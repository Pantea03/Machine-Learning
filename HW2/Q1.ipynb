{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b46fe41",
   "metadata": {
    "id": "1b46fe41"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - Course Code: 25737</h1>\n",
    "<h4 align=\"center\">Instructor: Dr. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "<h4 align=\"center\">Computer Assignment 3</h4>\n",
    "<h4 align=\"center\">\n",
    "\n",
    "Question 1\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0fc13",
   "metadata": {
    "id": "24a0fc13"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44babb65",
   "metadata": {
    "id": "44babb65"
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = 400101656\n",
    "Name = 'Pantea'\n",
    "Last_Name = 'Amoie'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a337a",
   "metadata": {
    "id": "ca4a337a"
   },
   "source": [
    "# Rules\n",
    "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
    "\n",
    "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
    "\n",
    "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b76789",
   "metadata": {
    "id": "12b76789",
    "outputId": "7e552bdb-7935-478f-8216-3040dca1910a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\legion\\anaconda3\\lib\\site-packages (1.20.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\legion\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\legion\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torchvision) (1.20.0)\n",
      "Requirement already satisfied: torch==2.3.0 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch==2.3.0->torchvision) (3.6.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.3.0->torchvision)\n",
      "  Downloading typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch==2.3.0->torchvision) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch==2.3.0->torchvision) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch==2.3.0->torchvision) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch==2.3.0->torchvision) (2022.7.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch==2.3.0->torchvision)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from jinja2->torch==2.3.0->torchvision) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from sympy->torch==2.3.0->torchvision) (1.2.1)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Downloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: tbb, intel-openmp, typing-extensions, mkl\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\legion\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\legion\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\legion\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: tbb, intel-openmp, typing-extensions, mkl\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torchvision\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886188c7",
   "metadata": {
    "id": "886188c7"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a0adcc",
   "metadata": {
    "id": "55a0adcc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510868",
   "metadata": {
    "id": "18510868"
   },
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8759e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc8759e2",
    "outputId": "3548c227-a9fc-4ce9-b7de-651e3a446ef3"
   },
   "outputs": [],
   "source": [
    "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df47fcb",
   "metadata": {
    "id": "5df47fcb"
   },
   "source": [
    "\n",
    "Here you have to calculate the number of classes amd input dimention of the first layer (how many pixels does each image have?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6763e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f6763e6",
    "outputId": "8f6ae41a-1ea0-49c0-f676-12cfd1c7e8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_set.classes)\n",
    "an_image, _ = train_set[0]\n",
    "input_dim = an_image.numel()\n",
    "print(num_classes)\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c695ff60",
   "metadata": {
    "id": "c695ff60"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dac6c2",
   "metadata": {
    "id": "f9dac6c2"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize 1 random image from each class by using `plt.subplots`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d6b0c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "e3d6b0c1",
    "outputId": "c861be7c-3d15-4c64-c652-31fc3916b0e8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = train_set.classes\n",
    "fig, axs = plt.subplots(1, num_classes, figsize=(10, 10))\n",
    "class_images = [None] * num_classes\n",
    "\n",
    "# train_loader has 2 elements: images, and their corresponding labels\n",
    "for images, labels in train_loader:\n",
    "    for i in range(len(labels)):\n",
    "        # convert tensor to scalar\n",
    "        label = labels[i].item()\n",
    "        if class_images[label] is None:\n",
    "            class_images[label] = images[i]\n",
    "\n",
    "    # stop the iteration if we have found an image for each class\n",
    "    if all(image is not None for image in class_images):\n",
    "        break\n",
    "\n",
    "for i in range(num_classes):\n",
    "    axs[i].imshow(class_images[i].squeeze(), cmap='gray')\n",
    "    axs[i].set_title(classes[i])\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5aba",
   "metadata": {
    "id": "a94c5aba"
   },
   "source": [
    "## Initializing model's parameters\n",
    "\n",
    "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d40952",
   "metadata": {
    "id": "e6d40952"
   },
   "outputs": [],
   "source": [
    "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
    "    \"\"\"\n",
    "    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
    "    \"\"\"\n",
    "    n_in, n_out = shape\n",
    "    with torch.no_grad():\n",
    "        w = torch.zeros(*shape, device=device)\n",
    "        # kaiming initialization for ReLU activations:\n",
    "        bound = 1 / np.sqrt(n_in).item()\n",
    "        w.uniform_(-bound, bound)\n",
    "        b = torch.zeros(n_out, device=device)  # no need to (1, n_out). it will broadcast itself.\n",
    "    w.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    # `i` is used to give numbers to parameter names\n",
    "    parameters.update({f'w{i}': w, f'b{i}': b})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce914706",
   "metadata": {
    "id": "ce914706"
   },
   "source": [
    "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3867d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f3867d7",
    "outputId": "f2018b4a-6021-4805-839e-e1ee8b53d582"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_dim : input dimention of the first layer, which you have calculated before.\n",
    "layers = [\n",
    "    (input_dim, 512),\n",
    "    (512, 256),\n",
    "    (256, 128),\n",
    "    (128, 64),\n",
    "    (64, num_classes)\n",
    "]\n",
    "num_layers = len(layers)\n",
    "parameters = {}\n",
    "\n",
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# add the parameters to the dictionary\n",
    "for i, shape in enumerate(layers):\n",
    "    add_linear_layer(parameters, shape, device, i)\n",
    "\n",
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd2c8e",
   "metadata": {
    "id": "8bfd2c8e"
   },
   "source": [
    "## Defining the required functions\n",
    "\n",
    "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b413d8",
   "metadata": {
    "id": "f3b413d8"
   },
   "source": [
    "Computing affine and relu outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebeeb0e",
   "metadata": {
    "id": "bebeeb0e"
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    # x = x.view(x.size(0), -1)\n",
    "    y = torch.matmul(x, w) + b\n",
    "    return y\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # relu: max(0, x)\n",
    "    y = torch.maximum(torch.tensor(0, dtype=x.dtype, device=x.device), x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9baa5e",
   "metadata": {
    "id": "5d9baa5e"
   },
   "source": [
    "Function `model` returns output of the whole model for the input `x` using the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2562962",
   "metadata": {
    "id": "d2562962"
   },
   "outputs": [],
   "source": [
    "def model(x: torch.Tensor, parameters, num_layers=num_layers):\n",
    "    # number of batches(B)\n",
    "    B = x.shape[0]\n",
    "    x = x.view(B, -1)\n",
    "\n",
    "    for layer_number in range(num_layers):\n",
    "        w = parameters[f'w{layer_number}']\n",
    "        b = parameters[f'b{layer_number}']\n",
    "        x = affine_forward(x, w, b) \n",
    "        # relu function on all the layers except the last one\n",
    "        if layer_number < num_layers - 1:\n",
    "            x = relu(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a9b4c",
   "metadata": {
    "id": "d17a9b4c"
   },
   "source": [
    "Implementing cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6959621c",
   "metadata": {
    "id": "6959621c"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(scores, y):\n",
    "    n = len(y)\n",
    "    # pi = e^si / (sum(e^sj))\n",
    "    # L = -sum(yi * log(pi))\n",
    "    # logpi = si - log(sum(e^sj))\n",
    "    log_p = scores - torch.log(torch.sum(torch.exp(scores), dim=1, keepdim=True))\n",
    "    # because y is one-hot encoded, we can extract the correct index of log_p instead of multiplying it by yi\n",
    "    log_p_i = log_p[range(n), y]\n",
    "    L = -torch.sum(log_p_i) / n\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a589af",
   "metadata": {
    "id": "15a589af"
   },
   "source": [
    "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3121c147",
   "metadata": {
    "id": "3121c147"
   },
   "outputs": [],
   "source": [
    "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
    "    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
    "    gradient. Finally, you should zero the gradients of the parameters after updating\n",
    "    the parameter value.'''\n",
    "#     for param_name, parameter in parameters.items():\n",
    "#         # updating parameters(w, b)\n",
    "#         parameter -= learning_rate * parameter.grad\n",
    "#         # zeroing the gradients of the parameters after updating the parameter value\n",
    "#         parameter.grad.zero_()\n",
    "    # Update parameters\n",
    "    for param in parameters.values():\n",
    "        if param.grad is not None:  # Only try to update parameters that have gradients\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "\n",
    "    # Zero the gradients after updating\n",
    "    for param in parameters.values():\n",
    "        if param.grad is not None:\n",
    "            param.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4cf8",
   "metadata": {
    "id": "e17b4cf8"
   },
   "source": [
    "Training functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c0f03b",
   "metadata": {
    "id": "76c0f03b"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    total_number = len(y_true)\n",
    "    correct_number = 0\n",
    "    for i in range(total_number):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            correct_number += 1\n",
    "    acc = correct_number / total_number\n",
    "    return acc\n",
    "\n",
    "def train(train_loader, learning_rate=0.001, epoch=None):\n",
    "    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
    "        1- Calculate the output of the model to the given input batch\n",
    "        2- Calculate the loss based on the model output\n",
    "        3- Update the gradients using backward method\n",
    "        4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
    "        5- Print the train loss (Show the epoch and batch as well)\n",
    "        '''\n",
    "    train_loss = 0\n",
    "    N_train = len(train_loader.dataset)\n",
    "\n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "\n",
    "        # zeroing the gradients\n",
    "        for param in parameters.values():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "        # calculating the loss\n",
    "        loss = cross_entropy_loss(p, y)\n",
    "        # using PyTorch's backward method to compute the gradients of the loss with respect to all the parameters\n",
    "        loss.backward()\n",
    "        # optimizing the model parameters\n",
    "        sgd_optimizer(parameters, learning_rate)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print(f\"Train: Epoch [{epoch}], Batch [{i+1}/{len(train_loader)}], Loss: {loss}\")\n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of train set: {acc}')\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def validate(loader, epoch=None, set_name=None):\n",
    "    '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
    "    the following on each batch:\n",
    "        1- Calculate the model output\n",
    "        2- Calculate the loss using the model output\n",
    "        3- Print the loss for each batch and epoch\n",
    "\n",
    "    Finally the function calculates the model accuracy.'''\n",
    "    total_loss = 0\n",
    "    N = len(loader.dataset)\n",
    "\n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "\n",
    "        # calculating the loss\n",
    "        loss = cross_entropy_loss(p, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation: Epoch [{epoch}], Batch [{i+1}/{len(train_loader)}], Loss: {loss}\")\n",
    "        \n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    total_loss /= N\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of {set_name} set: {acc}')\n",
    "\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87ebb4b6",
   "metadata": {
    "id": "87ebb4b6"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d4eb0b",
   "metadata": {
    "id": "28d4eb0b"
   },
   "outputs": [],
   "source": [
    "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
    "    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
    "    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
    "    train_loader, test_loader = dataloaders\n",
    "    for epoch in range(num_epochs):\n",
    "        # Calculating the train and test loss and accuracies for each epoch\n",
    "        \n",
    "        train_loss, train_acc = train(train_loader, learning_rate, epoch=epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        test_loss, test_acc = validate(test_loader, epoch=epoch, set_name='test')\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    # loss history of training and test sets\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss History')\n",
    "    plt.legend()\n",
    "\n",
    "    # accuracy history of training and test sets\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train')\n",
    "    plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ec4bdd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "2ec4bdd2",
    "outputId": "eb096df5-f9a9-4898-d6fc-7fb23e25a4dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [1/938], Loss: 2.283564567565918\n",
      "Train: Epoch [0], Batch [2/938], Loss: 2.2870607376098633\n",
      "Train: Epoch [0], Batch [3/938], Loss: 2.284881830215454\n",
      "Train: Epoch [0], Batch [4/938], Loss: 2.283128499984741\n",
      "Train: Epoch [0], Batch [5/938], Loss: 2.2827532291412354\n",
      "Train: Epoch [0], Batch [6/938], Loss: 2.281442403793335\n",
      "Train: Epoch [0], Batch [7/938], Loss: 2.2857718467712402\n",
      "Train: Epoch [0], Batch [8/938], Loss: 2.283212184906006\n",
      "Train: Epoch [0], Batch [9/938], Loss: 2.2845654487609863\n",
      "Train: Epoch [0], Batch [10/938], Loss: 2.2838845252990723\n",
      "Train: Epoch [0], Batch [11/938], Loss: 2.28432297706604\n",
      "Train: Epoch [0], Batch [12/938], Loss: 2.2852439880371094\n",
      "Train: Epoch [0], Batch [13/938], Loss: 2.2829413414001465\n",
      "Train: Epoch [0], Batch [14/938], Loss: 2.287461996078491\n",
      "Train: Epoch [0], Batch [15/938], Loss: 2.285722494125366\n",
      "Train: Epoch [0], Batch [16/938], Loss: 2.2834572792053223\n",
      "Train: Epoch [0], Batch [17/938], Loss: 2.284834861755371\n",
      "Train: Epoch [0], Batch [18/938], Loss: 2.2846295833587646\n",
      "Train: Epoch [0], Batch [19/938], Loss: 2.281266689300537\n",
      "Train: Epoch [0], Batch [20/938], Loss: 2.2858986854553223\n",
      "Train: Epoch [0], Batch [21/938], Loss: 2.282653331756592\n",
      "Train: Epoch [0], Batch [22/938], Loss: 2.2835729122161865\n",
      "Train: Epoch [0], Batch [23/938], Loss: 2.2807769775390625\n",
      "Train: Epoch [0], Batch [24/938], Loss: 2.2812886238098145\n",
      "Train: Epoch [0], Batch [25/938], Loss: 2.283843994140625\n",
      "Train: Epoch [0], Batch [26/938], Loss: 2.280503273010254\n",
      "Train: Epoch [0], Batch [27/938], Loss: 2.2849950790405273\n",
      "Train: Epoch [0], Batch [28/938], Loss: 2.2848901748657227\n",
      "Train: Epoch [0], Batch [29/938], Loss: 2.282580852508545\n",
      "Train: Epoch [0], Batch [30/938], Loss: 2.2809205055236816\n",
      "Train: Epoch [0], Batch [31/938], Loss: 2.2864551544189453\n",
      "Train: Epoch [0], Batch [32/938], Loss: 2.281963348388672\n",
      "Train: Epoch [0], Batch [33/938], Loss: 2.283613443374634\n",
      "Train: Epoch [0], Batch [34/938], Loss: 2.283841609954834\n",
      "Train: Epoch [0], Batch [35/938], Loss: 2.284087657928467\n",
      "Train: Epoch [0], Batch [36/938], Loss: 2.2873270511627197\n",
      "Train: Epoch [0], Batch [37/938], Loss: 2.2813591957092285\n",
      "Train: Epoch [0], Batch [38/938], Loss: 2.2785589694976807\n",
      "Train: Epoch [0], Batch [39/938], Loss: 2.2860336303710938\n",
      "Train: Epoch [0], Batch [40/938], Loss: 2.28102707862854\n",
      "Train: Epoch [0], Batch [41/938], Loss: 2.283111095428467\n",
      "Train: Epoch [0], Batch [42/938], Loss: 2.281909465789795\n",
      "Train: Epoch [0], Batch [43/938], Loss: 2.284299373626709\n",
      "Train: Epoch [0], Batch [44/938], Loss: 2.2824349403381348\n",
      "Train: Epoch [0], Batch [45/938], Loss: 2.2792136669158936\n",
      "Train: Epoch [0], Batch [46/938], Loss: 2.2853755950927734\n",
      "Train: Epoch [0], Batch [47/938], Loss: 2.2820096015930176\n",
      "Train: Epoch [0], Batch [48/938], Loss: 2.282010316848755\n",
      "Train: Epoch [0], Batch [49/938], Loss: 2.2808525562286377\n",
      "Train: Epoch [0], Batch [50/938], Loss: 2.280738353729248\n",
      "Train: Epoch [0], Batch [51/938], Loss: 2.2796106338500977\n",
      "Train: Epoch [0], Batch [52/938], Loss: 2.2815399169921875\n",
      "Train: Epoch [0], Batch [53/938], Loss: 2.2805933952331543\n",
      "Train: Epoch [0], Batch [54/938], Loss: 2.2832109928131104\n",
      "Train: Epoch [0], Batch [55/938], Loss: 2.2791590690612793\n",
      "Train: Epoch [0], Batch [56/938], Loss: 2.280977725982666\n",
      "Train: Epoch [0], Batch [57/938], Loss: 2.28033447265625\n",
      "Train: Epoch [0], Batch [58/938], Loss: 2.2816812992095947\n",
      "Train: Epoch [0], Batch [59/938], Loss: 2.282637119293213\n",
      "Train: Epoch [0], Batch [60/938], Loss: 2.279815196990967\n",
      "Train: Epoch [0], Batch [61/938], Loss: 2.281404495239258\n",
      "Train: Epoch [0], Batch [62/938], Loss: 2.2802329063415527\n",
      "Train: Epoch [0], Batch [63/938], Loss: 2.2828385829925537\n",
      "Train: Epoch [0], Batch [64/938], Loss: 2.278381109237671\n",
      "Train: Epoch [0], Batch [65/938], Loss: 2.2819368839263916\n",
      "Train: Epoch [0], Batch [66/938], Loss: 2.2790846824645996\n",
      "Train: Epoch [0], Batch [67/938], Loss: 2.2839765548706055\n",
      "Train: Epoch [0], Batch [68/938], Loss: 2.2807817459106445\n",
      "Train: Epoch [0], Batch [69/938], Loss: 2.278204917907715\n",
      "Train: Epoch [0], Batch [70/938], Loss: 2.2813196182250977\n",
      "Train: Epoch [0], Batch [71/938], Loss: 2.2776641845703125\n",
      "Train: Epoch [0], Batch [72/938], Loss: 2.2794876098632812\n",
      "Train: Epoch [0], Batch [73/938], Loss: 2.280491828918457\n",
      "Train: Epoch [0], Batch [74/938], Loss: 2.278797149658203\n",
      "Train: Epoch [0], Batch [75/938], Loss: 2.281790018081665\n",
      "Train: Epoch [0], Batch [76/938], Loss: 2.284060478210449\n",
      "Train: Epoch [0], Batch [77/938], Loss: 2.279042959213257\n",
      "Train: Epoch [0], Batch [78/938], Loss: 2.277914524078369\n",
      "Train: Epoch [0], Batch [79/938], Loss: 2.279928684234619\n",
      "Train: Epoch [0], Batch [80/938], Loss: 2.2775537967681885\n",
      "Train: Epoch [0], Batch [81/938], Loss: 2.2801427841186523\n",
      "Train: Epoch [0], Batch [82/938], Loss: 2.282773017883301\n",
      "Train: Epoch [0], Batch [83/938], Loss: 2.281186103820801\n",
      "Train: Epoch [0], Batch [84/938], Loss: 2.2833614349365234\n",
      "Train: Epoch [0], Batch [85/938], Loss: 2.2809455394744873\n",
      "Train: Epoch [0], Batch [86/938], Loss: 2.2803196907043457\n",
      "Train: Epoch [0], Batch [87/938], Loss: 2.2787117958068848\n",
      "Train: Epoch [0], Batch [88/938], Loss: 2.2821061611175537\n",
      "Train: Epoch [0], Batch [89/938], Loss: 2.280290126800537\n",
      "Train: Epoch [0], Batch [90/938], Loss: 2.280308246612549\n",
      "Train: Epoch [0], Batch [91/938], Loss: 2.2783758640289307\n",
      "Train: Epoch [0], Batch [92/938], Loss: 2.279538154602051\n",
      "Train: Epoch [0], Batch [93/938], Loss: 2.2797985076904297\n",
      "Train: Epoch [0], Batch [94/938], Loss: 2.2817234992980957\n",
      "Train: Epoch [0], Batch [95/938], Loss: 2.280838966369629\n",
      "Train: Epoch [0], Batch [96/938], Loss: 2.2771308422088623\n",
      "Train: Epoch [0], Batch [97/938], Loss: 2.2791032791137695\n",
      "Train: Epoch [0], Batch [98/938], Loss: 2.279804229736328\n",
      "Train: Epoch [0], Batch [99/938], Loss: 2.2802748680114746\n",
      "Train: Epoch [0], Batch [100/938], Loss: 2.2799627780914307\n",
      "Train: Epoch [0], Batch [101/938], Loss: 2.2781853675842285\n",
      "Train: Epoch [0], Batch [102/938], Loss: 2.2765355110168457\n",
      "Train: Epoch [0], Batch [103/938], Loss: 2.277902841567993\n",
      "Train: Epoch [0], Batch [104/938], Loss: 2.2802770137786865\n",
      "Train: Epoch [0], Batch [105/938], Loss: 2.2750473022460938\n",
      "Train: Epoch [0], Batch [106/938], Loss: 2.2788352966308594\n",
      "Train: Epoch [0], Batch [107/938], Loss: 2.2789642810821533\n",
      "Train: Epoch [0], Batch [108/938], Loss: 2.2761082649230957\n",
      "Train: Epoch [0], Batch [109/938], Loss: 2.2773215770721436\n",
      "Train: Epoch [0], Batch [110/938], Loss: 2.277519702911377\n",
      "Train: Epoch [0], Batch [111/938], Loss: 2.2798378467559814\n",
      "Train: Epoch [0], Batch [112/938], Loss: 2.2756764888763428\n",
      "Train: Epoch [0], Batch [113/938], Loss: 2.275775909423828\n",
      "Train: Epoch [0], Batch [114/938], Loss: 2.2710165977478027\n",
      "Train: Epoch [0], Batch [115/938], Loss: 2.2768194675445557\n",
      "Train: Epoch [0], Batch [116/938], Loss: 2.281100273132324\n",
      "Train: Epoch [0], Batch [117/938], Loss: 2.2746922969818115\n",
      "Train: Epoch [0], Batch [118/938], Loss: 2.2782626152038574\n",
      "Train: Epoch [0], Batch [119/938], Loss: 2.2805192470550537\n",
      "Train: Epoch [0], Batch [120/938], Loss: 2.2774431705474854\n",
      "Train: Epoch [0], Batch [121/938], Loss: 2.2726895809173584\n",
      "Train: Epoch [0], Batch [122/938], Loss: 2.281816244125366\n",
      "Train: Epoch [0], Batch [123/938], Loss: 2.2818801403045654\n",
      "Train: Epoch [0], Batch [124/938], Loss: 2.2771663665771484\n",
      "Train: Epoch [0], Batch [125/938], Loss: 2.2791428565979004\n",
      "Train: Epoch [0], Batch [126/938], Loss: 2.2785439491271973\n",
      "Train: Epoch [0], Batch [127/938], Loss: 2.277411699295044\n",
      "Train: Epoch [0], Batch [128/938], Loss: 2.275458812713623\n",
      "Train: Epoch [0], Batch [129/938], Loss: 2.275629997253418\n",
      "Train: Epoch [0], Batch [130/938], Loss: 2.275954246520996\n",
      "Train: Epoch [0], Batch [131/938], Loss: 2.2761406898498535\n",
      "Train: Epoch [0], Batch [132/938], Loss: 2.278244972229004\n",
      "Train: Epoch [0], Batch [133/938], Loss: 2.2768611907958984\n",
      "Train: Epoch [0], Batch [134/938], Loss: 2.2765653133392334\n",
      "Train: Epoch [0], Batch [135/938], Loss: 2.27760648727417\n",
      "Train: Epoch [0], Batch [136/938], Loss: 2.2831220626831055\n",
      "Train: Epoch [0], Batch [137/938], Loss: 2.2749757766723633\n",
      "Train: Epoch [0], Batch [138/938], Loss: 2.27771258354187\n",
      "Train: Epoch [0], Batch [139/938], Loss: 2.277339220046997\n",
      "Train: Epoch [0], Batch [140/938], Loss: 2.2802836894989014\n",
      "Train: Epoch [0], Batch [141/938], Loss: 2.2779064178466797\n",
      "Train: Epoch [0], Batch [142/938], Loss: 2.276109457015991\n",
      "Train: Epoch [0], Batch [143/938], Loss: 2.2771382331848145\n",
      "Train: Epoch [0], Batch [144/938], Loss: 2.278876304626465\n",
      "Train: Epoch [0], Batch [145/938], Loss: 2.2741212844848633\n",
      "Train: Epoch [0], Batch [146/938], Loss: 2.274730682373047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [147/938], Loss: 2.2756223678588867\n",
      "Train: Epoch [0], Batch [148/938], Loss: 2.2720413208007812\n",
      "Train: Epoch [0], Batch [149/938], Loss: 2.274250030517578\n",
      "Train: Epoch [0], Batch [150/938], Loss: 2.2814135551452637\n",
      "Train: Epoch [0], Batch [151/938], Loss: 2.275635242462158\n",
      "Train: Epoch [0], Batch [152/938], Loss: 2.2737836837768555\n",
      "Train: Epoch [0], Batch [153/938], Loss: 2.2764103412628174\n",
      "Train: Epoch [0], Batch [154/938], Loss: 2.2784883975982666\n",
      "Train: Epoch [0], Batch [155/938], Loss: 2.278552532196045\n",
      "Train: Epoch [0], Batch [156/938], Loss: 2.275369644165039\n",
      "Train: Epoch [0], Batch [157/938], Loss: 2.277127265930176\n",
      "Train: Epoch [0], Batch [158/938], Loss: 2.277954578399658\n",
      "Train: Epoch [0], Batch [159/938], Loss: 2.2787387371063232\n",
      "Train: Epoch [0], Batch [160/938], Loss: 2.2748360633850098\n",
      "Train: Epoch [0], Batch [161/938], Loss: 2.2781081199645996\n",
      "Train: Epoch [0], Batch [162/938], Loss: 2.2813456058502197\n",
      "Train: Epoch [0], Batch [163/938], Loss: 2.2769947052001953\n",
      "Train: Epoch [0], Batch [164/938], Loss: 2.2791380882263184\n",
      "Train: Epoch [0], Batch [165/938], Loss: 2.2741198539733887\n",
      "Train: Epoch [0], Batch [166/938], Loss: 2.273512363433838\n",
      "Train: Epoch [0], Batch [167/938], Loss: 2.276625633239746\n",
      "Train: Epoch [0], Batch [168/938], Loss: 2.2784223556518555\n",
      "Train: Epoch [0], Batch [169/938], Loss: 2.2768843173980713\n",
      "Train: Epoch [0], Batch [170/938], Loss: 2.2750144004821777\n",
      "Train: Epoch [0], Batch [171/938], Loss: 2.2748794555664062\n",
      "Train: Epoch [0], Batch [172/938], Loss: 2.271050214767456\n",
      "Train: Epoch [0], Batch [173/938], Loss: 2.2746334075927734\n",
      "Train: Epoch [0], Batch [174/938], Loss: 2.275925397872925\n",
      "Train: Epoch [0], Batch [175/938], Loss: 2.2740354537963867\n",
      "Train: Epoch [0], Batch [176/938], Loss: 2.2741291522979736\n",
      "Train: Epoch [0], Batch [177/938], Loss: 2.2734291553497314\n",
      "Train: Epoch [0], Batch [178/938], Loss: 2.2724051475524902\n",
      "Train: Epoch [0], Batch [179/938], Loss: 2.2741923332214355\n",
      "Train: Epoch [0], Batch [180/938], Loss: 2.274348258972168\n",
      "Train: Epoch [0], Batch [181/938], Loss: 2.2726798057556152\n",
      "Train: Epoch [0], Batch [182/938], Loss: 2.271544933319092\n",
      "Train: Epoch [0], Batch [183/938], Loss: 2.2730231285095215\n",
      "Train: Epoch [0], Batch [184/938], Loss: 2.271493911743164\n",
      "Train: Epoch [0], Batch [185/938], Loss: 2.2806434631347656\n",
      "Train: Epoch [0], Batch [186/938], Loss: 2.272739887237549\n",
      "Train: Epoch [0], Batch [187/938], Loss: 2.2763147354125977\n",
      "Train: Epoch [0], Batch [188/938], Loss: 2.2706921100616455\n",
      "Train: Epoch [0], Batch [189/938], Loss: 2.2781405448913574\n",
      "Train: Epoch [0], Batch [190/938], Loss: 2.2712466716766357\n",
      "Train: Epoch [0], Batch [191/938], Loss: 2.2766785621643066\n",
      "Train: Epoch [0], Batch [192/938], Loss: 2.2759366035461426\n",
      "Train: Epoch [0], Batch [193/938], Loss: 2.2755422592163086\n",
      "Train: Epoch [0], Batch [194/938], Loss: 2.2689919471740723\n",
      "Train: Epoch [0], Batch [195/938], Loss: 2.272728204727173\n",
      "Train: Epoch [0], Batch [196/938], Loss: 2.277812957763672\n",
      "Train: Epoch [0], Batch [197/938], Loss: 2.274630069732666\n",
      "Train: Epoch [0], Batch [198/938], Loss: 2.274761199951172\n",
      "Train: Epoch [0], Batch [199/938], Loss: 2.2730464935302734\n",
      "Train: Epoch [0], Batch [200/938], Loss: 2.27629017829895\n",
      "Train: Epoch [0], Batch [201/938], Loss: 2.2692806720733643\n",
      "Train: Epoch [0], Batch [202/938], Loss: 2.273803949356079\n",
      "Train: Epoch [0], Batch [203/938], Loss: 2.2758288383483887\n",
      "Train: Epoch [0], Batch [204/938], Loss: 2.2683892250061035\n",
      "Train: Epoch [0], Batch [205/938], Loss: 2.272487163543701\n",
      "Train: Epoch [0], Batch [206/938], Loss: 2.2763335704803467\n",
      "Train: Epoch [0], Batch [207/938], Loss: 2.2751574516296387\n",
      "Train: Epoch [0], Batch [208/938], Loss: 2.2712209224700928\n",
      "Train: Epoch [0], Batch [209/938], Loss: 2.2678074836730957\n",
      "Train: Epoch [0], Batch [210/938], Loss: 2.270081043243408\n",
      "Train: Epoch [0], Batch [211/938], Loss: 2.272705078125\n",
      "Train: Epoch [0], Batch [212/938], Loss: 2.2750587463378906\n",
      "Train: Epoch [0], Batch [213/938], Loss: 2.273015022277832\n",
      "Train: Epoch [0], Batch [214/938], Loss: 2.2736146450042725\n",
      "Train: Epoch [0], Batch [215/938], Loss: 2.2743520736694336\n",
      "Train: Epoch [0], Batch [216/938], Loss: 2.268521547317505\n",
      "Train: Epoch [0], Batch [217/938], Loss: 2.2750957012176514\n",
      "Train: Epoch [0], Batch [218/938], Loss: 2.2739250659942627\n",
      "Train: Epoch [0], Batch [219/938], Loss: 2.2704854011535645\n",
      "Train: Epoch [0], Batch [220/938], Loss: 2.2708497047424316\n",
      "Train: Epoch [0], Batch [221/938], Loss: 2.273460865020752\n",
      "Train: Epoch [0], Batch [222/938], Loss: 2.2745184898376465\n",
      "Train: Epoch [0], Batch [223/938], Loss: 2.270162582397461\n",
      "Train: Epoch [0], Batch [224/938], Loss: 2.273451328277588\n",
      "Train: Epoch [0], Batch [225/938], Loss: 2.2742786407470703\n",
      "Train: Epoch [0], Batch [226/938], Loss: 2.270904302597046\n",
      "Train: Epoch [0], Batch [227/938], Loss: 2.27366042137146\n",
      "Train: Epoch [0], Batch [228/938], Loss: 2.272221088409424\n",
      "Train: Epoch [0], Batch [229/938], Loss: 2.2681643962860107\n",
      "Train: Epoch [0], Batch [230/938], Loss: 2.267521619796753\n",
      "Train: Epoch [0], Batch [231/938], Loss: 2.2710914611816406\n",
      "Train: Epoch [0], Batch [232/938], Loss: 2.271810293197632\n",
      "Train: Epoch [0], Batch [233/938], Loss: 2.2734875679016113\n",
      "Train: Epoch [0], Batch [234/938], Loss: 2.273026466369629\n",
      "Train: Epoch [0], Batch [235/938], Loss: 2.270120143890381\n",
      "Train: Epoch [0], Batch [236/938], Loss: 2.272453546524048\n",
      "Train: Epoch [0], Batch [237/938], Loss: 2.274169445037842\n",
      "Train: Epoch [0], Batch [238/938], Loss: 2.268000602722168\n",
      "Train: Epoch [0], Batch [239/938], Loss: 2.2713942527770996\n",
      "Train: Epoch [0], Batch [240/938], Loss: 2.2704131603240967\n",
      "Train: Epoch [0], Batch [241/938], Loss: 2.2696590423583984\n",
      "Train: Epoch [0], Batch [242/938], Loss: 2.269228935241699\n",
      "Train: Epoch [0], Batch [243/938], Loss: 2.2728936672210693\n",
      "Train: Epoch [0], Batch [244/938], Loss: 2.2666163444519043\n",
      "Train: Epoch [0], Batch [245/938], Loss: 2.2715580463409424\n",
      "Train: Epoch [0], Batch [246/938], Loss: 2.2729296684265137\n",
      "Train: Epoch [0], Batch [247/938], Loss: 2.2724812030792236\n",
      "Train: Epoch [0], Batch [248/938], Loss: 2.270514965057373\n",
      "Train: Epoch [0], Batch [249/938], Loss: 2.2669878005981445\n",
      "Train: Epoch [0], Batch [250/938], Loss: 2.272588014602661\n",
      "Train: Epoch [0], Batch [251/938], Loss: 2.2696285247802734\n",
      "Train: Epoch [0], Batch [252/938], Loss: 2.267430305480957\n",
      "Train: Epoch [0], Batch [253/938], Loss: 2.268651247024536\n",
      "Train: Epoch [0], Batch [254/938], Loss: 2.267021894454956\n",
      "Train: Epoch [0], Batch [255/938], Loss: 2.2680411338806152\n",
      "Train: Epoch [0], Batch [256/938], Loss: 2.269874334335327\n",
      "Train: Epoch [0], Batch [257/938], Loss: 2.274290084838867\n",
      "Train: Epoch [0], Batch [258/938], Loss: 2.268815279006958\n",
      "Train: Epoch [0], Batch [259/938], Loss: 2.2672810554504395\n",
      "Train: Epoch [0], Batch [260/938], Loss: 2.2709012031555176\n",
      "Train: Epoch [0], Batch [261/938], Loss: 2.2687454223632812\n",
      "Train: Epoch [0], Batch [262/938], Loss: 2.2658333778381348\n",
      "Train: Epoch [0], Batch [263/938], Loss: 2.2628917694091797\n",
      "Train: Epoch [0], Batch [264/938], Loss: 2.2668209075927734\n",
      "Train: Epoch [0], Batch [265/938], Loss: 2.2670347690582275\n",
      "Train: Epoch [0], Batch [266/938], Loss: 2.2656233310699463\n",
      "Train: Epoch [0], Batch [267/938], Loss: 2.267576217651367\n",
      "Train: Epoch [0], Batch [268/938], Loss: 2.2695395946502686\n",
      "Train: Epoch [0], Batch [269/938], Loss: 2.268214225769043\n",
      "Train: Epoch [0], Batch [270/938], Loss: 2.262906789779663\n",
      "Train: Epoch [0], Batch [271/938], Loss: 2.264138698577881\n",
      "Train: Epoch [0], Batch [272/938], Loss: 2.2712695598602295\n",
      "Train: Epoch [0], Batch [273/938], Loss: 2.2691140174865723\n",
      "Train: Epoch [0], Batch [274/938], Loss: 2.266312837600708\n",
      "Train: Epoch [0], Batch [275/938], Loss: 2.266427993774414\n",
      "Train: Epoch [0], Batch [276/938], Loss: 2.270618438720703\n",
      "Train: Epoch [0], Batch [277/938], Loss: 2.2667975425720215\n",
      "Train: Epoch [0], Batch [278/938], Loss: 2.267665386199951\n",
      "Train: Epoch [0], Batch [279/938], Loss: 2.267383575439453\n",
      "Train: Epoch [0], Batch [280/938], Loss: 2.267118215560913\n",
      "Train: Epoch [0], Batch [281/938], Loss: 2.265711784362793\n",
      "Train: Epoch [0], Batch [282/938], Loss: 2.2700300216674805\n",
      "Train: Epoch [0], Batch [283/938], Loss: 2.2785117626190186\n",
      "Train: Epoch [0], Batch [284/938], Loss: 2.267693042755127\n",
      "Train: Epoch [0], Batch [285/938], Loss: 2.263648748397827\n",
      "Train: Epoch [0], Batch [286/938], Loss: 2.271432638168335\n",
      "Train: Epoch [0], Batch [287/938], Loss: 2.266401767730713\n",
      "Train: Epoch [0], Batch [288/938], Loss: 2.2705087661743164\n",
      "Train: Epoch [0], Batch [289/938], Loss: 2.272542715072632\n",
      "Train: Epoch [0], Batch [290/938], Loss: 2.270718574523926\n",
      "Train: Epoch [0], Batch [291/938], Loss: 2.270705223083496\n",
      "Train: Epoch [0], Batch [292/938], Loss: 2.2726452350616455\n",
      "Train: Epoch [0], Batch [293/938], Loss: 2.2620368003845215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [294/938], Loss: 2.2693305015563965\n",
      "Train: Epoch [0], Batch [295/938], Loss: 2.2729687690734863\n",
      "Train: Epoch [0], Batch [296/938], Loss: 2.267772912979126\n",
      "Train: Epoch [0], Batch [297/938], Loss: 2.269451379776001\n",
      "Train: Epoch [0], Batch [298/938], Loss: 2.266000747680664\n",
      "Train: Epoch [0], Batch [299/938], Loss: 2.268476963043213\n",
      "Train: Epoch [0], Batch [300/938], Loss: 2.2614054679870605\n",
      "Train: Epoch [0], Batch [301/938], Loss: 2.260551929473877\n",
      "Train: Epoch [0], Batch [302/938], Loss: 2.2677268981933594\n",
      "Train: Epoch [0], Batch [303/938], Loss: 2.264730453491211\n",
      "Train: Epoch [0], Batch [304/938], Loss: 2.269667625427246\n",
      "Train: Epoch [0], Batch [305/938], Loss: 2.264319658279419\n",
      "Train: Epoch [0], Batch [306/938], Loss: 2.267458438873291\n",
      "Train: Epoch [0], Batch [307/938], Loss: 2.2668018341064453\n",
      "Train: Epoch [0], Batch [308/938], Loss: 2.2679009437561035\n",
      "Train: Epoch [0], Batch [309/938], Loss: 2.2656145095825195\n",
      "Train: Epoch [0], Batch [310/938], Loss: 2.265383243560791\n",
      "Train: Epoch [0], Batch [311/938], Loss: 2.2624032497406006\n",
      "Train: Epoch [0], Batch [312/938], Loss: 2.2627646923065186\n",
      "Train: Epoch [0], Batch [313/938], Loss: 2.264705181121826\n",
      "Train: Epoch [0], Batch [314/938], Loss: 2.266786575317383\n",
      "Train: Epoch [0], Batch [315/938], Loss: 2.261909008026123\n",
      "Train: Epoch [0], Batch [316/938], Loss: 2.2611184120178223\n",
      "Train: Epoch [0], Batch [317/938], Loss: 2.266477584838867\n",
      "Train: Epoch [0], Batch [318/938], Loss: 2.262566089630127\n",
      "Train: Epoch [0], Batch [319/938], Loss: 2.2653398513793945\n",
      "Train: Epoch [0], Batch [320/938], Loss: 2.2596213817596436\n",
      "Train: Epoch [0], Batch [321/938], Loss: 2.2611143589019775\n",
      "Train: Epoch [0], Batch [322/938], Loss: 2.261835813522339\n",
      "Train: Epoch [0], Batch [323/938], Loss: 2.2628064155578613\n",
      "Train: Epoch [0], Batch [324/938], Loss: 2.2667856216430664\n",
      "Train: Epoch [0], Batch [325/938], Loss: 2.2645788192749023\n",
      "Train: Epoch [0], Batch [326/938], Loss: 2.261963129043579\n",
      "Train: Epoch [0], Batch [327/938], Loss: 2.263613700866699\n",
      "Train: Epoch [0], Batch [328/938], Loss: 2.264791488647461\n",
      "Train: Epoch [0], Batch [329/938], Loss: 2.2676334381103516\n",
      "Train: Epoch [0], Batch [330/938], Loss: 2.263096332550049\n",
      "Train: Epoch [0], Batch [331/938], Loss: 2.261232852935791\n",
      "Train: Epoch [0], Batch [332/938], Loss: 2.263780117034912\n",
      "Train: Epoch [0], Batch [333/938], Loss: 2.2579543590545654\n",
      "Train: Epoch [0], Batch [334/938], Loss: 2.2636852264404297\n",
      "Train: Epoch [0], Batch [335/938], Loss: 2.2692713737487793\n",
      "Train: Epoch [0], Batch [336/938], Loss: 2.2616307735443115\n",
      "Train: Epoch [0], Batch [337/938], Loss: 2.261993646621704\n",
      "Train: Epoch [0], Batch [338/938], Loss: 2.2649598121643066\n",
      "Train: Epoch [0], Batch [339/938], Loss: 2.263731002807617\n",
      "Train: Epoch [0], Batch [340/938], Loss: 2.258490562438965\n",
      "Train: Epoch [0], Batch [341/938], Loss: 2.27475643157959\n",
      "Train: Epoch [0], Batch [342/938], Loss: 2.2635531425476074\n",
      "Train: Epoch [0], Batch [343/938], Loss: 2.2654218673706055\n",
      "Train: Epoch [0], Batch [344/938], Loss: 2.266106605529785\n",
      "Train: Epoch [0], Batch [345/938], Loss: 2.260328531265259\n",
      "Train: Epoch [0], Batch [346/938], Loss: 2.2655487060546875\n",
      "Train: Epoch [0], Batch [347/938], Loss: 2.2585597038269043\n",
      "Train: Epoch [0], Batch [348/938], Loss: 2.2580270767211914\n",
      "Train: Epoch [0], Batch [349/938], Loss: 2.2652037143707275\n",
      "Train: Epoch [0], Batch [350/938], Loss: 2.256580352783203\n",
      "Train: Epoch [0], Batch [351/938], Loss: 2.2642173767089844\n",
      "Train: Epoch [0], Batch [352/938], Loss: 2.259967088699341\n",
      "Train: Epoch [0], Batch [353/938], Loss: 2.2661447525024414\n",
      "Train: Epoch [0], Batch [354/938], Loss: 2.2635765075683594\n",
      "Train: Epoch [0], Batch [355/938], Loss: 2.264988899230957\n",
      "Train: Epoch [0], Batch [356/938], Loss: 2.2697319984436035\n",
      "Train: Epoch [0], Batch [357/938], Loss: 2.2650222778320312\n",
      "Train: Epoch [0], Batch [358/938], Loss: 2.2634787559509277\n",
      "Train: Epoch [0], Batch [359/938], Loss: 2.2624807357788086\n",
      "Train: Epoch [0], Batch [360/938], Loss: 2.258230447769165\n",
      "Train: Epoch [0], Batch [361/938], Loss: 2.260049343109131\n",
      "Train: Epoch [0], Batch [362/938], Loss: 2.255380868911743\n",
      "Train: Epoch [0], Batch [363/938], Loss: 2.2551426887512207\n",
      "Train: Epoch [0], Batch [364/938], Loss: 2.259044647216797\n",
      "Train: Epoch [0], Batch [365/938], Loss: 2.259526014328003\n",
      "Train: Epoch [0], Batch [366/938], Loss: 2.2672953605651855\n",
      "Train: Epoch [0], Batch [367/938], Loss: 2.259340524673462\n",
      "Train: Epoch [0], Batch [368/938], Loss: 2.262113571166992\n",
      "Train: Epoch [0], Batch [369/938], Loss: 2.2597010135650635\n",
      "Train: Epoch [0], Batch [370/938], Loss: 2.2547144889831543\n",
      "Train: Epoch [0], Batch [371/938], Loss: 2.2546324729919434\n",
      "Train: Epoch [0], Batch [372/938], Loss: 2.2538628578186035\n",
      "Train: Epoch [0], Batch [373/938], Loss: 2.2570724487304688\n",
      "Train: Epoch [0], Batch [374/938], Loss: 2.2575223445892334\n",
      "Train: Epoch [0], Batch [375/938], Loss: 2.2623074054718018\n",
      "Train: Epoch [0], Batch [376/938], Loss: 2.2579116821289062\n",
      "Train: Epoch [0], Batch [377/938], Loss: 2.25701904296875\n",
      "Train: Epoch [0], Batch [378/938], Loss: 2.2627339363098145\n",
      "Train: Epoch [0], Batch [379/938], Loss: 2.2666075229644775\n",
      "Train: Epoch [0], Batch [380/938], Loss: 2.2601733207702637\n",
      "Train: Epoch [0], Batch [381/938], Loss: 2.265934467315674\n",
      "Train: Epoch [0], Batch [382/938], Loss: 2.256582021713257\n",
      "Train: Epoch [0], Batch [383/938], Loss: 2.26472806930542\n",
      "Train: Epoch [0], Batch [384/938], Loss: 2.257394790649414\n",
      "Train: Epoch [0], Batch [385/938], Loss: 2.2614660263061523\n",
      "Train: Epoch [0], Batch [386/938], Loss: 2.254814624786377\n",
      "Train: Epoch [0], Batch [387/938], Loss: 2.261063575744629\n",
      "Train: Epoch [0], Batch [388/938], Loss: 2.26613712310791\n",
      "Train: Epoch [0], Batch [389/938], Loss: 2.260129690170288\n",
      "Train: Epoch [0], Batch [390/938], Loss: 2.2644288539886475\n",
      "Train: Epoch [0], Batch [391/938], Loss: 2.259026288986206\n",
      "Train: Epoch [0], Batch [392/938], Loss: 2.259610414505005\n",
      "Train: Epoch [0], Batch [393/938], Loss: 2.246535301208496\n",
      "Train: Epoch [0], Batch [394/938], Loss: 2.2517213821411133\n",
      "Train: Epoch [0], Batch [395/938], Loss: 2.2529454231262207\n",
      "Train: Epoch [0], Batch [396/938], Loss: 2.261948347091675\n",
      "Train: Epoch [0], Batch [397/938], Loss: 2.252096176147461\n",
      "Train: Epoch [0], Batch [398/938], Loss: 2.2585678100585938\n",
      "Train: Epoch [0], Batch [399/938], Loss: 2.2643606662750244\n",
      "Train: Epoch [0], Batch [400/938], Loss: 2.2610459327697754\n",
      "Train: Epoch [0], Batch [401/938], Loss: 2.2578446865081787\n",
      "Train: Epoch [0], Batch [402/938], Loss: 2.265462875366211\n",
      "Train: Epoch [0], Batch [403/938], Loss: 2.261383533477783\n",
      "Train: Epoch [0], Batch [404/938], Loss: 2.249513626098633\n",
      "Train: Epoch [0], Batch [405/938], Loss: 2.257812023162842\n",
      "Train: Epoch [0], Batch [406/938], Loss: 2.247464418411255\n",
      "Train: Epoch [0], Batch [407/938], Loss: 2.255654811859131\n",
      "Train: Epoch [0], Batch [408/938], Loss: 2.2494266033172607\n",
      "Train: Epoch [0], Batch [409/938], Loss: 2.25313663482666\n",
      "Train: Epoch [0], Batch [410/938], Loss: 2.25714111328125\n",
      "Train: Epoch [0], Batch [411/938], Loss: 2.2514395713806152\n",
      "Train: Epoch [0], Batch [412/938], Loss: 2.2560625076293945\n",
      "Train: Epoch [0], Batch [413/938], Loss: 2.2504425048828125\n",
      "Train: Epoch [0], Batch [414/938], Loss: 2.25305438041687\n",
      "Train: Epoch [0], Batch [415/938], Loss: 2.252617359161377\n",
      "Train: Epoch [0], Batch [416/938], Loss: 2.2538585662841797\n",
      "Train: Epoch [0], Batch [417/938], Loss: 2.2591753005981445\n",
      "Train: Epoch [0], Batch [418/938], Loss: 2.259481191635132\n",
      "Train: Epoch [0], Batch [419/938], Loss: 2.2646536827087402\n",
      "Train: Epoch [0], Batch [420/938], Loss: 2.250317096710205\n",
      "Train: Epoch [0], Batch [421/938], Loss: 2.255246162414551\n",
      "Train: Epoch [0], Batch [422/938], Loss: 2.2553586959838867\n",
      "Train: Epoch [0], Batch [423/938], Loss: 2.257683753967285\n",
      "Train: Epoch [0], Batch [424/938], Loss: 2.2428627014160156\n",
      "Train: Epoch [0], Batch [425/938], Loss: 2.2485194206237793\n",
      "Train: Epoch [0], Batch [426/938], Loss: 2.26177978515625\n",
      "Train: Epoch [0], Batch [427/938], Loss: 2.2480616569519043\n",
      "Train: Epoch [0], Batch [428/938], Loss: 2.256805896759033\n",
      "Train: Epoch [0], Batch [429/938], Loss: 2.249927282333374\n",
      "Train: Epoch [0], Batch [430/938], Loss: 2.2463865280151367\n",
      "Train: Epoch [0], Batch [431/938], Loss: 2.2482948303222656\n",
      "Train: Epoch [0], Batch [432/938], Loss: 2.255186080932617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [433/938], Loss: 2.2566092014312744\n",
      "Train: Epoch [0], Batch [434/938], Loss: 2.2505455017089844\n",
      "Train: Epoch [0], Batch [435/938], Loss: 2.251321315765381\n",
      "Train: Epoch [0], Batch [436/938], Loss: 2.2514891624450684\n",
      "Train: Epoch [0], Batch [437/938], Loss: 2.2529776096343994\n",
      "Train: Epoch [0], Batch [438/938], Loss: 2.244709014892578\n",
      "Train: Epoch [0], Batch [439/938], Loss: 2.2452313899993896\n",
      "Train: Epoch [0], Batch [440/938], Loss: 2.2545080184936523\n",
      "Train: Epoch [0], Batch [441/938], Loss: 2.2516393661499023\n",
      "Train: Epoch [0], Batch [442/938], Loss: 2.2533493041992188\n",
      "Train: Epoch [0], Batch [443/938], Loss: 2.2445359230041504\n",
      "Train: Epoch [0], Batch [444/938], Loss: 2.249009132385254\n",
      "Train: Epoch [0], Batch [445/938], Loss: 2.252932548522949\n",
      "Train: Epoch [0], Batch [446/938], Loss: 2.2524495124816895\n",
      "Train: Epoch [0], Batch [447/938], Loss: 2.254394769668579\n",
      "Train: Epoch [0], Batch [448/938], Loss: 2.2497012615203857\n",
      "Train: Epoch [0], Batch [449/938], Loss: 2.2578821182250977\n",
      "Train: Epoch [0], Batch [450/938], Loss: 2.2472305297851562\n",
      "Train: Epoch [0], Batch [451/938], Loss: 2.256434917449951\n",
      "Train: Epoch [0], Batch [452/938], Loss: 2.2576217651367188\n",
      "Train: Epoch [0], Batch [453/938], Loss: 2.249054431915283\n",
      "Train: Epoch [0], Batch [454/938], Loss: 2.2511653900146484\n",
      "Train: Epoch [0], Batch [455/938], Loss: 2.246839761734009\n",
      "Train: Epoch [0], Batch [456/938], Loss: 2.2485742568969727\n",
      "Train: Epoch [0], Batch [457/938], Loss: 2.2455968856811523\n",
      "Train: Epoch [0], Batch [458/938], Loss: 2.2464613914489746\n",
      "Train: Epoch [0], Batch [459/938], Loss: 2.2449727058410645\n",
      "Train: Epoch [0], Batch [460/938], Loss: 2.2516465187072754\n",
      "Train: Epoch [0], Batch [461/938], Loss: 2.252523899078369\n",
      "Train: Epoch [0], Batch [462/938], Loss: 2.255418300628662\n",
      "Train: Epoch [0], Batch [463/938], Loss: 2.243666172027588\n",
      "Train: Epoch [0], Batch [464/938], Loss: 2.2476000785827637\n",
      "Train: Epoch [0], Batch [465/938], Loss: 2.2454733848571777\n",
      "Train: Epoch [0], Batch [466/938], Loss: 2.2571723461151123\n",
      "Train: Epoch [0], Batch [467/938], Loss: 2.246368885040283\n",
      "Train: Epoch [0], Batch [468/938], Loss: 2.249833822250366\n",
      "Train: Epoch [0], Batch [469/938], Loss: 2.244508743286133\n",
      "Train: Epoch [0], Batch [470/938], Loss: 2.239612340927124\n",
      "Train: Epoch [0], Batch [471/938], Loss: 2.248430013656616\n",
      "Train: Epoch [0], Batch [472/938], Loss: 2.2547402381896973\n",
      "Train: Epoch [0], Batch [473/938], Loss: 2.2540595531463623\n",
      "Train: Epoch [0], Batch [474/938], Loss: 2.2505455017089844\n",
      "Train: Epoch [0], Batch [475/938], Loss: 2.24627685546875\n",
      "Train: Epoch [0], Batch [476/938], Loss: 2.2528347969055176\n",
      "Train: Epoch [0], Batch [477/938], Loss: 2.23966121673584\n",
      "Train: Epoch [0], Batch [478/938], Loss: 2.2386622428894043\n",
      "Train: Epoch [0], Batch [479/938], Loss: 2.237537145614624\n",
      "Train: Epoch [0], Batch [480/938], Loss: 2.2407312393188477\n",
      "Train: Epoch [0], Batch [481/938], Loss: 2.239450454711914\n",
      "Train: Epoch [0], Batch [482/938], Loss: 2.241055965423584\n",
      "Train: Epoch [0], Batch [483/938], Loss: 2.242249011993408\n",
      "Train: Epoch [0], Batch [484/938], Loss: 2.244032382965088\n",
      "Train: Epoch [0], Batch [485/938], Loss: 2.2420220375061035\n",
      "Train: Epoch [0], Batch [486/938], Loss: 2.242947816848755\n",
      "Train: Epoch [0], Batch [487/938], Loss: 2.2426774501800537\n",
      "Train: Epoch [0], Batch [488/938], Loss: 2.2469241619110107\n",
      "Train: Epoch [0], Batch [489/938], Loss: 2.250336170196533\n",
      "Train: Epoch [0], Batch [490/938], Loss: 2.2468199729919434\n",
      "Train: Epoch [0], Batch [491/938], Loss: 2.244752883911133\n",
      "Train: Epoch [0], Batch [492/938], Loss: 2.2516651153564453\n",
      "Train: Epoch [0], Batch [493/938], Loss: 2.24652099609375\n",
      "Train: Epoch [0], Batch [494/938], Loss: 2.246127128601074\n",
      "Train: Epoch [0], Batch [495/938], Loss: 2.244175434112549\n",
      "Train: Epoch [0], Batch [496/938], Loss: 2.2419886589050293\n",
      "Train: Epoch [0], Batch [497/938], Loss: 2.2500174045562744\n",
      "Train: Epoch [0], Batch [498/938], Loss: 2.2438559532165527\n",
      "Train: Epoch [0], Batch [499/938], Loss: 2.2369751930236816\n",
      "Train: Epoch [0], Batch [500/938], Loss: 2.248464584350586\n",
      "Train: Epoch [0], Batch [501/938], Loss: 2.245607852935791\n",
      "Train: Epoch [0], Batch [502/938], Loss: 2.2605409622192383\n",
      "Train: Epoch [0], Batch [503/938], Loss: 2.2354249954223633\n",
      "Train: Epoch [0], Batch [504/938], Loss: 2.2476158142089844\n",
      "Train: Epoch [0], Batch [505/938], Loss: 2.239908456802368\n",
      "Train: Epoch [0], Batch [506/938], Loss: 2.244544506072998\n",
      "Train: Epoch [0], Batch [507/938], Loss: 2.2353813648223877\n",
      "Train: Epoch [0], Batch [508/938], Loss: 2.2315833568573\n",
      "Train: Epoch [0], Batch [509/938], Loss: 2.2455010414123535\n",
      "Train: Epoch [0], Batch [510/938], Loss: 2.2373619079589844\n",
      "Train: Epoch [0], Batch [511/938], Loss: 2.2366600036621094\n",
      "Train: Epoch [0], Batch [512/938], Loss: 2.251791477203369\n",
      "Train: Epoch [0], Batch [513/938], Loss: 2.240509510040283\n",
      "Train: Epoch [0], Batch [514/938], Loss: 2.2317557334899902\n",
      "Train: Epoch [0], Batch [515/938], Loss: 2.2473607063293457\n",
      "Train: Epoch [0], Batch [516/938], Loss: 2.2463607788085938\n",
      "Train: Epoch [0], Batch [517/938], Loss: 2.243942975997925\n",
      "Train: Epoch [0], Batch [518/938], Loss: 2.234358310699463\n",
      "Train: Epoch [0], Batch [519/938], Loss: 2.240934371948242\n",
      "Train: Epoch [0], Batch [520/938], Loss: 2.238941192626953\n",
      "Train: Epoch [0], Batch [521/938], Loss: 2.250372886657715\n",
      "Train: Epoch [0], Batch [522/938], Loss: 2.232750415802002\n",
      "Train: Epoch [0], Batch [523/938], Loss: 2.2516121864318848\n",
      "Train: Epoch [0], Batch [524/938], Loss: 2.232337474822998\n",
      "Train: Epoch [0], Batch [525/938], Loss: 2.236388921737671\n",
      "Train: Epoch [0], Batch [526/938], Loss: 2.239896535873413\n",
      "Train: Epoch [0], Batch [527/938], Loss: 2.2337193489074707\n",
      "Train: Epoch [0], Batch [528/938], Loss: 2.239485740661621\n",
      "Train: Epoch [0], Batch [529/938], Loss: 2.2432878017425537\n",
      "Train: Epoch [0], Batch [530/938], Loss: 2.2466416358947754\n",
      "Train: Epoch [0], Batch [531/938], Loss: 2.2387354373931885\n",
      "Train: Epoch [0], Batch [532/938], Loss: 2.2380268573760986\n",
      "Train: Epoch [0], Batch [533/938], Loss: 2.235525369644165\n",
      "Train: Epoch [0], Batch [534/938], Loss: 2.233177661895752\n",
      "Train: Epoch [0], Batch [535/938], Loss: 2.2345571517944336\n",
      "Train: Epoch [0], Batch [536/938], Loss: 2.237759590148926\n",
      "Train: Epoch [0], Batch [537/938], Loss: 2.2446584701538086\n",
      "Train: Epoch [0], Batch [538/938], Loss: 2.238129138946533\n",
      "Train: Epoch [0], Batch [539/938], Loss: 2.242234230041504\n",
      "Train: Epoch [0], Batch [540/938], Loss: 2.241818904876709\n",
      "Train: Epoch [0], Batch [541/938], Loss: 2.241140842437744\n",
      "Train: Epoch [0], Batch [542/938], Loss: 2.2371578216552734\n",
      "Train: Epoch [0], Batch [543/938], Loss: 2.2467613220214844\n",
      "Train: Epoch [0], Batch [544/938], Loss: 2.245422601699829\n",
      "Train: Epoch [0], Batch [545/938], Loss: 2.2497408390045166\n",
      "Train: Epoch [0], Batch [546/938], Loss: 2.2278006076812744\n",
      "Train: Epoch [0], Batch [547/938], Loss: 2.2339072227478027\n",
      "Train: Epoch [0], Batch [548/938], Loss: 2.228302240371704\n",
      "Train: Epoch [0], Batch [549/938], Loss: 2.238032579421997\n",
      "Train: Epoch [0], Batch [550/938], Loss: 2.23030424118042\n",
      "Train: Epoch [0], Batch [551/938], Loss: 2.235607624053955\n",
      "Train: Epoch [0], Batch [552/938], Loss: 2.2309389114379883\n",
      "Train: Epoch [0], Batch [553/938], Loss: 2.2270069122314453\n",
      "Train: Epoch [0], Batch [554/938], Loss: 2.2379980087280273\n",
      "Train: Epoch [0], Batch [555/938], Loss: 2.240424156188965\n",
      "Train: Epoch [0], Batch [556/938], Loss: 2.232093095779419\n",
      "Train: Epoch [0], Batch [557/938], Loss: 2.230259656906128\n",
      "Train: Epoch [0], Batch [558/938], Loss: 2.2380552291870117\n",
      "Train: Epoch [0], Batch [559/938], Loss: 2.2399067878723145\n",
      "Train: Epoch [0], Batch [560/938], Loss: 2.242687702178955\n",
      "Train: Epoch [0], Batch [561/938], Loss: 2.233344316482544\n",
      "Train: Epoch [0], Batch [562/938], Loss: 2.231283664703369\n",
      "Train: Epoch [0], Batch [563/938], Loss: 2.220742702484131\n",
      "Train: Epoch [0], Batch [564/938], Loss: 2.237597703933716\n",
      "Train: Epoch [0], Batch [565/938], Loss: 2.2280683517456055\n",
      "Train: Epoch [0], Batch [566/938], Loss: 2.2473764419555664\n",
      "Train: Epoch [0], Batch [567/938], Loss: 2.230558395385742\n",
      "Train: Epoch [0], Batch [568/938], Loss: 2.2302279472351074\n",
      "Train: Epoch [0], Batch [569/938], Loss: 2.2270827293395996\n",
      "Train: Epoch [0], Batch [570/938], Loss: 2.2470765113830566\n",
      "Train: Epoch [0], Batch [571/938], Loss: 2.2320094108581543\n",
      "Train: Epoch [0], Batch [572/938], Loss: 2.235995292663574\n",
      "Train: Epoch [0], Batch [573/938], Loss: 2.227008819580078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [574/938], Loss: 2.2296338081359863\n",
      "Train: Epoch [0], Batch [575/938], Loss: 2.2380051612854004\n",
      "Train: Epoch [0], Batch [576/938], Loss: 2.2223072052001953\n",
      "Train: Epoch [0], Batch [577/938], Loss: 2.2165961265563965\n",
      "Train: Epoch [0], Batch [578/938], Loss: 2.227627754211426\n",
      "Train: Epoch [0], Batch [579/938], Loss: 2.240460157394409\n",
      "Train: Epoch [0], Batch [580/938], Loss: 2.24308705329895\n",
      "Train: Epoch [0], Batch [581/938], Loss: 2.2281904220581055\n",
      "Train: Epoch [0], Batch [582/938], Loss: 2.2344980239868164\n",
      "Train: Epoch [0], Batch [583/938], Loss: 2.2234883308410645\n",
      "Train: Epoch [0], Batch [584/938], Loss: 2.235384464263916\n",
      "Train: Epoch [0], Batch [585/938], Loss: 2.2150790691375732\n",
      "Train: Epoch [0], Batch [586/938], Loss: 2.2248902320861816\n",
      "Train: Epoch [0], Batch [587/938], Loss: 2.223452568054199\n",
      "Train: Epoch [0], Batch [588/938], Loss: 2.2203726768493652\n",
      "Train: Epoch [0], Batch [589/938], Loss: 2.223392963409424\n",
      "Train: Epoch [0], Batch [590/938], Loss: 2.231322765350342\n",
      "Train: Epoch [0], Batch [591/938], Loss: 2.2329304218292236\n",
      "Train: Epoch [0], Batch [592/938], Loss: 2.2276782989501953\n",
      "Train: Epoch [0], Batch [593/938], Loss: 2.2294154167175293\n",
      "Train: Epoch [0], Batch [594/938], Loss: 2.23046875\n",
      "Train: Epoch [0], Batch [595/938], Loss: 2.2114057540893555\n",
      "Train: Epoch [0], Batch [596/938], Loss: 2.219393253326416\n",
      "Train: Epoch [0], Batch [597/938], Loss: 2.2272024154663086\n",
      "Train: Epoch [0], Batch [598/938], Loss: 2.21628999710083\n",
      "Train: Epoch [0], Batch [599/938], Loss: 2.227128744125366\n",
      "Train: Epoch [0], Batch [600/938], Loss: 2.231574535369873\n",
      "Train: Epoch [0], Batch [601/938], Loss: 2.2361316680908203\n",
      "Train: Epoch [0], Batch [602/938], Loss: 2.213165521621704\n",
      "Train: Epoch [0], Batch [603/938], Loss: 2.230581283569336\n",
      "Train: Epoch [0], Batch [604/938], Loss: 2.2288222312927246\n",
      "Train: Epoch [0], Batch [605/938], Loss: 2.2174124717712402\n",
      "Train: Epoch [0], Batch [606/938], Loss: 2.2270619869232178\n",
      "Train: Epoch [0], Batch [607/938], Loss: 2.2223868370056152\n",
      "Train: Epoch [0], Batch [608/938], Loss: 2.2284631729125977\n",
      "Train: Epoch [0], Batch [609/938], Loss: 2.228132724761963\n",
      "Train: Epoch [0], Batch [610/938], Loss: 2.202542543411255\n",
      "Train: Epoch [0], Batch [611/938], Loss: 2.227543830871582\n",
      "Train: Epoch [0], Batch [612/938], Loss: 2.223313808441162\n",
      "Train: Epoch [0], Batch [613/938], Loss: 2.223001480102539\n",
      "Train: Epoch [0], Batch [614/938], Loss: 2.215451717376709\n",
      "Train: Epoch [0], Batch [615/938], Loss: 2.230430841445923\n",
      "Train: Epoch [0], Batch [616/938], Loss: 2.223174810409546\n",
      "Train: Epoch [0], Batch [617/938], Loss: 2.217935085296631\n",
      "Train: Epoch [0], Batch [618/938], Loss: 2.2266154289245605\n",
      "Train: Epoch [0], Batch [619/938], Loss: 2.2120790481567383\n",
      "Train: Epoch [0], Batch [620/938], Loss: 2.2269513607025146\n",
      "Train: Epoch [0], Batch [621/938], Loss: 2.2200918197631836\n",
      "Train: Epoch [0], Batch [622/938], Loss: 2.2303664684295654\n",
      "Train: Epoch [0], Batch [623/938], Loss: 2.226408004760742\n",
      "Train: Epoch [0], Batch [624/938], Loss: 2.2150397300720215\n",
      "Train: Epoch [0], Batch [625/938], Loss: 2.222932815551758\n",
      "Train: Epoch [0], Batch [626/938], Loss: 2.207275867462158\n",
      "Train: Epoch [0], Batch [627/938], Loss: 2.2209765911102295\n",
      "Train: Epoch [0], Batch [628/938], Loss: 2.2280445098876953\n",
      "Train: Epoch [0], Batch [629/938], Loss: 2.2107863426208496\n",
      "Train: Epoch [0], Batch [630/938], Loss: 2.232882261276245\n",
      "Train: Epoch [0], Batch [631/938], Loss: 2.215484142303467\n",
      "Train: Epoch [0], Batch [632/938], Loss: 2.2254090309143066\n",
      "Train: Epoch [0], Batch [633/938], Loss: 2.2202930450439453\n",
      "Train: Epoch [0], Batch [634/938], Loss: 2.2256598472595215\n",
      "Train: Epoch [0], Batch [635/938], Loss: 2.20029878616333\n",
      "Train: Epoch [0], Batch [636/938], Loss: 2.2178421020507812\n",
      "Train: Epoch [0], Batch [637/938], Loss: 2.202523708343506\n",
      "Train: Epoch [0], Batch [638/938], Loss: 2.2147421836853027\n",
      "Train: Epoch [0], Batch [639/938], Loss: 2.2197184562683105\n",
      "Train: Epoch [0], Batch [640/938], Loss: 2.220547676086426\n",
      "Train: Epoch [0], Batch [641/938], Loss: 2.217306613922119\n",
      "Train: Epoch [0], Batch [642/938], Loss: 2.216308116912842\n",
      "Train: Epoch [0], Batch [643/938], Loss: 2.2173404693603516\n",
      "Train: Epoch [0], Batch [644/938], Loss: 2.2021656036376953\n",
      "Train: Epoch [0], Batch [645/938], Loss: 2.2136995792388916\n",
      "Train: Epoch [0], Batch [646/938], Loss: 2.1994881629943848\n",
      "Train: Epoch [0], Batch [647/938], Loss: 2.2209181785583496\n",
      "Train: Epoch [0], Batch [648/938], Loss: 2.214054584503174\n",
      "Train: Epoch [0], Batch [649/938], Loss: 2.2203896045684814\n",
      "Train: Epoch [0], Batch [650/938], Loss: 2.206768035888672\n",
      "Train: Epoch [0], Batch [651/938], Loss: 2.2027950286865234\n",
      "Train: Epoch [0], Batch [652/938], Loss: 2.20786190032959\n",
      "Train: Epoch [0], Batch [653/938], Loss: 2.208587646484375\n",
      "Train: Epoch [0], Batch [654/938], Loss: 2.203420639038086\n",
      "Train: Epoch [0], Batch [655/938], Loss: 2.2201712131500244\n",
      "Train: Epoch [0], Batch [656/938], Loss: 2.225675106048584\n",
      "Train: Epoch [0], Batch [657/938], Loss: 2.211289882659912\n",
      "Train: Epoch [0], Batch [658/938], Loss: 2.1981983184814453\n",
      "Train: Epoch [0], Batch [659/938], Loss: 2.2211289405822754\n",
      "Train: Epoch [0], Batch [660/938], Loss: 2.2066242694854736\n",
      "Train: Epoch [0], Batch [661/938], Loss: 2.2086992263793945\n",
      "Train: Epoch [0], Batch [662/938], Loss: 2.2228598594665527\n",
      "Train: Epoch [0], Batch [663/938], Loss: 2.214897394180298\n",
      "Train: Epoch [0], Batch [664/938], Loss: 2.2006359100341797\n",
      "Train: Epoch [0], Batch [665/938], Loss: 2.2092788219451904\n",
      "Train: Epoch [0], Batch [666/938], Loss: 2.2246899604797363\n",
      "Train: Epoch [0], Batch [667/938], Loss: 2.206892728805542\n",
      "Train: Epoch [0], Batch [668/938], Loss: 2.192962408065796\n",
      "Train: Epoch [0], Batch [669/938], Loss: 2.224062919616699\n",
      "Train: Epoch [0], Batch [670/938], Loss: 2.213473081588745\n",
      "Train: Epoch [0], Batch [671/938], Loss: 2.2134175300598145\n",
      "Train: Epoch [0], Batch [672/938], Loss: 2.2057089805603027\n",
      "Train: Epoch [0], Batch [673/938], Loss: 2.202054977416992\n",
      "Train: Epoch [0], Batch [674/938], Loss: 2.2022790908813477\n",
      "Train: Epoch [0], Batch [675/938], Loss: 2.196146249771118\n",
      "Train: Epoch [0], Batch [676/938], Loss: 2.1968626976013184\n",
      "Train: Epoch [0], Batch [677/938], Loss: 2.1991095542907715\n",
      "Train: Epoch [0], Batch [678/938], Loss: 2.2032527923583984\n",
      "Train: Epoch [0], Batch [679/938], Loss: 2.1968517303466797\n",
      "Train: Epoch [0], Batch [680/938], Loss: 2.197880744934082\n",
      "Train: Epoch [0], Batch [681/938], Loss: 2.212066173553467\n",
      "Train: Epoch [0], Batch [682/938], Loss: 2.188931465148926\n",
      "Train: Epoch [0], Batch [683/938], Loss: 2.190160036087036\n",
      "Train: Epoch [0], Batch [684/938], Loss: 2.199735164642334\n",
      "Train: Epoch [0], Batch [685/938], Loss: 2.1929779052734375\n",
      "Train: Epoch [0], Batch [686/938], Loss: 2.200439929962158\n",
      "Train: Epoch [0], Batch [687/938], Loss: 2.202355146408081\n",
      "Train: Epoch [0], Batch [688/938], Loss: 2.203080177307129\n",
      "Train: Epoch [0], Batch [689/938], Loss: 2.1823368072509766\n",
      "Train: Epoch [0], Batch [690/938], Loss: 2.2030725479125977\n",
      "Train: Epoch [0], Batch [691/938], Loss: 2.218388557434082\n",
      "Train: Epoch [0], Batch [692/938], Loss: 2.205822229385376\n",
      "Train: Epoch [0], Batch [693/938], Loss: 2.2086024284362793\n",
      "Train: Epoch [0], Batch [694/938], Loss: 2.198366165161133\n",
      "Train: Epoch [0], Batch [695/938], Loss: 2.1951725482940674\n",
      "Train: Epoch [0], Batch [696/938], Loss: 2.197134494781494\n",
      "Train: Epoch [0], Batch [697/938], Loss: 2.200597047805786\n",
      "Train: Epoch [0], Batch [698/938], Loss: 2.2045798301696777\n",
      "Train: Epoch [0], Batch [699/938], Loss: 2.1979188919067383\n",
      "Train: Epoch [0], Batch [700/938], Loss: 2.173168659210205\n",
      "Train: Epoch [0], Batch [701/938], Loss: 2.196956157684326\n",
      "Train: Epoch [0], Batch [702/938], Loss: 2.198715925216675\n",
      "Train: Epoch [0], Batch [703/938], Loss: 2.1985692977905273\n",
      "Train: Epoch [0], Batch [704/938], Loss: 2.1997313499450684\n",
      "Train: Epoch [0], Batch [705/938], Loss: 2.2074172496795654\n",
      "Train: Epoch [0], Batch [706/938], Loss: 2.1891136169433594\n",
      "Train: Epoch [0], Batch [707/938], Loss: 2.196160316467285\n",
      "Train: Epoch [0], Batch [708/938], Loss: 2.1963050365448\n",
      "Train: Epoch [0], Batch [709/938], Loss: 2.1965012550354004\n",
      "Train: Epoch [0], Batch [710/938], Loss: 2.1881909370422363\n",
      "Train: Epoch [0], Batch [711/938], Loss: 2.2023065090179443\n",
      "Train: Epoch [0], Batch [712/938], Loss: 2.194561243057251\n",
      "Train: Epoch [0], Batch [713/938], Loss: 2.183572769165039\n",
      "Train: Epoch [0], Batch [714/938], Loss: 2.192934274673462\n",
      "Train: Epoch [0], Batch [715/938], Loss: 2.1928839683532715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [716/938], Loss: 2.1951911449432373\n",
      "Train: Epoch [0], Batch [717/938], Loss: 2.1826047897338867\n",
      "Train: Epoch [0], Batch [718/938], Loss: 2.205321788787842\n",
      "Train: Epoch [0], Batch [719/938], Loss: 2.2001280784606934\n",
      "Train: Epoch [0], Batch [720/938], Loss: 2.202157497406006\n",
      "Train: Epoch [0], Batch [721/938], Loss: 2.2112069129943848\n",
      "Train: Epoch [0], Batch [722/938], Loss: 2.189237356185913\n",
      "Train: Epoch [0], Batch [723/938], Loss: 2.174299478530884\n",
      "Train: Epoch [0], Batch [724/938], Loss: 2.188809394836426\n",
      "Train: Epoch [0], Batch [725/938], Loss: 2.1927242279052734\n",
      "Train: Epoch [0], Batch [726/938], Loss: 2.197504997253418\n",
      "Train: Epoch [0], Batch [727/938], Loss: 2.1732754707336426\n",
      "Train: Epoch [0], Batch [728/938], Loss: 2.180999994277954\n",
      "Train: Epoch [0], Batch [729/938], Loss: 2.184335708618164\n",
      "Train: Epoch [0], Batch [730/938], Loss: 2.1976163387298584\n",
      "Train: Epoch [0], Batch [731/938], Loss: 2.1728286743164062\n",
      "Train: Epoch [0], Batch [732/938], Loss: 2.1818652153015137\n",
      "Train: Epoch [0], Batch [733/938], Loss: 2.1750235557556152\n",
      "Train: Epoch [0], Batch [734/938], Loss: 2.1910433769226074\n",
      "Train: Epoch [0], Batch [735/938], Loss: 2.2050514221191406\n",
      "Train: Epoch [0], Batch [736/938], Loss: 2.1854171752929688\n",
      "Train: Epoch [0], Batch [737/938], Loss: 2.194021701812744\n",
      "Train: Epoch [0], Batch [738/938], Loss: 2.201846122741699\n",
      "Train: Epoch [0], Batch [739/938], Loss: 2.1884827613830566\n",
      "Train: Epoch [0], Batch [740/938], Loss: 2.1779274940490723\n",
      "Train: Epoch [0], Batch [741/938], Loss: 2.1910595893859863\n",
      "Train: Epoch [0], Batch [742/938], Loss: 2.1742167472839355\n",
      "Train: Epoch [0], Batch [743/938], Loss: 2.1759777069091797\n",
      "Train: Epoch [0], Batch [744/938], Loss: 2.1886754035949707\n",
      "Train: Epoch [0], Batch [745/938], Loss: 2.177927017211914\n",
      "Train: Epoch [0], Batch [746/938], Loss: 2.1914892196655273\n",
      "Train: Epoch [0], Batch [747/938], Loss: 2.1851117610931396\n",
      "Train: Epoch [0], Batch [748/938], Loss: 2.182631731033325\n",
      "Train: Epoch [0], Batch [749/938], Loss: 2.1661601066589355\n",
      "Train: Epoch [0], Batch [750/938], Loss: 2.184009552001953\n",
      "Train: Epoch [0], Batch [751/938], Loss: 2.1936609745025635\n",
      "Train: Epoch [0], Batch [752/938], Loss: 2.190101146697998\n",
      "Train: Epoch [0], Batch [753/938], Loss: 2.20245099067688\n",
      "Train: Epoch [0], Batch [754/938], Loss: 2.1573290824890137\n",
      "Train: Epoch [0], Batch [755/938], Loss: 2.1762771606445312\n",
      "Train: Epoch [0], Batch [756/938], Loss: 2.174154758453369\n",
      "Train: Epoch [0], Batch [757/938], Loss: 2.1720168590545654\n",
      "Train: Epoch [0], Batch [758/938], Loss: 2.148268222808838\n",
      "Train: Epoch [0], Batch [759/938], Loss: 2.1916608810424805\n",
      "Train: Epoch [0], Batch [760/938], Loss: 2.1723809242248535\n",
      "Train: Epoch [0], Batch [761/938], Loss: 2.1556546688079834\n",
      "Train: Epoch [0], Batch [762/938], Loss: 2.1752686500549316\n",
      "Train: Epoch [0], Batch [763/938], Loss: 2.1497931480407715\n",
      "Train: Epoch [0], Batch [764/938], Loss: 2.1801609992980957\n",
      "Train: Epoch [0], Batch [765/938], Loss: 2.1720218658447266\n",
      "Train: Epoch [0], Batch [766/938], Loss: 2.1719210147857666\n",
      "Train: Epoch [0], Batch [767/938], Loss: 2.187549591064453\n",
      "Train: Epoch [0], Batch [768/938], Loss: 2.1863365173339844\n",
      "Train: Epoch [0], Batch [769/938], Loss: 2.1610193252563477\n",
      "Train: Epoch [0], Batch [770/938], Loss: 2.1782939434051514\n",
      "Train: Epoch [0], Batch [771/938], Loss: 2.1822028160095215\n",
      "Train: Epoch [0], Batch [772/938], Loss: 2.1508729457855225\n",
      "Train: Epoch [0], Batch [773/938], Loss: 2.1569676399230957\n",
      "Train: Epoch [0], Batch [774/938], Loss: 2.162619113922119\n",
      "Train: Epoch [0], Batch [775/938], Loss: 2.171480655670166\n",
      "Train: Epoch [0], Batch [776/938], Loss: 2.173861503601074\n",
      "Train: Epoch [0], Batch [777/938], Loss: 2.1387948989868164\n",
      "Train: Epoch [0], Batch [778/938], Loss: 2.1484179496765137\n",
      "Train: Epoch [0], Batch [779/938], Loss: 2.1775193214416504\n",
      "Train: Epoch [0], Batch [780/938], Loss: 2.1515984535217285\n",
      "Train: Epoch [0], Batch [781/938], Loss: 2.176725149154663\n",
      "Train: Epoch [0], Batch [782/938], Loss: 2.1552698612213135\n",
      "Train: Epoch [0], Batch [783/938], Loss: 2.174773693084717\n",
      "Train: Epoch [0], Batch [784/938], Loss: 2.165738582611084\n",
      "Train: Epoch [0], Batch [785/938], Loss: 2.147268772125244\n",
      "Train: Epoch [0], Batch [786/938], Loss: 2.1331000328063965\n",
      "Train: Epoch [0], Batch [787/938], Loss: 2.175189733505249\n",
      "Train: Epoch [0], Batch [788/938], Loss: 2.1467833518981934\n",
      "Train: Epoch [0], Batch [789/938], Loss: 2.1470577716827393\n",
      "Train: Epoch [0], Batch [790/938], Loss: 2.151182174682617\n",
      "Train: Epoch [0], Batch [791/938], Loss: 2.151609182357788\n",
      "Train: Epoch [0], Batch [792/938], Loss: 2.182854652404785\n",
      "Train: Epoch [0], Batch [793/938], Loss: 2.122889995574951\n",
      "Train: Epoch [0], Batch [794/938], Loss: 2.1675984859466553\n",
      "Train: Epoch [0], Batch [795/938], Loss: 2.175158977508545\n",
      "Train: Epoch [0], Batch [796/938], Loss: 2.1810719966888428\n",
      "Train: Epoch [0], Batch [797/938], Loss: 2.167128086090088\n",
      "Train: Epoch [0], Batch [798/938], Loss: 2.1617603302001953\n",
      "Train: Epoch [0], Batch [799/938], Loss: 2.1553807258605957\n",
      "Train: Epoch [0], Batch [800/938], Loss: 2.1546196937561035\n",
      "Train: Epoch [0], Batch [801/938], Loss: 2.1749987602233887\n",
      "Train: Epoch [0], Batch [802/938], Loss: 2.1646547317504883\n",
      "Train: Epoch [0], Batch [803/938], Loss: 2.156491279602051\n",
      "Train: Epoch [0], Batch [804/938], Loss: 2.131666660308838\n",
      "Train: Epoch [0], Batch [805/938], Loss: 2.1416893005371094\n",
      "Train: Epoch [0], Batch [806/938], Loss: 2.169102668762207\n",
      "Train: Epoch [0], Batch [807/938], Loss: 2.14398193359375\n",
      "Train: Epoch [0], Batch [808/938], Loss: 2.152448892593384\n",
      "Train: Epoch [0], Batch [809/938], Loss: 2.1711699962615967\n",
      "Train: Epoch [0], Batch [810/938], Loss: 2.1527962684631348\n",
      "Train: Epoch [0], Batch [811/938], Loss: 2.1523289680480957\n",
      "Train: Epoch [0], Batch [812/938], Loss: 2.1405444145202637\n",
      "Train: Epoch [0], Batch [813/938], Loss: 2.1503677368164062\n",
      "Train: Epoch [0], Batch [814/938], Loss: 2.136284351348877\n",
      "Train: Epoch [0], Batch [815/938], Loss: 2.1295013427734375\n",
      "Train: Epoch [0], Batch [816/938], Loss: 2.142270088195801\n",
      "Train: Epoch [0], Batch [817/938], Loss: 2.117281913757324\n",
      "Train: Epoch [0], Batch [818/938], Loss: 2.1203842163085938\n",
      "Train: Epoch [0], Batch [819/938], Loss: 2.1469857692718506\n",
      "Train: Epoch [0], Batch [820/938], Loss: 2.1606316566467285\n",
      "Train: Epoch [0], Batch [821/938], Loss: 2.151109457015991\n",
      "Train: Epoch [0], Batch [822/938], Loss: 2.150892734527588\n",
      "Train: Epoch [0], Batch [823/938], Loss: 2.138819694519043\n",
      "Train: Epoch [0], Batch [824/938], Loss: 2.149289131164551\n",
      "Train: Epoch [0], Batch [825/938], Loss: 2.155470848083496\n",
      "Train: Epoch [0], Batch [826/938], Loss: 2.141153335571289\n",
      "Train: Epoch [0], Batch [827/938], Loss: 2.127030372619629\n",
      "Train: Epoch [0], Batch [828/938], Loss: 2.166138172149658\n",
      "Train: Epoch [0], Batch [829/938], Loss: 2.1447505950927734\n",
      "Train: Epoch [0], Batch [830/938], Loss: 2.1656136512756348\n",
      "Train: Epoch [0], Batch [831/938], Loss: 2.135925531387329\n",
      "Train: Epoch [0], Batch [832/938], Loss: 2.1044113636016846\n",
      "Train: Epoch [0], Batch [833/938], Loss: 2.1209027767181396\n",
      "Train: Epoch [0], Batch [834/938], Loss: 2.1603760719299316\n",
      "Train: Epoch [0], Batch [835/938], Loss: 2.120685577392578\n",
      "Train: Epoch [0], Batch [836/938], Loss: 2.166445016860962\n",
      "Train: Epoch [0], Batch [837/938], Loss: 2.132739543914795\n",
      "Train: Epoch [0], Batch [838/938], Loss: 2.1319098472595215\n",
      "Train: Epoch [0], Batch [839/938], Loss: 2.111366033554077\n",
      "Train: Epoch [0], Batch [840/938], Loss: 2.1389598846435547\n",
      "Train: Epoch [0], Batch [841/938], Loss: 2.155757188796997\n",
      "Train: Epoch [0], Batch [842/938], Loss: 2.1400341987609863\n",
      "Train: Epoch [0], Batch [843/938], Loss: 2.1542065143585205\n",
      "Train: Epoch [0], Batch [844/938], Loss: 2.1409521102905273\n",
      "Train: Epoch [0], Batch [845/938], Loss: 2.14174222946167\n",
      "Train: Epoch [0], Batch [846/938], Loss: 2.1354928016662598\n",
      "Train: Epoch [0], Batch [847/938], Loss: 2.1148886680603027\n",
      "Train: Epoch [0], Batch [848/938], Loss: 2.107027053833008\n",
      "Train: Epoch [0], Batch [849/938], Loss: 2.1068215370178223\n",
      "Train: Epoch [0], Batch [850/938], Loss: 2.1515419483184814\n",
      "Train: Epoch [0], Batch [851/938], Loss: 2.123544692993164\n",
      "Train: Epoch [0], Batch [852/938], Loss: 2.1167120933532715\n",
      "Train: Epoch [0], Batch [853/938], Loss: 2.133561611175537\n",
      "Train: Epoch [0], Batch [854/938], Loss: 2.1131229400634766\n",
      "Train: Epoch [0], Batch [855/938], Loss: 2.116130828857422\n",
      "Train: Epoch [0], Batch [856/938], Loss: 2.1320278644561768\n",
      "Train: Epoch [0], Batch [857/938], Loss: 2.1150500774383545\n",
      "Train: Epoch [0], Batch [858/938], Loss: 2.109262466430664\n",
      "Train: Epoch [0], Batch [859/938], Loss: 2.122734546661377\n",
      "Train: Epoch [0], Batch [860/938], Loss: 2.0999014377593994\n",
      "Train: Epoch [0], Batch [861/938], Loss: 2.1220386028289795\n",
      "Train: Epoch [0], Batch [862/938], Loss: 2.132082462310791\n",
      "Train: Epoch [0], Batch [863/938], Loss: 2.116588592529297\n",
      "Train: Epoch [0], Batch [864/938], Loss: 2.1082606315612793\n",
      "Train: Epoch [0], Batch [865/938], Loss: 2.1076931953430176\n",
      "Train: Epoch [0], Batch [866/938], Loss: 2.15722393989563\n",
      "Train: Epoch [0], Batch [867/938], Loss: 2.114621162414551\n",
      "Train: Epoch [0], Batch [868/938], Loss: 2.114060640335083\n",
      "Train: Epoch [0], Batch [869/938], Loss: 2.1282875537872314\n",
      "Train: Epoch [0], Batch [870/938], Loss: 2.105670213699341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [0], Batch [871/938], Loss: 2.123739004135132\n",
      "Train: Epoch [0], Batch [872/938], Loss: 2.137207508087158\n",
      "Train: Epoch [0], Batch [873/938], Loss: 2.104180097579956\n",
      "Train: Epoch [0], Batch [874/938], Loss: 2.0848793983459473\n",
      "Train: Epoch [0], Batch [875/938], Loss: 2.09716796875\n",
      "Train: Epoch [0], Batch [876/938], Loss: 2.113476276397705\n",
      "Train: Epoch [0], Batch [877/938], Loss: 2.114123821258545\n",
      "Train: Epoch [0], Batch [878/938], Loss: 2.1347742080688477\n",
      "Train: Epoch [0], Batch [879/938], Loss: 2.1208999156951904\n",
      "Train: Epoch [0], Batch [880/938], Loss: 2.1054773330688477\n",
      "Train: Epoch [0], Batch [881/938], Loss: 2.099236011505127\n",
      "Train: Epoch [0], Batch [882/938], Loss: 2.1532392501831055\n",
      "Train: Epoch [0], Batch [883/938], Loss: 2.0893003940582275\n",
      "Train: Epoch [0], Batch [884/938], Loss: 2.1008896827697754\n",
      "Train: Epoch [0], Batch [885/938], Loss: 2.128916025161743\n",
      "Train: Epoch [0], Batch [886/938], Loss: 2.0965726375579834\n",
      "Train: Epoch [0], Batch [887/938], Loss: 2.094677686691284\n",
      "Train: Epoch [0], Batch [888/938], Loss: 2.136111259460449\n",
      "Train: Epoch [0], Batch [889/938], Loss: 2.0965185165405273\n",
      "Train: Epoch [0], Batch [890/938], Loss: 2.086350440979004\n",
      "Train: Epoch [0], Batch [891/938], Loss: 2.121265172958374\n",
      "Train: Epoch [0], Batch [892/938], Loss: 2.1107068061828613\n",
      "Train: Epoch [0], Batch [893/938], Loss: 2.115900993347168\n",
      "Train: Epoch [0], Batch [894/938], Loss: 2.0639400482177734\n",
      "Train: Epoch [0], Batch [895/938], Loss: 2.110905170440674\n",
      "Train: Epoch [0], Batch [896/938], Loss: 2.1098217964172363\n",
      "Train: Epoch [0], Batch [897/938], Loss: 2.0798721313476562\n",
      "Train: Epoch [0], Batch [898/938], Loss: 2.070873975753784\n",
      "Train: Epoch [0], Batch [899/938], Loss: 2.1131956577301025\n",
      "Train: Epoch [0], Batch [900/938], Loss: 2.0871188640594482\n",
      "Train: Epoch [0], Batch [901/938], Loss: 2.1476802825927734\n",
      "Train: Epoch [0], Batch [902/938], Loss: 2.057098388671875\n",
      "Train: Epoch [0], Batch [903/938], Loss: 2.0722672939300537\n",
      "Train: Epoch [0], Batch [904/938], Loss: 2.0395922660827637\n",
      "Train: Epoch [0], Batch [905/938], Loss: 2.0915207862854004\n",
      "Train: Epoch [0], Batch [906/938], Loss: 2.091134548187256\n",
      "Train: Epoch [0], Batch [907/938], Loss: 2.0928595066070557\n",
      "Train: Epoch [0], Batch [908/938], Loss: 2.059419631958008\n",
      "Train: Epoch [0], Batch [909/938], Loss: 2.0747814178466797\n",
      "Train: Epoch [0], Batch [910/938], Loss: 2.0645434856414795\n",
      "Train: Epoch [0], Batch [911/938], Loss: 2.049320936203003\n",
      "Train: Epoch [0], Batch [912/938], Loss: 2.0644679069519043\n",
      "Train: Epoch [0], Batch [913/938], Loss: 2.0905706882476807\n",
      "Train: Epoch [0], Batch [914/938], Loss: 2.0526483058929443\n",
      "Train: Epoch [0], Batch [915/938], Loss: 2.0957393646240234\n",
      "Train: Epoch [0], Batch [916/938], Loss: 2.0926308631896973\n",
      "Train: Epoch [0], Batch [917/938], Loss: 2.079310894012451\n",
      "Train: Epoch [0], Batch [918/938], Loss: 2.036538600921631\n",
      "Train: Epoch [0], Batch [919/938], Loss: 2.076657295227051\n",
      "Train: Epoch [0], Batch [920/938], Loss: 2.090775489807129\n",
      "Train: Epoch [0], Batch [921/938], Loss: 2.0559213161468506\n",
      "Train: Epoch [0], Batch [922/938], Loss: 2.081697940826416\n",
      "Train: Epoch [0], Batch [923/938], Loss: 2.0961036682128906\n",
      "Train: Epoch [0], Batch [924/938], Loss: 2.0987889766693115\n",
      "Train: Epoch [0], Batch [925/938], Loss: 2.0698728561401367\n",
      "Train: Epoch [0], Batch [926/938], Loss: 2.101992130279541\n",
      "Train: Epoch [0], Batch [927/938], Loss: 2.039276123046875\n",
      "Train: Epoch [0], Batch [928/938], Loss: 2.0749080181121826\n",
      "Train: Epoch [0], Batch [929/938], Loss: 2.074432373046875\n",
      "Train: Epoch [0], Batch [930/938], Loss: 2.0831847190856934\n",
      "Train: Epoch [0], Batch [931/938], Loss: 2.0874667167663574\n",
      "Train: Epoch [0], Batch [932/938], Loss: 2.062321186065674\n",
      "Train: Epoch [0], Batch [933/938], Loss: 2.057539463043213\n",
      "Train: Epoch [0], Batch [934/938], Loss: 2.0479586124420166\n",
      "Train: Epoch [0], Batch [935/938], Loss: 2.131466865539551\n",
      "Train: Epoch [0], Batch [936/938], Loss: 2.0825836658477783\n",
      "Train: Epoch [0], Batch [937/938], Loss: 2.1032657623291016\n",
      "Train: Epoch [0], Batch [938/938], Loss: 1.9986951351165771\n",
      "Accuracy of train set: 0.4068833333333333\n",
      "Validation: Epoch [0], Batch [1/938], Loss: 2.0706429481506348\n",
      "Validation: Epoch [0], Batch [2/938], Loss: 2.0668556690216064\n",
      "Validation: Epoch [0], Batch [3/938], Loss: 2.0554800033569336\n",
      "Validation: Epoch [0], Batch [4/938], Loss: 2.0628795623779297\n",
      "Validation: Epoch [0], Batch [5/938], Loss: 2.0649259090423584\n",
      "Validation: Epoch [0], Batch [6/938], Loss: 2.022035598754883\n",
      "Validation: Epoch [0], Batch [7/938], Loss: 2.0636420249938965\n",
      "Validation: Epoch [0], Batch [8/938], Loss: 2.0663015842437744\n",
      "Validation: Epoch [0], Batch [9/938], Loss: 2.0886387825012207\n",
      "Validation: Epoch [0], Batch [10/938], Loss: 2.088332414627075\n",
      "Validation: Epoch [0], Batch [11/938], Loss: 2.0604987144470215\n",
      "Validation: Epoch [0], Batch [12/938], Loss: 2.074735641479492\n",
      "Validation: Epoch [0], Batch [13/938], Loss: 2.0522806644439697\n",
      "Validation: Epoch [0], Batch [14/938], Loss: 2.059968948364258\n",
      "Validation: Epoch [0], Batch [15/938], Loss: 2.06571626663208\n",
      "Validation: Epoch [0], Batch [16/938], Loss: 2.0357255935668945\n",
      "Validation: Epoch [0], Batch [17/938], Loss: 2.0264666080474854\n",
      "Validation: Epoch [0], Batch [18/938], Loss: 2.0720183849334717\n",
      "Validation: Epoch [0], Batch [19/938], Loss: 2.0817954540252686\n",
      "Validation: Epoch [0], Batch [20/938], Loss: 2.0585696697235107\n",
      "Validation: Epoch [0], Batch [21/938], Loss: 2.0471725463867188\n",
      "Validation: Epoch [0], Batch [22/938], Loss: 2.076730728149414\n",
      "Validation: Epoch [0], Batch [23/938], Loss: 2.035278797149658\n",
      "Validation: Epoch [0], Batch [24/938], Loss: 2.0324535369873047\n",
      "Validation: Epoch [0], Batch [25/938], Loss: 2.0935184955596924\n",
      "Validation: Epoch [0], Batch [26/938], Loss: 2.0540871620178223\n",
      "Validation: Epoch [0], Batch [27/938], Loss: 2.027207851409912\n",
      "Validation: Epoch [0], Batch [28/938], Loss: 2.0739808082580566\n",
      "Validation: Epoch [0], Batch [29/938], Loss: 2.058790683746338\n",
      "Validation: Epoch [0], Batch [30/938], Loss: 2.0370631217956543\n",
      "Validation: Epoch [0], Batch [31/938], Loss: 2.09847354888916\n",
      "Validation: Epoch [0], Batch [32/938], Loss: 2.062790870666504\n",
      "Validation: Epoch [0], Batch [33/938], Loss: 2.0856800079345703\n",
      "Validation: Epoch [0], Batch [34/938], Loss: 2.082812786102295\n",
      "Validation: Epoch [0], Batch [35/938], Loss: 2.071300983428955\n",
      "Validation: Epoch [0], Batch [36/938], Loss: 2.0365469455718994\n",
      "Validation: Epoch [0], Batch [37/938], Loss: 2.039989709854126\n",
      "Validation: Epoch [0], Batch [38/938], Loss: 2.095508575439453\n",
      "Validation: Epoch [0], Batch [39/938], Loss: 2.0477991104125977\n",
      "Validation: Epoch [0], Batch [40/938], Loss: 2.034289836883545\n",
      "Validation: Epoch [0], Batch [41/938], Loss: 2.045215368270874\n",
      "Validation: Epoch [0], Batch [42/938], Loss: 2.0513575077056885\n",
      "Validation: Epoch [0], Batch [43/938], Loss: 2.0926241874694824\n",
      "Validation: Epoch [0], Batch [44/938], Loss: 2.063891887664795\n",
      "Validation: Epoch [0], Batch [45/938], Loss: 2.0111515522003174\n",
      "Validation: Epoch [0], Batch [46/938], Loss: 2.077892303466797\n",
      "Validation: Epoch [0], Batch [47/938], Loss: 2.0547308921813965\n",
      "Validation: Epoch [0], Batch [48/938], Loss: 2.0382726192474365\n",
      "Validation: Epoch [0], Batch [49/938], Loss: 2.0773985385894775\n",
      "Validation: Epoch [0], Batch [50/938], Loss: 2.077643871307373\n",
      "Validation: Epoch [0], Batch [51/938], Loss: 2.055931806564331\n",
      "Validation: Epoch [0], Batch [52/938], Loss: 2.1002984046936035\n",
      "Validation: Epoch [0], Batch [53/938], Loss: 2.0220344066619873\n",
      "Validation: Epoch [0], Batch [54/938], Loss: 2.0621838569641113\n",
      "Validation: Epoch [0], Batch [55/938], Loss: 2.047858238220215\n",
      "Validation: Epoch [0], Batch [56/938], Loss: 2.01855206489563\n",
      "Validation: Epoch [0], Batch [57/938], Loss: 2.065335750579834\n",
      "Validation: Epoch [0], Batch [58/938], Loss: 2.08968448638916\n",
      "Validation: Epoch [0], Batch [59/938], Loss: 2.0698561668395996\n",
      "Validation: Epoch [0], Batch [60/938], Loss: 2.085577964782715\n",
      "Validation: Epoch [0], Batch [61/938], Loss: 2.09658145904541\n",
      "Validation: Epoch [0], Batch [62/938], Loss: 2.087881088256836\n",
      "Validation: Epoch [0], Batch [63/938], Loss: 2.034679889678955\n",
      "Validation: Epoch [0], Batch [64/938], Loss: 2.057021141052246\n",
      "Validation: Epoch [0], Batch [65/938], Loss: 2.038590431213379\n",
      "Validation: Epoch [0], Batch [66/938], Loss: 2.079969644546509\n",
      "Validation: Epoch [0], Batch [67/938], Loss: 2.0607941150665283\n",
      "Validation: Epoch [0], Batch [68/938], Loss: 2.0309956073760986\n",
      "Validation: Epoch [0], Batch [69/938], Loss: 2.079044818878174\n",
      "Validation: Epoch [0], Batch [70/938], Loss: 2.0234179496765137\n",
      "Validation: Epoch [0], Batch [71/938], Loss: 2.0698490142822266\n",
      "Validation: Epoch [0], Batch [72/938], Loss: 2.0762593746185303\n",
      "Validation: Epoch [0], Batch [73/938], Loss: 2.0638322830200195\n",
      "Validation: Epoch [0], Batch [74/938], Loss: 2.026731491088867\n",
      "Validation: Epoch [0], Batch [75/938], Loss: 2.066667079925537\n",
      "Validation: Epoch [0], Batch [76/938], Loss: 2.088881731033325\n",
      "Validation: Epoch [0], Batch [77/938], Loss: 2.068049192428589\n",
      "Validation: Epoch [0], Batch [78/938], Loss: 2.024228572845459\n",
      "Validation: Epoch [0], Batch [79/938], Loss: 2.077928066253662\n",
      "Validation: Epoch [0], Batch [80/938], Loss: 2.0660157203674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [81/938], Loss: 2.0626349449157715\n",
      "Validation: Epoch [0], Batch [82/938], Loss: 2.0694189071655273\n",
      "Validation: Epoch [0], Batch [83/938], Loss: 2.077744960784912\n",
      "Validation: Epoch [0], Batch [84/938], Loss: 2.0547170639038086\n",
      "Validation: Epoch [0], Batch [85/938], Loss: 2.0800387859344482\n",
      "Validation: Epoch [0], Batch [86/938], Loss: 2.0468478202819824\n",
      "Validation: Epoch [0], Batch [87/938], Loss: 2.037994861602783\n",
      "Validation: Epoch [0], Batch [88/938], Loss: 2.0468688011169434\n",
      "Validation: Epoch [0], Batch [89/938], Loss: 2.065927028656006\n",
      "Validation: Epoch [0], Batch [90/938], Loss: 2.0465316772460938\n",
      "Validation: Epoch [0], Batch [91/938], Loss: 2.043755054473877\n",
      "Validation: Epoch [0], Batch [92/938], Loss: 2.0612711906433105\n",
      "Validation: Epoch [0], Batch [93/938], Loss: 2.055262565612793\n",
      "Validation: Epoch [0], Batch [94/938], Loss: 2.0547468662261963\n",
      "Validation: Epoch [0], Batch [95/938], Loss: 2.016857385635376\n",
      "Validation: Epoch [0], Batch [96/938], Loss: 2.0832009315490723\n",
      "Validation: Epoch [0], Batch [97/938], Loss: 2.0365638732910156\n",
      "Validation: Epoch [0], Batch [98/938], Loss: 2.0429329872131348\n",
      "Validation: Epoch [0], Batch [99/938], Loss: 2.0689327716827393\n",
      "Validation: Epoch [0], Batch [100/938], Loss: 2.0633180141448975\n",
      "Validation: Epoch [0], Batch [101/938], Loss: 2.078629493713379\n",
      "Validation: Epoch [0], Batch [102/938], Loss: 2.058203935623169\n",
      "Validation: Epoch [0], Batch [103/938], Loss: 2.0197830200195312\n",
      "Validation: Epoch [0], Batch [104/938], Loss: 2.0378737449645996\n",
      "Validation: Epoch [0], Batch [105/938], Loss: 2.0905356407165527\n",
      "Validation: Epoch [0], Batch [106/938], Loss: 2.060112476348877\n",
      "Validation: Epoch [0], Batch [107/938], Loss: 2.029341220855713\n",
      "Validation: Epoch [0], Batch [108/938], Loss: 2.086996555328369\n",
      "Validation: Epoch [0], Batch [109/938], Loss: 2.0873899459838867\n",
      "Validation: Epoch [0], Batch [110/938], Loss: 2.0560741424560547\n",
      "Validation: Epoch [0], Batch [111/938], Loss: 2.073052406311035\n",
      "Validation: Epoch [0], Batch [112/938], Loss: 2.0424675941467285\n",
      "Validation: Epoch [0], Batch [113/938], Loss: 2.08516263961792\n",
      "Validation: Epoch [0], Batch [114/938], Loss: 2.0453925132751465\n",
      "Validation: Epoch [0], Batch [115/938], Loss: 2.0518035888671875\n",
      "Validation: Epoch [0], Batch [116/938], Loss: 2.0372376441955566\n",
      "Validation: Epoch [0], Batch [117/938], Loss: 2.068502426147461\n",
      "Validation: Epoch [0], Batch [118/938], Loss: 2.0604920387268066\n",
      "Validation: Epoch [0], Batch [119/938], Loss: 2.073563575744629\n",
      "Validation: Epoch [0], Batch [120/938], Loss: 2.076540470123291\n",
      "Validation: Epoch [0], Batch [121/938], Loss: 2.0566461086273193\n",
      "Validation: Epoch [0], Batch [122/938], Loss: 2.041205406188965\n",
      "Validation: Epoch [0], Batch [123/938], Loss: 2.0775339603424072\n",
      "Validation: Epoch [0], Batch [124/938], Loss: 2.0724282264709473\n",
      "Validation: Epoch [0], Batch [125/938], Loss: 2.067643165588379\n",
      "Validation: Epoch [0], Batch [126/938], Loss: 2.029472827911377\n",
      "Validation: Epoch [0], Batch [127/938], Loss: 2.0451855659484863\n",
      "Validation: Epoch [0], Batch [128/938], Loss: 2.1141059398651123\n",
      "Validation: Epoch [0], Batch [129/938], Loss: 2.0553040504455566\n",
      "Validation: Epoch [0], Batch [130/938], Loss: 2.0501632690429688\n",
      "Validation: Epoch [0], Batch [131/938], Loss: 2.093346118927002\n",
      "Validation: Epoch [0], Batch [132/938], Loss: 2.0807607173919678\n",
      "Validation: Epoch [0], Batch [133/938], Loss: 2.0653371810913086\n",
      "Validation: Epoch [0], Batch [134/938], Loss: 2.0590343475341797\n",
      "Validation: Epoch [0], Batch [135/938], Loss: 2.052011489868164\n",
      "Validation: Epoch [0], Batch [136/938], Loss: 2.0738682746887207\n",
      "Validation: Epoch [0], Batch [137/938], Loss: 2.0794172286987305\n",
      "Validation: Epoch [0], Batch [138/938], Loss: 2.0368642807006836\n",
      "Validation: Epoch [0], Batch [139/938], Loss: 2.0719399452209473\n",
      "Validation: Epoch [0], Batch [140/938], Loss: 2.0743093490600586\n",
      "Validation: Epoch [0], Batch [141/938], Loss: 2.039222240447998\n",
      "Validation: Epoch [0], Batch [142/938], Loss: 2.058269500732422\n",
      "Validation: Epoch [0], Batch [143/938], Loss: 2.051720142364502\n",
      "Validation: Epoch [0], Batch [144/938], Loss: 2.0529351234436035\n",
      "Validation: Epoch [0], Batch [145/938], Loss: 2.050772190093994\n",
      "Validation: Epoch [0], Batch [146/938], Loss: 2.0713696479797363\n",
      "Validation: Epoch [0], Batch [147/938], Loss: 2.0804810523986816\n",
      "Validation: Epoch [0], Batch [148/938], Loss: 2.032405376434326\n",
      "Validation: Epoch [0], Batch [149/938], Loss: 2.0636701583862305\n",
      "Validation: Epoch [0], Batch [150/938], Loss: 2.0668044090270996\n",
      "Validation: Epoch [0], Batch [151/938], Loss: 2.0479776859283447\n",
      "Validation: Epoch [0], Batch [152/938], Loss: 2.065741539001465\n",
      "Validation: Epoch [0], Batch [153/938], Loss: 2.0898542404174805\n",
      "Validation: Epoch [0], Batch [154/938], Loss: 2.0886754989624023\n",
      "Validation: Epoch [0], Batch [155/938], Loss: 2.0166077613830566\n",
      "Validation: Epoch [0], Batch [156/938], Loss: 2.0445613861083984\n",
      "Validation: Epoch [0], Batch [157/938], Loss: 2.0576248168945312\n",
      "Validation: Epoch [0], Batch [158/938], Loss: 2.068370819091797\n",
      "Validation: Epoch [0], Batch [159/938], Loss: 2.1095807552337646\n",
      "Validation: Epoch [0], Batch [160/938], Loss: 2.095539093017578\n",
      "Validation: Epoch [0], Batch [161/938], Loss: 2.080418825149536\n",
      "Validation: Epoch [0], Batch [162/938], Loss: 2.042984962463379\n",
      "Validation: Epoch [0], Batch [163/938], Loss: 2.0696334838867188\n",
      "Validation: Epoch [0], Batch [164/938], Loss: 2.064175605773926\n",
      "Validation: Epoch [0], Batch [165/938], Loss: 2.054598093032837\n",
      "Validation: Epoch [0], Batch [166/938], Loss: 2.022597312927246\n",
      "Validation: Epoch [0], Batch [167/938], Loss: 2.050110340118408\n",
      "Validation: Epoch [0], Batch [168/938], Loss: 2.080699920654297\n",
      "Validation: Epoch [0], Batch [169/938], Loss: 2.070124864578247\n",
      "Validation: Epoch [0], Batch [170/938], Loss: 2.055098295211792\n",
      "Validation: Epoch [0], Batch [171/938], Loss: 2.0670924186706543\n",
      "Validation: Epoch [0], Batch [172/938], Loss: 2.067601442337036\n",
      "Validation: Epoch [0], Batch [173/938], Loss: 2.040712594985962\n",
      "Validation: Epoch [0], Batch [174/938], Loss: 2.09275484085083\n",
      "Validation: Epoch [0], Batch [175/938], Loss: 2.0441009998321533\n",
      "Validation: Epoch [0], Batch [176/938], Loss: 2.056147813796997\n",
      "Validation: Epoch [0], Batch [177/938], Loss: 2.037785053253174\n",
      "Validation: Epoch [0], Batch [178/938], Loss: 2.0856754779815674\n",
      "Validation: Epoch [0], Batch [179/938], Loss: 2.06441068649292\n",
      "Validation: Epoch [0], Batch [180/938], Loss: 2.034123420715332\n",
      "Validation: Epoch [0], Batch [181/938], Loss: 2.047791004180908\n",
      "Validation: Epoch [0], Batch [182/938], Loss: 2.033665895462036\n",
      "Validation: Epoch [0], Batch [183/938], Loss: 2.0435702800750732\n",
      "Validation: Epoch [0], Batch [184/938], Loss: 2.0529565811157227\n",
      "Validation: Epoch [0], Batch [185/938], Loss: 2.050093173980713\n",
      "Validation: Epoch [0], Batch [186/938], Loss: 2.096144676208496\n",
      "Validation: Epoch [0], Batch [187/938], Loss: 2.096679925918579\n",
      "Validation: Epoch [0], Batch [188/938], Loss: 2.0582902431488037\n",
      "Validation: Epoch [0], Batch [189/938], Loss: 2.0477981567382812\n",
      "Validation: Epoch [0], Batch [190/938], Loss: 2.0540060997009277\n",
      "Validation: Epoch [0], Batch [191/938], Loss: 2.0876219272613525\n",
      "Validation: Epoch [0], Batch [192/938], Loss: 2.0791409015655518\n",
      "Validation: Epoch [0], Batch [193/938], Loss: 2.0504794120788574\n",
      "Validation: Epoch [0], Batch [194/938], Loss: 2.0586352348327637\n",
      "Validation: Epoch [0], Batch [195/938], Loss: 2.0773539543151855\n",
      "Validation: Epoch [0], Batch [196/938], Loss: 2.0758092403411865\n",
      "Validation: Epoch [0], Batch [197/938], Loss: 2.061464309692383\n",
      "Validation: Epoch [0], Batch [198/938], Loss: 2.1117992401123047\n",
      "Validation: Epoch [0], Batch [199/938], Loss: 2.0388710498809814\n",
      "Validation: Epoch [0], Batch [200/938], Loss: 2.075147867202759\n",
      "Validation: Epoch [0], Batch [201/938], Loss: 2.044963836669922\n",
      "Validation: Epoch [0], Batch [202/938], Loss: 2.0487685203552246\n",
      "Validation: Epoch [0], Batch [203/938], Loss: 2.0308823585510254\n",
      "Validation: Epoch [0], Batch [204/938], Loss: 2.106795310974121\n",
      "Validation: Epoch [0], Batch [205/938], Loss: 2.0513052940368652\n",
      "Validation: Epoch [0], Batch [206/938], Loss: 2.0588810443878174\n",
      "Validation: Epoch [0], Batch [207/938], Loss: 2.1065115928649902\n",
      "Validation: Epoch [0], Batch [208/938], Loss: 2.0140016078948975\n",
      "Validation: Epoch [0], Batch [209/938], Loss: 2.079343795776367\n",
      "Validation: Epoch [0], Batch [210/938], Loss: 2.0438547134399414\n",
      "Validation: Epoch [0], Batch [211/938], Loss: 2.055713176727295\n",
      "Validation: Epoch [0], Batch [212/938], Loss: 2.0404839515686035\n",
      "Validation: Epoch [0], Batch [213/938], Loss: 2.1036791801452637\n",
      "Validation: Epoch [0], Batch [214/938], Loss: 2.0377116203308105\n",
      "Validation: Epoch [0], Batch [215/938], Loss: 2.0474843978881836\n",
      "Validation: Epoch [0], Batch [216/938], Loss: 2.0710132122039795\n",
      "Validation: Epoch [0], Batch [217/938], Loss: 2.1017119884490967\n",
      "Validation: Epoch [0], Batch [218/938], Loss: 2.0628905296325684\n",
      "Validation: Epoch [0], Batch [219/938], Loss: 2.046083688735962\n",
      "Validation: Epoch [0], Batch [220/938], Loss: 2.104018211364746\n",
      "Validation: Epoch [0], Batch [221/938], Loss: 2.05316162109375\n",
      "Validation: Epoch [0], Batch [222/938], Loss: 2.016803741455078\n",
      "Validation: Epoch [0], Batch [223/938], Loss: 2.0350022315979004\n",
      "Validation: Epoch [0], Batch [224/938], Loss: 2.0868711471557617\n",
      "Validation: Epoch [0], Batch [225/938], Loss: 2.0447282791137695\n",
      "Validation: Epoch [0], Batch [226/938], Loss: 2.055022716522217\n",
      "Validation: Epoch [0], Batch [227/938], Loss: 2.0650734901428223\n",
      "Validation: Epoch [0], Batch [228/938], Loss: 2.021941661834717\n",
      "Validation: Epoch [0], Batch [229/938], Loss: 2.0582711696624756\n",
      "Validation: Epoch [0], Batch [230/938], Loss: 2.019667625427246\n",
      "Validation: Epoch [0], Batch [231/938], Loss: 2.0269381999969482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [232/938], Loss: 2.0853123664855957\n",
      "Validation: Epoch [0], Batch [233/938], Loss: 2.0372419357299805\n",
      "Validation: Epoch [0], Batch [234/938], Loss: 2.0310049057006836\n",
      "Validation: Epoch [0], Batch [235/938], Loss: 2.0540432929992676\n",
      "Validation: Epoch [0], Batch [236/938], Loss: 2.0472264289855957\n",
      "Validation: Epoch [0], Batch [237/938], Loss: 2.1099352836608887\n",
      "Validation: Epoch [0], Batch [238/938], Loss: 2.076939105987549\n",
      "Validation: Epoch [0], Batch [239/938], Loss: 2.1031014919281006\n",
      "Validation: Epoch [0], Batch [240/938], Loss: 2.0725154876708984\n",
      "Validation: Epoch [0], Batch [241/938], Loss: 2.0447967052459717\n",
      "Validation: Epoch [0], Batch [242/938], Loss: 2.062084674835205\n",
      "Validation: Epoch [0], Batch [243/938], Loss: 2.070680856704712\n",
      "Validation: Epoch [0], Batch [244/938], Loss: 2.0814614295959473\n",
      "Validation: Epoch [0], Batch [245/938], Loss: 2.0310044288635254\n",
      "Validation: Epoch [0], Batch [246/938], Loss: 2.0688843727111816\n",
      "Validation: Epoch [0], Batch [247/938], Loss: 2.040038585662842\n",
      "Validation: Epoch [0], Batch [248/938], Loss: 2.0930376052856445\n",
      "Validation: Epoch [0], Batch [249/938], Loss: 2.0762124061584473\n",
      "Validation: Epoch [0], Batch [250/938], Loss: 2.065145969390869\n",
      "Validation: Epoch [0], Batch [251/938], Loss: 2.0696182250976562\n",
      "Validation: Epoch [0], Batch [252/938], Loss: 2.0456857681274414\n",
      "Validation: Epoch [0], Batch [253/938], Loss: 2.034078598022461\n",
      "Validation: Epoch [0], Batch [254/938], Loss: 2.0426127910614014\n",
      "Validation: Epoch [0], Batch [255/938], Loss: 2.062551975250244\n",
      "Validation: Epoch [0], Batch [256/938], Loss: 2.0570287704467773\n",
      "Validation: Epoch [0], Batch [257/938], Loss: 2.0221633911132812\n",
      "Validation: Epoch [0], Batch [258/938], Loss: 2.032824993133545\n",
      "Validation: Epoch [0], Batch [259/938], Loss: 2.0789437294006348\n",
      "Validation: Epoch [0], Batch [260/938], Loss: 2.0545761585235596\n",
      "Validation: Epoch [0], Batch [261/938], Loss: 2.0695133209228516\n",
      "Validation: Epoch [0], Batch [262/938], Loss: 2.014580011367798\n",
      "Validation: Epoch [0], Batch [263/938], Loss: 2.0499534606933594\n",
      "Validation: Epoch [0], Batch [264/938], Loss: 2.053546190261841\n",
      "Validation: Epoch [0], Batch [265/938], Loss: 2.0550432205200195\n",
      "Validation: Epoch [0], Batch [266/938], Loss: 2.0468082427978516\n",
      "Validation: Epoch [0], Batch [267/938], Loss: 2.10760498046875\n",
      "Validation: Epoch [0], Batch [268/938], Loss: 2.049398422241211\n",
      "Validation: Epoch [0], Batch [269/938], Loss: 2.051632881164551\n",
      "Validation: Epoch [0], Batch [270/938], Loss: 2.0267837047576904\n",
      "Validation: Epoch [0], Batch [271/938], Loss: 2.0753021240234375\n",
      "Validation: Epoch [0], Batch [272/938], Loss: 2.0410025119781494\n",
      "Validation: Epoch [0], Batch [273/938], Loss: 2.0711851119995117\n",
      "Validation: Epoch [0], Batch [274/938], Loss: 2.0550456047058105\n",
      "Validation: Epoch [0], Batch [275/938], Loss: 2.0157628059387207\n",
      "Validation: Epoch [0], Batch [276/938], Loss: 2.1011667251586914\n",
      "Validation: Epoch [0], Batch [277/938], Loss: 2.072502613067627\n",
      "Validation: Epoch [0], Batch [278/938], Loss: 2.066774368286133\n",
      "Validation: Epoch [0], Batch [279/938], Loss: 2.0320191383361816\n",
      "Validation: Epoch [0], Batch [280/938], Loss: 2.0749316215515137\n",
      "Validation: Epoch [0], Batch [281/938], Loss: 2.0457284450531006\n",
      "Validation: Epoch [0], Batch [282/938], Loss: 2.050171136856079\n",
      "Validation: Epoch [0], Batch [283/938], Loss: 2.041193962097168\n",
      "Validation: Epoch [0], Batch [284/938], Loss: 2.0588908195495605\n",
      "Validation: Epoch [0], Batch [285/938], Loss: 2.063943862915039\n",
      "Validation: Epoch [0], Batch [286/938], Loss: 2.076368570327759\n",
      "Validation: Epoch [0], Batch [287/938], Loss: 2.0207226276397705\n",
      "Validation: Epoch [0], Batch [288/938], Loss: 2.0669963359832764\n",
      "Validation: Epoch [0], Batch [289/938], Loss: 2.0832772254943848\n",
      "Validation: Epoch [0], Batch [290/938], Loss: 2.0718612670898438\n",
      "Validation: Epoch [0], Batch [291/938], Loss: 2.059239625930786\n",
      "Validation: Epoch [0], Batch [292/938], Loss: 2.057788848876953\n",
      "Validation: Epoch [0], Batch [293/938], Loss: 2.082193613052368\n",
      "Validation: Epoch [0], Batch [294/938], Loss: 2.082239866256714\n",
      "Validation: Epoch [0], Batch [295/938], Loss: 2.0538830757141113\n",
      "Validation: Epoch [0], Batch [296/938], Loss: 2.0548887252807617\n",
      "Validation: Epoch [0], Batch [297/938], Loss: 2.0342907905578613\n",
      "Validation: Epoch [0], Batch [298/938], Loss: 2.054476022720337\n",
      "Validation: Epoch [0], Batch [299/938], Loss: 2.0550999641418457\n",
      "Validation: Epoch [0], Batch [300/938], Loss: 2.0362296104431152\n",
      "Validation: Epoch [0], Batch [301/938], Loss: 2.0763630867004395\n",
      "Validation: Epoch [0], Batch [302/938], Loss: 2.045651912689209\n",
      "Validation: Epoch [0], Batch [303/938], Loss: 2.0622763633728027\n",
      "Validation: Epoch [0], Batch [304/938], Loss: 2.03450083732605\n",
      "Validation: Epoch [0], Batch [305/938], Loss: 2.071721315383911\n",
      "Validation: Epoch [0], Batch [306/938], Loss: 2.087913990020752\n",
      "Validation: Epoch [0], Batch [307/938], Loss: 2.0724196434020996\n",
      "Validation: Epoch [0], Batch [308/938], Loss: 2.01865816116333\n",
      "Validation: Epoch [0], Batch [309/938], Loss: 2.0935208797454834\n",
      "Validation: Epoch [0], Batch [310/938], Loss: 2.090001106262207\n",
      "Validation: Epoch [0], Batch [311/938], Loss: 2.084563732147217\n",
      "Validation: Epoch [0], Batch [312/938], Loss: 2.0852789878845215\n",
      "Validation: Epoch [0], Batch [313/938], Loss: 2.0371358394622803\n",
      "Validation: Epoch [0], Batch [314/938], Loss: 2.053981304168701\n",
      "Validation: Epoch [0], Batch [315/938], Loss: 2.035006523132324\n",
      "Validation: Epoch [0], Batch [316/938], Loss: 2.040558338165283\n",
      "Validation: Epoch [0], Batch [317/938], Loss: 2.0716254711151123\n",
      "Validation: Epoch [0], Batch [318/938], Loss: 2.056103467941284\n",
      "Validation: Epoch [0], Batch [319/938], Loss: 2.07358455657959\n",
      "Validation: Epoch [0], Batch [320/938], Loss: 2.0762627124786377\n",
      "Validation: Epoch [0], Batch [321/938], Loss: 2.0595662593841553\n",
      "Validation: Epoch [0], Batch [322/938], Loss: 2.1068060398101807\n",
      "Validation: Epoch [0], Batch [323/938], Loss: 2.097031354904175\n",
      "Validation: Epoch [0], Batch [324/938], Loss: 2.061960458755493\n",
      "Validation: Epoch [0], Batch [325/938], Loss: 2.0634899139404297\n",
      "Validation: Epoch [0], Batch [326/938], Loss: 2.0505783557891846\n",
      "Validation: Epoch [0], Batch [327/938], Loss: 2.0684046745300293\n",
      "Validation: Epoch [0], Batch [328/938], Loss: 2.085758924484253\n",
      "Validation: Epoch [0], Batch [329/938], Loss: 2.0828592777252197\n",
      "Validation: Epoch [0], Batch [330/938], Loss: 2.075103759765625\n",
      "Validation: Epoch [0], Batch [331/938], Loss: 2.0353057384490967\n",
      "Validation: Epoch [0], Batch [332/938], Loss: 2.0698986053466797\n",
      "Validation: Epoch [0], Batch [333/938], Loss: 2.0327320098876953\n",
      "Validation: Epoch [0], Batch [334/938], Loss: 2.075199604034424\n",
      "Validation: Epoch [0], Batch [335/938], Loss: 2.079500675201416\n",
      "Validation: Epoch [0], Batch [336/938], Loss: 2.0979676246643066\n",
      "Validation: Epoch [0], Batch [337/938], Loss: 2.092858076095581\n",
      "Validation: Epoch [0], Batch [338/938], Loss: 2.0588841438293457\n",
      "Validation: Epoch [0], Batch [339/938], Loss: 2.0467350482940674\n",
      "Validation: Epoch [0], Batch [340/938], Loss: 2.0484514236450195\n",
      "Validation: Epoch [0], Batch [341/938], Loss: 2.0430803298950195\n",
      "Validation: Epoch [0], Batch [342/938], Loss: 2.0628933906555176\n",
      "Validation: Epoch [0], Batch [343/938], Loss: 2.091444969177246\n",
      "Validation: Epoch [0], Batch [344/938], Loss: 2.0681190490722656\n",
      "Validation: Epoch [0], Batch [345/938], Loss: 2.0793685913085938\n",
      "Validation: Epoch [0], Batch [346/938], Loss: 2.1260008811950684\n",
      "Validation: Epoch [0], Batch [347/938], Loss: 2.044968605041504\n",
      "Validation: Epoch [0], Batch [348/938], Loss: 2.07028865814209\n",
      "Validation: Epoch [0], Batch [349/938], Loss: 2.066563367843628\n",
      "Validation: Epoch [0], Batch [350/938], Loss: 2.106567859649658\n",
      "Validation: Epoch [0], Batch [351/938], Loss: 2.062931537628174\n",
      "Validation: Epoch [0], Batch [352/938], Loss: 2.0624160766601562\n",
      "Validation: Epoch [0], Batch [353/938], Loss: 2.073523998260498\n",
      "Validation: Epoch [0], Batch [354/938], Loss: 2.0559041500091553\n",
      "Validation: Epoch [0], Batch [355/938], Loss: 2.0770082473754883\n",
      "Validation: Epoch [0], Batch [356/938], Loss: 2.0717973709106445\n",
      "Validation: Epoch [0], Batch [357/938], Loss: 2.075981616973877\n",
      "Validation: Epoch [0], Batch [358/938], Loss: 2.100269317626953\n",
      "Validation: Epoch [0], Batch [359/938], Loss: 2.052922248840332\n",
      "Validation: Epoch [0], Batch [360/938], Loss: 2.042790174484253\n",
      "Validation: Epoch [0], Batch [361/938], Loss: 2.0810348987579346\n",
      "Validation: Epoch [0], Batch [362/938], Loss: 2.0331950187683105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [363/938], Loss: 2.0220580101013184\n",
      "Validation: Epoch [0], Batch [364/938], Loss: 2.0978431701660156\n",
      "Validation: Epoch [0], Batch [365/938], Loss: 2.0274009704589844\n",
      "Validation: Epoch [0], Batch [366/938], Loss: 2.0613021850585938\n",
      "Validation: Epoch [0], Batch [367/938], Loss: 2.0446434020996094\n",
      "Validation: Epoch [0], Batch [368/938], Loss: 2.0216426849365234\n",
      "Validation: Epoch [0], Batch [369/938], Loss: 2.0751147270202637\n",
      "Validation: Epoch [0], Batch [370/938], Loss: 2.0322322845458984\n",
      "Validation: Epoch [0], Batch [371/938], Loss: 2.044166326522827\n",
      "Validation: Epoch [0], Batch [372/938], Loss: 2.072728157043457\n",
      "Validation: Epoch [0], Batch [373/938], Loss: 2.059879779815674\n",
      "Validation: Epoch [0], Batch [374/938], Loss: 2.048356533050537\n",
      "Validation: Epoch [0], Batch [375/938], Loss: 2.04473614692688\n",
      "Validation: Epoch [0], Batch [376/938], Loss: 2.0680155754089355\n",
      "Validation: Epoch [0], Batch [377/938], Loss: 2.0591087341308594\n",
      "Validation: Epoch [0], Batch [378/938], Loss: 2.0640621185302734\n",
      "Validation: Epoch [0], Batch [379/938], Loss: 2.081400156021118\n",
      "Validation: Epoch [0], Batch [380/938], Loss: 2.072094440460205\n",
      "Validation: Epoch [0], Batch [381/938], Loss: 2.0821127891540527\n",
      "Validation: Epoch [0], Batch [382/938], Loss: 2.0433897972106934\n",
      "Validation: Epoch [0], Batch [383/938], Loss: 2.0759263038635254\n",
      "Validation: Epoch [0], Batch [384/938], Loss: 2.082124710083008\n",
      "Validation: Epoch [0], Batch [385/938], Loss: 2.069344997406006\n",
      "Validation: Epoch [0], Batch [386/938], Loss: 2.0602715015411377\n",
      "Validation: Epoch [0], Batch [387/938], Loss: 2.046527147293091\n",
      "Validation: Epoch [0], Batch [388/938], Loss: 2.098968029022217\n",
      "Validation: Epoch [0], Batch [389/938], Loss: 2.096400022506714\n",
      "Validation: Epoch [0], Batch [390/938], Loss: 2.072023391723633\n",
      "Validation: Epoch [0], Batch [391/938], Loss: 2.040632963180542\n",
      "Validation: Epoch [0], Batch [392/938], Loss: 2.048213481903076\n",
      "Validation: Epoch [0], Batch [393/938], Loss: 2.047337293624878\n",
      "Validation: Epoch [0], Batch [394/938], Loss: 2.0733909606933594\n",
      "Validation: Epoch [0], Batch [395/938], Loss: 2.076474189758301\n",
      "Validation: Epoch [0], Batch [396/938], Loss: 2.1130495071411133\n",
      "Validation: Epoch [0], Batch [397/938], Loss: 2.056323289871216\n",
      "Validation: Epoch [0], Batch [398/938], Loss: 2.0767412185668945\n",
      "Validation: Epoch [0], Batch [399/938], Loss: 2.0743508338928223\n",
      "Validation: Epoch [0], Batch [400/938], Loss: 2.1066713333129883\n",
      "Validation: Epoch [0], Batch [401/938], Loss: 2.048992395401001\n",
      "Validation: Epoch [0], Batch [402/938], Loss: 2.045151710510254\n",
      "Validation: Epoch [0], Batch [403/938], Loss: 2.0725512504577637\n",
      "Validation: Epoch [0], Batch [404/938], Loss: 2.063784122467041\n",
      "Validation: Epoch [0], Batch [405/938], Loss: 2.039093255996704\n",
      "Validation: Epoch [0], Batch [406/938], Loss: 2.0715246200561523\n",
      "Validation: Epoch [0], Batch [407/938], Loss: 2.0793166160583496\n",
      "Validation: Epoch [0], Batch [408/938], Loss: 2.0571632385253906\n",
      "Validation: Epoch [0], Batch [409/938], Loss: 2.0582571029663086\n",
      "Validation: Epoch [0], Batch [410/938], Loss: 2.039621114730835\n",
      "Validation: Epoch [0], Batch [411/938], Loss: 2.0591330528259277\n",
      "Validation: Epoch [0], Batch [412/938], Loss: 2.056994676589966\n",
      "Validation: Epoch [0], Batch [413/938], Loss: 2.06892466545105\n",
      "Validation: Epoch [0], Batch [414/938], Loss: 2.0558323860168457\n",
      "Validation: Epoch [0], Batch [415/938], Loss: 2.1205801963806152\n",
      "Validation: Epoch [0], Batch [416/938], Loss: 2.0609045028686523\n",
      "Validation: Epoch [0], Batch [417/938], Loss: 2.078094005584717\n",
      "Validation: Epoch [0], Batch [418/938], Loss: 2.054340362548828\n",
      "Validation: Epoch [0], Batch [419/938], Loss: 2.048562526702881\n",
      "Validation: Epoch [0], Batch [420/938], Loss: 2.0524888038635254\n",
      "Validation: Epoch [0], Batch [421/938], Loss: 2.0363385677337646\n",
      "Validation: Epoch [0], Batch [422/938], Loss: 2.0365421772003174\n",
      "Validation: Epoch [0], Batch [423/938], Loss: 2.09810733795166\n",
      "Validation: Epoch [0], Batch [424/938], Loss: 2.0914523601531982\n",
      "Validation: Epoch [0], Batch [425/938], Loss: 2.0258491039276123\n",
      "Validation: Epoch [0], Batch [426/938], Loss: 2.073714256286621\n",
      "Validation: Epoch [0], Batch [427/938], Loss: 2.0669517517089844\n",
      "Validation: Epoch [0], Batch [428/938], Loss: 2.0916011333465576\n",
      "Validation: Epoch [0], Batch [429/938], Loss: 2.0519070625305176\n",
      "Validation: Epoch [0], Batch [430/938], Loss: 2.1019554138183594\n",
      "Validation: Epoch [0], Batch [431/938], Loss: 2.0362677574157715\n",
      "Validation: Epoch [0], Batch [432/938], Loss: 2.0692920684814453\n",
      "Validation: Epoch [0], Batch [433/938], Loss: 2.071741819381714\n",
      "Validation: Epoch [0], Batch [434/938], Loss: 2.070614814758301\n",
      "Validation: Epoch [0], Batch [435/938], Loss: 2.058992385864258\n",
      "Validation: Epoch [0], Batch [436/938], Loss: 2.045551300048828\n",
      "Validation: Epoch [0], Batch [437/938], Loss: 2.0901577472686768\n",
      "Validation: Epoch [0], Batch [438/938], Loss: 2.0225412845611572\n",
      "Validation: Epoch [0], Batch [439/938], Loss: 2.1005210876464844\n",
      "Validation: Epoch [0], Batch [440/938], Loss: 2.0538830757141113\n",
      "Validation: Epoch [0], Batch [441/938], Loss: 2.0395405292510986\n",
      "Validation: Epoch [0], Batch [442/938], Loss: 2.101994514465332\n",
      "Validation: Epoch [0], Batch [443/938], Loss: 2.1062614917755127\n",
      "Validation: Epoch [0], Batch [444/938], Loss: 2.0579559803009033\n",
      "Validation: Epoch [0], Batch [445/938], Loss: 2.1003055572509766\n",
      "Validation: Epoch [0], Batch [446/938], Loss: 2.106691598892212\n",
      "Validation: Epoch [0], Batch [447/938], Loss: 2.050877094268799\n",
      "Validation: Epoch [0], Batch [448/938], Loss: 2.0724263191223145\n",
      "Validation: Epoch [0], Batch [449/938], Loss: 2.106098175048828\n",
      "Validation: Epoch [0], Batch [450/938], Loss: 2.0243606567382812\n",
      "Validation: Epoch [0], Batch [451/938], Loss: 2.05651593208313\n",
      "Validation: Epoch [0], Batch [452/938], Loss: 2.086949110031128\n",
      "Validation: Epoch [0], Batch [453/938], Loss: 2.083667755126953\n",
      "Validation: Epoch [0], Batch [454/938], Loss: 2.0693564414978027\n",
      "Validation: Epoch [0], Batch [455/938], Loss: 2.0436363220214844\n",
      "Validation: Epoch [0], Batch [456/938], Loss: 2.0529122352600098\n",
      "Validation: Epoch [0], Batch [457/938], Loss: 2.1059744358062744\n",
      "Validation: Epoch [0], Batch [458/938], Loss: 2.060877799987793\n",
      "Validation: Epoch [0], Batch [459/938], Loss: 2.03311824798584\n",
      "Validation: Epoch [0], Batch [460/938], Loss: 2.0349318981170654\n",
      "Validation: Epoch [0], Batch [461/938], Loss: 2.0759315490722656\n",
      "Validation: Epoch [0], Batch [462/938], Loss: 2.097836494445801\n",
      "Validation: Epoch [0], Batch [463/938], Loss: 2.0511975288391113\n",
      "Validation: Epoch [0], Batch [464/938], Loss: 2.044978141784668\n",
      "Validation: Epoch [0], Batch [465/938], Loss: 2.0374717712402344\n",
      "Validation: Epoch [0], Batch [466/938], Loss: 2.0695173740386963\n",
      "Validation: Epoch [0], Batch [467/938], Loss: 2.080242395401001\n",
      "Validation: Epoch [0], Batch [468/938], Loss: 2.0638623237609863\n",
      "Validation: Epoch [0], Batch [469/938], Loss: 2.0951757431030273\n",
      "Validation: Epoch [0], Batch [470/938], Loss: 2.08595609664917\n",
      "Validation: Epoch [0], Batch [471/938], Loss: 2.0862603187561035\n",
      "Validation: Epoch [0], Batch [472/938], Loss: 2.035871982574463\n",
      "Validation: Epoch [0], Batch [473/938], Loss: 2.0692923069000244\n",
      "Validation: Epoch [0], Batch [474/938], Loss: 2.0065975189208984\n",
      "Validation: Epoch [0], Batch [475/938], Loss: 2.0744152069091797\n",
      "Validation: Epoch [0], Batch [476/938], Loss: 2.0777153968811035\n",
      "Validation: Epoch [0], Batch [477/938], Loss: 2.0704493522644043\n",
      "Validation: Epoch [0], Batch [478/938], Loss: 2.0269365310668945\n",
      "Validation: Epoch [0], Batch [479/938], Loss: 2.052194118499756\n",
      "Validation: Epoch [0], Batch [480/938], Loss: 2.0358424186706543\n",
      "Validation: Epoch [0], Batch [481/938], Loss: 2.040212631225586\n",
      "Validation: Epoch [0], Batch [482/938], Loss: 2.065541982650757\n",
      "Validation: Epoch [0], Batch [483/938], Loss: 2.034731864929199\n",
      "Validation: Epoch [0], Batch [484/938], Loss: 2.0353341102600098\n",
      "Validation: Epoch [0], Batch [485/938], Loss: 2.091597080230713\n",
      "Validation: Epoch [0], Batch [486/938], Loss: 2.055274248123169\n",
      "Validation: Epoch [0], Batch [487/938], Loss: 2.0473337173461914\n",
      "Validation: Epoch [0], Batch [488/938], Loss: 2.0565738677978516\n",
      "Validation: Epoch [0], Batch [489/938], Loss: 2.073153495788574\n",
      "Validation: Epoch [0], Batch [490/938], Loss: 2.064936399459839\n",
      "Validation: Epoch [0], Batch [491/938], Loss: 2.0996079444885254\n",
      "Validation: Epoch [0], Batch [492/938], Loss: 2.070016384124756\n",
      "Validation: Epoch [0], Batch [493/938], Loss: 2.0632967948913574\n",
      "Validation: Epoch [0], Batch [494/938], Loss: 2.0707054138183594\n",
      "Validation: Epoch [0], Batch [495/938], Loss: 2.0584778785705566\n",
      "Validation: Epoch [0], Batch [496/938], Loss: 2.056547164916992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [497/938], Loss: 2.0532946586608887\n",
      "Validation: Epoch [0], Batch [498/938], Loss: 2.0489501953125\n",
      "Validation: Epoch [0], Batch [499/938], Loss: 2.117974042892456\n",
      "Validation: Epoch [0], Batch [500/938], Loss: 2.06141996383667\n",
      "Validation: Epoch [0], Batch [501/938], Loss: 2.0576224327087402\n",
      "Validation: Epoch [0], Batch [502/938], Loss: 2.064080238342285\n",
      "Validation: Epoch [0], Batch [503/938], Loss: 2.0708305835723877\n",
      "Validation: Epoch [0], Batch [504/938], Loss: 2.069176197052002\n",
      "Validation: Epoch [0], Batch [505/938], Loss: 2.058565616607666\n",
      "Validation: Epoch [0], Batch [506/938], Loss: 2.0897059440612793\n",
      "Validation: Epoch [0], Batch [507/938], Loss: 2.0634632110595703\n",
      "Validation: Epoch [0], Batch [508/938], Loss: 2.0430166721343994\n",
      "Validation: Epoch [0], Batch [509/938], Loss: 2.078646183013916\n",
      "Validation: Epoch [0], Batch [510/938], Loss: 2.0895376205444336\n",
      "Validation: Epoch [0], Batch [511/938], Loss: 2.036449909210205\n",
      "Validation: Epoch [0], Batch [512/938], Loss: 2.082348346710205\n",
      "Validation: Epoch [0], Batch [513/938], Loss: 2.064757823944092\n",
      "Validation: Epoch [0], Batch [514/938], Loss: 2.0574698448181152\n",
      "Validation: Epoch [0], Batch [515/938], Loss: 2.0634007453918457\n",
      "Validation: Epoch [0], Batch [516/938], Loss: 2.068347692489624\n",
      "Validation: Epoch [0], Batch [517/938], Loss: 2.053709030151367\n",
      "Validation: Epoch [0], Batch [518/938], Loss: 2.071465492248535\n",
      "Validation: Epoch [0], Batch [519/938], Loss: 2.098001480102539\n",
      "Validation: Epoch [0], Batch [520/938], Loss: 2.060061454772949\n",
      "Validation: Epoch [0], Batch [521/938], Loss: 2.076045274734497\n",
      "Validation: Epoch [0], Batch [522/938], Loss: 2.0516304969787598\n",
      "Validation: Epoch [0], Batch [523/938], Loss: 2.0455284118652344\n",
      "Validation: Epoch [0], Batch [524/938], Loss: 2.0579771995544434\n",
      "Validation: Epoch [0], Batch [525/938], Loss: 2.067440986633301\n",
      "Validation: Epoch [0], Batch [526/938], Loss: 2.084463357925415\n",
      "Validation: Epoch [0], Batch [527/938], Loss: 2.0776991844177246\n",
      "Validation: Epoch [0], Batch [528/938], Loss: 2.0569796562194824\n",
      "Validation: Epoch [0], Batch [529/938], Loss: 2.046990394592285\n",
      "Validation: Epoch [0], Batch [530/938], Loss: 2.0632617473602295\n",
      "Validation: Epoch [0], Batch [531/938], Loss: 2.063431739807129\n",
      "Validation: Epoch [0], Batch [532/938], Loss: 2.062408924102783\n",
      "Validation: Epoch [0], Batch [533/938], Loss: 2.0938165187835693\n",
      "Validation: Epoch [0], Batch [534/938], Loss: 2.059269428253174\n",
      "Validation: Epoch [0], Batch [535/938], Loss: 2.074852466583252\n",
      "Validation: Epoch [0], Batch [536/938], Loss: 2.0450642108917236\n",
      "Validation: Epoch [0], Batch [537/938], Loss: 2.0539629459381104\n",
      "Validation: Epoch [0], Batch [538/938], Loss: 2.0509283542633057\n",
      "Validation: Epoch [0], Batch [539/938], Loss: 2.030569076538086\n",
      "Validation: Epoch [0], Batch [540/938], Loss: 2.0443625450134277\n",
      "Validation: Epoch [0], Batch [541/938], Loss: 2.036710500717163\n",
      "Validation: Epoch [0], Batch [542/938], Loss: 2.0773324966430664\n",
      "Validation: Epoch [0], Batch [543/938], Loss: 2.079103708267212\n",
      "Validation: Epoch [0], Batch [544/938], Loss: 2.030738592147827\n",
      "Validation: Epoch [0], Batch [545/938], Loss: 2.0846633911132812\n",
      "Validation: Epoch [0], Batch [546/938], Loss: 2.0658366680145264\n",
      "Validation: Epoch [0], Batch [547/938], Loss: 2.0617623329162598\n",
      "Validation: Epoch [0], Batch [548/938], Loss: 2.067699670791626\n",
      "Validation: Epoch [0], Batch [549/938], Loss: 2.097654342651367\n",
      "Validation: Epoch [0], Batch [550/938], Loss: 2.0603342056274414\n",
      "Validation: Epoch [0], Batch [551/938], Loss: 2.0561742782592773\n",
      "Validation: Epoch [0], Batch [552/938], Loss: 2.0732386112213135\n",
      "Validation: Epoch [0], Batch [553/938], Loss: 2.0752058029174805\n",
      "Validation: Epoch [0], Batch [554/938], Loss: 2.0714495182037354\n",
      "Validation: Epoch [0], Batch [555/938], Loss: 2.0740561485290527\n",
      "Validation: Epoch [0], Batch [556/938], Loss: 2.0621583461761475\n",
      "Validation: Epoch [0], Batch [557/938], Loss: 2.0437629222869873\n",
      "Validation: Epoch [0], Batch [558/938], Loss: 2.041353225708008\n",
      "Validation: Epoch [0], Batch [559/938], Loss: 2.0460500717163086\n",
      "Validation: Epoch [0], Batch [560/938], Loss: 2.0565154552459717\n",
      "Validation: Epoch [0], Batch [561/938], Loss: 2.038174629211426\n",
      "Validation: Epoch [0], Batch [562/938], Loss: 2.058600902557373\n",
      "Validation: Epoch [0], Batch [563/938], Loss: 2.0617215633392334\n",
      "Validation: Epoch [0], Batch [564/938], Loss: 2.083193302154541\n",
      "Validation: Epoch [0], Batch [565/938], Loss: 2.0623974800109863\n",
      "Validation: Epoch [0], Batch [566/938], Loss: 2.0736207962036133\n",
      "Validation: Epoch [0], Batch [567/938], Loss: 2.0403757095336914\n",
      "Validation: Epoch [0], Batch [568/938], Loss: 2.0175914764404297\n",
      "Validation: Epoch [0], Batch [569/938], Loss: 2.042757511138916\n",
      "Validation: Epoch [0], Batch [570/938], Loss: 2.0624215602874756\n",
      "Validation: Epoch [0], Batch [571/938], Loss: 2.055995464324951\n",
      "Validation: Epoch [0], Batch [572/938], Loss: 2.092129707336426\n",
      "Validation: Epoch [0], Batch [573/938], Loss: 2.063594341278076\n",
      "Validation: Epoch [0], Batch [574/938], Loss: 2.0753402709960938\n",
      "Validation: Epoch [0], Batch [575/938], Loss: 2.085465431213379\n",
      "Validation: Epoch [0], Batch [576/938], Loss: 2.0385823249816895\n",
      "Validation: Epoch [0], Batch [577/938], Loss: 2.052523612976074\n",
      "Validation: Epoch [0], Batch [578/938], Loss: 2.0452709197998047\n",
      "Validation: Epoch [0], Batch [579/938], Loss: 2.0381662845611572\n",
      "Validation: Epoch [0], Batch [580/938], Loss: 2.08355712890625\n",
      "Validation: Epoch [0], Batch [581/938], Loss: 2.070783853530884\n",
      "Validation: Epoch [0], Batch [582/938], Loss: 2.0686378479003906\n",
      "Validation: Epoch [0], Batch [583/938], Loss: 2.100579261779785\n",
      "Validation: Epoch [0], Batch [584/938], Loss: 2.0689258575439453\n",
      "Validation: Epoch [0], Batch [585/938], Loss: 2.038421630859375\n",
      "Validation: Epoch [0], Batch [586/938], Loss: 2.0722086429595947\n",
      "Validation: Epoch [0], Batch [587/938], Loss: 2.038562774658203\n",
      "Validation: Epoch [0], Batch [588/938], Loss: 2.0593671798706055\n",
      "Validation: Epoch [0], Batch [589/938], Loss: 2.1397576332092285\n",
      "Validation: Epoch [0], Batch [590/938], Loss: 2.060396194458008\n",
      "Validation: Epoch [0], Batch [591/938], Loss: 2.067352056503296\n",
      "Validation: Epoch [0], Batch [592/938], Loss: 2.068638801574707\n",
      "Validation: Epoch [0], Batch [593/938], Loss: 2.063131809234619\n",
      "Validation: Epoch [0], Batch [594/938], Loss: 2.0959343910217285\n",
      "Validation: Epoch [0], Batch [595/938], Loss: 2.084989070892334\n",
      "Validation: Epoch [0], Batch [596/938], Loss: 2.0632221698760986\n",
      "Validation: Epoch [0], Batch [597/938], Loss: 2.0854599475860596\n",
      "Validation: Epoch [0], Batch [598/938], Loss: 2.02500319480896\n",
      "Validation: Epoch [0], Batch [599/938], Loss: 2.094557762145996\n",
      "Validation: Epoch [0], Batch [600/938], Loss: 2.0306878089904785\n",
      "Validation: Epoch [0], Batch [601/938], Loss: 2.0698537826538086\n",
      "Validation: Epoch [0], Batch [602/938], Loss: 2.0596156120300293\n",
      "Validation: Epoch [0], Batch [603/938], Loss: 2.0620057582855225\n",
      "Validation: Epoch [0], Batch [604/938], Loss: 2.0357370376586914\n",
      "Validation: Epoch [0], Batch [605/938], Loss: 2.0314788818359375\n",
      "Validation: Epoch [0], Batch [606/938], Loss: 2.0845513343811035\n",
      "Validation: Epoch [0], Batch [607/938], Loss: 2.0460994243621826\n",
      "Validation: Epoch [0], Batch [608/938], Loss: 2.04231595993042\n",
      "Validation: Epoch [0], Batch [609/938], Loss: 2.0497286319732666\n",
      "Validation: Epoch [0], Batch [610/938], Loss: 2.065830707550049\n",
      "Validation: Epoch [0], Batch [611/938], Loss: 2.0537822246551514\n",
      "Validation: Epoch [0], Batch [612/938], Loss: 2.0432772636413574\n",
      "Validation: Epoch [0], Batch [613/938], Loss: 2.0854806900024414\n",
      "Validation: Epoch [0], Batch [614/938], Loss: 2.0660953521728516\n",
      "Validation: Epoch [0], Batch [615/938], Loss: 2.0607333183288574\n",
      "Validation: Epoch [0], Batch [616/938], Loss: 2.029844284057617\n",
      "Validation: Epoch [0], Batch [617/938], Loss: 2.0635018348693848\n",
      "Validation: Epoch [0], Batch [618/938], Loss: 2.12553071975708\n",
      "Validation: Epoch [0], Batch [619/938], Loss: 2.0431067943573\n",
      "Validation: Epoch [0], Batch [620/938], Loss: 2.0297744274139404\n",
      "Validation: Epoch [0], Batch [621/938], Loss: 2.0779058933258057\n",
      "Validation: Epoch [0], Batch [622/938], Loss: 2.031611442565918\n",
      "Validation: Epoch [0], Batch [623/938], Loss: 2.0788488388061523\n",
      "Validation: Epoch [0], Batch [624/938], Loss: 2.0620133876800537\n",
      "Validation: Epoch [0], Batch [625/938], Loss: 2.0771894454956055\n",
      "Validation: Epoch [0], Batch [626/938], Loss: 2.070115089416504\n",
      "Validation: Epoch [0], Batch [627/938], Loss: 2.092745304107666\n",
      "Validation: Epoch [0], Batch [628/938], Loss: 2.091277837753296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [629/938], Loss: 2.0681357383728027\n",
      "Validation: Epoch [0], Batch [630/938], Loss: 2.043241024017334\n",
      "Validation: Epoch [0], Batch [631/938], Loss: 2.091376543045044\n",
      "Validation: Epoch [0], Batch [632/938], Loss: 2.0491812229156494\n",
      "Validation: Epoch [0], Batch [633/938], Loss: 2.088071823120117\n",
      "Validation: Epoch [0], Batch [634/938], Loss: 2.0609583854675293\n",
      "Validation: Epoch [0], Batch [635/938], Loss: 2.049799919128418\n",
      "Validation: Epoch [0], Batch [636/938], Loss: 2.038525104522705\n",
      "Validation: Epoch [0], Batch [637/938], Loss: 2.060857057571411\n",
      "Validation: Epoch [0], Batch [638/938], Loss: 2.1059353351593018\n",
      "Validation: Epoch [0], Batch [639/938], Loss: 2.079702138900757\n",
      "Validation: Epoch [0], Batch [640/938], Loss: 2.0324926376342773\n",
      "Validation: Epoch [0], Batch [641/938], Loss: 2.060333251953125\n",
      "Validation: Epoch [0], Batch [642/938], Loss: 2.046891927719116\n",
      "Validation: Epoch [0], Batch [643/938], Loss: 2.0278944969177246\n",
      "Validation: Epoch [0], Batch [644/938], Loss: 2.092207193374634\n",
      "Validation: Epoch [0], Batch [645/938], Loss: 2.0770788192749023\n",
      "Validation: Epoch [0], Batch [646/938], Loss: 2.0421273708343506\n",
      "Validation: Epoch [0], Batch [647/938], Loss: 2.044877529144287\n",
      "Validation: Epoch [0], Batch [648/938], Loss: 2.050236225128174\n",
      "Validation: Epoch [0], Batch [649/938], Loss: 2.056607723236084\n",
      "Validation: Epoch [0], Batch [650/938], Loss: 2.078808069229126\n",
      "Validation: Epoch [0], Batch [651/938], Loss: 2.0666093826293945\n",
      "Validation: Epoch [0], Batch [652/938], Loss: 2.041545867919922\n",
      "Validation: Epoch [0], Batch [653/938], Loss: 2.0624897480010986\n",
      "Validation: Epoch [0], Batch [654/938], Loss: 2.029644012451172\n",
      "Validation: Epoch [0], Batch [655/938], Loss: 2.066932439804077\n",
      "Validation: Epoch [0], Batch [656/938], Loss: 2.079085350036621\n",
      "Validation: Epoch [0], Batch [657/938], Loss: 2.0612988471984863\n",
      "Validation: Epoch [0], Batch [658/938], Loss: 2.0461266040802\n",
      "Validation: Epoch [0], Batch [659/938], Loss: 2.0622830390930176\n",
      "Validation: Epoch [0], Batch [660/938], Loss: 2.0659923553466797\n",
      "Validation: Epoch [0], Batch [661/938], Loss: 2.0903396606445312\n",
      "Validation: Epoch [0], Batch [662/938], Loss: 2.076810359954834\n",
      "Validation: Epoch [0], Batch [663/938], Loss: 2.0905203819274902\n",
      "Validation: Epoch [0], Batch [664/938], Loss: 2.0897207260131836\n",
      "Validation: Epoch [0], Batch [665/938], Loss: 2.0492210388183594\n",
      "Validation: Epoch [0], Batch [666/938], Loss: 2.031249761581421\n",
      "Validation: Epoch [0], Batch [667/938], Loss: 2.055755138397217\n",
      "Validation: Epoch [0], Batch [668/938], Loss: 2.074132204055786\n",
      "Validation: Epoch [0], Batch [669/938], Loss: 2.0702710151672363\n",
      "Validation: Epoch [0], Batch [670/938], Loss: 2.068223237991333\n",
      "Validation: Epoch [0], Batch [671/938], Loss: 2.0991601943969727\n",
      "Validation: Epoch [0], Batch [672/938], Loss: 2.067880630493164\n",
      "Validation: Epoch [0], Batch [673/938], Loss: 2.0561769008636475\n",
      "Validation: Epoch [0], Batch [674/938], Loss: 2.097968578338623\n",
      "Validation: Epoch [0], Batch [675/938], Loss: 2.0637943744659424\n",
      "Validation: Epoch [0], Batch [676/938], Loss: 2.0600123405456543\n",
      "Validation: Epoch [0], Batch [677/938], Loss: 2.0404250621795654\n",
      "Validation: Epoch [0], Batch [678/938], Loss: 2.0847043991088867\n",
      "Validation: Epoch [0], Batch [679/938], Loss: 2.052689790725708\n",
      "Validation: Epoch [0], Batch [680/938], Loss: 2.0951924324035645\n",
      "Validation: Epoch [0], Batch [681/938], Loss: 2.058197498321533\n",
      "Validation: Epoch [0], Batch [682/938], Loss: 2.050278902053833\n",
      "Validation: Epoch [0], Batch [683/938], Loss: 2.0382308959960938\n",
      "Validation: Epoch [0], Batch [684/938], Loss: 2.10477352142334\n",
      "Validation: Epoch [0], Batch [685/938], Loss: 2.0203607082366943\n",
      "Validation: Epoch [0], Batch [686/938], Loss: 2.0337982177734375\n",
      "Validation: Epoch [0], Batch [687/938], Loss: 2.0284488201141357\n",
      "Validation: Epoch [0], Batch [688/938], Loss: 2.04522705078125\n",
      "Validation: Epoch [0], Batch [689/938], Loss: 2.0818803310394287\n",
      "Validation: Epoch [0], Batch [690/938], Loss: 2.0402109622955322\n",
      "Validation: Epoch [0], Batch [691/938], Loss: 2.0402474403381348\n",
      "Validation: Epoch [0], Batch [692/938], Loss: 2.062323570251465\n",
      "Validation: Epoch [0], Batch [693/938], Loss: 2.0238401889801025\n",
      "Validation: Epoch [0], Batch [694/938], Loss: 2.062535524368286\n",
      "Validation: Epoch [0], Batch [695/938], Loss: 2.070713520050049\n",
      "Validation: Epoch [0], Batch [696/938], Loss: 2.0562925338745117\n",
      "Validation: Epoch [0], Batch [697/938], Loss: 2.055809497833252\n",
      "Validation: Epoch [0], Batch [698/938], Loss: 2.0487923622131348\n",
      "Validation: Epoch [0], Batch [699/938], Loss: 2.093636989593506\n",
      "Validation: Epoch [0], Batch [700/938], Loss: 2.096736431121826\n",
      "Validation: Epoch [0], Batch [701/938], Loss: 2.079627752304077\n",
      "Validation: Epoch [0], Batch [702/938], Loss: 2.0803840160369873\n",
      "Validation: Epoch [0], Batch [703/938], Loss: 2.0561776161193848\n",
      "Validation: Epoch [0], Batch [704/938], Loss: 2.0606586933135986\n",
      "Validation: Epoch [0], Batch [705/938], Loss: 2.083676815032959\n",
      "Validation: Epoch [0], Batch [706/938], Loss: 2.048335075378418\n",
      "Validation: Epoch [0], Batch [707/938], Loss: 2.049866199493408\n",
      "Validation: Epoch [0], Batch [708/938], Loss: 2.037759780883789\n",
      "Validation: Epoch [0], Batch [709/938], Loss: 2.061575174331665\n",
      "Validation: Epoch [0], Batch [710/938], Loss: 2.0633175373077393\n",
      "Validation: Epoch [0], Batch [711/938], Loss: 2.0678350925445557\n",
      "Validation: Epoch [0], Batch [712/938], Loss: 2.030670642852783\n",
      "Validation: Epoch [0], Batch [713/938], Loss: 2.035238027572632\n",
      "Validation: Epoch [0], Batch [714/938], Loss: 2.0286784172058105\n",
      "Validation: Epoch [0], Batch [715/938], Loss: 2.0708250999450684\n",
      "Validation: Epoch [0], Batch [716/938], Loss: 2.0405495166778564\n",
      "Validation: Epoch [0], Batch [717/938], Loss: 2.0545783042907715\n",
      "Validation: Epoch [0], Batch [718/938], Loss: 2.070181369781494\n",
      "Validation: Epoch [0], Batch [719/938], Loss: 2.083240509033203\n",
      "Validation: Epoch [0], Batch [720/938], Loss: 2.0780129432678223\n",
      "Validation: Epoch [0], Batch [721/938], Loss: 2.0408639907836914\n",
      "Validation: Epoch [0], Batch [722/938], Loss: 2.0463430881500244\n",
      "Validation: Epoch [0], Batch [723/938], Loss: 2.0516252517700195\n",
      "Validation: Epoch [0], Batch [724/938], Loss: 2.029402256011963\n",
      "Validation: Epoch [0], Batch [725/938], Loss: 2.102977752685547\n",
      "Validation: Epoch [0], Batch [726/938], Loss: 2.036653757095337\n",
      "Validation: Epoch [0], Batch [727/938], Loss: 2.053861141204834\n",
      "Validation: Epoch [0], Batch [728/938], Loss: 2.0876801013946533\n",
      "Validation: Epoch [0], Batch [729/938], Loss: 2.040853977203369\n",
      "Validation: Epoch [0], Batch [730/938], Loss: 2.0898046493530273\n",
      "Validation: Epoch [0], Batch [731/938], Loss: 2.0818777084350586\n",
      "Validation: Epoch [0], Batch [732/938], Loss: 2.027676820755005\n",
      "Validation: Epoch [0], Batch [733/938], Loss: 2.0684561729431152\n",
      "Validation: Epoch [0], Batch [734/938], Loss: 2.037647247314453\n",
      "Validation: Epoch [0], Batch [735/938], Loss: 2.046661615371704\n",
      "Validation: Epoch [0], Batch [736/938], Loss: 2.068087100982666\n",
      "Validation: Epoch [0], Batch [737/938], Loss: 2.075885534286499\n",
      "Validation: Epoch [0], Batch [738/938], Loss: 2.0622997283935547\n",
      "Validation: Epoch [0], Batch [739/938], Loss: 2.067493200302124\n",
      "Validation: Epoch [0], Batch [740/938], Loss: 2.081224203109741\n",
      "Validation: Epoch [0], Batch [741/938], Loss: 2.0902976989746094\n",
      "Validation: Epoch [0], Batch [742/938], Loss: 2.050175905227661\n",
      "Validation: Epoch [0], Batch [743/938], Loss: 2.0458552837371826\n",
      "Validation: Epoch [0], Batch [744/938], Loss: 2.061232566833496\n",
      "Validation: Epoch [0], Batch [745/938], Loss: 2.049466609954834\n",
      "Validation: Epoch [0], Batch [746/938], Loss: 2.064560651779175\n",
      "Validation: Epoch [0], Batch [747/938], Loss: 2.0853710174560547\n",
      "Validation: Epoch [0], Batch [748/938], Loss: 2.078805923461914\n",
      "Validation: Epoch [0], Batch [749/938], Loss: 2.075920581817627\n",
      "Validation: Epoch [0], Batch [750/938], Loss: 2.0649232864379883\n",
      "Validation: Epoch [0], Batch [751/938], Loss: 2.029247522354126\n",
      "Validation: Epoch [0], Batch [752/938], Loss: 2.0456976890563965\n",
      "Validation: Epoch [0], Batch [753/938], Loss: 2.060990810394287\n",
      "Validation: Epoch [0], Batch [754/938], Loss: 2.0519371032714844\n",
      "Validation: Epoch [0], Batch [755/938], Loss: 2.0385823249816895\n",
      "Validation: Epoch [0], Batch [756/938], Loss: 2.0710954666137695\n",
      "Validation: Epoch [0], Batch [757/938], Loss: 2.0960817337036133\n",
      "Validation: Epoch [0], Batch [758/938], Loss: 2.070878505706787\n",
      "Validation: Epoch [0], Batch [759/938], Loss: 2.0398941040039062\n",
      "Validation: Epoch [0], Batch [760/938], Loss: 2.0395820140838623\n",
      "Validation: Epoch [0], Batch [761/938], Loss: 2.0716135501861572\n",
      "Validation: Epoch [0], Batch [762/938], Loss: 2.0744400024414062\n",
      "Validation: Epoch [0], Batch [763/938], Loss: 2.0577025413513184\n",
      "Validation: Epoch [0], Batch [764/938], Loss: 2.060755491256714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [765/938], Loss: 2.053352117538452\n",
      "Validation: Epoch [0], Batch [766/938], Loss: 2.0793588161468506\n",
      "Validation: Epoch [0], Batch [767/938], Loss: 2.0650134086608887\n",
      "Validation: Epoch [0], Batch [768/938], Loss: 2.0851898193359375\n",
      "Validation: Epoch [0], Batch [769/938], Loss: 2.058795690536499\n",
      "Validation: Epoch [0], Batch [770/938], Loss: 2.0704777240753174\n",
      "Validation: Epoch [0], Batch [771/938], Loss: 2.047133684158325\n",
      "Validation: Epoch [0], Batch [772/938], Loss: 2.076364040374756\n",
      "Validation: Epoch [0], Batch [773/938], Loss: 2.0385499000549316\n",
      "Validation: Epoch [0], Batch [774/938], Loss: 2.03320574760437\n",
      "Validation: Epoch [0], Batch [775/938], Loss: 2.089970111846924\n",
      "Validation: Epoch [0], Batch [776/938], Loss: 2.033262252807617\n",
      "Validation: Epoch [0], Batch [777/938], Loss: 2.0696651935577393\n",
      "Validation: Epoch [0], Batch [778/938], Loss: 2.0644335746765137\n",
      "Validation: Epoch [0], Batch [779/938], Loss: 2.058501958847046\n",
      "Validation: Epoch [0], Batch [780/938], Loss: 2.0823092460632324\n",
      "Validation: Epoch [0], Batch [781/938], Loss: 2.0296003818511963\n",
      "Validation: Epoch [0], Batch [782/938], Loss: 2.0687801837921143\n",
      "Validation: Epoch [0], Batch [783/938], Loss: 2.0561771392822266\n",
      "Validation: Epoch [0], Batch [784/938], Loss: 2.0377931594848633\n",
      "Validation: Epoch [0], Batch [785/938], Loss: 2.0617194175720215\n",
      "Validation: Epoch [0], Batch [786/938], Loss: 2.1142382621765137\n",
      "Validation: Epoch [0], Batch [787/938], Loss: 2.0814731121063232\n",
      "Validation: Epoch [0], Batch [788/938], Loss: 2.058600902557373\n",
      "Validation: Epoch [0], Batch [789/938], Loss: 2.0843560695648193\n",
      "Validation: Epoch [0], Batch [790/938], Loss: 2.047614097595215\n",
      "Validation: Epoch [0], Batch [791/938], Loss: 2.0763401985168457\n",
      "Validation: Epoch [0], Batch [792/938], Loss: 2.0476512908935547\n",
      "Validation: Epoch [0], Batch [793/938], Loss: 2.0604896545410156\n",
      "Validation: Epoch [0], Batch [794/938], Loss: 2.0455422401428223\n",
      "Validation: Epoch [0], Batch [795/938], Loss: 2.0388007164001465\n",
      "Validation: Epoch [0], Batch [796/938], Loss: 2.056161642074585\n",
      "Validation: Epoch [0], Batch [797/938], Loss: 2.066895008087158\n",
      "Validation: Epoch [0], Batch [798/938], Loss: 2.043919801712036\n",
      "Validation: Epoch [0], Batch [799/938], Loss: 2.043473243713379\n",
      "Validation: Epoch [0], Batch [800/938], Loss: 2.057579517364502\n",
      "Validation: Epoch [0], Batch [801/938], Loss: 2.0429773330688477\n",
      "Validation: Epoch [0], Batch [802/938], Loss: 2.0827109813690186\n",
      "Validation: Epoch [0], Batch [803/938], Loss: 2.0960426330566406\n",
      "Validation: Epoch [0], Batch [804/938], Loss: 2.072869062423706\n",
      "Validation: Epoch [0], Batch [805/938], Loss: 2.070054531097412\n",
      "Validation: Epoch [0], Batch [806/938], Loss: 2.0770740509033203\n",
      "Validation: Epoch [0], Batch [807/938], Loss: 2.047889471054077\n",
      "Validation: Epoch [0], Batch [808/938], Loss: 2.057262897491455\n",
      "Validation: Epoch [0], Batch [809/938], Loss: 2.0728468894958496\n",
      "Validation: Epoch [0], Batch [810/938], Loss: 2.0751075744628906\n",
      "Validation: Epoch [0], Batch [811/938], Loss: 2.0731472969055176\n",
      "Validation: Epoch [0], Batch [812/938], Loss: 2.03823184967041\n",
      "Validation: Epoch [0], Batch [813/938], Loss: 2.079681396484375\n",
      "Validation: Epoch [0], Batch [814/938], Loss: 2.0492329597473145\n",
      "Validation: Epoch [0], Batch [815/938], Loss: 2.0456924438476562\n",
      "Validation: Epoch [0], Batch [816/938], Loss: 2.089355945587158\n",
      "Validation: Epoch [0], Batch [817/938], Loss: 2.0832324028015137\n",
      "Validation: Epoch [0], Batch [818/938], Loss: 2.062074899673462\n",
      "Validation: Epoch [0], Batch [819/938], Loss: 2.077486515045166\n",
      "Validation: Epoch [0], Batch [820/938], Loss: 2.0638532638549805\n",
      "Validation: Epoch [0], Batch [821/938], Loss: 2.0698142051696777\n",
      "Validation: Epoch [0], Batch [822/938], Loss: 2.0523667335510254\n",
      "Validation: Epoch [0], Batch [823/938], Loss: 2.0744307041168213\n",
      "Validation: Epoch [0], Batch [824/938], Loss: 2.0599114894866943\n",
      "Validation: Epoch [0], Batch [825/938], Loss: 2.087965488433838\n",
      "Validation: Epoch [0], Batch [826/938], Loss: 2.072432518005371\n",
      "Validation: Epoch [0], Batch [827/938], Loss: 2.047116756439209\n",
      "Validation: Epoch [0], Batch [828/938], Loss: 2.043294906616211\n",
      "Validation: Epoch [0], Batch [829/938], Loss: 2.0384340286254883\n",
      "Validation: Epoch [0], Batch [830/938], Loss: 2.0556979179382324\n",
      "Validation: Epoch [0], Batch [831/938], Loss: 2.0630834102630615\n",
      "Validation: Epoch [0], Batch [832/938], Loss: 2.066688299179077\n",
      "Validation: Epoch [0], Batch [833/938], Loss: 2.047776222229004\n",
      "Validation: Epoch [0], Batch [834/938], Loss: 2.0656068325042725\n",
      "Validation: Epoch [0], Batch [835/938], Loss: 2.0342531204223633\n",
      "Validation: Epoch [0], Batch [836/938], Loss: 2.0336856842041016\n",
      "Validation: Epoch [0], Batch [837/938], Loss: 2.033787727355957\n",
      "Validation: Epoch [0], Batch [838/938], Loss: 2.091810703277588\n",
      "Validation: Epoch [0], Batch [839/938], Loss: 2.0555520057678223\n",
      "Validation: Epoch [0], Batch [840/938], Loss: 2.050602436065674\n",
      "Validation: Epoch [0], Batch [841/938], Loss: 2.0715160369873047\n",
      "Validation: Epoch [0], Batch [842/938], Loss: 2.0645461082458496\n",
      "Validation: Epoch [0], Batch [843/938], Loss: 2.04205322265625\n",
      "Validation: Epoch [0], Batch [844/938], Loss: 2.067969799041748\n",
      "Validation: Epoch [0], Batch [845/938], Loss: 2.052473545074463\n",
      "Validation: Epoch [0], Batch [846/938], Loss: 2.04327392578125\n",
      "Validation: Epoch [0], Batch [847/938], Loss: 2.046060800552368\n",
      "Validation: Epoch [0], Batch [848/938], Loss: 2.038825511932373\n",
      "Validation: Epoch [0], Batch [849/938], Loss: 2.0688953399658203\n",
      "Validation: Epoch [0], Batch [850/938], Loss: 2.0900676250457764\n",
      "Validation: Epoch [0], Batch [851/938], Loss: 2.0493104457855225\n",
      "Validation: Epoch [0], Batch [852/938], Loss: 2.072445869445801\n",
      "Validation: Epoch [0], Batch [853/938], Loss: 2.084656000137329\n",
      "Validation: Epoch [0], Batch [854/938], Loss: 2.070451021194458\n",
      "Validation: Epoch [0], Batch [855/938], Loss: 2.0104942321777344\n",
      "Validation: Epoch [0], Batch [856/938], Loss: 2.058727741241455\n",
      "Validation: Epoch [0], Batch [857/938], Loss: 2.1037731170654297\n",
      "Validation: Epoch [0], Batch [858/938], Loss: 2.025153160095215\n",
      "Validation: Epoch [0], Batch [859/938], Loss: 2.0637922286987305\n",
      "Validation: Epoch [0], Batch [860/938], Loss: 2.028611421585083\n",
      "Validation: Epoch [0], Batch [861/938], Loss: 2.072359085083008\n",
      "Validation: Epoch [0], Batch [862/938], Loss: 2.062602996826172\n",
      "Validation: Epoch [0], Batch [863/938], Loss: 2.1160495281219482\n",
      "Validation: Epoch [0], Batch [864/938], Loss: 2.0910637378692627\n",
      "Validation: Epoch [0], Batch [865/938], Loss: 2.0494916439056396\n",
      "Validation: Epoch [0], Batch [866/938], Loss: 2.0453543663024902\n",
      "Validation: Epoch [0], Batch [867/938], Loss: 2.0607962608337402\n",
      "Validation: Epoch [0], Batch [868/938], Loss: 2.0922646522521973\n",
      "Validation: Epoch [0], Batch [869/938], Loss: 2.0400638580322266\n",
      "Validation: Epoch [0], Batch [870/938], Loss: 2.0507378578186035\n",
      "Validation: Epoch [0], Batch [871/938], Loss: 2.090082883834839\n",
      "Validation: Epoch [0], Batch [872/938], Loss: 2.0189132690429688\n",
      "Validation: Epoch [0], Batch [873/938], Loss: 2.0422520637512207\n",
      "Validation: Epoch [0], Batch [874/938], Loss: 2.066518545150757\n",
      "Validation: Epoch [0], Batch [875/938], Loss: 2.0753414630889893\n",
      "Validation: Epoch [0], Batch [876/938], Loss: 2.0754902362823486\n",
      "Validation: Epoch [0], Batch [877/938], Loss: 2.060811758041382\n",
      "Validation: Epoch [0], Batch [878/938], Loss: 2.094729423522949\n",
      "Validation: Epoch [0], Batch [879/938], Loss: 2.0846714973449707\n",
      "Validation: Epoch [0], Batch [880/938], Loss: 2.0626163482666016\n",
      "Validation: Epoch [0], Batch [881/938], Loss: 2.114872455596924\n",
      "Validation: Epoch [0], Batch [882/938], Loss: 2.046699285507202\n",
      "Validation: Epoch [0], Batch [883/938], Loss: 2.0471739768981934\n",
      "Validation: Epoch [0], Batch [884/938], Loss: 2.052171230316162\n",
      "Validation: Epoch [0], Batch [885/938], Loss: 2.0466771125793457\n",
      "Validation: Epoch [0], Batch [886/938], Loss: 2.037022829055786\n",
      "Validation: Epoch [0], Batch [887/938], Loss: 2.0927982330322266\n",
      "Validation: Epoch [0], Batch [888/938], Loss: 2.0700507164001465\n",
      "Validation: Epoch [0], Batch [889/938], Loss: 2.0452046394348145\n",
      "Validation: Epoch [0], Batch [890/938], Loss: 2.058173656463623\n",
      "Validation: Epoch [0], Batch [891/938], Loss: 2.099848985671997\n",
      "Validation: Epoch [0], Batch [892/938], Loss: 2.0510873794555664\n",
      "Validation: Epoch [0], Batch [893/938], Loss: 2.069610595703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [0], Batch [894/938], Loss: 2.0373950004577637\n",
      "Validation: Epoch [0], Batch [895/938], Loss: 2.0373995304107666\n",
      "Validation: Epoch [0], Batch [896/938], Loss: 2.0840349197387695\n",
      "Validation: Epoch [0], Batch [897/938], Loss: 2.0755412578582764\n",
      "Validation: Epoch [0], Batch [898/938], Loss: 2.0878138542175293\n",
      "Validation: Epoch [0], Batch [899/938], Loss: 2.0835256576538086\n",
      "Validation: Epoch [0], Batch [900/938], Loss: 2.072981834411621\n",
      "Validation: Epoch [0], Batch [901/938], Loss: 2.0709733963012695\n",
      "Validation: Epoch [0], Batch [902/938], Loss: 2.046447515487671\n",
      "Validation: Epoch [0], Batch [903/938], Loss: 2.090273380279541\n",
      "Validation: Epoch [0], Batch [904/938], Loss: 2.0759265422821045\n",
      "Validation: Epoch [0], Batch [905/938], Loss: 2.039395809173584\n",
      "Validation: Epoch [0], Batch [906/938], Loss: 2.0985333919525146\n",
      "Validation: Epoch [0], Batch [907/938], Loss: 2.0647952556610107\n",
      "Validation: Epoch [0], Batch [908/938], Loss: 2.0541491508483887\n",
      "Validation: Epoch [0], Batch [909/938], Loss: 2.056875705718994\n",
      "Validation: Epoch [0], Batch [910/938], Loss: 2.065784454345703\n",
      "Validation: Epoch [0], Batch [911/938], Loss: 2.0415587425231934\n",
      "Validation: Epoch [0], Batch [912/938], Loss: 2.0536632537841797\n",
      "Validation: Epoch [0], Batch [913/938], Loss: 2.043562889099121\n",
      "Validation: Epoch [0], Batch [914/938], Loss: 2.0911173820495605\n",
      "Validation: Epoch [0], Batch [915/938], Loss: 2.0429258346557617\n",
      "Validation: Epoch [0], Batch [916/938], Loss: 2.050633668899536\n",
      "Validation: Epoch [0], Batch [917/938], Loss: 2.0818588733673096\n",
      "Validation: Epoch [0], Batch [918/938], Loss: 2.0971243381500244\n",
      "Validation: Epoch [0], Batch [919/938], Loss: 2.0748353004455566\n",
      "Validation: Epoch [0], Batch [920/938], Loss: 2.0723459720611572\n",
      "Validation: Epoch [0], Batch [921/938], Loss: 2.0955684185028076\n",
      "Validation: Epoch [0], Batch [922/938], Loss: 2.038290500640869\n",
      "Validation: Epoch [0], Batch [923/938], Loss: 2.0651276111602783\n",
      "Validation: Epoch [0], Batch [924/938], Loss: 2.0493125915527344\n",
      "Validation: Epoch [0], Batch [925/938], Loss: 2.0664072036743164\n",
      "Validation: Epoch [0], Batch [926/938], Loss: 2.051241159439087\n",
      "Validation: Epoch [0], Batch [927/938], Loss: 2.0650506019592285\n",
      "Validation: Epoch [0], Batch [928/938], Loss: 2.0554914474487305\n",
      "Validation: Epoch [0], Batch [929/938], Loss: 2.0788350105285645\n",
      "Validation: Epoch [0], Batch [930/938], Loss: 2.0385565757751465\n",
      "Validation: Epoch [0], Batch [931/938], Loss: 2.039524555206299\n",
      "Validation: Epoch [0], Batch [932/938], Loss: 2.0342655181884766\n",
      "Validation: Epoch [0], Batch [933/938], Loss: 2.043215274810791\n",
      "Validation: Epoch [0], Batch [934/938], Loss: 2.0863685607910156\n",
      "Validation: Epoch [0], Batch [935/938], Loss: 2.044123649597168\n",
      "Validation: Epoch [0], Batch [936/938], Loss: 2.0947461128234863\n",
      "Validation: Epoch [0], Batch [937/938], Loss: 2.0877952575683594\n",
      "Validation: Epoch [0], Batch [938/938], Loss: 2.0652596950531006\n",
      "Accuracy of test set: 0.2838833333333333\n",
      "Train: Epoch [1], Batch [1/938], Loss: 2.0756797790527344\n",
      "Train: Epoch [1], Batch [2/938], Loss: 2.047711133956909\n",
      "Train: Epoch [1], Batch [3/938], Loss: 2.0697622299194336\n",
      "Train: Epoch [1], Batch [4/938], Loss: 2.055863857269287\n",
      "Train: Epoch [1], Batch [5/938], Loss: 2.0672519207000732\n",
      "Train: Epoch [1], Batch [6/938], Loss: 2.041059970855713\n",
      "Train: Epoch [1], Batch [7/938], Loss: 2.0424113273620605\n",
      "Train: Epoch [1], Batch [8/938], Loss: 2.0450665950775146\n",
      "Train: Epoch [1], Batch [9/938], Loss: 2.038926124572754\n",
      "Train: Epoch [1], Batch [10/938], Loss: 2.063873767852783\n",
      "Train: Epoch [1], Batch [11/938], Loss: 2.055995464324951\n",
      "Train: Epoch [1], Batch [12/938], Loss: 2.0346264839172363\n",
      "Train: Epoch [1], Batch [13/938], Loss: 2.0717780590057373\n",
      "Train: Epoch [1], Batch [14/938], Loss: 2.0215811729431152\n",
      "Train: Epoch [1], Batch [15/938], Loss: 2.0315732955932617\n",
      "Train: Epoch [1], Batch [16/938], Loss: 2.076955795288086\n",
      "Train: Epoch [1], Batch [17/938], Loss: 2.063516139984131\n",
      "Train: Epoch [1], Batch [18/938], Loss: 2.050846815109253\n",
      "Train: Epoch [1], Batch [19/938], Loss: 2.02164363861084\n",
      "Train: Epoch [1], Batch [20/938], Loss: 2.053488254547119\n",
      "Train: Epoch [1], Batch [21/938], Loss: 2.058332920074463\n",
      "Train: Epoch [1], Batch [22/938], Loss: 2.0734713077545166\n",
      "Train: Epoch [1], Batch [23/938], Loss: 2.086482286453247\n",
      "Train: Epoch [1], Batch [24/938], Loss: 2.028423309326172\n",
      "Train: Epoch [1], Batch [25/938], Loss: 2.0268921852111816\n",
      "Train: Epoch [1], Batch [26/938], Loss: 2.0388665199279785\n",
      "Train: Epoch [1], Batch [27/938], Loss: 2.017597198486328\n",
      "Train: Epoch [1], Batch [28/938], Loss: 2.035806655883789\n",
      "Train: Epoch [1], Batch [29/938], Loss: 2.048184394836426\n",
      "Train: Epoch [1], Batch [30/938], Loss: 2.0243380069732666\n",
      "Train: Epoch [1], Batch [31/938], Loss: 2.043395757675171\n",
      "Train: Epoch [1], Batch [32/938], Loss: 2.0038180351257324\n",
      "Train: Epoch [1], Batch [33/938], Loss: 2.0270676612854004\n",
      "Train: Epoch [1], Batch [34/938], Loss: 2.0348830223083496\n",
      "Train: Epoch [1], Batch [35/938], Loss: 2.0427134037017822\n",
      "Train: Epoch [1], Batch [36/938], Loss: 2.0431714057922363\n",
      "Train: Epoch [1], Batch [37/938], Loss: 2.046088218688965\n",
      "Train: Epoch [1], Batch [38/938], Loss: 1.9896515607833862\n",
      "Train: Epoch [1], Batch [39/938], Loss: 2.0046260356903076\n",
      "Train: Epoch [1], Batch [40/938], Loss: 2.0103375911712646\n",
      "Train: Epoch [1], Batch [41/938], Loss: 2.022775888442993\n",
      "Train: Epoch [1], Batch [42/938], Loss: 2.0178191661834717\n",
      "Train: Epoch [1], Batch [43/938], Loss: 2.0042247772216797\n",
      "Train: Epoch [1], Batch [44/938], Loss: 1.9635229110717773\n",
      "Train: Epoch [1], Batch [45/938], Loss: 2.014697313308716\n",
      "Train: Epoch [1], Batch [46/938], Loss: 2.0610134601593018\n",
      "Train: Epoch [1], Batch [47/938], Loss: 2.0237233638763428\n",
      "Train: Epoch [1], Batch [48/938], Loss: 2.0418944358825684\n",
      "Train: Epoch [1], Batch [49/938], Loss: 2.045924186706543\n",
      "Train: Epoch [1], Batch [50/938], Loss: 1.9995346069335938\n",
      "Train: Epoch [1], Batch [51/938], Loss: 2.026878833770752\n",
      "Train: Epoch [1], Batch [52/938], Loss: 2.043771505355835\n",
      "Train: Epoch [1], Batch [53/938], Loss: 2.01145339012146\n",
      "Train: Epoch [1], Batch [54/938], Loss: 1.9862726926803589\n",
      "Train: Epoch [1], Batch [55/938], Loss: 2.0144591331481934\n",
      "Train: Epoch [1], Batch [56/938], Loss: 2.011061429977417\n",
      "Train: Epoch [1], Batch [57/938], Loss: 2.003852605819702\n",
      "Train: Epoch [1], Batch [58/938], Loss: 2.04782772064209\n",
      "Train: Epoch [1], Batch [59/938], Loss: 1.957969069480896\n",
      "Train: Epoch [1], Batch [60/938], Loss: 1.9827170372009277\n",
      "Train: Epoch [1], Batch [61/938], Loss: 1.9608869552612305\n",
      "Train: Epoch [1], Batch [62/938], Loss: 1.9882187843322754\n",
      "Train: Epoch [1], Batch [63/938], Loss: 1.9948675632476807\n",
      "Train: Epoch [1], Batch [64/938], Loss: 2.0156984329223633\n",
      "Train: Epoch [1], Batch [65/938], Loss: 2.0367555618286133\n",
      "Train: Epoch [1], Batch [66/938], Loss: 2.0161561965942383\n",
      "Train: Epoch [1], Batch [67/938], Loss: 1.9438631534576416\n",
      "Train: Epoch [1], Batch [68/938], Loss: 1.9458940029144287\n",
      "Train: Epoch [1], Batch [69/938], Loss: 1.9230332374572754\n",
      "Train: Epoch [1], Batch [70/938], Loss: 1.9862873554229736\n",
      "Train: Epoch [1], Batch [71/938], Loss: 1.9288804531097412\n",
      "Train: Epoch [1], Batch [72/938], Loss: 1.9575164318084717\n",
      "Train: Epoch [1], Batch [73/938], Loss: 2.0071680545806885\n",
      "Train: Epoch [1], Batch [74/938], Loss: 1.9464116096496582\n",
      "Train: Epoch [1], Batch [75/938], Loss: 1.9723222255706787\n",
      "Train: Epoch [1], Batch [76/938], Loss: 1.9680891036987305\n",
      "Train: Epoch [1], Batch [77/938], Loss: 1.9720948934555054\n",
      "Train: Epoch [1], Batch [78/938], Loss: 1.9411473274230957\n",
      "Train: Epoch [1], Batch [79/938], Loss: 2.0075035095214844\n",
      "Train: Epoch [1], Batch [80/938], Loss: 2.015312910079956\n",
      "Train: Epoch [1], Batch [81/938], Loss: 1.9683367013931274\n",
      "Train: Epoch [1], Batch [82/938], Loss: 1.9709467887878418\n",
      "Train: Epoch [1], Batch [83/938], Loss: 1.9457333087921143\n",
      "Train: Epoch [1], Batch [84/938], Loss: 1.9868162870407104\n",
      "Train: Epoch [1], Batch [85/938], Loss: 1.9977205991744995\n",
      "Train: Epoch [1], Batch [86/938], Loss: 2.0206503868103027\n",
      "Train: Epoch [1], Batch [87/938], Loss: 1.9560627937316895\n",
      "Train: Epoch [1], Batch [88/938], Loss: 2.0339901447296143\n",
      "Train: Epoch [1], Batch [89/938], Loss: 1.9116997718811035\n",
      "Train: Epoch [1], Batch [90/938], Loss: 1.9523990154266357\n",
      "Train: Epoch [1], Batch [91/938], Loss: 1.9757440090179443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [92/938], Loss: 1.986998438835144\n",
      "Train: Epoch [1], Batch [93/938], Loss: 1.944669246673584\n",
      "Train: Epoch [1], Batch [94/938], Loss: 1.9336297512054443\n",
      "Train: Epoch [1], Batch [95/938], Loss: 2.000274181365967\n",
      "Train: Epoch [1], Batch [96/938], Loss: 1.987224817276001\n",
      "Train: Epoch [1], Batch [97/938], Loss: 1.916752815246582\n",
      "Train: Epoch [1], Batch [98/938], Loss: 1.9969425201416016\n",
      "Train: Epoch [1], Batch [99/938], Loss: 1.906682014465332\n",
      "Train: Epoch [1], Batch [100/938], Loss: 1.940049409866333\n",
      "Train: Epoch [1], Batch [101/938], Loss: 1.9647111892700195\n",
      "Train: Epoch [1], Batch [102/938], Loss: 1.9463605880737305\n",
      "Train: Epoch [1], Batch [103/938], Loss: 1.9942365884780884\n",
      "Train: Epoch [1], Batch [104/938], Loss: 1.9608248472213745\n",
      "Train: Epoch [1], Batch [105/938], Loss: 2.0105485916137695\n",
      "Train: Epoch [1], Batch [106/938], Loss: 1.9921138286590576\n",
      "Train: Epoch [1], Batch [107/938], Loss: 1.9156198501586914\n",
      "Train: Epoch [1], Batch [108/938], Loss: 1.9139986038208008\n",
      "Train: Epoch [1], Batch [109/938], Loss: 1.9095417261123657\n",
      "Train: Epoch [1], Batch [110/938], Loss: 1.928619384765625\n",
      "Train: Epoch [1], Batch [111/938], Loss: 1.9911696910858154\n",
      "Train: Epoch [1], Batch [112/938], Loss: 1.949267864227295\n",
      "Train: Epoch [1], Batch [113/938], Loss: 1.9600703716278076\n",
      "Train: Epoch [1], Batch [114/938], Loss: 1.9484326839447021\n",
      "Train: Epoch [1], Batch [115/938], Loss: 1.9608571529388428\n",
      "Train: Epoch [1], Batch [116/938], Loss: 1.938590168952942\n",
      "Train: Epoch [1], Batch [117/938], Loss: 1.922505497932434\n",
      "Train: Epoch [1], Batch [118/938], Loss: 1.9105563163757324\n",
      "Train: Epoch [1], Batch [119/938], Loss: 1.9462311267852783\n",
      "Train: Epoch [1], Batch [120/938], Loss: 1.8826861381530762\n",
      "Train: Epoch [1], Batch [121/938], Loss: 1.934186339378357\n",
      "Train: Epoch [1], Batch [122/938], Loss: 1.96946382522583\n",
      "Train: Epoch [1], Batch [123/938], Loss: 1.9485809803009033\n",
      "Train: Epoch [1], Batch [124/938], Loss: 1.8880717754364014\n",
      "Train: Epoch [1], Batch [125/938], Loss: 1.9525322914123535\n",
      "Train: Epoch [1], Batch [126/938], Loss: 1.8987460136413574\n",
      "Train: Epoch [1], Batch [127/938], Loss: 1.8877112865447998\n",
      "Train: Epoch [1], Batch [128/938], Loss: 1.9797229766845703\n",
      "Train: Epoch [1], Batch [129/938], Loss: 1.9293454885482788\n",
      "Train: Epoch [1], Batch [130/938], Loss: 1.9317609071731567\n",
      "Train: Epoch [1], Batch [131/938], Loss: 1.8787872791290283\n",
      "Train: Epoch [1], Batch [132/938], Loss: 1.9018257856369019\n",
      "Train: Epoch [1], Batch [133/938], Loss: 1.9563450813293457\n",
      "Train: Epoch [1], Batch [134/938], Loss: 1.8863017559051514\n",
      "Train: Epoch [1], Batch [135/938], Loss: 1.8906251192092896\n",
      "Train: Epoch [1], Batch [136/938], Loss: 1.9600119590759277\n",
      "Train: Epoch [1], Batch [137/938], Loss: 1.9711368083953857\n",
      "Train: Epoch [1], Batch [138/938], Loss: 1.9341692924499512\n",
      "Train: Epoch [1], Batch [139/938], Loss: 1.9006812572479248\n",
      "Train: Epoch [1], Batch [140/938], Loss: 1.9810513257980347\n",
      "Train: Epoch [1], Batch [141/938], Loss: 1.985929012298584\n",
      "Train: Epoch [1], Batch [142/938], Loss: 1.8557038307189941\n",
      "Train: Epoch [1], Batch [143/938], Loss: 1.9774049520492554\n",
      "Train: Epoch [1], Batch [144/938], Loss: 1.9025566577911377\n",
      "Train: Epoch [1], Batch [145/938], Loss: 1.9006210565567017\n",
      "Train: Epoch [1], Batch [146/938], Loss: 1.8662165403366089\n",
      "Train: Epoch [1], Batch [147/938], Loss: 1.9265007972717285\n",
      "Train: Epoch [1], Batch [148/938], Loss: 1.9705770015716553\n",
      "Train: Epoch [1], Batch [149/938], Loss: 1.8194794654846191\n",
      "Train: Epoch [1], Batch [150/938], Loss: 1.920556902885437\n",
      "Train: Epoch [1], Batch [151/938], Loss: 1.8601391315460205\n",
      "Train: Epoch [1], Batch [152/938], Loss: 1.9366282224655151\n",
      "Train: Epoch [1], Batch [153/938], Loss: 1.8894774913787842\n",
      "Train: Epoch [1], Batch [154/938], Loss: 1.9540188312530518\n",
      "Train: Epoch [1], Batch [155/938], Loss: 1.9298515319824219\n",
      "Train: Epoch [1], Batch [156/938], Loss: 1.9197949171066284\n",
      "Train: Epoch [1], Batch [157/938], Loss: 1.8674147129058838\n",
      "Train: Epoch [1], Batch [158/938], Loss: 1.8799445629119873\n",
      "Train: Epoch [1], Batch [159/938], Loss: 1.8438957929611206\n",
      "Train: Epoch [1], Batch [160/938], Loss: 1.886156678199768\n",
      "Train: Epoch [1], Batch [161/938], Loss: 1.8438243865966797\n",
      "Train: Epoch [1], Batch [162/938], Loss: 1.951721429824829\n",
      "Train: Epoch [1], Batch [163/938], Loss: 1.884362816810608\n",
      "Train: Epoch [1], Batch [164/938], Loss: 1.8703430891036987\n",
      "Train: Epoch [1], Batch [165/938], Loss: 1.8817609548568726\n",
      "Train: Epoch [1], Batch [166/938], Loss: 1.8823660612106323\n",
      "Train: Epoch [1], Batch [167/938], Loss: 1.8987817764282227\n",
      "Train: Epoch [1], Batch [168/938], Loss: 1.8686399459838867\n",
      "Train: Epoch [1], Batch [169/938], Loss: 1.879232406616211\n",
      "Train: Epoch [1], Batch [170/938], Loss: 1.852600336074829\n",
      "Train: Epoch [1], Batch [171/938], Loss: 1.8514344692230225\n",
      "Train: Epoch [1], Batch [172/938], Loss: 1.7919796705245972\n",
      "Train: Epoch [1], Batch [173/938], Loss: 1.8916099071502686\n",
      "Train: Epoch [1], Batch [174/938], Loss: 1.8594670295715332\n",
      "Train: Epoch [1], Batch [175/938], Loss: 1.8422608375549316\n",
      "Train: Epoch [1], Batch [176/938], Loss: 1.9333064556121826\n",
      "Train: Epoch [1], Batch [177/938], Loss: 1.8572217226028442\n",
      "Train: Epoch [1], Batch [178/938], Loss: 1.8295907974243164\n",
      "Train: Epoch [1], Batch [179/938], Loss: 1.857029914855957\n",
      "Train: Epoch [1], Batch [180/938], Loss: 1.8554952144622803\n",
      "Train: Epoch [1], Batch [181/938], Loss: 1.8559153079986572\n",
      "Train: Epoch [1], Batch [182/938], Loss: 1.8311021327972412\n",
      "Train: Epoch [1], Batch [183/938], Loss: 1.8628123998641968\n",
      "Train: Epoch [1], Batch [184/938], Loss: 1.8141734600067139\n",
      "Train: Epoch [1], Batch [185/938], Loss: 1.7842137813568115\n",
      "Train: Epoch [1], Batch [186/938], Loss: 1.8032206296920776\n",
      "Train: Epoch [1], Batch [187/938], Loss: 1.8744029998779297\n",
      "Train: Epoch [1], Batch [188/938], Loss: 1.8629987239837646\n",
      "Train: Epoch [1], Batch [189/938], Loss: 1.7926336526870728\n",
      "Train: Epoch [1], Batch [190/938], Loss: 1.8448460102081299\n",
      "Train: Epoch [1], Batch [191/938], Loss: 1.8568651676177979\n",
      "Train: Epoch [1], Batch [192/938], Loss: 1.8510147333145142\n",
      "Train: Epoch [1], Batch [193/938], Loss: 1.8112845420837402\n",
      "Train: Epoch [1], Batch [194/938], Loss: 1.881666898727417\n",
      "Train: Epoch [1], Batch [195/938], Loss: 1.8095858097076416\n",
      "Train: Epoch [1], Batch [196/938], Loss: 1.8287628889083862\n",
      "Train: Epoch [1], Batch [197/938], Loss: 1.7786535024642944\n",
      "Train: Epoch [1], Batch [198/938], Loss: 1.913577675819397\n",
      "Train: Epoch [1], Batch [199/938], Loss: 1.8131400346755981\n",
      "Train: Epoch [1], Batch [200/938], Loss: 1.8631972074508667\n",
      "Train: Epoch [1], Batch [201/938], Loss: 1.8328583240509033\n",
      "Train: Epoch [1], Batch [202/938], Loss: 1.785729169845581\n",
      "Train: Epoch [1], Batch [203/938], Loss: 1.8929622173309326\n",
      "Train: Epoch [1], Batch [204/938], Loss: 1.8316540718078613\n",
      "Train: Epoch [1], Batch [205/938], Loss: 1.8392010927200317\n",
      "Train: Epoch [1], Batch [206/938], Loss: 1.7730603218078613\n",
      "Train: Epoch [1], Batch [207/938], Loss: 1.792092204093933\n",
      "Train: Epoch [1], Batch [208/938], Loss: 1.7162914276123047\n",
      "Train: Epoch [1], Batch [209/938], Loss: 1.7469717264175415\n",
      "Train: Epoch [1], Batch [210/938], Loss: 1.7956366539001465\n",
      "Train: Epoch [1], Batch [211/938], Loss: 1.732764482498169\n",
      "Train: Epoch [1], Batch [212/938], Loss: 1.8422552347183228\n",
      "Train: Epoch [1], Batch [213/938], Loss: 1.774385690689087\n",
      "Train: Epoch [1], Batch [214/938], Loss: 1.7755653858184814\n",
      "Train: Epoch [1], Batch [215/938], Loss: 1.7358288764953613\n",
      "Train: Epoch [1], Batch [216/938], Loss: 1.7362059354782104\n",
      "Train: Epoch [1], Batch [217/938], Loss: 1.8351576328277588\n",
      "Train: Epoch [1], Batch [218/938], Loss: 1.7811355590820312\n",
      "Train: Epoch [1], Batch [219/938], Loss: 1.787480354309082\n",
      "Train: Epoch [1], Batch [220/938], Loss: 1.8254085779190063\n",
      "Train: Epoch [1], Batch [221/938], Loss: 1.804189920425415\n",
      "Train: Epoch [1], Batch [222/938], Loss: 1.8223187923431396\n",
      "Train: Epoch [1], Batch [223/938], Loss: 1.803464412689209\n",
      "Train: Epoch [1], Batch [224/938], Loss: 1.7743732929229736\n",
      "Train: Epoch [1], Batch [225/938], Loss: 1.849457859992981\n",
      "Train: Epoch [1], Batch [226/938], Loss: 1.7498266696929932\n",
      "Train: Epoch [1], Batch [227/938], Loss: 1.8115131855010986\n",
      "Train: Epoch [1], Batch [228/938], Loss: 1.8011155128479004\n",
      "Train: Epoch [1], Batch [229/938], Loss: 1.7072951793670654\n",
      "Train: Epoch [1], Batch [230/938], Loss: 1.8631858825683594\n",
      "Train: Epoch [1], Batch [231/938], Loss: 1.7665164470672607\n",
      "Train: Epoch [1], Batch [232/938], Loss: 1.7689764499664307\n",
      "Train: Epoch [1], Batch [233/938], Loss: 1.7379896640777588\n",
      "Train: Epoch [1], Batch [234/938], Loss: 1.7588850259780884\n",
      "Train: Epoch [1], Batch [235/938], Loss: 1.795890212059021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [236/938], Loss: 1.7238194942474365\n",
      "Train: Epoch [1], Batch [237/938], Loss: 1.7760229110717773\n",
      "Train: Epoch [1], Batch [238/938], Loss: 1.7511132955551147\n",
      "Train: Epoch [1], Batch [239/938], Loss: 1.6992709636688232\n",
      "Train: Epoch [1], Batch [240/938], Loss: 1.739888072013855\n",
      "Train: Epoch [1], Batch [241/938], Loss: 1.7622840404510498\n",
      "Train: Epoch [1], Batch [242/938], Loss: 1.799797534942627\n",
      "Train: Epoch [1], Batch [243/938], Loss: 1.7528640031814575\n",
      "Train: Epoch [1], Batch [244/938], Loss: 1.8271349668502808\n",
      "Train: Epoch [1], Batch [245/938], Loss: 1.7647361755371094\n",
      "Train: Epoch [1], Batch [246/938], Loss: 1.7278380393981934\n",
      "Train: Epoch [1], Batch [247/938], Loss: 1.7267292737960815\n",
      "Train: Epoch [1], Batch [248/938], Loss: 1.7763866186141968\n",
      "Train: Epoch [1], Batch [249/938], Loss: 1.7376534938812256\n",
      "Train: Epoch [1], Batch [250/938], Loss: 1.753680944442749\n",
      "Train: Epoch [1], Batch [251/938], Loss: 1.812898874282837\n",
      "Train: Epoch [1], Batch [252/938], Loss: 1.8316079378128052\n",
      "Train: Epoch [1], Batch [253/938], Loss: 1.7800230979919434\n",
      "Train: Epoch [1], Batch [254/938], Loss: 1.723899006843567\n",
      "Train: Epoch [1], Batch [255/938], Loss: 1.776667594909668\n",
      "Train: Epoch [1], Batch [256/938], Loss: 1.7201340198516846\n",
      "Train: Epoch [1], Batch [257/938], Loss: 1.7722423076629639\n",
      "Train: Epoch [1], Batch [258/938], Loss: 1.7969900369644165\n",
      "Train: Epoch [1], Batch [259/938], Loss: 1.7219250202178955\n",
      "Train: Epoch [1], Batch [260/938], Loss: 1.6765152215957642\n",
      "Train: Epoch [1], Batch [261/938], Loss: 1.7329484224319458\n",
      "Train: Epoch [1], Batch [262/938], Loss: 1.731967568397522\n",
      "Train: Epoch [1], Batch [263/938], Loss: 1.6770110130310059\n",
      "Train: Epoch [1], Batch [264/938], Loss: 1.7327120304107666\n",
      "Train: Epoch [1], Batch [265/938], Loss: 1.727061152458191\n",
      "Train: Epoch [1], Batch [266/938], Loss: 1.678525447845459\n",
      "Train: Epoch [1], Batch [267/938], Loss: 1.725609540939331\n",
      "Train: Epoch [1], Batch [268/938], Loss: 1.7673399448394775\n",
      "Train: Epoch [1], Batch [269/938], Loss: 1.6546919345855713\n",
      "Train: Epoch [1], Batch [270/938], Loss: 1.6906545162200928\n",
      "Train: Epoch [1], Batch [271/938], Loss: 1.7224925756454468\n",
      "Train: Epoch [1], Batch [272/938], Loss: 1.7334568500518799\n",
      "Train: Epoch [1], Batch [273/938], Loss: 1.7433018684387207\n",
      "Train: Epoch [1], Batch [274/938], Loss: 1.6664237976074219\n",
      "Train: Epoch [1], Batch [275/938], Loss: 1.7562588453292847\n",
      "Train: Epoch [1], Batch [276/938], Loss: 1.7290501594543457\n",
      "Train: Epoch [1], Batch [277/938], Loss: 1.7384101152420044\n",
      "Train: Epoch [1], Batch [278/938], Loss: 1.7550718784332275\n",
      "Train: Epoch [1], Batch [279/938], Loss: 1.7743436098098755\n",
      "Train: Epoch [1], Batch [280/938], Loss: 1.65028977394104\n",
      "Train: Epoch [1], Batch [281/938], Loss: 1.7073934078216553\n",
      "Train: Epoch [1], Batch [282/938], Loss: 1.7246875762939453\n",
      "Train: Epoch [1], Batch [283/938], Loss: 1.7370766401290894\n",
      "Train: Epoch [1], Batch [284/938], Loss: 1.7188528776168823\n",
      "Train: Epoch [1], Batch [285/938], Loss: 1.7034459114074707\n",
      "Train: Epoch [1], Batch [286/938], Loss: 1.6477866172790527\n",
      "Train: Epoch [1], Batch [287/938], Loss: 1.6676872968673706\n",
      "Train: Epoch [1], Batch [288/938], Loss: 1.6995669603347778\n",
      "Train: Epoch [1], Batch [289/938], Loss: 1.689037561416626\n",
      "Train: Epoch [1], Batch [290/938], Loss: 1.719355821609497\n",
      "Train: Epoch [1], Batch [291/938], Loss: 1.6310105323791504\n",
      "Train: Epoch [1], Batch [292/938], Loss: 1.6406948566436768\n",
      "Train: Epoch [1], Batch [293/938], Loss: 1.7353229522705078\n",
      "Train: Epoch [1], Batch [294/938], Loss: 1.6736444234848022\n",
      "Train: Epoch [1], Batch [295/938], Loss: 1.6551588773727417\n",
      "Train: Epoch [1], Batch [296/938], Loss: 1.6476833820343018\n",
      "Train: Epoch [1], Batch [297/938], Loss: 1.7656476497650146\n",
      "Train: Epoch [1], Batch [298/938], Loss: 1.7082164287567139\n",
      "Train: Epoch [1], Batch [299/938], Loss: 1.79340660572052\n",
      "Train: Epoch [1], Batch [300/938], Loss: 1.7017452716827393\n",
      "Train: Epoch [1], Batch [301/938], Loss: 1.6866716146469116\n",
      "Train: Epoch [1], Batch [302/938], Loss: 1.6859650611877441\n",
      "Train: Epoch [1], Batch [303/938], Loss: 1.6872730255126953\n",
      "Train: Epoch [1], Batch [304/938], Loss: 1.7634859085083008\n",
      "Train: Epoch [1], Batch [305/938], Loss: 1.705758810043335\n",
      "Train: Epoch [1], Batch [306/938], Loss: 1.6416178941726685\n",
      "Train: Epoch [1], Batch [307/938], Loss: 1.6665213108062744\n",
      "Train: Epoch [1], Batch [308/938], Loss: 1.692077875137329\n",
      "Train: Epoch [1], Batch [309/938], Loss: 1.6461899280548096\n",
      "Train: Epoch [1], Batch [310/938], Loss: 1.56695556640625\n",
      "Train: Epoch [1], Batch [311/938], Loss: 1.5969998836517334\n",
      "Train: Epoch [1], Batch [312/938], Loss: 1.702106237411499\n",
      "Train: Epoch [1], Batch [313/938], Loss: 1.6619017124176025\n",
      "Train: Epoch [1], Batch [314/938], Loss: 1.7264353036880493\n",
      "Train: Epoch [1], Batch [315/938], Loss: 1.7038195133209229\n",
      "Train: Epoch [1], Batch [316/938], Loss: 1.667309045791626\n",
      "Train: Epoch [1], Batch [317/938], Loss: 1.7483150959014893\n",
      "Train: Epoch [1], Batch [318/938], Loss: 1.5813322067260742\n",
      "Train: Epoch [1], Batch [319/938], Loss: 1.7033888101577759\n",
      "Train: Epoch [1], Batch [320/938], Loss: 1.6459100246429443\n",
      "Train: Epoch [1], Batch [321/938], Loss: 1.666097640991211\n",
      "Train: Epoch [1], Batch [322/938], Loss: 1.6847516298294067\n",
      "Train: Epoch [1], Batch [323/938], Loss: 1.5849412679672241\n",
      "Train: Epoch [1], Batch [324/938], Loss: 1.6645036935806274\n",
      "Train: Epoch [1], Batch [325/938], Loss: 1.702762246131897\n",
      "Train: Epoch [1], Batch [326/938], Loss: 1.6355904340744019\n",
      "Train: Epoch [1], Batch [327/938], Loss: 1.6174473762512207\n",
      "Train: Epoch [1], Batch [328/938], Loss: 1.6641089916229248\n",
      "Train: Epoch [1], Batch [329/938], Loss: 1.627476453781128\n",
      "Train: Epoch [1], Batch [330/938], Loss: 1.6296091079711914\n",
      "Train: Epoch [1], Batch [331/938], Loss: 1.6404956579208374\n",
      "Train: Epoch [1], Batch [332/938], Loss: 1.6110633611679077\n",
      "Train: Epoch [1], Batch [333/938], Loss: 1.5555357933044434\n",
      "Train: Epoch [1], Batch [334/938], Loss: 1.639012336730957\n",
      "Train: Epoch [1], Batch [335/938], Loss: 1.5807543992996216\n",
      "Train: Epoch [1], Batch [336/938], Loss: 1.5532879829406738\n",
      "Train: Epoch [1], Batch [337/938], Loss: 1.629075050354004\n",
      "Train: Epoch [1], Batch [338/938], Loss: 1.6267975568771362\n",
      "Train: Epoch [1], Batch [339/938], Loss: 1.6501827239990234\n",
      "Train: Epoch [1], Batch [340/938], Loss: 1.6514685153961182\n",
      "Train: Epoch [1], Batch [341/938], Loss: 1.5234622955322266\n",
      "Train: Epoch [1], Batch [342/938], Loss: 1.6423221826553345\n",
      "Train: Epoch [1], Batch [343/938], Loss: 1.6459977626800537\n",
      "Train: Epoch [1], Batch [344/938], Loss: 1.6114604473114014\n",
      "Train: Epoch [1], Batch [345/938], Loss: 1.5925796031951904\n",
      "Train: Epoch [1], Batch [346/938], Loss: 1.6490018367767334\n",
      "Train: Epoch [1], Batch [347/938], Loss: 1.7241528034210205\n",
      "Train: Epoch [1], Batch [348/938], Loss: 1.6075084209442139\n",
      "Train: Epoch [1], Batch [349/938], Loss: 1.5683287382125854\n",
      "Train: Epoch [1], Batch [350/938], Loss: 1.6466422080993652\n",
      "Train: Epoch [1], Batch [351/938], Loss: 1.622380018234253\n",
      "Train: Epoch [1], Batch [352/938], Loss: 1.6421291828155518\n",
      "Train: Epoch [1], Batch [353/938], Loss: 1.6931052207946777\n",
      "Train: Epoch [1], Batch [354/938], Loss: 1.5164408683776855\n",
      "Train: Epoch [1], Batch [355/938], Loss: 1.5832862854003906\n",
      "Train: Epoch [1], Batch [356/938], Loss: 1.5985883474349976\n",
      "Train: Epoch [1], Batch [357/938], Loss: 1.663515329360962\n",
      "Train: Epoch [1], Batch [358/938], Loss: 1.5800358057022095\n",
      "Train: Epoch [1], Batch [359/938], Loss: 1.6050763130187988\n",
      "Train: Epoch [1], Batch [360/938], Loss: 1.7029215097427368\n",
      "Train: Epoch [1], Batch [361/938], Loss: 1.651798963546753\n",
      "Train: Epoch [1], Batch [362/938], Loss: 1.607680082321167\n",
      "Train: Epoch [1], Batch [363/938], Loss: 1.5629140138626099\n",
      "Train: Epoch [1], Batch [364/938], Loss: 1.581709861755371\n",
      "Train: Epoch [1], Batch [365/938], Loss: 1.5692991018295288\n",
      "Train: Epoch [1], Batch [366/938], Loss: 1.6364243030548096\n",
      "Train: Epoch [1], Batch [367/938], Loss: 1.561575174331665\n",
      "Train: Epoch [1], Batch [368/938], Loss: 1.595571517944336\n",
      "Train: Epoch [1], Batch [369/938], Loss: 1.6120517253875732\n",
      "Train: Epoch [1], Batch [370/938], Loss: 1.5970938205718994\n",
      "Train: Epoch [1], Batch [371/938], Loss: 1.6094202995300293\n",
      "Train: Epoch [1], Batch [372/938], Loss: 1.654909610748291\n",
      "Train: Epoch [1], Batch [373/938], Loss: 1.6386563777923584\n",
      "Train: Epoch [1], Batch [374/938], Loss: 1.55129075050354\n",
      "Train: Epoch [1], Batch [375/938], Loss: 1.6344302892684937\n",
      "Train: Epoch [1], Batch [376/938], Loss: 1.5671933889389038\n",
      "Train: Epoch [1], Batch [377/938], Loss: 1.5952696800231934\n",
      "Train: Epoch [1], Batch [378/938], Loss: 1.5909514427185059\n",
      "Train: Epoch [1], Batch [379/938], Loss: 1.5055274963378906\n",
      "Train: Epoch [1], Batch [380/938], Loss: 1.5410802364349365\n",
      "Train: Epoch [1], Batch [381/938], Loss: 1.5493853092193604\n",
      "Train: Epoch [1], Batch [382/938], Loss: 1.6030173301696777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [383/938], Loss: 1.5882937908172607\n",
      "Train: Epoch [1], Batch [384/938], Loss: 1.529463768005371\n",
      "Train: Epoch [1], Batch [385/938], Loss: 1.4868714809417725\n",
      "Train: Epoch [1], Batch [386/938], Loss: 1.638634204864502\n",
      "Train: Epoch [1], Batch [387/938], Loss: 1.6087861061096191\n",
      "Train: Epoch [1], Batch [388/938], Loss: 1.5636162757873535\n",
      "Train: Epoch [1], Batch [389/938], Loss: 1.514115571975708\n",
      "Train: Epoch [1], Batch [390/938], Loss: 1.5130932331085205\n",
      "Train: Epoch [1], Batch [391/938], Loss: 1.524794578552246\n",
      "Train: Epoch [1], Batch [392/938], Loss: 1.5967727899551392\n",
      "Train: Epoch [1], Batch [393/938], Loss: 1.5346863269805908\n",
      "Train: Epoch [1], Batch [394/938], Loss: 1.6085903644561768\n",
      "Train: Epoch [1], Batch [395/938], Loss: 1.5874085426330566\n",
      "Train: Epoch [1], Batch [396/938], Loss: 1.4760901927947998\n",
      "Train: Epoch [1], Batch [397/938], Loss: 1.4780101776123047\n",
      "Train: Epoch [1], Batch [398/938], Loss: 1.5833125114440918\n",
      "Train: Epoch [1], Batch [399/938], Loss: 1.5624704360961914\n",
      "Train: Epoch [1], Batch [400/938], Loss: 1.5608384609222412\n",
      "Train: Epoch [1], Batch [401/938], Loss: 1.474846363067627\n",
      "Train: Epoch [1], Batch [402/938], Loss: 1.5257935523986816\n",
      "Train: Epoch [1], Batch [403/938], Loss: 1.4786185026168823\n",
      "Train: Epoch [1], Batch [404/938], Loss: 1.5068129301071167\n",
      "Train: Epoch [1], Batch [405/938], Loss: 1.5049258470535278\n",
      "Train: Epoch [1], Batch [406/938], Loss: 1.5105082988739014\n",
      "Train: Epoch [1], Batch [407/938], Loss: 1.5942705869674683\n",
      "Train: Epoch [1], Batch [408/938], Loss: 1.4981907606124878\n",
      "Train: Epoch [1], Batch [409/938], Loss: 1.5982584953308105\n",
      "Train: Epoch [1], Batch [410/938], Loss: 1.4999934434890747\n",
      "Train: Epoch [1], Batch [411/938], Loss: 1.5468666553497314\n",
      "Train: Epoch [1], Batch [412/938], Loss: 1.547703742980957\n",
      "Train: Epoch [1], Batch [413/938], Loss: 1.5317813158035278\n",
      "Train: Epoch [1], Batch [414/938], Loss: 1.4651120901107788\n",
      "Train: Epoch [1], Batch [415/938], Loss: 1.5249624252319336\n",
      "Train: Epoch [1], Batch [416/938], Loss: 1.4278521537780762\n",
      "Train: Epoch [1], Batch [417/938], Loss: 1.5055510997772217\n",
      "Train: Epoch [1], Batch [418/938], Loss: 1.5103709697723389\n",
      "Train: Epoch [1], Batch [419/938], Loss: 1.5900723934173584\n",
      "Train: Epoch [1], Batch [420/938], Loss: 1.564880132675171\n",
      "Train: Epoch [1], Batch [421/938], Loss: 1.538069725036621\n",
      "Train: Epoch [1], Batch [422/938], Loss: 1.5147135257720947\n",
      "Train: Epoch [1], Batch [423/938], Loss: 1.5993620157241821\n",
      "Train: Epoch [1], Batch [424/938], Loss: 1.4561433792114258\n",
      "Train: Epoch [1], Batch [425/938], Loss: 1.4511210918426514\n",
      "Train: Epoch [1], Batch [426/938], Loss: 1.5125623941421509\n",
      "Train: Epoch [1], Batch [427/938], Loss: 1.4964947700500488\n",
      "Train: Epoch [1], Batch [428/938], Loss: 1.5642930269241333\n",
      "Train: Epoch [1], Batch [429/938], Loss: 1.5020101070404053\n",
      "Train: Epoch [1], Batch [430/938], Loss: 1.536696434020996\n",
      "Train: Epoch [1], Batch [431/938], Loss: 1.528746485710144\n",
      "Train: Epoch [1], Batch [432/938], Loss: 1.5425347089767456\n",
      "Train: Epoch [1], Batch [433/938], Loss: 1.4309494495391846\n",
      "Train: Epoch [1], Batch [434/938], Loss: 1.4974644184112549\n",
      "Train: Epoch [1], Batch [435/938], Loss: 1.4721815586090088\n",
      "Train: Epoch [1], Batch [436/938], Loss: 1.4913089275360107\n",
      "Train: Epoch [1], Batch [437/938], Loss: 1.582963228225708\n",
      "Train: Epoch [1], Batch [438/938], Loss: 1.5240604877471924\n",
      "Train: Epoch [1], Batch [439/938], Loss: 1.4093739986419678\n",
      "Train: Epoch [1], Batch [440/938], Loss: 1.512223482131958\n",
      "Train: Epoch [1], Batch [441/938], Loss: 1.4998012781143188\n",
      "Train: Epoch [1], Batch [442/938], Loss: 1.5137702226638794\n",
      "Train: Epoch [1], Batch [443/938], Loss: 1.5095486640930176\n",
      "Train: Epoch [1], Batch [444/938], Loss: 1.5178766250610352\n",
      "Train: Epoch [1], Batch [445/938], Loss: 1.4628264904022217\n",
      "Train: Epoch [1], Batch [446/938], Loss: 1.4928317070007324\n",
      "Train: Epoch [1], Batch [447/938], Loss: 1.5735933780670166\n",
      "Train: Epoch [1], Batch [448/938], Loss: 1.4817572832107544\n",
      "Train: Epoch [1], Batch [449/938], Loss: 1.496242880821228\n",
      "Train: Epoch [1], Batch [450/938], Loss: 1.5879580974578857\n",
      "Train: Epoch [1], Batch [451/938], Loss: 1.5635056495666504\n",
      "Train: Epoch [1], Batch [452/938], Loss: 1.5160586833953857\n",
      "Train: Epoch [1], Batch [453/938], Loss: 1.4710443019866943\n",
      "Train: Epoch [1], Batch [454/938], Loss: 1.5503689050674438\n",
      "Train: Epoch [1], Batch [455/938], Loss: 1.442552089691162\n",
      "Train: Epoch [1], Batch [456/938], Loss: 1.4943437576293945\n",
      "Train: Epoch [1], Batch [457/938], Loss: 1.5537137985229492\n",
      "Train: Epoch [1], Batch [458/938], Loss: 1.5473477840423584\n",
      "Train: Epoch [1], Batch [459/938], Loss: 1.5432156324386597\n",
      "Train: Epoch [1], Batch [460/938], Loss: 1.4799320697784424\n",
      "Train: Epoch [1], Batch [461/938], Loss: 1.4706072807312012\n",
      "Train: Epoch [1], Batch [462/938], Loss: 1.4213323593139648\n",
      "Train: Epoch [1], Batch [463/938], Loss: 1.4404038190841675\n",
      "Train: Epoch [1], Batch [464/938], Loss: 1.3963232040405273\n",
      "Train: Epoch [1], Batch [465/938], Loss: 1.5550436973571777\n",
      "Train: Epoch [1], Batch [466/938], Loss: 1.4819763898849487\n",
      "Train: Epoch [1], Batch [467/938], Loss: 1.5097107887268066\n",
      "Train: Epoch [1], Batch [468/938], Loss: 1.469433069229126\n",
      "Train: Epoch [1], Batch [469/938], Loss: 1.4284164905548096\n",
      "Train: Epoch [1], Batch [470/938], Loss: 1.4940388202667236\n",
      "Train: Epoch [1], Batch [471/938], Loss: 1.4640229940414429\n",
      "Train: Epoch [1], Batch [472/938], Loss: 1.5528863668441772\n",
      "Train: Epoch [1], Batch [473/938], Loss: 1.5007085800170898\n",
      "Train: Epoch [1], Batch [474/938], Loss: 1.4751077890396118\n",
      "Train: Epoch [1], Batch [475/938], Loss: 1.5371336936950684\n",
      "Train: Epoch [1], Batch [476/938], Loss: 1.5186253786087036\n",
      "Train: Epoch [1], Batch [477/938], Loss: 1.4915180206298828\n",
      "Train: Epoch [1], Batch [478/938], Loss: 1.4821953773498535\n",
      "Train: Epoch [1], Batch [479/938], Loss: 1.3418012857437134\n",
      "Train: Epoch [1], Batch [480/938], Loss: 1.4273796081542969\n",
      "Train: Epoch [1], Batch [481/938], Loss: 1.4453521966934204\n",
      "Train: Epoch [1], Batch [482/938], Loss: 1.6051251888275146\n",
      "Train: Epoch [1], Batch [483/938], Loss: 1.4119713306427002\n",
      "Train: Epoch [1], Batch [484/938], Loss: 1.3880136013031006\n",
      "Train: Epoch [1], Batch [485/938], Loss: 1.4482464790344238\n",
      "Train: Epoch [1], Batch [486/938], Loss: 1.4641194343566895\n",
      "Train: Epoch [1], Batch [487/938], Loss: 1.4474929571151733\n",
      "Train: Epoch [1], Batch [488/938], Loss: 1.4071459770202637\n",
      "Train: Epoch [1], Batch [489/938], Loss: 1.367336392402649\n",
      "Train: Epoch [1], Batch [490/938], Loss: 1.4654271602630615\n",
      "Train: Epoch [1], Batch [491/938], Loss: 1.346498966217041\n",
      "Train: Epoch [1], Batch [492/938], Loss: 1.5234088897705078\n",
      "Train: Epoch [1], Batch [493/938], Loss: 1.3735653162002563\n",
      "Train: Epoch [1], Batch [494/938], Loss: 1.4746067523956299\n",
      "Train: Epoch [1], Batch [495/938], Loss: 1.4276920557022095\n",
      "Train: Epoch [1], Batch [496/938], Loss: 1.4512561559677124\n",
      "Train: Epoch [1], Batch [497/938], Loss: 1.3613801002502441\n",
      "Train: Epoch [1], Batch [498/938], Loss: 1.3911570310592651\n",
      "Train: Epoch [1], Batch [499/938], Loss: 1.501338243484497\n",
      "Train: Epoch [1], Batch [500/938], Loss: 1.3502881526947021\n",
      "Train: Epoch [1], Batch [501/938], Loss: 1.4554309844970703\n",
      "Train: Epoch [1], Batch [502/938], Loss: 1.4562532901763916\n",
      "Train: Epoch [1], Batch [503/938], Loss: 1.5039184093475342\n",
      "Train: Epoch [1], Batch [504/938], Loss: 1.4962223768234253\n",
      "Train: Epoch [1], Batch [505/938], Loss: 1.3924096822738647\n",
      "Train: Epoch [1], Batch [506/938], Loss: 1.5819275379180908\n",
      "Train: Epoch [1], Batch [507/938], Loss: 1.346254587173462\n",
      "Train: Epoch [1], Batch [508/938], Loss: 1.425457239151001\n",
      "Train: Epoch [1], Batch [509/938], Loss: 1.4789059162139893\n",
      "Train: Epoch [1], Batch [510/938], Loss: 1.4551873207092285\n",
      "Train: Epoch [1], Batch [511/938], Loss: 1.3918590545654297\n",
      "Train: Epoch [1], Batch [512/938], Loss: 1.427746295928955\n",
      "Train: Epoch [1], Batch [513/938], Loss: 1.4471101760864258\n",
      "Train: Epoch [1], Batch [514/938], Loss: 1.5440123081207275\n",
      "Train: Epoch [1], Batch [515/938], Loss: 1.545137882232666\n",
      "Train: Epoch [1], Batch [516/938], Loss: 1.5081385374069214\n",
      "Train: Epoch [1], Batch [517/938], Loss: 1.4684748649597168\n",
      "Train: Epoch [1], Batch [518/938], Loss: 1.3697456121444702\n",
      "Train: Epoch [1], Batch [519/938], Loss: 1.431119680404663\n",
      "Train: Epoch [1], Batch [520/938], Loss: 1.4470579624176025\n",
      "Train: Epoch [1], Batch [521/938], Loss: 1.4524331092834473\n",
      "Train: Epoch [1], Batch [522/938], Loss: 1.3895478248596191\n",
      "Train: Epoch [1], Batch [523/938], Loss: 1.3683133125305176\n",
      "Train: Epoch [1], Batch [524/938], Loss: 1.387606143951416\n",
      "Train: Epoch [1], Batch [525/938], Loss: 1.582643985748291\n",
      "Train: Epoch [1], Batch [526/938], Loss: 1.3551404476165771\n",
      "Train: Epoch [1], Batch [527/938], Loss: 1.390998363494873\n",
      "Train: Epoch [1], Batch [528/938], Loss: 1.3260765075683594\n",
      "Train: Epoch [1], Batch [529/938], Loss: 1.384963035583496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [530/938], Loss: 1.3559715747833252\n",
      "Train: Epoch [1], Batch [531/938], Loss: 1.3720216751098633\n",
      "Train: Epoch [1], Batch [532/938], Loss: 1.29678213596344\n",
      "Train: Epoch [1], Batch [533/938], Loss: 1.407805323600769\n",
      "Train: Epoch [1], Batch [534/938], Loss: 1.430019736289978\n",
      "Train: Epoch [1], Batch [535/938], Loss: 1.345619559288025\n",
      "Train: Epoch [1], Batch [536/938], Loss: 1.406402349472046\n",
      "Train: Epoch [1], Batch [537/938], Loss: 1.4371662139892578\n",
      "Train: Epoch [1], Batch [538/938], Loss: 1.2968631982803345\n",
      "Train: Epoch [1], Batch [539/938], Loss: 1.373668909072876\n",
      "Train: Epoch [1], Batch [540/938], Loss: 1.4149682521820068\n",
      "Train: Epoch [1], Batch [541/938], Loss: 1.3677077293395996\n",
      "Train: Epoch [1], Batch [542/938], Loss: 1.3379414081573486\n",
      "Train: Epoch [1], Batch [543/938], Loss: 1.3888955116271973\n",
      "Train: Epoch [1], Batch [544/938], Loss: 1.381818175315857\n",
      "Train: Epoch [1], Batch [545/938], Loss: 1.3569512367248535\n",
      "Train: Epoch [1], Batch [546/938], Loss: 1.3390085697174072\n",
      "Train: Epoch [1], Batch [547/938], Loss: 1.370755672454834\n",
      "Train: Epoch [1], Batch [548/938], Loss: 1.3814804553985596\n",
      "Train: Epoch [1], Batch [549/938], Loss: 1.3751025199890137\n",
      "Train: Epoch [1], Batch [550/938], Loss: 1.344266653060913\n",
      "Train: Epoch [1], Batch [551/938], Loss: 1.361527681350708\n",
      "Train: Epoch [1], Batch [552/938], Loss: 1.3134393692016602\n",
      "Train: Epoch [1], Batch [553/938], Loss: 1.304340124130249\n",
      "Train: Epoch [1], Batch [554/938], Loss: 1.3925676345825195\n",
      "Train: Epoch [1], Batch [555/938], Loss: 1.392745018005371\n",
      "Train: Epoch [1], Batch [556/938], Loss: 1.356397032737732\n",
      "Train: Epoch [1], Batch [557/938], Loss: 1.352894902229309\n",
      "Train: Epoch [1], Batch [558/938], Loss: 1.3269679546356201\n",
      "Train: Epoch [1], Batch [559/938], Loss: 1.3869842290878296\n",
      "Train: Epoch [1], Batch [560/938], Loss: 1.3969817161560059\n",
      "Train: Epoch [1], Batch [561/938], Loss: 1.3917787075042725\n",
      "Train: Epoch [1], Batch [562/938], Loss: 1.302936315536499\n",
      "Train: Epoch [1], Batch [563/938], Loss: 1.4063401222229004\n",
      "Train: Epoch [1], Batch [564/938], Loss: 1.3342490196228027\n",
      "Train: Epoch [1], Batch [565/938], Loss: 1.3241664171218872\n",
      "Train: Epoch [1], Batch [566/938], Loss: 1.3373849391937256\n",
      "Train: Epoch [1], Batch [567/938], Loss: 1.3603739738464355\n",
      "Train: Epoch [1], Batch [568/938], Loss: 1.3185999393463135\n",
      "Train: Epoch [1], Batch [569/938], Loss: 1.3583312034606934\n",
      "Train: Epoch [1], Batch [570/938], Loss: 1.335396409034729\n",
      "Train: Epoch [1], Batch [571/938], Loss: 1.3326566219329834\n",
      "Train: Epoch [1], Batch [572/938], Loss: 1.3509503602981567\n",
      "Train: Epoch [1], Batch [573/938], Loss: 1.2675634622573853\n",
      "Train: Epoch [1], Batch [574/938], Loss: 1.3720059394836426\n",
      "Train: Epoch [1], Batch [575/938], Loss: 1.3270796537399292\n",
      "Train: Epoch [1], Batch [576/938], Loss: 1.3707195520401\n",
      "Train: Epoch [1], Batch [577/938], Loss: 1.3816604614257812\n",
      "Train: Epoch [1], Batch [578/938], Loss: 1.3150466680526733\n",
      "Train: Epoch [1], Batch [579/938], Loss: 1.3741369247436523\n",
      "Train: Epoch [1], Batch [580/938], Loss: 1.3590322732925415\n",
      "Train: Epoch [1], Batch [581/938], Loss: 1.3008453845977783\n",
      "Train: Epoch [1], Batch [582/938], Loss: 1.3935935497283936\n",
      "Train: Epoch [1], Batch [583/938], Loss: 1.2869203090667725\n",
      "Train: Epoch [1], Batch [584/938], Loss: 1.337065577507019\n",
      "Train: Epoch [1], Batch [585/938], Loss: 1.363294005393982\n",
      "Train: Epoch [1], Batch [586/938], Loss: 1.3470275402069092\n",
      "Train: Epoch [1], Batch [587/938], Loss: 1.4185469150543213\n",
      "Train: Epoch [1], Batch [588/938], Loss: 1.2831870317459106\n",
      "Train: Epoch [1], Batch [589/938], Loss: 1.4286980628967285\n",
      "Train: Epoch [1], Batch [590/938], Loss: 1.2975776195526123\n",
      "Train: Epoch [1], Batch [591/938], Loss: 1.3997352123260498\n",
      "Train: Epoch [1], Batch [592/938], Loss: 1.300980567932129\n",
      "Train: Epoch [1], Batch [593/938], Loss: 1.3488938808441162\n",
      "Train: Epoch [1], Batch [594/938], Loss: 1.3859379291534424\n",
      "Train: Epoch [1], Batch [595/938], Loss: 1.2947566509246826\n",
      "Train: Epoch [1], Batch [596/938], Loss: 1.4217045307159424\n",
      "Train: Epoch [1], Batch [597/938], Loss: 1.3310952186584473\n",
      "Train: Epoch [1], Batch [598/938], Loss: 1.3932933807373047\n",
      "Train: Epoch [1], Batch [599/938], Loss: 1.386118769645691\n",
      "Train: Epoch [1], Batch [600/938], Loss: 1.3897711038589478\n",
      "Train: Epoch [1], Batch [601/938], Loss: 1.2735283374786377\n",
      "Train: Epoch [1], Batch [602/938], Loss: 1.2609858512878418\n",
      "Train: Epoch [1], Batch [603/938], Loss: 1.2795305252075195\n",
      "Train: Epoch [1], Batch [604/938], Loss: 1.2824633121490479\n",
      "Train: Epoch [1], Batch [605/938], Loss: 1.296102523803711\n",
      "Train: Epoch [1], Batch [606/938], Loss: 1.3216813802719116\n",
      "Train: Epoch [1], Batch [607/938], Loss: 1.2654268741607666\n",
      "Train: Epoch [1], Batch [608/938], Loss: 1.3021080493927002\n",
      "Train: Epoch [1], Batch [609/938], Loss: 1.3492175340652466\n",
      "Train: Epoch [1], Batch [610/938], Loss: 1.3391404151916504\n",
      "Train: Epoch [1], Batch [611/938], Loss: 1.3094737529754639\n",
      "Train: Epoch [1], Batch [612/938], Loss: 1.4065580368041992\n",
      "Train: Epoch [1], Batch [613/938], Loss: 1.2897436618804932\n",
      "Train: Epoch [1], Batch [614/938], Loss: 1.2596371173858643\n",
      "Train: Epoch [1], Batch [615/938], Loss: 1.2599387168884277\n",
      "Train: Epoch [1], Batch [616/938], Loss: 1.3456130027770996\n",
      "Train: Epoch [1], Batch [617/938], Loss: 1.3948955535888672\n",
      "Train: Epoch [1], Batch [618/938], Loss: 1.2746813297271729\n",
      "Train: Epoch [1], Batch [619/938], Loss: 1.3428939580917358\n",
      "Train: Epoch [1], Batch [620/938], Loss: 1.3777573108673096\n",
      "Train: Epoch [1], Batch [621/938], Loss: 1.3635671138763428\n",
      "Train: Epoch [1], Batch [622/938], Loss: 1.301579475402832\n",
      "Train: Epoch [1], Batch [623/938], Loss: 1.4379076957702637\n",
      "Train: Epoch [1], Batch [624/938], Loss: 1.33228600025177\n",
      "Train: Epoch [1], Batch [625/938], Loss: 1.3919516801834106\n",
      "Train: Epoch [1], Batch [626/938], Loss: 1.2441327571868896\n",
      "Train: Epoch [1], Batch [627/938], Loss: 1.441711187362671\n",
      "Train: Epoch [1], Batch [628/938], Loss: 1.3483192920684814\n",
      "Train: Epoch [1], Batch [629/938], Loss: 1.4053126573562622\n",
      "Train: Epoch [1], Batch [630/938], Loss: 1.3662137985229492\n",
      "Train: Epoch [1], Batch [631/938], Loss: 1.2098181247711182\n",
      "Train: Epoch [1], Batch [632/938], Loss: 1.3179324865341187\n",
      "Train: Epoch [1], Batch [633/938], Loss: 1.3231468200683594\n",
      "Train: Epoch [1], Batch [634/938], Loss: 1.2774405479431152\n",
      "Train: Epoch [1], Batch [635/938], Loss: 1.2401947975158691\n",
      "Train: Epoch [1], Batch [636/938], Loss: 1.274561882019043\n",
      "Train: Epoch [1], Batch [637/938], Loss: 1.2558331489562988\n",
      "Train: Epoch [1], Batch [638/938], Loss: 1.3038115501403809\n",
      "Train: Epoch [1], Batch [639/938], Loss: 1.281501293182373\n",
      "Train: Epoch [1], Batch [640/938], Loss: 1.2282047271728516\n",
      "Train: Epoch [1], Batch [641/938], Loss: 1.403854489326477\n",
      "Train: Epoch [1], Batch [642/938], Loss: 1.2749595642089844\n",
      "Train: Epoch [1], Batch [643/938], Loss: 1.3134866952896118\n",
      "Train: Epoch [1], Batch [644/938], Loss: 1.2621124982833862\n",
      "Train: Epoch [1], Batch [645/938], Loss: 1.298628807067871\n",
      "Train: Epoch [1], Batch [646/938], Loss: 1.2174246311187744\n",
      "Train: Epoch [1], Batch [647/938], Loss: 1.3172016143798828\n",
      "Train: Epoch [1], Batch [648/938], Loss: 1.2595162391662598\n",
      "Train: Epoch [1], Batch [649/938], Loss: 1.220122218132019\n",
      "Train: Epoch [1], Batch [650/938], Loss: 1.1910910606384277\n",
      "Train: Epoch [1], Batch [651/938], Loss: 1.3959095478057861\n",
      "Train: Epoch [1], Batch [652/938], Loss: 1.2771575450897217\n",
      "Train: Epoch [1], Batch [653/938], Loss: 1.2214081287384033\n",
      "Train: Epoch [1], Batch [654/938], Loss: 1.3510122299194336\n",
      "Train: Epoch [1], Batch [655/938], Loss: 1.2680697441101074\n",
      "Train: Epoch [1], Batch [656/938], Loss: 1.3314812183380127\n",
      "Train: Epoch [1], Batch [657/938], Loss: 1.2985992431640625\n",
      "Train: Epoch [1], Batch [658/938], Loss: 1.1517844200134277\n",
      "Train: Epoch [1], Batch [659/938], Loss: 1.2905769348144531\n",
      "Train: Epoch [1], Batch [660/938], Loss: 1.3097442388534546\n",
      "Train: Epoch [1], Batch [661/938], Loss: 1.296992540359497\n",
      "Train: Epoch [1], Batch [662/938], Loss: 1.2698326110839844\n",
      "Train: Epoch [1], Batch [663/938], Loss: 1.1874017715454102\n",
      "Train: Epoch [1], Batch [664/938], Loss: 1.3780813217163086\n",
      "Train: Epoch [1], Batch [665/938], Loss: 1.288069248199463\n",
      "Train: Epoch [1], Batch [666/938], Loss: 1.3028204441070557\n",
      "Train: Epoch [1], Batch [667/938], Loss: 1.2672464847564697\n",
      "Train: Epoch [1], Batch [668/938], Loss: 1.272364616394043\n",
      "Train: Epoch [1], Batch [669/938], Loss: 1.3332021236419678\n",
      "Train: Epoch [1], Batch [670/938], Loss: 1.183915376663208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [671/938], Loss: 1.4129798412322998\n",
      "Train: Epoch [1], Batch [672/938], Loss: 1.253525733947754\n",
      "Train: Epoch [1], Batch [673/938], Loss: 1.181749701499939\n",
      "Train: Epoch [1], Batch [674/938], Loss: 1.3160011768341064\n",
      "Train: Epoch [1], Batch [675/938], Loss: 1.3465487957000732\n",
      "Train: Epoch [1], Batch [676/938], Loss: 1.3773493766784668\n",
      "Train: Epoch [1], Batch [677/938], Loss: 1.2047967910766602\n",
      "Train: Epoch [1], Batch [678/938], Loss: 1.2622833251953125\n",
      "Train: Epoch [1], Batch [679/938], Loss: 1.3174399137496948\n",
      "Train: Epoch [1], Batch [680/938], Loss: 1.308521032333374\n",
      "Train: Epoch [1], Batch [681/938], Loss: 1.2069274187088013\n",
      "Train: Epoch [1], Batch [682/938], Loss: 1.2593003511428833\n",
      "Train: Epoch [1], Batch [683/938], Loss: 1.1873385906219482\n",
      "Train: Epoch [1], Batch [684/938], Loss: 1.247265100479126\n",
      "Train: Epoch [1], Batch [685/938], Loss: 1.27847158908844\n",
      "Train: Epoch [1], Batch [686/938], Loss: 1.244077205657959\n",
      "Train: Epoch [1], Batch [687/938], Loss: 1.1992347240447998\n",
      "Train: Epoch [1], Batch [688/938], Loss: 1.2239699363708496\n",
      "Train: Epoch [1], Batch [689/938], Loss: 1.481208324432373\n",
      "Train: Epoch [1], Batch [690/938], Loss: 1.2109978199005127\n",
      "Train: Epoch [1], Batch [691/938], Loss: 1.2579331398010254\n",
      "Train: Epoch [1], Batch [692/938], Loss: 1.2506747245788574\n",
      "Train: Epoch [1], Batch [693/938], Loss: 1.2033743858337402\n",
      "Train: Epoch [1], Batch [694/938], Loss: 1.2548950910568237\n",
      "Train: Epoch [1], Batch [695/938], Loss: 1.2197062969207764\n",
      "Train: Epoch [1], Batch [696/938], Loss: 1.222240924835205\n",
      "Train: Epoch [1], Batch [697/938], Loss: 1.2969467639923096\n",
      "Train: Epoch [1], Batch [698/938], Loss: 1.1242051124572754\n",
      "Train: Epoch [1], Batch [699/938], Loss: 1.3343908786773682\n",
      "Train: Epoch [1], Batch [700/938], Loss: 1.2643331289291382\n",
      "Train: Epoch [1], Batch [701/938], Loss: 1.1737675666809082\n",
      "Train: Epoch [1], Batch [702/938], Loss: 1.2394672632217407\n",
      "Train: Epoch [1], Batch [703/938], Loss: 1.1743640899658203\n",
      "Train: Epoch [1], Batch [704/938], Loss: 1.1645952463150024\n",
      "Train: Epoch [1], Batch [705/938], Loss: 1.3551743030548096\n",
      "Train: Epoch [1], Batch [706/938], Loss: 1.2143497467041016\n",
      "Train: Epoch [1], Batch [707/938], Loss: 1.2619305849075317\n",
      "Train: Epoch [1], Batch [708/938], Loss: 1.23236083984375\n",
      "Train: Epoch [1], Batch [709/938], Loss: 1.214486002922058\n",
      "Train: Epoch [1], Batch [710/938], Loss: 1.2744765281677246\n",
      "Train: Epoch [1], Batch [711/938], Loss: 1.1243400573730469\n",
      "Train: Epoch [1], Batch [712/938], Loss: 1.4441497325897217\n",
      "Train: Epoch [1], Batch [713/938], Loss: 1.3044488430023193\n",
      "Train: Epoch [1], Batch [714/938], Loss: 1.2013887166976929\n",
      "Train: Epoch [1], Batch [715/938], Loss: 1.2537853717803955\n",
      "Train: Epoch [1], Batch [716/938], Loss: 1.3347052335739136\n",
      "Train: Epoch [1], Batch [717/938], Loss: 1.2320212125778198\n",
      "Train: Epoch [1], Batch [718/938], Loss: 1.2713558673858643\n",
      "Train: Epoch [1], Batch [719/938], Loss: 1.2262790203094482\n",
      "Train: Epoch [1], Batch [720/938], Loss: 1.226963996887207\n",
      "Train: Epoch [1], Batch [721/938], Loss: 1.1918132305145264\n",
      "Train: Epoch [1], Batch [722/938], Loss: 1.2940263748168945\n",
      "Train: Epoch [1], Batch [723/938], Loss: 1.233778715133667\n",
      "Train: Epoch [1], Batch [724/938], Loss: 1.3355971574783325\n",
      "Train: Epoch [1], Batch [725/938], Loss: 1.212157964706421\n",
      "Train: Epoch [1], Batch [726/938], Loss: 1.2971208095550537\n",
      "Train: Epoch [1], Batch [727/938], Loss: 1.168360948562622\n",
      "Train: Epoch [1], Batch [728/938], Loss: 1.1967737674713135\n",
      "Train: Epoch [1], Batch [729/938], Loss: 1.296146273612976\n",
      "Train: Epoch [1], Batch [730/938], Loss: 1.1596975326538086\n",
      "Train: Epoch [1], Batch [731/938], Loss: 1.2325387001037598\n",
      "Train: Epoch [1], Batch [732/938], Loss: 1.2983105182647705\n",
      "Train: Epoch [1], Batch [733/938], Loss: 1.2807400226593018\n",
      "Train: Epoch [1], Batch [734/938], Loss: 1.250788927078247\n",
      "Train: Epoch [1], Batch [735/938], Loss: 1.2089025974273682\n",
      "Train: Epoch [1], Batch [736/938], Loss: 1.251834750175476\n",
      "Train: Epoch [1], Batch [737/938], Loss: 1.2462495565414429\n",
      "Train: Epoch [1], Batch [738/938], Loss: 1.3781888484954834\n",
      "Train: Epoch [1], Batch [739/938], Loss: 1.2427856922149658\n",
      "Train: Epoch [1], Batch [740/938], Loss: 1.2675360441207886\n",
      "Train: Epoch [1], Batch [741/938], Loss: 1.3293979167938232\n",
      "Train: Epoch [1], Batch [742/938], Loss: 1.2108896970748901\n",
      "Train: Epoch [1], Batch [743/938], Loss: 1.262925386428833\n",
      "Train: Epoch [1], Batch [744/938], Loss: 1.1276217699050903\n",
      "Train: Epoch [1], Batch [745/938], Loss: 1.2329542636871338\n",
      "Train: Epoch [1], Batch [746/938], Loss: 1.4040749073028564\n",
      "Train: Epoch [1], Batch [747/938], Loss: 1.2835979461669922\n",
      "Train: Epoch [1], Batch [748/938], Loss: 1.2243409156799316\n",
      "Train: Epoch [1], Batch [749/938], Loss: 1.2648425102233887\n",
      "Train: Epoch [1], Batch [750/938], Loss: 1.1780922412872314\n",
      "Train: Epoch [1], Batch [751/938], Loss: 1.3140480518341064\n",
      "Train: Epoch [1], Batch [752/938], Loss: 1.181281566619873\n",
      "Train: Epoch [1], Batch [753/938], Loss: 1.2050442695617676\n",
      "Train: Epoch [1], Batch [754/938], Loss: 1.2350728511810303\n",
      "Train: Epoch [1], Batch [755/938], Loss: 1.076169729232788\n",
      "Train: Epoch [1], Batch [756/938], Loss: 1.296661138534546\n",
      "Train: Epoch [1], Batch [757/938], Loss: 1.191212773323059\n",
      "Train: Epoch [1], Batch [758/938], Loss: 1.197648286819458\n",
      "Train: Epoch [1], Batch [759/938], Loss: 1.2175453901290894\n",
      "Train: Epoch [1], Batch [760/938], Loss: 1.2181780338287354\n",
      "Train: Epoch [1], Batch [761/938], Loss: 1.2253695726394653\n",
      "Train: Epoch [1], Batch [762/938], Loss: 1.2274492979049683\n",
      "Train: Epoch [1], Batch [763/938], Loss: 1.4589899778366089\n",
      "Train: Epoch [1], Batch [764/938], Loss: 1.2688705921173096\n",
      "Train: Epoch [1], Batch [765/938], Loss: 1.1654616594314575\n",
      "Train: Epoch [1], Batch [766/938], Loss: 1.0931799411773682\n",
      "Train: Epoch [1], Batch [767/938], Loss: 1.2335652112960815\n",
      "Train: Epoch [1], Batch [768/938], Loss: 1.2694196701049805\n",
      "Train: Epoch [1], Batch [769/938], Loss: 1.151402473449707\n",
      "Train: Epoch [1], Batch [770/938], Loss: 1.241365909576416\n",
      "Train: Epoch [1], Batch [771/938], Loss: 1.0588278770446777\n",
      "Train: Epoch [1], Batch [772/938], Loss: 1.1593619585037231\n",
      "Train: Epoch [1], Batch [773/938], Loss: 1.1135004758834839\n",
      "Train: Epoch [1], Batch [774/938], Loss: 1.1639821529388428\n",
      "Train: Epoch [1], Batch [775/938], Loss: 1.0780034065246582\n",
      "Train: Epoch [1], Batch [776/938], Loss: 1.1079440116882324\n",
      "Train: Epoch [1], Batch [777/938], Loss: 1.2693020105361938\n",
      "Train: Epoch [1], Batch [778/938], Loss: 1.2313456535339355\n",
      "Train: Epoch [1], Batch [779/938], Loss: 1.1467275619506836\n",
      "Train: Epoch [1], Batch [780/938], Loss: 1.220949411392212\n",
      "Train: Epoch [1], Batch [781/938], Loss: 1.2397499084472656\n",
      "Train: Epoch [1], Batch [782/938], Loss: 1.2409679889678955\n",
      "Train: Epoch [1], Batch [783/938], Loss: 1.2252603769302368\n",
      "Train: Epoch [1], Batch [784/938], Loss: 1.186124324798584\n",
      "Train: Epoch [1], Batch [785/938], Loss: 1.2197792530059814\n",
      "Train: Epoch [1], Batch [786/938], Loss: 1.1142840385437012\n",
      "Train: Epoch [1], Batch [787/938], Loss: 1.274968147277832\n",
      "Train: Epoch [1], Batch [788/938], Loss: 1.2171190977096558\n",
      "Train: Epoch [1], Batch [789/938], Loss: 1.1525317430496216\n",
      "Train: Epoch [1], Batch [790/938], Loss: 1.1387548446655273\n",
      "Train: Epoch [1], Batch [791/938], Loss: 1.1424165964126587\n",
      "Train: Epoch [1], Batch [792/938], Loss: 1.1823279857635498\n",
      "Train: Epoch [1], Batch [793/938], Loss: 1.1147675514221191\n",
      "Train: Epoch [1], Batch [794/938], Loss: 1.208404302597046\n",
      "Train: Epoch [1], Batch [795/938], Loss: 1.3003073930740356\n",
      "Train: Epoch [1], Batch [796/938], Loss: 1.149648666381836\n",
      "Train: Epoch [1], Batch [797/938], Loss: 1.1520531177520752\n",
      "Train: Epoch [1], Batch [798/938], Loss: 1.1969993114471436\n",
      "Train: Epoch [1], Batch [799/938], Loss: 1.2582157850265503\n",
      "Train: Epoch [1], Batch [800/938], Loss: 1.3266687393188477\n",
      "Train: Epoch [1], Batch [801/938], Loss: 1.218464970588684\n",
      "Train: Epoch [1], Batch [802/938], Loss: 1.1068544387817383\n",
      "Train: Epoch [1], Batch [803/938], Loss: 1.1659612655639648\n",
      "Train: Epoch [1], Batch [804/938], Loss: 1.1872118711471558\n",
      "Train: Epoch [1], Batch [805/938], Loss: 1.1427726745605469\n",
      "Train: Epoch [1], Batch [806/938], Loss: 1.0861401557922363\n",
      "Train: Epoch [1], Batch [807/938], Loss: 1.0674161911010742\n",
      "Train: Epoch [1], Batch [808/938], Loss: 1.1855604648590088\n",
      "Train: Epoch [1], Batch [809/938], Loss: 1.16012442111969\n",
      "Train: Epoch [1], Batch [810/938], Loss: 1.1778206825256348\n",
      "Train: Epoch [1], Batch [811/938], Loss: 1.2522478103637695\n",
      "Train: Epoch [1], Batch [812/938], Loss: 1.1491906642913818\n",
      "Train: Epoch [1], Batch [813/938], Loss: 1.1666862964630127\n",
      "Train: Epoch [1], Batch [814/938], Loss: 1.1794171333312988\n",
      "Train: Epoch [1], Batch [815/938], Loss: 1.1079750061035156\n",
      "Train: Epoch [1], Batch [816/938], Loss: 1.0148277282714844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [817/938], Loss: 1.219930648803711\n",
      "Train: Epoch [1], Batch [818/938], Loss: 1.204026222229004\n",
      "Train: Epoch [1], Batch [819/938], Loss: 1.1789512634277344\n",
      "Train: Epoch [1], Batch [820/938], Loss: 1.2257273197174072\n",
      "Train: Epoch [1], Batch [821/938], Loss: 1.1028143167495728\n",
      "Train: Epoch [1], Batch [822/938], Loss: 1.0758130550384521\n",
      "Train: Epoch [1], Batch [823/938], Loss: 1.1371946334838867\n",
      "Train: Epoch [1], Batch [824/938], Loss: 1.1967082023620605\n",
      "Train: Epoch [1], Batch [825/938], Loss: 1.2407244443893433\n",
      "Train: Epoch [1], Batch [826/938], Loss: 1.1470947265625\n",
      "Train: Epoch [1], Batch [827/938], Loss: 1.0751371383666992\n",
      "Train: Epoch [1], Batch [828/938], Loss: 1.1543242931365967\n",
      "Train: Epoch [1], Batch [829/938], Loss: 1.5153417587280273\n",
      "Train: Epoch [1], Batch [830/938], Loss: 1.238423466682434\n",
      "Train: Epoch [1], Batch [831/938], Loss: 1.076431393623352\n",
      "Train: Epoch [1], Batch [832/938], Loss: 1.1901373863220215\n",
      "Train: Epoch [1], Batch [833/938], Loss: 1.143516182899475\n",
      "Train: Epoch [1], Batch [834/938], Loss: 1.0349547863006592\n",
      "Train: Epoch [1], Batch [835/938], Loss: 1.1547138690948486\n",
      "Train: Epoch [1], Batch [836/938], Loss: 1.2204045057296753\n",
      "Train: Epoch [1], Batch [837/938], Loss: 1.1402870416641235\n",
      "Train: Epoch [1], Batch [838/938], Loss: 1.1427699327468872\n",
      "Train: Epoch [1], Batch [839/938], Loss: 1.316330075263977\n",
      "Train: Epoch [1], Batch [840/938], Loss: 1.1952733993530273\n",
      "Train: Epoch [1], Batch [841/938], Loss: 1.1016374826431274\n",
      "Train: Epoch [1], Batch [842/938], Loss: 1.1527631282806396\n",
      "Train: Epoch [1], Batch [843/938], Loss: 1.2015494108200073\n",
      "Train: Epoch [1], Batch [844/938], Loss: 1.0945372581481934\n",
      "Train: Epoch [1], Batch [845/938], Loss: 1.0641344785690308\n",
      "Train: Epoch [1], Batch [846/938], Loss: 1.1844322681427002\n",
      "Train: Epoch [1], Batch [847/938], Loss: 1.1456503868103027\n",
      "Train: Epoch [1], Batch [848/938], Loss: 1.1292108297348022\n",
      "Train: Epoch [1], Batch [849/938], Loss: 1.1899360418319702\n",
      "Train: Epoch [1], Batch [850/938], Loss: 1.2024941444396973\n",
      "Train: Epoch [1], Batch [851/938], Loss: 1.157050609588623\n",
      "Train: Epoch [1], Batch [852/938], Loss: 0.9847798943519592\n",
      "Train: Epoch [1], Batch [853/938], Loss: 1.1032545566558838\n",
      "Train: Epoch [1], Batch [854/938], Loss: 1.0633792877197266\n",
      "Train: Epoch [1], Batch [855/938], Loss: 1.2834241390228271\n",
      "Train: Epoch [1], Batch [856/938], Loss: 1.1557390689849854\n",
      "Train: Epoch [1], Batch [857/938], Loss: 1.2098963260650635\n",
      "Train: Epoch [1], Batch [858/938], Loss: 1.0451725721359253\n",
      "Train: Epoch [1], Batch [859/938], Loss: 1.1864137649536133\n",
      "Train: Epoch [1], Batch [860/938], Loss: 1.2233493328094482\n",
      "Train: Epoch [1], Batch [861/938], Loss: 1.1838274002075195\n",
      "Train: Epoch [1], Batch [862/938], Loss: 1.1776951551437378\n",
      "Train: Epoch [1], Batch [863/938], Loss: 1.1408092975616455\n",
      "Train: Epoch [1], Batch [864/938], Loss: 1.1952362060546875\n",
      "Train: Epoch [1], Batch [865/938], Loss: 1.0976225137710571\n",
      "Train: Epoch [1], Batch [866/938], Loss: 1.1605793237686157\n",
      "Train: Epoch [1], Batch [867/938], Loss: 1.173645257949829\n",
      "Train: Epoch [1], Batch [868/938], Loss: 1.1378211975097656\n",
      "Train: Epoch [1], Batch [869/938], Loss: 1.1121644973754883\n",
      "Train: Epoch [1], Batch [870/938], Loss: 1.143661618232727\n",
      "Train: Epoch [1], Batch [871/938], Loss: 1.2171962261199951\n",
      "Train: Epoch [1], Batch [872/938], Loss: 1.08513343334198\n",
      "Train: Epoch [1], Batch [873/938], Loss: 1.226593017578125\n",
      "Train: Epoch [1], Batch [874/938], Loss: 1.1297448873519897\n",
      "Train: Epoch [1], Batch [875/938], Loss: 1.2193793058395386\n",
      "Train: Epoch [1], Batch [876/938], Loss: 1.1300936937332153\n",
      "Train: Epoch [1], Batch [877/938], Loss: 1.0754623413085938\n",
      "Train: Epoch [1], Batch [878/938], Loss: 1.0754050016403198\n",
      "Train: Epoch [1], Batch [879/938], Loss: 1.1106762886047363\n",
      "Train: Epoch [1], Batch [880/938], Loss: 1.1579952239990234\n",
      "Train: Epoch [1], Batch [881/938], Loss: 1.1702325344085693\n",
      "Train: Epoch [1], Batch [882/938], Loss: 1.268224835395813\n",
      "Train: Epoch [1], Batch [883/938], Loss: 1.1117132902145386\n",
      "Train: Epoch [1], Batch [884/938], Loss: 1.1101620197296143\n",
      "Train: Epoch [1], Batch [885/938], Loss: 1.2304596900939941\n",
      "Train: Epoch [1], Batch [886/938], Loss: 1.1175531148910522\n",
      "Train: Epoch [1], Batch [887/938], Loss: 1.113741397857666\n",
      "Train: Epoch [1], Batch [888/938], Loss: 1.2437026500701904\n",
      "Train: Epoch [1], Batch [889/938], Loss: 1.1391723155975342\n",
      "Train: Epoch [1], Batch [890/938], Loss: 1.1133002042770386\n",
      "Train: Epoch [1], Batch [891/938], Loss: 1.2082085609436035\n",
      "Train: Epoch [1], Batch [892/938], Loss: 1.1077563762664795\n",
      "Train: Epoch [1], Batch [893/938], Loss: 1.1862666606903076\n",
      "Train: Epoch [1], Batch [894/938], Loss: 1.0977725982666016\n",
      "Train: Epoch [1], Batch [895/938], Loss: 1.1372931003570557\n",
      "Train: Epoch [1], Batch [896/938], Loss: 1.1398862600326538\n",
      "Train: Epoch [1], Batch [897/938], Loss: 1.1237505674362183\n",
      "Train: Epoch [1], Batch [898/938], Loss: 1.0728843212127686\n",
      "Train: Epoch [1], Batch [899/938], Loss: 1.1760145425796509\n",
      "Train: Epoch [1], Batch [900/938], Loss: 0.9993469715118408\n",
      "Train: Epoch [1], Batch [901/938], Loss: 1.1343915462493896\n",
      "Train: Epoch [1], Batch [902/938], Loss: 0.9957431554794312\n",
      "Train: Epoch [1], Batch [903/938], Loss: 1.1842774152755737\n",
      "Train: Epoch [1], Batch [904/938], Loss: 1.2260117530822754\n",
      "Train: Epoch [1], Batch [905/938], Loss: 1.0427567958831787\n",
      "Train: Epoch [1], Batch [906/938], Loss: 1.0346136093139648\n",
      "Train: Epoch [1], Batch [907/938], Loss: 1.189345121383667\n",
      "Train: Epoch [1], Batch [908/938], Loss: 1.2452679872512817\n",
      "Train: Epoch [1], Batch [909/938], Loss: 1.1710537672042847\n",
      "Train: Epoch [1], Batch [910/938], Loss: 1.0961285829544067\n",
      "Train: Epoch [1], Batch [911/938], Loss: 1.0160478353500366\n",
      "Train: Epoch [1], Batch [912/938], Loss: 1.048648715019226\n",
      "Train: Epoch [1], Batch [913/938], Loss: 1.132863163948059\n",
      "Train: Epoch [1], Batch [914/938], Loss: 1.1228886842727661\n",
      "Train: Epoch [1], Batch [915/938], Loss: 0.998830258846283\n",
      "Train: Epoch [1], Batch [916/938], Loss: 1.0843451023101807\n",
      "Train: Epoch [1], Batch [917/938], Loss: 1.0704916715621948\n",
      "Train: Epoch [1], Batch [918/938], Loss: 1.0457699298858643\n",
      "Train: Epoch [1], Batch [919/938], Loss: 1.0591497421264648\n",
      "Train: Epoch [1], Batch [920/938], Loss: 1.3315836191177368\n",
      "Train: Epoch [1], Batch [921/938], Loss: 1.079934000968933\n",
      "Train: Epoch [1], Batch [922/938], Loss: 1.0193889141082764\n",
      "Train: Epoch [1], Batch [923/938], Loss: 1.0992352962493896\n",
      "Train: Epoch [1], Batch [924/938], Loss: 1.09895920753479\n",
      "Train: Epoch [1], Batch [925/938], Loss: 1.0826892852783203\n",
      "Train: Epoch [1], Batch [926/938], Loss: 1.053644061088562\n",
      "Train: Epoch [1], Batch [927/938], Loss: 1.1549105644226074\n",
      "Train: Epoch [1], Batch [928/938], Loss: 1.0947775840759277\n",
      "Train: Epoch [1], Batch [929/938], Loss: 1.1614937782287598\n",
      "Train: Epoch [1], Batch [930/938], Loss: 1.0189790725708008\n",
      "Train: Epoch [1], Batch [931/938], Loss: 1.0805233716964722\n",
      "Train: Epoch [1], Batch [932/938], Loss: 1.2544240951538086\n",
      "Train: Epoch [1], Batch [933/938], Loss: 1.024277687072754\n",
      "Train: Epoch [1], Batch [934/938], Loss: 1.082923412322998\n",
      "Train: Epoch [1], Batch [935/938], Loss: 1.1442244052886963\n",
      "Train: Epoch [1], Batch [936/938], Loss: 1.1843153238296509\n",
      "Train: Epoch [1], Batch [937/938], Loss: 1.0488483905792236\n",
      "Train: Epoch [1], Batch [938/938], Loss: 1.261747121810913\n",
      "Accuracy of train set: 0.44521666666666665\n",
      "Validation: Epoch [1], Batch [1/938], Loss: 1.0680230855941772\n",
      "Validation: Epoch [1], Batch [2/938], Loss: 1.0352497100830078\n",
      "Validation: Epoch [1], Batch [3/938], Loss: 1.1463688611984253\n",
      "Validation: Epoch [1], Batch [4/938], Loss: 1.1538121700286865\n",
      "Validation: Epoch [1], Batch [5/938], Loss: 1.2077504396438599\n",
      "Validation: Epoch [1], Batch [6/938], Loss: 1.0697808265686035\n",
      "Validation: Epoch [1], Batch [7/938], Loss: 1.0885127782821655\n",
      "Validation: Epoch [1], Batch [8/938], Loss: 1.244055986404419\n",
      "Validation: Epoch [1], Batch [9/938], Loss: 1.122710943222046\n",
      "Validation: Epoch [1], Batch [10/938], Loss: 1.230485439300537\n",
      "Validation: Epoch [1], Batch [11/938], Loss: 1.191532015800476\n",
      "Validation: Epoch [1], Batch [12/938], Loss: 1.067940354347229\n",
      "Validation: Epoch [1], Batch [13/938], Loss: 1.051072359085083\n",
      "Validation: Epoch [1], Batch [14/938], Loss: 1.1973403692245483\n",
      "Validation: Epoch [1], Batch [15/938], Loss: 1.124999761581421\n",
      "Validation: Epoch [1], Batch [16/938], Loss: 1.1959969997406006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [17/938], Loss: 0.9651353359222412\n",
      "Validation: Epoch [1], Batch [18/938], Loss: 1.1918851137161255\n",
      "Validation: Epoch [1], Batch [19/938], Loss: 1.044440746307373\n",
      "Validation: Epoch [1], Batch [20/938], Loss: 1.072771668434143\n",
      "Validation: Epoch [1], Batch [21/938], Loss: 1.0435712337493896\n",
      "Validation: Epoch [1], Batch [22/938], Loss: 0.9554988145828247\n",
      "Validation: Epoch [1], Batch [23/938], Loss: 1.0582046508789062\n",
      "Validation: Epoch [1], Batch [24/938], Loss: 1.0386216640472412\n",
      "Validation: Epoch [1], Batch [25/938], Loss: 1.0735104084014893\n",
      "Validation: Epoch [1], Batch [26/938], Loss: 1.0894557237625122\n",
      "Validation: Epoch [1], Batch [27/938], Loss: 1.0676432847976685\n",
      "Validation: Epoch [1], Batch [28/938], Loss: 1.0786575078964233\n",
      "Validation: Epoch [1], Batch [29/938], Loss: 1.09042227268219\n",
      "Validation: Epoch [1], Batch [30/938], Loss: 1.0221524238586426\n",
      "Validation: Epoch [1], Batch [31/938], Loss: 0.9890334606170654\n",
      "Validation: Epoch [1], Batch [32/938], Loss: 1.1363279819488525\n",
      "Validation: Epoch [1], Batch [33/938], Loss: 1.0589603185653687\n",
      "Validation: Epoch [1], Batch [34/938], Loss: 1.1703100204467773\n",
      "Validation: Epoch [1], Batch [35/938], Loss: 1.337160587310791\n",
      "Validation: Epoch [1], Batch [36/938], Loss: 0.9971534013748169\n",
      "Validation: Epoch [1], Batch [37/938], Loss: 1.0887253284454346\n",
      "Validation: Epoch [1], Batch [38/938], Loss: 1.075595736503601\n",
      "Validation: Epoch [1], Batch [39/938], Loss: 1.150343656539917\n",
      "Validation: Epoch [1], Batch [40/938], Loss: 1.1327135562896729\n",
      "Validation: Epoch [1], Batch [41/938], Loss: 1.1084246635437012\n",
      "Validation: Epoch [1], Batch [42/938], Loss: 1.2246527671813965\n",
      "Validation: Epoch [1], Batch [43/938], Loss: 1.0868839025497437\n",
      "Validation: Epoch [1], Batch [44/938], Loss: 1.0787913799285889\n",
      "Validation: Epoch [1], Batch [45/938], Loss: 1.0864224433898926\n",
      "Validation: Epoch [1], Batch [46/938], Loss: 1.218519687652588\n",
      "Validation: Epoch [1], Batch [47/938], Loss: 1.2699074745178223\n",
      "Validation: Epoch [1], Batch [48/938], Loss: 1.1474719047546387\n",
      "Validation: Epoch [1], Batch [49/938], Loss: 1.2905172109603882\n",
      "Validation: Epoch [1], Batch [50/938], Loss: 1.0923967361450195\n",
      "Validation: Epoch [1], Batch [51/938], Loss: 0.9637818932533264\n",
      "Validation: Epoch [1], Batch [52/938], Loss: 1.2359012365341187\n",
      "Validation: Epoch [1], Batch [53/938], Loss: 1.059464931488037\n",
      "Validation: Epoch [1], Batch [54/938], Loss: 1.0796231031417847\n",
      "Validation: Epoch [1], Batch [55/938], Loss: 0.983866274356842\n",
      "Validation: Epoch [1], Batch [56/938], Loss: 1.1466786861419678\n",
      "Validation: Epoch [1], Batch [57/938], Loss: 1.0211334228515625\n",
      "Validation: Epoch [1], Batch [58/938], Loss: 1.1470088958740234\n",
      "Validation: Epoch [1], Batch [59/938], Loss: 1.2626855373382568\n",
      "Validation: Epoch [1], Batch [60/938], Loss: 1.1020328998565674\n",
      "Validation: Epoch [1], Batch [61/938], Loss: 1.0731041431427002\n",
      "Validation: Epoch [1], Batch [62/938], Loss: 0.9543507099151611\n",
      "Validation: Epoch [1], Batch [63/938], Loss: 1.1203101873397827\n",
      "Validation: Epoch [1], Batch [64/938], Loss: 1.0953166484832764\n",
      "Validation: Epoch [1], Batch [65/938], Loss: 1.2167959213256836\n",
      "Validation: Epoch [1], Batch [66/938], Loss: 1.1872423887252808\n",
      "Validation: Epoch [1], Batch [67/938], Loss: 1.173184871673584\n",
      "Validation: Epoch [1], Batch [68/938], Loss: 1.2052803039550781\n",
      "Validation: Epoch [1], Batch [69/938], Loss: 1.1379280090332031\n",
      "Validation: Epoch [1], Batch [70/938], Loss: 1.1397781372070312\n",
      "Validation: Epoch [1], Batch [71/938], Loss: 1.0700759887695312\n",
      "Validation: Epoch [1], Batch [72/938], Loss: 1.0988370180130005\n",
      "Validation: Epoch [1], Batch [73/938], Loss: 1.038982629776001\n",
      "Validation: Epoch [1], Batch [74/938], Loss: 1.1469981670379639\n",
      "Validation: Epoch [1], Batch [75/938], Loss: 1.039536714553833\n",
      "Validation: Epoch [1], Batch [76/938], Loss: 1.1245425939559937\n",
      "Validation: Epoch [1], Batch [77/938], Loss: 1.1547492742538452\n",
      "Validation: Epoch [1], Batch [78/938], Loss: 1.2176523208618164\n",
      "Validation: Epoch [1], Batch [79/938], Loss: 1.2166398763656616\n",
      "Validation: Epoch [1], Batch [80/938], Loss: 1.1962471008300781\n",
      "Validation: Epoch [1], Batch [81/938], Loss: 1.0371918678283691\n",
      "Validation: Epoch [1], Batch [82/938], Loss: 1.118397831916809\n",
      "Validation: Epoch [1], Batch [83/938], Loss: 1.301996111869812\n",
      "Validation: Epoch [1], Batch [84/938], Loss: 1.1592451333999634\n",
      "Validation: Epoch [1], Batch [85/938], Loss: 1.1528459787368774\n",
      "Validation: Epoch [1], Batch [86/938], Loss: 1.210291862487793\n",
      "Validation: Epoch [1], Batch [87/938], Loss: 1.2095904350280762\n",
      "Validation: Epoch [1], Batch [88/938], Loss: 1.0500885248184204\n",
      "Validation: Epoch [1], Batch [89/938], Loss: 0.9823548793792725\n",
      "Validation: Epoch [1], Batch [90/938], Loss: 1.2573161125183105\n",
      "Validation: Epoch [1], Batch [91/938], Loss: 0.9720161557197571\n",
      "Validation: Epoch [1], Batch [92/938], Loss: 0.9212513566017151\n",
      "Validation: Epoch [1], Batch [93/938], Loss: 1.129483699798584\n",
      "Validation: Epoch [1], Batch [94/938], Loss: 1.165225863456726\n",
      "Validation: Epoch [1], Batch [95/938], Loss: 1.0751945972442627\n",
      "Validation: Epoch [1], Batch [96/938], Loss: 1.145505666732788\n",
      "Validation: Epoch [1], Batch [97/938], Loss: 1.058312177658081\n",
      "Validation: Epoch [1], Batch [98/938], Loss: 1.1192938089370728\n",
      "Validation: Epoch [1], Batch [99/938], Loss: 0.9933091998100281\n",
      "Validation: Epoch [1], Batch [100/938], Loss: 0.9987583160400391\n",
      "Validation: Epoch [1], Batch [101/938], Loss: 1.1606419086456299\n",
      "Validation: Epoch [1], Batch [102/938], Loss: 1.0943963527679443\n",
      "Validation: Epoch [1], Batch [103/938], Loss: 1.0379548072814941\n",
      "Validation: Epoch [1], Batch [104/938], Loss: 1.1905616521835327\n",
      "Validation: Epoch [1], Batch [105/938], Loss: 1.056169033050537\n",
      "Validation: Epoch [1], Batch [106/938], Loss: 0.9847151637077332\n",
      "Validation: Epoch [1], Batch [107/938], Loss: 1.16702401638031\n",
      "Validation: Epoch [1], Batch [108/938], Loss: 1.1300371885299683\n",
      "Validation: Epoch [1], Batch [109/938], Loss: 1.0331964492797852\n",
      "Validation: Epoch [1], Batch [110/938], Loss: 1.1239476203918457\n",
      "Validation: Epoch [1], Batch [111/938], Loss: 1.0164484977722168\n",
      "Validation: Epoch [1], Batch [112/938], Loss: 1.235724687576294\n",
      "Validation: Epoch [1], Batch [113/938], Loss: 1.0459139347076416\n",
      "Validation: Epoch [1], Batch [114/938], Loss: 1.149855136871338\n",
      "Validation: Epoch [1], Batch [115/938], Loss: 1.2605969905853271\n",
      "Validation: Epoch [1], Batch [116/938], Loss: 1.0247918367385864\n",
      "Validation: Epoch [1], Batch [117/938], Loss: 0.9954260587692261\n",
      "Validation: Epoch [1], Batch [118/938], Loss: 1.1460691690444946\n",
      "Validation: Epoch [1], Batch [119/938], Loss: 1.100792407989502\n",
      "Validation: Epoch [1], Batch [120/938], Loss: 1.124403476715088\n",
      "Validation: Epoch [1], Batch [121/938], Loss: 1.1091892719268799\n",
      "Validation: Epoch [1], Batch [122/938], Loss: 1.1762855052947998\n",
      "Validation: Epoch [1], Batch [123/938], Loss: 1.0628595352172852\n",
      "Validation: Epoch [1], Batch [124/938], Loss: 1.1547324657440186\n",
      "Validation: Epoch [1], Batch [125/938], Loss: 1.078263282775879\n",
      "Validation: Epoch [1], Batch [126/938], Loss: 1.2286105155944824\n",
      "Validation: Epoch [1], Batch [127/938], Loss: 1.149547815322876\n",
      "Validation: Epoch [1], Batch [128/938], Loss: 1.1307687759399414\n",
      "Validation: Epoch [1], Batch [129/938], Loss: 1.0862953662872314\n",
      "Validation: Epoch [1], Batch [130/938], Loss: 1.1419886350631714\n",
      "Validation: Epoch [1], Batch [131/938], Loss: 1.116486668586731\n",
      "Validation: Epoch [1], Batch [132/938], Loss: 1.0197782516479492\n",
      "Validation: Epoch [1], Batch [133/938], Loss: 1.0624477863311768\n",
      "Validation: Epoch [1], Batch [134/938], Loss: 1.1255395412445068\n",
      "Validation: Epoch [1], Batch [135/938], Loss: 1.0704700946807861\n",
      "Validation: Epoch [1], Batch [136/938], Loss: 1.0256447792053223\n",
      "Validation: Epoch [1], Batch [137/938], Loss: 1.08573579788208\n",
      "Validation: Epoch [1], Batch [138/938], Loss: 1.031714677810669\n",
      "Validation: Epoch [1], Batch [139/938], Loss: 1.0766990184783936\n",
      "Validation: Epoch [1], Batch [140/938], Loss: 1.0047428607940674\n",
      "Validation: Epoch [1], Batch [141/938], Loss: 1.0938862562179565\n",
      "Validation: Epoch [1], Batch [142/938], Loss: 1.1824846267700195\n",
      "Validation: Epoch [1], Batch [143/938], Loss: 0.8717032670974731\n",
      "Validation: Epoch [1], Batch [144/938], Loss: 1.1068308353424072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [145/938], Loss: 1.1280758380889893\n",
      "Validation: Epoch [1], Batch [146/938], Loss: 1.0881189107894897\n",
      "Validation: Epoch [1], Batch [147/938], Loss: 1.0332045555114746\n",
      "Validation: Epoch [1], Batch [148/938], Loss: 1.079660177230835\n",
      "Validation: Epoch [1], Batch [149/938], Loss: 1.0809648036956787\n",
      "Validation: Epoch [1], Batch [150/938], Loss: 1.1315393447875977\n",
      "Validation: Epoch [1], Batch [151/938], Loss: 1.1734137535095215\n",
      "Validation: Epoch [1], Batch [152/938], Loss: 1.0439420938491821\n",
      "Validation: Epoch [1], Batch [153/938], Loss: 1.0706326961517334\n",
      "Validation: Epoch [1], Batch [154/938], Loss: 1.127849817276001\n",
      "Validation: Epoch [1], Batch [155/938], Loss: 1.009334921836853\n",
      "Validation: Epoch [1], Batch [156/938], Loss: 1.0766937732696533\n",
      "Validation: Epoch [1], Batch [157/938], Loss: 1.2376199960708618\n",
      "Validation: Epoch [1], Batch [158/938], Loss: 1.168556809425354\n",
      "Validation: Epoch [1], Batch [159/938], Loss: 1.0673882961273193\n",
      "Validation: Epoch [1], Batch [160/938], Loss: 1.1894361972808838\n",
      "Validation: Epoch [1], Batch [161/938], Loss: 1.0621302127838135\n",
      "Validation: Epoch [1], Batch [162/938], Loss: 1.0133408308029175\n",
      "Validation: Epoch [1], Batch [163/938], Loss: 1.0483717918395996\n",
      "Validation: Epoch [1], Batch [164/938], Loss: 1.0704201459884644\n",
      "Validation: Epoch [1], Batch [165/938], Loss: 1.131197214126587\n",
      "Validation: Epoch [1], Batch [166/938], Loss: 1.2234787940979004\n",
      "Validation: Epoch [1], Batch [167/938], Loss: 1.0998131036758423\n",
      "Validation: Epoch [1], Batch [168/938], Loss: 1.1064077615737915\n",
      "Validation: Epoch [1], Batch [169/938], Loss: 1.1254793405532837\n",
      "Validation: Epoch [1], Batch [170/938], Loss: 1.238940954208374\n",
      "Validation: Epoch [1], Batch [171/938], Loss: 1.170609712600708\n",
      "Validation: Epoch [1], Batch [172/938], Loss: 1.1724210977554321\n",
      "Validation: Epoch [1], Batch [173/938], Loss: 1.0522327423095703\n",
      "Validation: Epoch [1], Batch [174/938], Loss: 1.1222115755081177\n",
      "Validation: Epoch [1], Batch [175/938], Loss: 1.1521457433700562\n",
      "Validation: Epoch [1], Batch [176/938], Loss: 1.229388952255249\n",
      "Validation: Epoch [1], Batch [177/938], Loss: 1.005159854888916\n",
      "Validation: Epoch [1], Batch [178/938], Loss: 1.1036272048950195\n",
      "Validation: Epoch [1], Batch [179/938], Loss: 1.0133609771728516\n",
      "Validation: Epoch [1], Batch [180/938], Loss: 1.0830684900283813\n",
      "Validation: Epoch [1], Batch [181/938], Loss: 1.0734834671020508\n",
      "Validation: Epoch [1], Batch [182/938], Loss: 1.2607322931289673\n",
      "Validation: Epoch [1], Batch [183/938], Loss: 1.1674504280090332\n",
      "Validation: Epoch [1], Batch [184/938], Loss: 1.052443265914917\n",
      "Validation: Epoch [1], Batch [185/938], Loss: 1.0367357730865479\n",
      "Validation: Epoch [1], Batch [186/938], Loss: 1.1249310970306396\n",
      "Validation: Epoch [1], Batch [187/938], Loss: 1.0239670276641846\n",
      "Validation: Epoch [1], Batch [188/938], Loss: 1.0957380533218384\n",
      "Validation: Epoch [1], Batch [189/938], Loss: 1.2019619941711426\n",
      "Validation: Epoch [1], Batch [190/938], Loss: 1.2114884853363037\n",
      "Validation: Epoch [1], Batch [191/938], Loss: 1.0404709577560425\n",
      "Validation: Epoch [1], Batch [192/938], Loss: 1.0129146575927734\n",
      "Validation: Epoch [1], Batch [193/938], Loss: 1.0882402658462524\n",
      "Validation: Epoch [1], Batch [194/938], Loss: 1.1477994918823242\n",
      "Validation: Epoch [1], Batch [195/938], Loss: 1.0684785842895508\n",
      "Validation: Epoch [1], Batch [196/938], Loss: 1.150642991065979\n",
      "Validation: Epoch [1], Batch [197/938], Loss: 1.1003973484039307\n",
      "Validation: Epoch [1], Batch [198/938], Loss: 1.0876551866531372\n",
      "Validation: Epoch [1], Batch [199/938], Loss: 1.212829351425171\n",
      "Validation: Epoch [1], Batch [200/938], Loss: 1.013930320739746\n",
      "Validation: Epoch [1], Batch [201/938], Loss: 1.094996690750122\n",
      "Validation: Epoch [1], Batch [202/938], Loss: 1.0526883602142334\n",
      "Validation: Epoch [1], Batch [203/938], Loss: 1.1772443056106567\n",
      "Validation: Epoch [1], Batch [204/938], Loss: 1.0632526874542236\n",
      "Validation: Epoch [1], Batch [205/938], Loss: 1.1418523788452148\n",
      "Validation: Epoch [1], Batch [206/938], Loss: 1.2792174816131592\n",
      "Validation: Epoch [1], Batch [207/938], Loss: 1.0954562425613403\n",
      "Validation: Epoch [1], Batch [208/938], Loss: 1.0829294919967651\n",
      "Validation: Epoch [1], Batch [209/938], Loss: 1.077157735824585\n",
      "Validation: Epoch [1], Batch [210/938], Loss: 1.0442216396331787\n",
      "Validation: Epoch [1], Batch [211/938], Loss: 1.1995084285736084\n",
      "Validation: Epoch [1], Batch [212/938], Loss: 1.198913812637329\n",
      "Validation: Epoch [1], Batch [213/938], Loss: 1.0575884580612183\n",
      "Validation: Epoch [1], Batch [214/938], Loss: 1.1629867553710938\n",
      "Validation: Epoch [1], Batch [215/938], Loss: 1.0266821384429932\n",
      "Validation: Epoch [1], Batch [216/938], Loss: 1.0247620344161987\n",
      "Validation: Epoch [1], Batch [217/938], Loss: 1.1599063873291016\n",
      "Validation: Epoch [1], Batch [218/938], Loss: 1.1719733476638794\n",
      "Validation: Epoch [1], Batch [219/938], Loss: 1.2180536985397339\n",
      "Validation: Epoch [1], Batch [220/938], Loss: 1.1619620323181152\n",
      "Validation: Epoch [1], Batch [221/938], Loss: 1.0254695415496826\n",
      "Validation: Epoch [1], Batch [222/938], Loss: 1.1649360656738281\n",
      "Validation: Epoch [1], Batch [223/938], Loss: 1.1280497312545776\n",
      "Validation: Epoch [1], Batch [224/938], Loss: 1.2484774589538574\n",
      "Validation: Epoch [1], Batch [225/938], Loss: 1.143977165222168\n",
      "Validation: Epoch [1], Batch [226/938], Loss: 1.1020617485046387\n",
      "Validation: Epoch [1], Batch [227/938], Loss: 1.1316053867340088\n",
      "Validation: Epoch [1], Batch [228/938], Loss: 1.166882038116455\n",
      "Validation: Epoch [1], Batch [229/938], Loss: 1.1182278394699097\n",
      "Validation: Epoch [1], Batch [230/938], Loss: 1.0988032817840576\n",
      "Validation: Epoch [1], Batch [231/938], Loss: 1.0335837602615356\n",
      "Validation: Epoch [1], Batch [232/938], Loss: 0.9538617730140686\n",
      "Validation: Epoch [1], Batch [233/938], Loss: 1.2104363441467285\n",
      "Validation: Epoch [1], Batch [234/938], Loss: 1.0978119373321533\n",
      "Validation: Epoch [1], Batch [235/938], Loss: 1.0356032848358154\n",
      "Validation: Epoch [1], Batch [236/938], Loss: 1.110145926475525\n",
      "Validation: Epoch [1], Batch [237/938], Loss: 1.0268174409866333\n",
      "Validation: Epoch [1], Batch [238/938], Loss: 1.1491692066192627\n",
      "Validation: Epoch [1], Batch [239/938], Loss: 0.9407603144645691\n",
      "Validation: Epoch [1], Batch [240/938], Loss: 1.0405328273773193\n",
      "Validation: Epoch [1], Batch [241/938], Loss: 1.0234365463256836\n",
      "Validation: Epoch [1], Batch [242/938], Loss: 1.0893458127975464\n",
      "Validation: Epoch [1], Batch [243/938], Loss: 1.030031442642212\n",
      "Validation: Epoch [1], Batch [244/938], Loss: 1.3063976764678955\n",
      "Validation: Epoch [1], Batch [245/938], Loss: 1.1179494857788086\n",
      "Validation: Epoch [1], Batch [246/938], Loss: 1.2030166387557983\n",
      "Validation: Epoch [1], Batch [247/938], Loss: 1.1787652969360352\n",
      "Validation: Epoch [1], Batch [248/938], Loss: 1.032492995262146\n",
      "Validation: Epoch [1], Batch [249/938], Loss: 0.9794180989265442\n",
      "Validation: Epoch [1], Batch [250/938], Loss: 1.0881845951080322\n",
      "Validation: Epoch [1], Batch [251/938], Loss: 1.1313493251800537\n",
      "Validation: Epoch [1], Batch [252/938], Loss: 1.0201102495193481\n",
      "Validation: Epoch [1], Batch [253/938], Loss: 1.1471893787384033\n",
      "Validation: Epoch [1], Batch [254/938], Loss: 1.1330031156539917\n",
      "Validation: Epoch [1], Batch [255/938], Loss: 1.2182714939117432\n",
      "Validation: Epoch [1], Batch [256/938], Loss: 1.1115317344665527\n",
      "Validation: Epoch [1], Batch [257/938], Loss: 1.133776068687439\n",
      "Validation: Epoch [1], Batch [258/938], Loss: 1.1180287599563599\n",
      "Validation: Epoch [1], Batch [259/938], Loss: 1.1816028356552124\n",
      "Validation: Epoch [1], Batch [260/938], Loss: 1.1487088203430176\n",
      "Validation: Epoch [1], Batch [261/938], Loss: 1.1983283758163452\n",
      "Validation: Epoch [1], Batch [262/938], Loss: 1.1009161472320557\n",
      "Validation: Epoch [1], Batch [263/938], Loss: 1.0356709957122803\n",
      "Validation: Epoch [1], Batch [264/938], Loss: 1.1644835472106934\n",
      "Validation: Epoch [1], Batch [265/938], Loss: 1.0824278593063354\n",
      "Validation: Epoch [1], Batch [266/938], Loss: 1.0468509197235107\n",
      "Validation: Epoch [1], Batch [267/938], Loss: 1.0185801982879639\n",
      "Validation: Epoch [1], Batch [268/938], Loss: 1.1025431156158447\n",
      "Validation: Epoch [1], Batch [269/938], Loss: 0.9929554462432861\n",
      "Validation: Epoch [1], Batch [270/938], Loss: 1.0445908308029175\n",
      "Validation: Epoch [1], Batch [271/938], Loss: 1.085026741027832\n",
      "Validation: Epoch [1], Batch [272/938], Loss: 1.1513781547546387\n",
      "Validation: Epoch [1], Batch [273/938], Loss: 1.0302143096923828\n",
      "Validation: Epoch [1], Batch [274/938], Loss: 1.1479687690734863\n",
      "Validation: Epoch [1], Batch [275/938], Loss: 1.0588128566741943\n",
      "Validation: Epoch [1], Batch [276/938], Loss: 1.0684771537780762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [277/938], Loss: 1.3520984649658203\n",
      "Validation: Epoch [1], Batch [278/938], Loss: 1.0611166954040527\n",
      "Validation: Epoch [1], Batch [279/938], Loss: 1.2075257301330566\n",
      "Validation: Epoch [1], Batch [280/938], Loss: 1.0839920043945312\n",
      "Validation: Epoch [1], Batch [281/938], Loss: 0.9853579998016357\n",
      "Validation: Epoch [1], Batch [282/938], Loss: 1.0556433200836182\n",
      "Validation: Epoch [1], Batch [283/938], Loss: 1.0401763916015625\n",
      "Validation: Epoch [1], Batch [284/938], Loss: 1.2293972969055176\n",
      "Validation: Epoch [1], Batch [285/938], Loss: 1.0129700899124146\n",
      "Validation: Epoch [1], Batch [286/938], Loss: 1.0635952949523926\n",
      "Validation: Epoch [1], Batch [287/938], Loss: 1.0454089641571045\n",
      "Validation: Epoch [1], Batch [288/938], Loss: 1.1388177871704102\n",
      "Validation: Epoch [1], Batch [289/938], Loss: 1.31378173828125\n",
      "Validation: Epoch [1], Batch [290/938], Loss: 1.1662392616271973\n",
      "Validation: Epoch [1], Batch [291/938], Loss: 1.2155990600585938\n",
      "Validation: Epoch [1], Batch [292/938], Loss: 1.1940813064575195\n",
      "Validation: Epoch [1], Batch [293/938], Loss: 1.0871222019195557\n",
      "Validation: Epoch [1], Batch [294/938], Loss: 1.0481255054473877\n",
      "Validation: Epoch [1], Batch [295/938], Loss: 1.0535168647766113\n",
      "Validation: Epoch [1], Batch [296/938], Loss: 1.1410127878189087\n",
      "Validation: Epoch [1], Batch [297/938], Loss: 1.1211941242218018\n",
      "Validation: Epoch [1], Batch [298/938], Loss: 1.0532233715057373\n",
      "Validation: Epoch [1], Batch [299/938], Loss: 1.1347286701202393\n",
      "Validation: Epoch [1], Batch [300/938], Loss: 1.2790215015411377\n",
      "Validation: Epoch [1], Batch [301/938], Loss: 1.2081634998321533\n",
      "Validation: Epoch [1], Batch [302/938], Loss: 1.2653862237930298\n",
      "Validation: Epoch [1], Batch [303/938], Loss: 1.288198471069336\n",
      "Validation: Epoch [1], Batch [304/938], Loss: 1.2733935117721558\n",
      "Validation: Epoch [1], Batch [305/938], Loss: 1.08555269241333\n",
      "Validation: Epoch [1], Batch [306/938], Loss: 1.0746567249298096\n",
      "Validation: Epoch [1], Batch [307/938], Loss: 1.038313865661621\n",
      "Validation: Epoch [1], Batch [308/938], Loss: 1.0865648984909058\n",
      "Validation: Epoch [1], Batch [309/938], Loss: 1.1319259405136108\n",
      "Validation: Epoch [1], Batch [310/938], Loss: 1.0735918283462524\n",
      "Validation: Epoch [1], Batch [311/938], Loss: 1.089073657989502\n",
      "Validation: Epoch [1], Batch [312/938], Loss: 1.0188343524932861\n",
      "Validation: Epoch [1], Batch [313/938], Loss: 1.1280454397201538\n",
      "Validation: Epoch [1], Batch [314/938], Loss: 1.1199467182159424\n",
      "Validation: Epoch [1], Batch [315/938], Loss: 1.0763707160949707\n",
      "Validation: Epoch [1], Batch [316/938], Loss: 1.1488229036331177\n",
      "Validation: Epoch [1], Batch [317/938], Loss: 1.0862151384353638\n",
      "Validation: Epoch [1], Batch [318/938], Loss: 1.0352702140808105\n",
      "Validation: Epoch [1], Batch [319/938], Loss: 1.0334932804107666\n",
      "Validation: Epoch [1], Batch [320/938], Loss: 1.1531925201416016\n",
      "Validation: Epoch [1], Batch [321/938], Loss: 1.1608167886734009\n",
      "Validation: Epoch [1], Batch [322/938], Loss: 1.1851557493209839\n",
      "Validation: Epoch [1], Batch [323/938], Loss: 1.059755802154541\n",
      "Validation: Epoch [1], Batch [324/938], Loss: 0.9791382551193237\n",
      "Validation: Epoch [1], Batch [325/938], Loss: 1.076058268547058\n",
      "Validation: Epoch [1], Batch [326/938], Loss: 1.053381323814392\n",
      "Validation: Epoch [1], Batch [327/938], Loss: 1.1606591939926147\n",
      "Validation: Epoch [1], Batch [328/938], Loss: 1.0559380054473877\n",
      "Validation: Epoch [1], Batch [329/938], Loss: 1.1605031490325928\n",
      "Validation: Epoch [1], Batch [330/938], Loss: 1.1016111373901367\n",
      "Validation: Epoch [1], Batch [331/938], Loss: 1.0805367231369019\n",
      "Validation: Epoch [1], Batch [332/938], Loss: 1.0914816856384277\n",
      "Validation: Epoch [1], Batch [333/938], Loss: 1.194394588470459\n",
      "Validation: Epoch [1], Batch [334/938], Loss: 1.2623717784881592\n",
      "Validation: Epoch [1], Batch [335/938], Loss: 0.9916375875473022\n",
      "Validation: Epoch [1], Batch [336/938], Loss: 1.0258581638336182\n",
      "Validation: Epoch [1], Batch [337/938], Loss: 1.0882153511047363\n",
      "Validation: Epoch [1], Batch [338/938], Loss: 1.2002121210098267\n",
      "Validation: Epoch [1], Batch [339/938], Loss: 1.020432710647583\n",
      "Validation: Epoch [1], Batch [340/938], Loss: 1.0847786664962769\n",
      "Validation: Epoch [1], Batch [341/938], Loss: 1.1806720495224\n",
      "Validation: Epoch [1], Batch [342/938], Loss: 1.0740139484405518\n",
      "Validation: Epoch [1], Batch [343/938], Loss: 1.1934638023376465\n",
      "Validation: Epoch [1], Batch [344/938], Loss: 0.9880632162094116\n",
      "Validation: Epoch [1], Batch [345/938], Loss: 1.0890638828277588\n",
      "Validation: Epoch [1], Batch [346/938], Loss: 1.1282007694244385\n",
      "Validation: Epoch [1], Batch [347/938], Loss: 1.1140124797821045\n",
      "Validation: Epoch [1], Batch [348/938], Loss: 1.1540895700454712\n",
      "Validation: Epoch [1], Batch [349/938], Loss: 1.0897512435913086\n",
      "Validation: Epoch [1], Batch [350/938], Loss: 1.11676025390625\n",
      "Validation: Epoch [1], Batch [351/938], Loss: 1.1171815395355225\n",
      "Validation: Epoch [1], Batch [352/938], Loss: 1.0994222164154053\n",
      "Validation: Epoch [1], Batch [353/938], Loss: 1.0229966640472412\n",
      "Validation: Epoch [1], Batch [354/938], Loss: 1.262157678604126\n",
      "Validation: Epoch [1], Batch [355/938], Loss: 1.0165057182312012\n",
      "Validation: Epoch [1], Batch [356/938], Loss: 1.078157663345337\n",
      "Validation: Epoch [1], Batch [357/938], Loss: 1.0215559005737305\n",
      "Validation: Epoch [1], Batch [358/938], Loss: 1.289505124092102\n",
      "Validation: Epoch [1], Batch [359/938], Loss: 1.0762830972671509\n",
      "Validation: Epoch [1], Batch [360/938], Loss: 1.1117804050445557\n",
      "Validation: Epoch [1], Batch [361/938], Loss: 1.2116727828979492\n",
      "Validation: Epoch [1], Batch [362/938], Loss: 1.1022390127182007\n",
      "Validation: Epoch [1], Batch [363/938], Loss: 1.2594997882843018\n",
      "Validation: Epoch [1], Batch [364/938], Loss: 1.154995322227478\n",
      "Validation: Epoch [1], Batch [365/938], Loss: 1.1267001628875732\n",
      "Validation: Epoch [1], Batch [366/938], Loss: 1.1316757202148438\n",
      "Validation: Epoch [1], Batch [367/938], Loss: 1.2045302391052246\n",
      "Validation: Epoch [1], Batch [368/938], Loss: 0.9520411491394043\n",
      "Validation: Epoch [1], Batch [369/938], Loss: 1.2311818599700928\n",
      "Validation: Epoch [1], Batch [370/938], Loss: 1.1011017560958862\n",
      "Validation: Epoch [1], Batch [371/938], Loss: 1.1106650829315186\n",
      "Validation: Epoch [1], Batch [372/938], Loss: 1.0402977466583252\n",
      "Validation: Epoch [1], Batch [373/938], Loss: 1.1670347452163696\n",
      "Validation: Epoch [1], Batch [374/938], Loss: 1.180619478225708\n",
      "Validation: Epoch [1], Batch [375/938], Loss: 1.1862671375274658\n",
      "Validation: Epoch [1], Batch [376/938], Loss: 1.439526081085205\n",
      "Validation: Epoch [1], Batch [377/938], Loss: 1.0888136625289917\n",
      "Validation: Epoch [1], Batch [378/938], Loss: 1.117966651916504\n",
      "Validation: Epoch [1], Batch [379/938], Loss: 1.045074224472046\n",
      "Validation: Epoch [1], Batch [380/938], Loss: 1.0858979225158691\n",
      "Validation: Epoch [1], Batch [381/938], Loss: 1.113055944442749\n",
      "Validation: Epoch [1], Batch [382/938], Loss: 1.1857402324676514\n",
      "Validation: Epoch [1], Batch [383/938], Loss: 1.1574668884277344\n",
      "Validation: Epoch [1], Batch [384/938], Loss: 1.0866750478744507\n",
      "Validation: Epoch [1], Batch [385/938], Loss: 1.1338942050933838\n",
      "Validation: Epoch [1], Batch [386/938], Loss: 1.1985111236572266\n",
      "Validation: Epoch [1], Batch [387/938], Loss: 1.1036183834075928\n",
      "Validation: Epoch [1], Batch [388/938], Loss: 1.15907621383667\n",
      "Validation: Epoch [1], Batch [389/938], Loss: 0.9851704239845276\n",
      "Validation: Epoch [1], Batch [390/938], Loss: 1.1618338823318481\n",
      "Validation: Epoch [1], Batch [391/938], Loss: 1.1856070756912231\n",
      "Validation: Epoch [1], Batch [392/938], Loss: 1.0841023921966553\n",
      "Validation: Epoch [1], Batch [393/938], Loss: 1.1206386089324951\n",
      "Validation: Epoch [1], Batch [394/938], Loss: 1.0405431985855103\n",
      "Validation: Epoch [1], Batch [395/938], Loss: 1.1494195461273193\n",
      "Validation: Epoch [1], Batch [396/938], Loss: 1.1168200969696045\n",
      "Validation: Epoch [1], Batch [397/938], Loss: 1.0637580156326294\n",
      "Validation: Epoch [1], Batch [398/938], Loss: 1.1725177764892578\n",
      "Validation: Epoch [1], Batch [399/938], Loss: 1.0673422813415527\n",
      "Validation: Epoch [1], Batch [400/938], Loss: 1.1096904277801514\n",
      "Validation: Epoch [1], Batch [401/938], Loss: 1.0088893175125122\n",
      "Validation: Epoch [1], Batch [402/938], Loss: 1.311030387878418\n",
      "Validation: Epoch [1], Batch [403/938], Loss: 1.3663432598114014\n",
      "Validation: Epoch [1], Batch [404/938], Loss: 1.017600178718567\n",
      "Validation: Epoch [1], Batch [405/938], Loss: 1.146822452545166\n",
      "Validation: Epoch [1], Batch [406/938], Loss: 1.1226098537445068\n",
      "Validation: Epoch [1], Batch [407/938], Loss: 1.123654842376709\n",
      "Validation: Epoch [1], Batch [408/938], Loss: 1.1152901649475098\n",
      "Validation: Epoch [1], Batch [409/938], Loss: 1.1378071308135986\n",
      "Validation: Epoch [1], Batch [410/938], Loss: 1.1679991483688354\n",
      "Validation: Epoch [1], Batch [411/938], Loss: 1.2249608039855957\n",
      "Validation: Epoch [1], Batch [412/938], Loss: 1.13140869140625\n",
      "Validation: Epoch [1], Batch [413/938], Loss: 1.023376703262329\n",
      "Validation: Epoch [1], Batch [414/938], Loss: 1.2071447372436523\n",
      "Validation: Epoch [1], Batch [415/938], Loss: 1.0598788261413574\n",
      "Validation: Epoch [1], Batch [416/938], Loss: 1.217565894126892\n",
      "Validation: Epoch [1], Batch [417/938], Loss: 1.1201519966125488\n",
      "Validation: Epoch [1], Batch [418/938], Loss: 1.1837821006774902\n",
      "Validation: Epoch [1], Batch [419/938], Loss: 1.064683198928833\n",
      "Validation: Epoch [1], Batch [420/938], Loss: 1.0996019840240479\n",
      "Validation: Epoch [1], Batch [421/938], Loss: 1.216513752937317\n",
      "Validation: Epoch [1], Batch [422/938], Loss: 1.145088791847229\n",
      "Validation: Epoch [1], Batch [423/938], Loss: 1.055876612663269\n",
      "Validation: Epoch [1], Batch [424/938], Loss: 1.0446529388427734\n",
      "Validation: Epoch [1], Batch [425/938], Loss: 1.0556174516677856\n",
      "Validation: Epoch [1], Batch [426/938], Loss: 1.026039481163025\n",
      "Validation: Epoch [1], Batch [427/938], Loss: 0.967475414276123\n",
      "Validation: Epoch [1], Batch [428/938], Loss: 1.118802547454834\n",
      "Validation: Epoch [1], Batch [429/938], Loss: 1.0533676147460938\n",
      "Validation: Epoch [1], Batch [430/938], Loss: 1.2081406116485596\n",
      "Validation: Epoch [1], Batch [431/938], Loss: 1.2203001976013184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [432/938], Loss: 0.9727885127067566\n",
      "Validation: Epoch [1], Batch [433/938], Loss: 1.133719801902771\n",
      "Validation: Epoch [1], Batch [434/938], Loss: 1.0160142183303833\n",
      "Validation: Epoch [1], Batch [435/938], Loss: 1.021432638168335\n",
      "Validation: Epoch [1], Batch [436/938], Loss: 1.2081165313720703\n",
      "Validation: Epoch [1], Batch [437/938], Loss: 1.048779845237732\n",
      "Validation: Epoch [1], Batch [438/938], Loss: 1.2073638439178467\n",
      "Validation: Epoch [1], Batch [439/938], Loss: 1.0940210819244385\n",
      "Validation: Epoch [1], Batch [440/938], Loss: 0.9674583673477173\n",
      "Validation: Epoch [1], Batch [441/938], Loss: 1.1826603412628174\n",
      "Validation: Epoch [1], Batch [442/938], Loss: 1.0427124500274658\n",
      "Validation: Epoch [1], Batch [443/938], Loss: 1.2036807537078857\n",
      "Validation: Epoch [1], Batch [444/938], Loss: 1.2476084232330322\n",
      "Validation: Epoch [1], Batch [445/938], Loss: 1.1238641738891602\n",
      "Validation: Epoch [1], Batch [446/938], Loss: 1.0468813180923462\n",
      "Validation: Epoch [1], Batch [447/938], Loss: 1.1316734552383423\n",
      "Validation: Epoch [1], Batch [448/938], Loss: 1.2094738483428955\n",
      "Validation: Epoch [1], Batch [449/938], Loss: 1.3061233758926392\n",
      "Validation: Epoch [1], Batch [450/938], Loss: 1.0302555561065674\n",
      "Validation: Epoch [1], Batch [451/938], Loss: 1.217786431312561\n",
      "Validation: Epoch [1], Batch [452/938], Loss: 1.0928022861480713\n",
      "Validation: Epoch [1], Batch [453/938], Loss: 1.000561237335205\n",
      "Validation: Epoch [1], Batch [454/938], Loss: 1.0943595170974731\n",
      "Validation: Epoch [1], Batch [455/938], Loss: 1.238165020942688\n",
      "Validation: Epoch [1], Batch [456/938], Loss: 1.1364424228668213\n",
      "Validation: Epoch [1], Batch [457/938], Loss: 1.2397935390472412\n",
      "Validation: Epoch [1], Batch [458/938], Loss: 1.1087563037872314\n",
      "Validation: Epoch [1], Batch [459/938], Loss: 1.0248119831085205\n",
      "Validation: Epoch [1], Batch [460/938], Loss: 1.0862503051757812\n",
      "Validation: Epoch [1], Batch [461/938], Loss: 1.101041316986084\n",
      "Validation: Epoch [1], Batch [462/938], Loss: 1.1121386289596558\n",
      "Validation: Epoch [1], Batch [463/938], Loss: 1.0148587226867676\n",
      "Validation: Epoch [1], Batch [464/938], Loss: 1.0685817003250122\n",
      "Validation: Epoch [1], Batch [465/938], Loss: 1.1117959022521973\n",
      "Validation: Epoch [1], Batch [466/938], Loss: 1.0894094705581665\n",
      "Validation: Epoch [1], Batch [467/938], Loss: 1.0505694150924683\n",
      "Validation: Epoch [1], Batch [468/938], Loss: 1.2118985652923584\n",
      "Validation: Epoch [1], Batch [469/938], Loss: 1.1369593143463135\n",
      "Validation: Epoch [1], Batch [470/938], Loss: 1.067792534828186\n",
      "Validation: Epoch [1], Batch [471/938], Loss: 1.1377867460250854\n",
      "Validation: Epoch [1], Batch [472/938], Loss: 1.0551331043243408\n",
      "Validation: Epoch [1], Batch [473/938], Loss: 1.0467066764831543\n",
      "Validation: Epoch [1], Batch [474/938], Loss: 1.1208302974700928\n",
      "Validation: Epoch [1], Batch [475/938], Loss: 1.0879957675933838\n",
      "Validation: Epoch [1], Batch [476/938], Loss: 1.0920679569244385\n",
      "Validation: Epoch [1], Batch [477/938], Loss: 1.0563801527023315\n",
      "Validation: Epoch [1], Batch [478/938], Loss: 1.1363582611083984\n",
      "Validation: Epoch [1], Batch [479/938], Loss: 1.108894944190979\n",
      "Validation: Epoch [1], Batch [480/938], Loss: 1.0666418075561523\n",
      "Validation: Epoch [1], Batch [481/938], Loss: 0.9847198724746704\n",
      "Validation: Epoch [1], Batch [482/938], Loss: 1.1949224472045898\n",
      "Validation: Epoch [1], Batch [483/938], Loss: 1.0899187326431274\n",
      "Validation: Epoch [1], Batch [484/938], Loss: 1.085777997970581\n",
      "Validation: Epoch [1], Batch [485/938], Loss: 1.2275487184524536\n",
      "Validation: Epoch [1], Batch [486/938], Loss: 1.098081111907959\n",
      "Validation: Epoch [1], Batch [487/938], Loss: 1.0562183856964111\n",
      "Validation: Epoch [1], Batch [488/938], Loss: 1.1450650691986084\n",
      "Validation: Epoch [1], Batch [489/938], Loss: 1.0405864715576172\n",
      "Validation: Epoch [1], Batch [490/938], Loss: 1.1713988780975342\n",
      "Validation: Epoch [1], Batch [491/938], Loss: 1.042227029800415\n",
      "Validation: Epoch [1], Batch [492/938], Loss: 1.1164708137512207\n",
      "Validation: Epoch [1], Batch [493/938], Loss: 1.1918766498565674\n",
      "Validation: Epoch [1], Batch [494/938], Loss: 1.165640115737915\n",
      "Validation: Epoch [1], Batch [495/938], Loss: 1.1755839586257935\n",
      "Validation: Epoch [1], Batch [496/938], Loss: 1.3560847043991089\n",
      "Validation: Epoch [1], Batch [497/938], Loss: 1.0166513919830322\n",
      "Validation: Epoch [1], Batch [498/938], Loss: 1.2019298076629639\n",
      "Validation: Epoch [1], Batch [499/938], Loss: 1.1005406379699707\n",
      "Validation: Epoch [1], Batch [500/938], Loss: 1.114463448524475\n",
      "Validation: Epoch [1], Batch [501/938], Loss: 1.0950570106506348\n",
      "Validation: Epoch [1], Batch [502/938], Loss: 1.2309315204620361\n",
      "Validation: Epoch [1], Batch [503/938], Loss: 1.172149419784546\n",
      "Validation: Epoch [1], Batch [504/938], Loss: 1.0649735927581787\n",
      "Validation: Epoch [1], Batch [505/938], Loss: 1.183457612991333\n",
      "Validation: Epoch [1], Batch [506/938], Loss: 1.0195538997650146\n",
      "Validation: Epoch [1], Batch [507/938], Loss: 1.1213608980178833\n",
      "Validation: Epoch [1], Batch [508/938], Loss: 1.1231993436813354\n",
      "Validation: Epoch [1], Batch [509/938], Loss: 1.136715054512024\n",
      "Validation: Epoch [1], Batch [510/938], Loss: 1.1919790506362915\n",
      "Validation: Epoch [1], Batch [511/938], Loss: 1.1647608280181885\n",
      "Validation: Epoch [1], Batch [512/938], Loss: 0.9810343980789185\n",
      "Validation: Epoch [1], Batch [513/938], Loss: 1.172885775566101\n",
      "Validation: Epoch [1], Batch [514/938], Loss: 0.9769654870033264\n",
      "Validation: Epoch [1], Batch [515/938], Loss: 1.1482775211334229\n",
      "Validation: Epoch [1], Batch [516/938], Loss: 1.0798543691635132\n",
      "Validation: Epoch [1], Batch [517/938], Loss: 1.1205215454101562\n",
      "Validation: Epoch [1], Batch [518/938], Loss: 1.0939109325408936\n",
      "Validation: Epoch [1], Batch [519/938], Loss: 1.098737359046936\n",
      "Validation: Epoch [1], Batch [520/938], Loss: 0.9745615720748901\n",
      "Validation: Epoch [1], Batch [521/938], Loss: 1.0963307619094849\n",
      "Validation: Epoch [1], Batch [522/938], Loss: 1.0704317092895508\n",
      "Validation: Epoch [1], Batch [523/938], Loss: 1.053802490234375\n",
      "Validation: Epoch [1], Batch [524/938], Loss: 1.1440244913101196\n",
      "Validation: Epoch [1], Batch [525/938], Loss: 1.1357216835021973\n",
      "Validation: Epoch [1], Batch [526/938], Loss: 1.1663634777069092\n",
      "Validation: Epoch [1], Batch [527/938], Loss: 1.1311684846878052\n",
      "Validation: Epoch [1], Batch [528/938], Loss: 1.1815729141235352\n",
      "Validation: Epoch [1], Batch [529/938], Loss: 1.3095344305038452\n",
      "Validation: Epoch [1], Batch [530/938], Loss: 1.0619163513183594\n",
      "Validation: Epoch [1], Batch [531/938], Loss: 1.1433486938476562\n",
      "Validation: Epoch [1], Batch [532/938], Loss: 1.0756522417068481\n",
      "Validation: Epoch [1], Batch [533/938], Loss: 1.125938892364502\n",
      "Validation: Epoch [1], Batch [534/938], Loss: 1.1282448768615723\n",
      "Validation: Epoch [1], Batch [535/938], Loss: 1.1162607669830322\n",
      "Validation: Epoch [1], Batch [536/938], Loss: 1.1287305355072021\n",
      "Validation: Epoch [1], Batch [537/938], Loss: 1.0697972774505615\n",
      "Validation: Epoch [1], Batch [538/938], Loss: 1.124265193939209\n",
      "Validation: Epoch [1], Batch [539/938], Loss: 1.1071205139160156\n",
      "Validation: Epoch [1], Batch [540/938], Loss: 1.0637112855911255\n",
      "Validation: Epoch [1], Batch [541/938], Loss: 1.1703811883926392\n",
      "Validation: Epoch [1], Batch [542/938], Loss: 1.2739717960357666\n",
      "Validation: Epoch [1], Batch [543/938], Loss: 1.0803430080413818\n",
      "Validation: Epoch [1], Batch [544/938], Loss: 1.0646512508392334\n",
      "Validation: Epoch [1], Batch [545/938], Loss: 1.0685100555419922\n",
      "Validation: Epoch [1], Batch [546/938], Loss: 0.9960201382637024\n",
      "Validation: Epoch [1], Batch [547/938], Loss: 1.109762191772461\n",
      "Validation: Epoch [1], Batch [548/938], Loss: 1.1130766868591309\n",
      "Validation: Epoch [1], Batch [549/938], Loss: 1.1022084951400757\n",
      "Validation: Epoch [1], Batch [550/938], Loss: 1.121595859527588\n",
      "Validation: Epoch [1], Batch [551/938], Loss: 1.0104761123657227\n",
      "Validation: Epoch [1], Batch [552/938], Loss: 0.9594353437423706\n",
      "Validation: Epoch [1], Batch [553/938], Loss: 1.0864148139953613\n",
      "Validation: Epoch [1], Batch [554/938], Loss: 1.0467748641967773\n",
      "Validation: Epoch [1], Batch [555/938], Loss: 1.1026246547698975\n",
      "Validation: Epoch [1], Batch [556/938], Loss: 1.0483955144882202\n",
      "Validation: Epoch [1], Batch [557/938], Loss: 1.0706086158752441\n",
      "Validation: Epoch [1], Batch [558/938], Loss: 1.0706713199615479\n",
      "Validation: Epoch [1], Batch [559/938], Loss: 1.075225591659546\n",
      "Validation: Epoch [1], Batch [560/938], Loss: 1.1424198150634766\n",
      "Validation: Epoch [1], Batch [561/938], Loss: 1.2735767364501953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [562/938], Loss: 1.040388822555542\n",
      "Validation: Epoch [1], Batch [563/938], Loss: 1.1843550205230713\n",
      "Validation: Epoch [1], Batch [564/938], Loss: 1.2917001247406006\n",
      "Validation: Epoch [1], Batch [565/938], Loss: 1.0482759475708008\n",
      "Validation: Epoch [1], Batch [566/938], Loss: 1.1983044147491455\n",
      "Validation: Epoch [1], Batch [567/938], Loss: 1.1433370113372803\n",
      "Validation: Epoch [1], Batch [568/938], Loss: 1.155626893043518\n",
      "Validation: Epoch [1], Batch [569/938], Loss: 1.1222350597381592\n",
      "Validation: Epoch [1], Batch [570/938], Loss: 1.1597706079483032\n",
      "Validation: Epoch [1], Batch [571/938], Loss: 1.0977028608322144\n",
      "Validation: Epoch [1], Batch [572/938], Loss: 1.1258220672607422\n",
      "Validation: Epoch [1], Batch [573/938], Loss: 1.1291543245315552\n",
      "Validation: Epoch [1], Batch [574/938], Loss: 1.196868896484375\n",
      "Validation: Epoch [1], Batch [575/938], Loss: 1.1126277446746826\n",
      "Validation: Epoch [1], Batch [576/938], Loss: 1.0980920791625977\n",
      "Validation: Epoch [1], Batch [577/938], Loss: 1.070847749710083\n",
      "Validation: Epoch [1], Batch [578/938], Loss: 1.224181056022644\n",
      "Validation: Epoch [1], Batch [579/938], Loss: 1.0187855958938599\n",
      "Validation: Epoch [1], Batch [580/938], Loss: 1.1019822359085083\n",
      "Validation: Epoch [1], Batch [581/938], Loss: 1.1119863986968994\n",
      "Validation: Epoch [1], Batch [582/938], Loss: 1.1752445697784424\n",
      "Validation: Epoch [1], Batch [583/938], Loss: 1.2051632404327393\n",
      "Validation: Epoch [1], Batch [584/938], Loss: 1.2643871307373047\n",
      "Validation: Epoch [1], Batch [585/938], Loss: 1.154416799545288\n",
      "Validation: Epoch [1], Batch [586/938], Loss: 0.983738362789154\n",
      "Validation: Epoch [1], Batch [587/938], Loss: 1.1105356216430664\n",
      "Validation: Epoch [1], Batch [588/938], Loss: 1.1147538423538208\n",
      "Validation: Epoch [1], Batch [589/938], Loss: 0.9936206340789795\n",
      "Validation: Epoch [1], Batch [590/938], Loss: 1.0784742832183838\n",
      "Validation: Epoch [1], Batch [591/938], Loss: 1.2225651741027832\n",
      "Validation: Epoch [1], Batch [592/938], Loss: 1.186800241470337\n",
      "Validation: Epoch [1], Batch [593/938], Loss: 1.2282763719558716\n",
      "Validation: Epoch [1], Batch [594/938], Loss: 1.1250650882720947\n",
      "Validation: Epoch [1], Batch [595/938], Loss: 1.1787402629852295\n",
      "Validation: Epoch [1], Batch [596/938], Loss: 1.0202000141143799\n",
      "Validation: Epoch [1], Batch [597/938], Loss: 1.127556324005127\n",
      "Validation: Epoch [1], Batch [598/938], Loss: 1.1056101322174072\n",
      "Validation: Epoch [1], Batch [599/938], Loss: 1.1753981113433838\n",
      "Validation: Epoch [1], Batch [600/938], Loss: 1.0851655006408691\n",
      "Validation: Epoch [1], Batch [601/938], Loss: 1.1349024772644043\n",
      "Validation: Epoch [1], Batch [602/938], Loss: 1.2316863536834717\n",
      "Validation: Epoch [1], Batch [603/938], Loss: 1.1468005180358887\n",
      "Validation: Epoch [1], Batch [604/938], Loss: 1.2406085729599\n",
      "Validation: Epoch [1], Batch [605/938], Loss: 1.158846378326416\n",
      "Validation: Epoch [1], Batch [606/938], Loss: 1.0350171327590942\n",
      "Validation: Epoch [1], Batch [607/938], Loss: 1.057603120803833\n",
      "Validation: Epoch [1], Batch [608/938], Loss: 0.9202677011489868\n",
      "Validation: Epoch [1], Batch [609/938], Loss: 1.0708991289138794\n",
      "Validation: Epoch [1], Batch [610/938], Loss: 1.0696649551391602\n",
      "Validation: Epoch [1], Batch [611/938], Loss: 1.0772862434387207\n",
      "Validation: Epoch [1], Batch [612/938], Loss: 1.2976363897323608\n",
      "Validation: Epoch [1], Batch [613/938], Loss: 1.0872251987457275\n",
      "Validation: Epoch [1], Batch [614/938], Loss: 1.1280179023742676\n",
      "Validation: Epoch [1], Batch [615/938], Loss: 1.117693543434143\n",
      "Validation: Epoch [1], Batch [616/938], Loss: 1.0354156494140625\n",
      "Validation: Epoch [1], Batch [617/938], Loss: 1.0202934741973877\n",
      "Validation: Epoch [1], Batch [618/938], Loss: 1.0607606172561646\n",
      "Validation: Epoch [1], Batch [619/938], Loss: 1.0640445947647095\n",
      "Validation: Epoch [1], Batch [620/938], Loss: 1.1091750860214233\n",
      "Validation: Epoch [1], Batch [621/938], Loss: 1.0877506732940674\n",
      "Validation: Epoch [1], Batch [622/938], Loss: 1.047364592552185\n",
      "Validation: Epoch [1], Batch [623/938], Loss: 1.0170224905014038\n",
      "Validation: Epoch [1], Batch [624/938], Loss: 1.0274927616119385\n",
      "Validation: Epoch [1], Batch [625/938], Loss: 1.1241053342819214\n",
      "Validation: Epoch [1], Batch [626/938], Loss: 1.3019548654556274\n",
      "Validation: Epoch [1], Batch [627/938], Loss: 1.0692682266235352\n",
      "Validation: Epoch [1], Batch [628/938], Loss: 1.2446932792663574\n",
      "Validation: Epoch [1], Batch [629/938], Loss: 1.0993900299072266\n",
      "Validation: Epoch [1], Batch [630/938], Loss: 1.1736165285110474\n",
      "Validation: Epoch [1], Batch [631/938], Loss: 1.2327589988708496\n",
      "Validation: Epoch [1], Batch [632/938], Loss: 1.0426772832870483\n",
      "Validation: Epoch [1], Batch [633/938], Loss: 1.113631010055542\n",
      "Validation: Epoch [1], Batch [634/938], Loss: 1.0832927227020264\n",
      "Validation: Epoch [1], Batch [635/938], Loss: 1.0998938083648682\n",
      "Validation: Epoch [1], Batch [636/938], Loss: 1.0706405639648438\n",
      "Validation: Epoch [1], Batch [637/938], Loss: 1.2750896215438843\n",
      "Validation: Epoch [1], Batch [638/938], Loss: 1.1930146217346191\n",
      "Validation: Epoch [1], Batch [639/938], Loss: 1.3318610191345215\n",
      "Validation: Epoch [1], Batch [640/938], Loss: 1.1166319847106934\n",
      "Validation: Epoch [1], Batch [641/938], Loss: 1.0991606712341309\n",
      "Validation: Epoch [1], Batch [642/938], Loss: 1.1130805015563965\n",
      "Validation: Epoch [1], Batch [643/938], Loss: 1.12924325466156\n",
      "Validation: Epoch [1], Batch [644/938], Loss: 1.0146408081054688\n",
      "Validation: Epoch [1], Batch [645/938], Loss: 1.1238536834716797\n",
      "Validation: Epoch [1], Batch [646/938], Loss: 1.0824401378631592\n",
      "Validation: Epoch [1], Batch [647/938], Loss: 1.268301248550415\n",
      "Validation: Epoch [1], Batch [648/938], Loss: 1.2293016910552979\n",
      "Validation: Epoch [1], Batch [649/938], Loss: 1.1775448322296143\n",
      "Validation: Epoch [1], Batch [650/938], Loss: 1.0677809715270996\n",
      "Validation: Epoch [1], Batch [651/938], Loss: 1.1034977436065674\n",
      "Validation: Epoch [1], Batch [652/938], Loss: 0.9626719951629639\n",
      "Validation: Epoch [1], Batch [653/938], Loss: 1.1874173879623413\n",
      "Validation: Epoch [1], Batch [654/938], Loss: 1.027024507522583\n",
      "Validation: Epoch [1], Batch [655/938], Loss: 1.1121058464050293\n",
      "Validation: Epoch [1], Batch [656/938], Loss: 1.1101070642471313\n",
      "Validation: Epoch [1], Batch [657/938], Loss: 0.958121657371521\n",
      "Validation: Epoch [1], Batch [658/938], Loss: 1.01212477684021\n",
      "Validation: Epoch [1], Batch [659/938], Loss: 1.0046954154968262\n",
      "Validation: Epoch [1], Batch [660/938], Loss: 1.048249363899231\n",
      "Validation: Epoch [1], Batch [661/938], Loss: 1.1408761739730835\n",
      "Validation: Epoch [1], Batch [662/938], Loss: 0.958091139793396\n",
      "Validation: Epoch [1], Batch [663/938], Loss: 1.073988437652588\n",
      "Validation: Epoch [1], Batch [664/938], Loss: 1.0581059455871582\n",
      "Validation: Epoch [1], Batch [665/938], Loss: 1.1458009481430054\n",
      "Validation: Epoch [1], Batch [666/938], Loss: 1.0764447450637817\n",
      "Validation: Epoch [1], Batch [667/938], Loss: 0.9923195242881775\n",
      "Validation: Epoch [1], Batch [668/938], Loss: 1.0121955871582031\n",
      "Validation: Epoch [1], Batch [669/938], Loss: 1.1836676597595215\n",
      "Validation: Epoch [1], Batch [670/938], Loss: 1.2083789110183716\n",
      "Validation: Epoch [1], Batch [671/938], Loss: 1.0895088911056519\n",
      "Validation: Epoch [1], Batch [672/938], Loss: 0.9660519957542419\n",
      "Validation: Epoch [1], Batch [673/938], Loss: 1.0773252248764038\n",
      "Validation: Epoch [1], Batch [674/938], Loss: 1.0430059432983398\n",
      "Validation: Epoch [1], Batch [675/938], Loss: 1.0529468059539795\n",
      "Validation: Epoch [1], Batch [676/938], Loss: 1.0971078872680664\n",
      "Validation: Epoch [1], Batch [677/938], Loss: 1.2651042938232422\n",
      "Validation: Epoch [1], Batch [678/938], Loss: 1.3026081323623657\n",
      "Validation: Epoch [1], Batch [679/938], Loss: 1.192574143409729\n",
      "Validation: Epoch [1], Batch [680/938], Loss: 1.0884451866149902\n",
      "Validation: Epoch [1], Batch [681/938], Loss: 1.3513398170471191\n",
      "Validation: Epoch [1], Batch [682/938], Loss: 1.1864876747131348\n",
      "Validation: Epoch [1], Batch [683/938], Loss: 1.0344805717468262\n",
      "Validation: Epoch [1], Batch [684/938], Loss: 1.1303355693817139\n",
      "Validation: Epoch [1], Batch [685/938], Loss: 1.0372796058654785\n",
      "Validation: Epoch [1], Batch [686/938], Loss: 1.173124074935913\n",
      "Validation: Epoch [1], Batch [687/938], Loss: 1.0359039306640625\n",
      "Validation: Epoch [1], Batch [688/938], Loss: 1.067384123802185\n",
      "Validation: Epoch [1], Batch [689/938], Loss: 1.2094264030456543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [690/938], Loss: 1.083458662033081\n",
      "Validation: Epoch [1], Batch [691/938], Loss: 1.09891676902771\n",
      "Validation: Epoch [1], Batch [692/938], Loss: 1.156707525253296\n",
      "Validation: Epoch [1], Batch [693/938], Loss: 0.9852210283279419\n",
      "Validation: Epoch [1], Batch [694/938], Loss: 1.0751621723175049\n",
      "Validation: Epoch [1], Batch [695/938], Loss: 1.1112086772918701\n",
      "Validation: Epoch [1], Batch [696/938], Loss: 1.1666712760925293\n",
      "Validation: Epoch [1], Batch [697/938], Loss: 1.129831314086914\n",
      "Validation: Epoch [1], Batch [698/938], Loss: 1.1940457820892334\n",
      "Validation: Epoch [1], Batch [699/938], Loss: 1.081628680229187\n",
      "Validation: Epoch [1], Batch [700/938], Loss: 1.1055781841278076\n",
      "Validation: Epoch [1], Batch [701/938], Loss: 1.2136927843093872\n",
      "Validation: Epoch [1], Batch [702/938], Loss: 1.147566795349121\n",
      "Validation: Epoch [1], Batch [703/938], Loss: 1.0691983699798584\n",
      "Validation: Epoch [1], Batch [704/938], Loss: 1.0926755666732788\n",
      "Validation: Epoch [1], Batch [705/938], Loss: 1.1430716514587402\n",
      "Validation: Epoch [1], Batch [706/938], Loss: 1.2437527179718018\n",
      "Validation: Epoch [1], Batch [707/938], Loss: 1.1764297485351562\n",
      "Validation: Epoch [1], Batch [708/938], Loss: 1.1107975244522095\n",
      "Validation: Epoch [1], Batch [709/938], Loss: 1.134535789489746\n",
      "Validation: Epoch [1], Batch [710/938], Loss: 1.148111343383789\n",
      "Validation: Epoch [1], Batch [711/938], Loss: 1.0650215148925781\n",
      "Validation: Epoch [1], Batch [712/938], Loss: 1.1591839790344238\n",
      "Validation: Epoch [1], Batch [713/938], Loss: 1.0015573501586914\n",
      "Validation: Epoch [1], Batch [714/938], Loss: 1.1607660055160522\n",
      "Validation: Epoch [1], Batch [715/938], Loss: 1.1459813117980957\n",
      "Validation: Epoch [1], Batch [716/938], Loss: 1.0711817741394043\n",
      "Validation: Epoch [1], Batch [717/938], Loss: 1.1665289402008057\n",
      "Validation: Epoch [1], Batch [718/938], Loss: 1.0880482196807861\n",
      "Validation: Epoch [1], Batch [719/938], Loss: 1.2698791027069092\n",
      "Validation: Epoch [1], Batch [720/938], Loss: 1.097541093826294\n",
      "Validation: Epoch [1], Batch [721/938], Loss: 1.1752803325653076\n",
      "Validation: Epoch [1], Batch [722/938], Loss: 1.0890116691589355\n",
      "Validation: Epoch [1], Batch [723/938], Loss: 1.085568904876709\n",
      "Validation: Epoch [1], Batch [724/938], Loss: 1.1576013565063477\n",
      "Validation: Epoch [1], Batch [725/938], Loss: 1.069905400276184\n",
      "Validation: Epoch [1], Batch [726/938], Loss: 1.1384246349334717\n",
      "Validation: Epoch [1], Batch [727/938], Loss: 1.2149946689605713\n",
      "Validation: Epoch [1], Batch [728/938], Loss: 1.0620002746582031\n",
      "Validation: Epoch [1], Batch [729/938], Loss: 1.2564482688903809\n",
      "Validation: Epoch [1], Batch [730/938], Loss: 1.0869147777557373\n",
      "Validation: Epoch [1], Batch [731/938], Loss: 1.0764086246490479\n",
      "Validation: Epoch [1], Batch [732/938], Loss: 1.1348822116851807\n",
      "Validation: Epoch [1], Batch [733/938], Loss: 1.197501540184021\n",
      "Validation: Epoch [1], Batch [734/938], Loss: 1.1389474868774414\n",
      "Validation: Epoch [1], Batch [735/938], Loss: 1.2913203239440918\n",
      "Validation: Epoch [1], Batch [736/938], Loss: 1.1322250366210938\n",
      "Validation: Epoch [1], Batch [737/938], Loss: 1.199641227722168\n",
      "Validation: Epoch [1], Batch [738/938], Loss: 1.2737782001495361\n",
      "Validation: Epoch [1], Batch [739/938], Loss: 1.2611010074615479\n",
      "Validation: Epoch [1], Batch [740/938], Loss: 0.9453383684158325\n",
      "Validation: Epoch [1], Batch [741/938], Loss: 1.086045742034912\n",
      "Validation: Epoch [1], Batch [742/938], Loss: 1.1805803775787354\n",
      "Validation: Epoch [1], Batch [743/938], Loss: 1.1627978086471558\n",
      "Validation: Epoch [1], Batch [744/938], Loss: 1.1219592094421387\n",
      "Validation: Epoch [1], Batch [745/938], Loss: 1.0390205383300781\n",
      "Validation: Epoch [1], Batch [746/938], Loss: 1.0324245691299438\n",
      "Validation: Epoch [1], Batch [747/938], Loss: 1.1318873167037964\n",
      "Validation: Epoch [1], Batch [748/938], Loss: 1.1214268207550049\n",
      "Validation: Epoch [1], Batch [749/938], Loss: 0.9566437005996704\n",
      "Validation: Epoch [1], Batch [750/938], Loss: 1.2681525945663452\n",
      "Validation: Epoch [1], Batch [751/938], Loss: 1.1776907444000244\n",
      "Validation: Epoch [1], Batch [752/938], Loss: 1.1742104291915894\n",
      "Validation: Epoch [1], Batch [753/938], Loss: 0.9802820682525635\n",
      "Validation: Epoch [1], Batch [754/938], Loss: 1.057065486907959\n",
      "Validation: Epoch [1], Batch [755/938], Loss: 1.0586504936218262\n",
      "Validation: Epoch [1], Batch [756/938], Loss: 1.1530210971832275\n",
      "Validation: Epoch [1], Batch [757/938], Loss: 1.2002885341644287\n",
      "Validation: Epoch [1], Batch [758/938], Loss: 1.334625005722046\n",
      "Validation: Epoch [1], Batch [759/938], Loss: 1.4070041179656982\n",
      "Validation: Epoch [1], Batch [760/938], Loss: 1.2834124565124512\n",
      "Validation: Epoch [1], Batch [761/938], Loss: 1.0960519313812256\n",
      "Validation: Epoch [1], Batch [762/938], Loss: 1.0122843980789185\n",
      "Validation: Epoch [1], Batch [763/938], Loss: 1.12758469581604\n",
      "Validation: Epoch [1], Batch [764/938], Loss: 1.081808090209961\n",
      "Validation: Epoch [1], Batch [765/938], Loss: 1.0424890518188477\n",
      "Validation: Epoch [1], Batch [766/938], Loss: 1.1531260013580322\n",
      "Validation: Epoch [1], Batch [767/938], Loss: 1.0873535871505737\n",
      "Validation: Epoch [1], Batch [768/938], Loss: 1.1945250034332275\n",
      "Validation: Epoch [1], Batch [769/938], Loss: 1.1705923080444336\n",
      "Validation: Epoch [1], Batch [770/938], Loss: 0.9518194198608398\n",
      "Validation: Epoch [1], Batch [771/938], Loss: 1.1089757680892944\n",
      "Validation: Epoch [1], Batch [772/938], Loss: 1.1343433856964111\n",
      "Validation: Epoch [1], Batch [773/938], Loss: 1.109866976737976\n",
      "Validation: Epoch [1], Batch [774/938], Loss: 1.1254265308380127\n",
      "Validation: Epoch [1], Batch [775/938], Loss: 1.0020498037338257\n",
      "Validation: Epoch [1], Batch [776/938], Loss: 1.0581215620040894\n",
      "Validation: Epoch [1], Batch [777/938], Loss: 1.202868938446045\n",
      "Validation: Epoch [1], Batch [778/938], Loss: 1.0983827114105225\n",
      "Validation: Epoch [1], Batch [779/938], Loss: 1.2186590433120728\n",
      "Validation: Epoch [1], Batch [780/938], Loss: 1.107797622680664\n",
      "Validation: Epoch [1], Batch [781/938], Loss: 1.0890352725982666\n",
      "Validation: Epoch [1], Batch [782/938], Loss: 1.0682721138000488\n",
      "Validation: Epoch [1], Batch [783/938], Loss: 1.226313829421997\n",
      "Validation: Epoch [1], Batch [784/938], Loss: 1.051969051361084\n",
      "Validation: Epoch [1], Batch [785/938], Loss: 1.1082334518432617\n",
      "Validation: Epoch [1], Batch [786/938], Loss: 1.23567533493042\n",
      "Validation: Epoch [1], Batch [787/938], Loss: 1.1706444025039673\n",
      "Validation: Epoch [1], Batch [788/938], Loss: 1.0011779069900513\n",
      "Validation: Epoch [1], Batch [789/938], Loss: 1.066657543182373\n",
      "Validation: Epoch [1], Batch [790/938], Loss: 1.2327772378921509\n",
      "Validation: Epoch [1], Batch [791/938], Loss: 1.106544017791748\n",
      "Validation: Epoch [1], Batch [792/938], Loss: 1.1170084476470947\n",
      "Validation: Epoch [1], Batch [793/938], Loss: 1.152410864830017\n",
      "Validation: Epoch [1], Batch [794/938], Loss: 1.04056978225708\n",
      "Validation: Epoch [1], Batch [795/938], Loss: 1.0440279245376587\n",
      "Validation: Epoch [1], Batch [796/938], Loss: 1.1497807502746582\n",
      "Validation: Epoch [1], Batch [797/938], Loss: 1.0812345743179321\n",
      "Validation: Epoch [1], Batch [798/938], Loss: 1.0642147064208984\n",
      "Validation: Epoch [1], Batch [799/938], Loss: 1.1221699714660645\n",
      "Validation: Epoch [1], Batch [800/938], Loss: 1.1153075695037842\n",
      "Validation: Epoch [1], Batch [801/938], Loss: 1.147085428237915\n",
      "Validation: Epoch [1], Batch [802/938], Loss: 1.2438284158706665\n",
      "Validation: Epoch [1], Batch [803/938], Loss: 1.0916461944580078\n",
      "Validation: Epoch [1], Batch [804/938], Loss: 0.9821339249610901\n",
      "Validation: Epoch [1], Batch [805/938], Loss: 1.2659648656845093\n",
      "Validation: Epoch [1], Batch [806/938], Loss: 0.9853300452232361\n",
      "Validation: Epoch [1], Batch [807/938], Loss: 1.004856824874878\n",
      "Validation: Epoch [1], Batch [808/938], Loss: 1.0790679454803467\n",
      "Validation: Epoch [1], Batch [809/938], Loss: 1.1711108684539795\n",
      "Validation: Epoch [1], Batch [810/938], Loss: 1.0905499458312988\n",
      "Validation: Epoch [1], Batch [811/938], Loss: 1.089838981628418\n",
      "Validation: Epoch [1], Batch [812/938], Loss: 1.0813753604888916\n",
      "Validation: Epoch [1], Batch [813/938], Loss: 1.1701340675354004\n",
      "Validation: Epoch [1], Batch [814/938], Loss: 1.134208083152771\n",
      "Validation: Epoch [1], Batch [815/938], Loss: 1.0964399576187134\n",
      "Validation: Epoch [1], Batch [816/938], Loss: 1.1301738023757935\n",
      "Validation: Epoch [1], Batch [817/938], Loss: 1.163116693496704\n",
      "Validation: Epoch [1], Batch [818/938], Loss: 1.1079766750335693\n",
      "Validation: Epoch [1], Batch [819/938], Loss: 0.9794467687606812\n",
      "Validation: Epoch [1], Batch [820/938], Loss: 1.066202163696289\n",
      "Validation: Epoch [1], Batch [821/938], Loss: 1.0656468868255615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [1], Batch [822/938], Loss: 1.1063154935836792\n",
      "Validation: Epoch [1], Batch [823/938], Loss: 1.0123374462127686\n",
      "Validation: Epoch [1], Batch [824/938], Loss: 1.0910532474517822\n",
      "Validation: Epoch [1], Batch [825/938], Loss: 1.0306358337402344\n",
      "Validation: Epoch [1], Batch [826/938], Loss: 1.1456794738769531\n",
      "Validation: Epoch [1], Batch [827/938], Loss: 1.1905779838562012\n",
      "Validation: Epoch [1], Batch [828/938], Loss: 1.1412525177001953\n",
      "Validation: Epoch [1], Batch [829/938], Loss: 0.979178249835968\n",
      "Validation: Epoch [1], Batch [830/938], Loss: 1.156980037689209\n",
      "Validation: Epoch [1], Batch [831/938], Loss: 1.227046251296997\n",
      "Validation: Epoch [1], Batch [832/938], Loss: 1.0351370573043823\n",
      "Validation: Epoch [1], Batch [833/938], Loss: 1.1463727951049805\n",
      "Validation: Epoch [1], Batch [834/938], Loss: 1.1331053972244263\n",
      "Validation: Epoch [1], Batch [835/938], Loss: 1.1219887733459473\n",
      "Validation: Epoch [1], Batch [836/938], Loss: 1.116079568862915\n",
      "Validation: Epoch [1], Batch [837/938], Loss: 1.1521614789962769\n",
      "Validation: Epoch [1], Batch [838/938], Loss: 1.0387150049209595\n",
      "Validation: Epoch [1], Batch [839/938], Loss: 1.1827253103256226\n",
      "Validation: Epoch [1], Batch [840/938], Loss: 1.388124704360962\n",
      "Validation: Epoch [1], Batch [841/938], Loss: 1.0709431171417236\n",
      "Validation: Epoch [1], Batch [842/938], Loss: 1.0206985473632812\n",
      "Validation: Epoch [1], Batch [843/938], Loss: 1.15498948097229\n",
      "Validation: Epoch [1], Batch [844/938], Loss: 1.0532782077789307\n",
      "Validation: Epoch [1], Batch [845/938], Loss: 1.037270426750183\n",
      "Validation: Epoch [1], Batch [846/938], Loss: 1.1197614669799805\n",
      "Validation: Epoch [1], Batch [847/938], Loss: 1.0790324211120605\n",
      "Validation: Epoch [1], Batch [848/938], Loss: 1.2150850296020508\n",
      "Validation: Epoch [1], Batch [849/938], Loss: 1.1290152072906494\n",
      "Validation: Epoch [1], Batch [850/938], Loss: 1.063890814781189\n",
      "Validation: Epoch [1], Batch [851/938], Loss: 0.9857568740844727\n",
      "Validation: Epoch [1], Batch [852/938], Loss: 1.1153309345245361\n",
      "Validation: Epoch [1], Batch [853/938], Loss: 1.1571234464645386\n",
      "Validation: Epoch [1], Batch [854/938], Loss: 0.9754390120506287\n",
      "Validation: Epoch [1], Batch [855/938], Loss: 1.0895447731018066\n",
      "Validation: Epoch [1], Batch [856/938], Loss: 1.0646998882293701\n",
      "Validation: Epoch [1], Batch [857/938], Loss: 1.1101881265640259\n",
      "Validation: Epoch [1], Batch [858/938], Loss: 1.0897774696350098\n",
      "Validation: Epoch [1], Batch [859/938], Loss: 1.0805386304855347\n",
      "Validation: Epoch [1], Batch [860/938], Loss: 1.0659291744232178\n",
      "Validation: Epoch [1], Batch [861/938], Loss: 0.9982714056968689\n",
      "Validation: Epoch [1], Batch [862/938], Loss: 1.0171828269958496\n",
      "Validation: Epoch [1], Batch [863/938], Loss: 1.0456279516220093\n",
      "Validation: Epoch [1], Batch [864/938], Loss: 0.984574556350708\n",
      "Validation: Epoch [1], Batch [865/938], Loss: 0.9748448133468628\n",
      "Validation: Epoch [1], Batch [866/938], Loss: 1.050751805305481\n",
      "Validation: Epoch [1], Batch [867/938], Loss: 1.0505030155181885\n",
      "Validation: Epoch [1], Batch [868/938], Loss: 1.0609450340270996\n",
      "Validation: Epoch [1], Batch [869/938], Loss: 1.1037529706954956\n",
      "Validation: Epoch [1], Batch [870/938], Loss: 1.1084508895874023\n",
      "Validation: Epoch [1], Batch [871/938], Loss: 1.120046615600586\n",
      "Validation: Epoch [1], Batch [872/938], Loss: 1.1276280879974365\n",
      "Validation: Epoch [1], Batch [873/938], Loss: 1.0870003700256348\n",
      "Validation: Epoch [1], Batch [874/938], Loss: 1.0816998481750488\n",
      "Validation: Epoch [1], Batch [875/938], Loss: 1.2015774250030518\n",
      "Validation: Epoch [1], Batch [876/938], Loss: 1.1143412590026855\n",
      "Validation: Epoch [1], Batch [877/938], Loss: 1.1568920612335205\n",
      "Validation: Epoch [1], Batch [878/938], Loss: 1.1479777097702026\n",
      "Validation: Epoch [1], Batch [879/938], Loss: 1.1527371406555176\n",
      "Validation: Epoch [1], Batch [880/938], Loss: 1.1178196668624878\n",
      "Validation: Epoch [1], Batch [881/938], Loss: 1.1671435832977295\n",
      "Validation: Epoch [1], Batch [882/938], Loss: 1.0803956985473633\n",
      "Validation: Epoch [1], Batch [883/938], Loss: 1.043853998184204\n",
      "Validation: Epoch [1], Batch [884/938], Loss: 1.156722903251648\n",
      "Validation: Epoch [1], Batch [885/938], Loss: 1.2174327373504639\n",
      "Validation: Epoch [1], Batch [886/938], Loss: 1.0528182983398438\n",
      "Validation: Epoch [1], Batch [887/938], Loss: 1.1994774341583252\n",
      "Validation: Epoch [1], Batch [888/938], Loss: 0.9994339346885681\n",
      "Validation: Epoch [1], Batch [889/938], Loss: 1.108243703842163\n",
      "Validation: Epoch [1], Batch [890/938], Loss: 1.2087156772613525\n",
      "Validation: Epoch [1], Batch [891/938], Loss: 1.1974637508392334\n",
      "Validation: Epoch [1], Batch [892/938], Loss: 1.257006287574768\n",
      "Validation: Epoch [1], Batch [893/938], Loss: 1.287149429321289\n",
      "Validation: Epoch [1], Batch [894/938], Loss: 1.0413309335708618\n",
      "Validation: Epoch [1], Batch [895/938], Loss: 1.0576342344284058\n",
      "Validation: Epoch [1], Batch [896/938], Loss: 1.2790310382843018\n",
      "Validation: Epoch [1], Batch [897/938], Loss: 1.1847946643829346\n",
      "Validation: Epoch [1], Batch [898/938], Loss: 1.1488327980041504\n",
      "Validation: Epoch [1], Batch [899/938], Loss: 1.2627882957458496\n",
      "Validation: Epoch [1], Batch [900/938], Loss: 1.0992896556854248\n",
      "Validation: Epoch [1], Batch [901/938], Loss: 1.2189894914627075\n",
      "Validation: Epoch [1], Batch [902/938], Loss: 1.258251667022705\n",
      "Validation: Epoch [1], Batch [903/938], Loss: 1.4346548318862915\n",
      "Validation: Epoch [1], Batch [904/938], Loss: 1.1802177429199219\n",
      "Validation: Epoch [1], Batch [905/938], Loss: 0.9771470427513123\n",
      "Validation: Epoch [1], Batch [906/938], Loss: 1.2187297344207764\n",
      "Validation: Epoch [1], Batch [907/938], Loss: 1.1784353256225586\n",
      "Validation: Epoch [1], Batch [908/938], Loss: 1.0624704360961914\n",
      "Validation: Epoch [1], Batch [909/938], Loss: 1.0486105680465698\n",
      "Validation: Epoch [1], Batch [910/938], Loss: 1.1054325103759766\n",
      "Validation: Epoch [1], Batch [911/938], Loss: 1.4308202266693115\n",
      "Validation: Epoch [1], Batch [912/938], Loss: 1.0578224658966064\n",
      "Validation: Epoch [1], Batch [913/938], Loss: 1.2086148262023926\n",
      "Validation: Epoch [1], Batch [914/938], Loss: 1.220240592956543\n",
      "Validation: Epoch [1], Batch [915/938], Loss: 1.062559962272644\n",
      "Validation: Epoch [1], Batch [916/938], Loss: 1.1141468286514282\n",
      "Validation: Epoch [1], Batch [917/938], Loss: 1.0416960716247559\n",
      "Validation: Epoch [1], Batch [918/938], Loss: 1.179816484451294\n",
      "Validation: Epoch [1], Batch [919/938], Loss: 1.095644474029541\n",
      "Validation: Epoch [1], Batch [920/938], Loss: 1.1267690658569336\n",
      "Validation: Epoch [1], Batch [921/938], Loss: 1.0703508853912354\n",
      "Validation: Epoch [1], Batch [922/938], Loss: 1.0124335289001465\n",
      "Validation: Epoch [1], Batch [923/938], Loss: 1.0779037475585938\n",
      "Validation: Epoch [1], Batch [924/938], Loss: 1.227564811706543\n",
      "Validation: Epoch [1], Batch [925/938], Loss: 1.0198302268981934\n",
      "Validation: Epoch [1], Batch [926/938], Loss: 1.1459629535675049\n",
      "Validation: Epoch [1], Batch [927/938], Loss: 1.1213384866714478\n",
      "Validation: Epoch [1], Batch [928/938], Loss: 1.2012979984283447\n",
      "Validation: Epoch [1], Batch [929/938], Loss: 1.2144960165023804\n",
      "Validation: Epoch [1], Batch [930/938], Loss: 1.087526798248291\n",
      "Validation: Epoch [1], Batch [931/938], Loss: 1.2632670402526855\n",
      "Validation: Epoch [1], Batch [932/938], Loss: 1.1105842590332031\n",
      "Validation: Epoch [1], Batch [933/938], Loss: 0.9962782859802246\n",
      "Validation: Epoch [1], Batch [934/938], Loss: 1.0873289108276367\n",
      "Validation: Epoch [1], Batch [935/938], Loss: 1.1322410106658936\n",
      "Validation: Epoch [1], Batch [936/938], Loss: 0.9748550057411194\n",
      "Validation: Epoch [1], Batch [937/938], Loss: 1.1048481464385986\n",
      "Validation: Epoch [1], Batch [938/938], Loss: 1.2219328880310059\n",
      "Accuracy of test set: 0.58655\n",
      "Train: Epoch [2], Batch [1/938], Loss: 1.1536273956298828\n",
      "Train: Epoch [2], Batch [2/938], Loss: 1.109839916229248\n",
      "Train: Epoch [2], Batch [3/938], Loss: 1.1272025108337402\n",
      "Train: Epoch [2], Batch [4/938], Loss: 1.09112548828125\n",
      "Train: Epoch [2], Batch [5/938], Loss: 1.1491236686706543\n",
      "Train: Epoch [2], Batch [6/938], Loss: 1.0141083002090454\n",
      "Train: Epoch [2], Batch [7/938], Loss: 1.1972861289978027\n",
      "Train: Epoch [2], Batch [8/938], Loss: 1.09516441822052\n",
      "Train: Epoch [2], Batch [9/938], Loss: 1.2328717708587646\n",
      "Train: Epoch [2], Batch [10/938], Loss: 1.102803349494934\n",
      "Train: Epoch [2], Batch [11/938], Loss: 1.1000276803970337\n",
      "Train: Epoch [2], Batch [12/938], Loss: 1.1225632429122925\n",
      "Train: Epoch [2], Batch [13/938], Loss: 1.0557416677474976\n",
      "Train: Epoch [2], Batch [14/938], Loss: 1.110525369644165\n",
      "Train: Epoch [2], Batch [15/938], Loss: 1.262382984161377\n",
      "Train: Epoch [2], Batch [16/938], Loss: 1.0673291683197021\n",
      "Train: Epoch [2], Batch [17/938], Loss: 1.0006730556488037\n",
      "Train: Epoch [2], Batch [18/938], Loss: 1.1083950996398926\n",
      "Train: Epoch [2], Batch [19/938], Loss: 1.0556248426437378\n",
      "Train: Epoch [2], Batch [20/938], Loss: 0.9863951802253723\n",
      "Train: Epoch [2], Batch [21/938], Loss: 1.1859476566314697\n",
      "Train: Epoch [2], Batch [22/938], Loss: 1.0645582675933838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [23/938], Loss: 1.0159589052200317\n",
      "Train: Epoch [2], Batch [24/938], Loss: 1.1216320991516113\n",
      "Train: Epoch [2], Batch [25/938], Loss: 1.127604365348816\n",
      "Train: Epoch [2], Batch [26/938], Loss: 1.2536060810089111\n",
      "Train: Epoch [2], Batch [27/938], Loss: 1.0841434001922607\n",
      "Train: Epoch [2], Batch [28/938], Loss: 1.142730474472046\n",
      "Train: Epoch [2], Batch [29/938], Loss: 1.0701255798339844\n",
      "Train: Epoch [2], Batch [30/938], Loss: 1.0786454677581787\n",
      "Train: Epoch [2], Batch [31/938], Loss: 1.0311970710754395\n",
      "Train: Epoch [2], Batch [32/938], Loss: 0.9832124710083008\n",
      "Train: Epoch [2], Batch [33/938], Loss: 0.9530313014984131\n",
      "Train: Epoch [2], Batch [34/938], Loss: 1.0654398202896118\n",
      "Train: Epoch [2], Batch [35/938], Loss: 1.2209792137145996\n",
      "Train: Epoch [2], Batch [36/938], Loss: 1.184220314025879\n",
      "Train: Epoch [2], Batch [37/938], Loss: 1.0416107177734375\n",
      "Train: Epoch [2], Batch [38/938], Loss: 1.0736916065216064\n",
      "Train: Epoch [2], Batch [39/938], Loss: 1.0661791563034058\n",
      "Train: Epoch [2], Batch [40/938], Loss: 1.4121811389923096\n",
      "Train: Epoch [2], Batch [41/938], Loss: 1.0099751949310303\n",
      "Train: Epoch [2], Batch [42/938], Loss: 1.0562973022460938\n",
      "Train: Epoch [2], Batch [43/938], Loss: 1.2230274677276611\n",
      "Train: Epoch [2], Batch [44/938], Loss: 1.1394078731536865\n",
      "Train: Epoch [2], Batch [45/938], Loss: 1.0481404066085815\n",
      "Train: Epoch [2], Batch [46/938], Loss: 1.087641954421997\n",
      "Train: Epoch [2], Batch [47/938], Loss: 1.0767675638198853\n",
      "Train: Epoch [2], Batch [48/938], Loss: 0.9679459929466248\n",
      "Train: Epoch [2], Batch [49/938], Loss: 1.1265363693237305\n",
      "Train: Epoch [2], Batch [50/938], Loss: 1.0788726806640625\n",
      "Train: Epoch [2], Batch [51/938], Loss: 1.068906307220459\n",
      "Train: Epoch [2], Batch [52/938], Loss: 1.0323415994644165\n",
      "Train: Epoch [2], Batch [53/938], Loss: 1.2400788068771362\n",
      "Train: Epoch [2], Batch [54/938], Loss: 1.0490660667419434\n",
      "Train: Epoch [2], Batch [55/938], Loss: 1.1888015270233154\n",
      "Train: Epoch [2], Batch [56/938], Loss: 1.0529119968414307\n",
      "Train: Epoch [2], Batch [57/938], Loss: 0.9861428141593933\n",
      "Train: Epoch [2], Batch [58/938], Loss: 0.9686749577522278\n",
      "Train: Epoch [2], Batch [59/938], Loss: 1.10823392868042\n",
      "Train: Epoch [2], Batch [60/938], Loss: 1.1837482452392578\n",
      "Train: Epoch [2], Batch [61/938], Loss: 1.180545449256897\n",
      "Train: Epoch [2], Batch [62/938], Loss: 1.1820580959320068\n",
      "Train: Epoch [2], Batch [63/938], Loss: 0.9875257611274719\n",
      "Train: Epoch [2], Batch [64/938], Loss: 1.1584608554840088\n",
      "Train: Epoch [2], Batch [65/938], Loss: 1.0903069972991943\n",
      "Train: Epoch [2], Batch [66/938], Loss: 0.9428815245628357\n",
      "Train: Epoch [2], Batch [67/938], Loss: 1.1901839971542358\n",
      "Train: Epoch [2], Batch [68/938], Loss: 1.1558128595352173\n",
      "Train: Epoch [2], Batch [69/938], Loss: 1.0533819198608398\n",
      "Train: Epoch [2], Batch [70/938], Loss: 1.1516425609588623\n",
      "Train: Epoch [2], Batch [71/938], Loss: 1.1674144268035889\n",
      "Train: Epoch [2], Batch [72/938], Loss: 0.9808007478713989\n",
      "Train: Epoch [2], Batch [73/938], Loss: 1.0423610210418701\n",
      "Train: Epoch [2], Batch [74/938], Loss: 1.0621397495269775\n",
      "Train: Epoch [2], Batch [75/938], Loss: 1.053316354751587\n",
      "Train: Epoch [2], Batch [76/938], Loss: 1.249219536781311\n",
      "Train: Epoch [2], Batch [77/938], Loss: 1.2400083541870117\n",
      "Train: Epoch [2], Batch [78/938], Loss: 1.1553966999053955\n",
      "Train: Epoch [2], Batch [79/938], Loss: 1.0894043445587158\n",
      "Train: Epoch [2], Batch [80/938], Loss: 1.1086046695709229\n",
      "Train: Epoch [2], Batch [81/938], Loss: 1.1457722187042236\n",
      "Train: Epoch [2], Batch [82/938], Loss: 1.113695740699768\n",
      "Train: Epoch [2], Batch [83/938], Loss: 1.029901385307312\n",
      "Train: Epoch [2], Batch [84/938], Loss: 1.0625338554382324\n",
      "Train: Epoch [2], Batch [85/938], Loss: 1.0167971849441528\n",
      "Train: Epoch [2], Batch [86/938], Loss: 0.9240681529045105\n",
      "Train: Epoch [2], Batch [87/938], Loss: 1.1303765773773193\n",
      "Train: Epoch [2], Batch [88/938], Loss: 0.9270619750022888\n",
      "Train: Epoch [2], Batch [89/938], Loss: 0.9663212299346924\n",
      "Train: Epoch [2], Batch [90/938], Loss: 1.062089204788208\n",
      "Train: Epoch [2], Batch [91/938], Loss: 1.0940496921539307\n",
      "Train: Epoch [2], Batch [92/938], Loss: 1.3029016256332397\n",
      "Train: Epoch [2], Batch [93/938], Loss: 1.1360257863998413\n",
      "Train: Epoch [2], Batch [94/938], Loss: 1.1624197959899902\n",
      "Train: Epoch [2], Batch [95/938], Loss: 1.026677131652832\n",
      "Train: Epoch [2], Batch [96/938], Loss: 1.1068522930145264\n",
      "Train: Epoch [2], Batch [97/938], Loss: 1.1175016164779663\n",
      "Train: Epoch [2], Batch [98/938], Loss: 0.9940956830978394\n",
      "Train: Epoch [2], Batch [99/938], Loss: 0.9233051538467407\n",
      "Train: Epoch [2], Batch [100/938], Loss: 1.1100863218307495\n",
      "Train: Epoch [2], Batch [101/938], Loss: 0.9942246675491333\n",
      "Train: Epoch [2], Batch [102/938], Loss: 1.1067578792572021\n",
      "Train: Epoch [2], Batch [103/938], Loss: 1.0444118976593018\n",
      "Train: Epoch [2], Batch [104/938], Loss: 1.155909538269043\n",
      "Train: Epoch [2], Batch [105/938], Loss: 1.0717687606811523\n",
      "Train: Epoch [2], Batch [106/938], Loss: 1.0477792024612427\n",
      "Train: Epoch [2], Batch [107/938], Loss: 1.0483160018920898\n",
      "Train: Epoch [2], Batch [108/938], Loss: 1.1727750301361084\n",
      "Train: Epoch [2], Batch [109/938], Loss: 1.09270179271698\n",
      "Train: Epoch [2], Batch [110/938], Loss: 1.1249165534973145\n",
      "Train: Epoch [2], Batch [111/938], Loss: 1.1236504316329956\n",
      "Train: Epoch [2], Batch [112/938], Loss: 1.0842903852462769\n",
      "Train: Epoch [2], Batch [113/938], Loss: 1.1143040657043457\n",
      "Train: Epoch [2], Batch [114/938], Loss: 1.1309322118759155\n",
      "Train: Epoch [2], Batch [115/938], Loss: 0.9352728128433228\n",
      "Train: Epoch [2], Batch [116/938], Loss: 1.038539171218872\n",
      "Train: Epoch [2], Batch [117/938], Loss: 0.958312451839447\n",
      "Train: Epoch [2], Batch [118/938], Loss: 1.1762721538543701\n",
      "Train: Epoch [2], Batch [119/938], Loss: 1.003426194190979\n",
      "Train: Epoch [2], Batch [120/938], Loss: 1.207319974899292\n",
      "Train: Epoch [2], Batch [121/938], Loss: 1.0042659044265747\n",
      "Train: Epoch [2], Batch [122/938], Loss: 0.9995859861373901\n",
      "Train: Epoch [2], Batch [123/938], Loss: 1.3040181398391724\n",
      "Train: Epoch [2], Batch [124/938], Loss: 1.0552374124526978\n",
      "Train: Epoch [2], Batch [125/938], Loss: 1.018254280090332\n",
      "Train: Epoch [2], Batch [126/938], Loss: 1.1312402486801147\n",
      "Train: Epoch [2], Batch [127/938], Loss: 1.011064887046814\n",
      "Train: Epoch [2], Batch [128/938], Loss: 1.0813699960708618\n",
      "Train: Epoch [2], Batch [129/938], Loss: 1.1797616481781006\n",
      "Train: Epoch [2], Batch [130/938], Loss: 0.9297609925270081\n",
      "Train: Epoch [2], Batch [131/938], Loss: 0.929498016834259\n",
      "Train: Epoch [2], Batch [132/938], Loss: 1.1038050651550293\n",
      "Train: Epoch [2], Batch [133/938], Loss: 0.9528915882110596\n",
      "Train: Epoch [2], Batch [134/938], Loss: 0.9839894771575928\n",
      "Train: Epoch [2], Batch [135/938], Loss: 1.023467779159546\n",
      "Train: Epoch [2], Batch [136/938], Loss: 0.962909460067749\n",
      "Train: Epoch [2], Batch [137/938], Loss: 0.9040641784667969\n",
      "Train: Epoch [2], Batch [138/938], Loss: 1.0835776329040527\n",
      "Train: Epoch [2], Batch [139/938], Loss: 0.8728689551353455\n",
      "Train: Epoch [2], Batch [140/938], Loss: 0.9693308472633362\n",
      "Train: Epoch [2], Batch [141/938], Loss: 1.1203279495239258\n",
      "Train: Epoch [2], Batch [142/938], Loss: 1.020994782447815\n",
      "Train: Epoch [2], Batch [143/938], Loss: 1.002528429031372\n",
      "Train: Epoch [2], Batch [144/938], Loss: 1.0110974311828613\n",
      "Train: Epoch [2], Batch [145/938], Loss: 1.0295618772506714\n",
      "Train: Epoch [2], Batch [146/938], Loss: 1.014253854751587\n",
      "Train: Epoch [2], Batch [147/938], Loss: 0.9048651456832886\n",
      "Train: Epoch [2], Batch [148/938], Loss: 0.9910718202590942\n",
      "Train: Epoch [2], Batch [149/938], Loss: 1.150234341621399\n",
      "Train: Epoch [2], Batch [150/938], Loss: 1.0764527320861816\n",
      "Train: Epoch [2], Batch [151/938], Loss: 1.1473784446716309\n",
      "Train: Epoch [2], Batch [152/938], Loss: 1.0672826766967773\n",
      "Train: Epoch [2], Batch [153/938], Loss: 1.0002431869506836\n",
      "Train: Epoch [2], Batch [154/938], Loss: 0.95262610912323\n",
      "Train: Epoch [2], Batch [155/938], Loss: 0.8983327746391296\n",
      "Train: Epoch [2], Batch [156/938], Loss: 0.9781404733657837\n",
      "Train: Epoch [2], Batch [157/938], Loss: 1.1207129955291748\n",
      "Train: Epoch [2], Batch [158/938], Loss: 1.154954433441162\n",
      "Train: Epoch [2], Batch [159/938], Loss: 0.912247359752655\n",
      "Train: Epoch [2], Batch [160/938], Loss: 1.2410593032836914\n",
      "Train: Epoch [2], Batch [161/938], Loss: 0.9810606837272644\n",
      "Train: Epoch [2], Batch [162/938], Loss: 0.9836782813072205\n",
      "Train: Epoch [2], Batch [163/938], Loss: 1.131852149963379\n",
      "Train: Epoch [2], Batch [164/938], Loss: 0.9839134812355042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [165/938], Loss: 1.0648303031921387\n",
      "Train: Epoch [2], Batch [166/938], Loss: 0.9276348352432251\n",
      "Train: Epoch [2], Batch [167/938], Loss: 1.1269869804382324\n",
      "Train: Epoch [2], Batch [168/938], Loss: 1.0494242906570435\n",
      "Train: Epoch [2], Batch [169/938], Loss: 1.1327698230743408\n",
      "Train: Epoch [2], Batch [170/938], Loss: 1.1028006076812744\n",
      "Train: Epoch [2], Batch [171/938], Loss: 1.0359759330749512\n",
      "Train: Epoch [2], Batch [172/938], Loss: 1.05165696144104\n",
      "Train: Epoch [2], Batch [173/938], Loss: 1.0238198041915894\n",
      "Train: Epoch [2], Batch [174/938], Loss: 1.0554983615875244\n",
      "Train: Epoch [2], Batch [175/938], Loss: 1.0554852485656738\n",
      "Train: Epoch [2], Batch [176/938], Loss: 1.0233416557312012\n",
      "Train: Epoch [2], Batch [177/938], Loss: 0.9668675661087036\n",
      "Train: Epoch [2], Batch [178/938], Loss: 1.0856943130493164\n",
      "Train: Epoch [2], Batch [179/938], Loss: 1.06181001663208\n",
      "Train: Epoch [2], Batch [180/938], Loss: 1.199405312538147\n",
      "Train: Epoch [2], Batch [181/938], Loss: 1.0294572114944458\n",
      "Train: Epoch [2], Batch [182/938], Loss: 1.1902244091033936\n",
      "Train: Epoch [2], Batch [183/938], Loss: 1.0299913883209229\n",
      "Train: Epoch [2], Batch [184/938], Loss: 1.0966205596923828\n",
      "Train: Epoch [2], Batch [185/938], Loss: 0.9798749685287476\n",
      "Train: Epoch [2], Batch [186/938], Loss: 1.1348316669464111\n",
      "Train: Epoch [2], Batch [187/938], Loss: 0.9697802662849426\n",
      "Train: Epoch [2], Batch [188/938], Loss: 0.968510627746582\n",
      "Train: Epoch [2], Batch [189/938], Loss: 1.128687858581543\n",
      "Train: Epoch [2], Batch [190/938], Loss: 0.9834489822387695\n",
      "Train: Epoch [2], Batch [191/938], Loss: 1.2126286029815674\n",
      "Train: Epoch [2], Batch [192/938], Loss: 1.0273191928863525\n",
      "Train: Epoch [2], Batch [193/938], Loss: 1.0423266887664795\n",
      "Train: Epoch [2], Batch [194/938], Loss: 1.0295599699020386\n",
      "Train: Epoch [2], Batch [195/938], Loss: 1.166921854019165\n",
      "Train: Epoch [2], Batch [196/938], Loss: 1.0317937135696411\n",
      "Train: Epoch [2], Batch [197/938], Loss: 0.929601788520813\n",
      "Train: Epoch [2], Batch [198/938], Loss: 0.9731760025024414\n",
      "Train: Epoch [2], Batch [199/938], Loss: 0.9682573676109314\n",
      "Train: Epoch [2], Batch [200/938], Loss: 1.1019279956817627\n",
      "Train: Epoch [2], Batch [201/938], Loss: 1.1178313493728638\n",
      "Train: Epoch [2], Batch [202/938], Loss: 0.9089988470077515\n",
      "Train: Epoch [2], Batch [203/938], Loss: 0.9391075372695923\n",
      "Train: Epoch [2], Batch [204/938], Loss: 1.2413944005966187\n",
      "Train: Epoch [2], Batch [205/938], Loss: 1.1526960134506226\n",
      "Train: Epoch [2], Batch [206/938], Loss: 0.9253287315368652\n",
      "Train: Epoch [2], Batch [207/938], Loss: 1.0608620643615723\n",
      "Train: Epoch [2], Batch [208/938], Loss: 0.9367356896400452\n",
      "Train: Epoch [2], Batch [209/938], Loss: 1.0842182636260986\n",
      "Train: Epoch [2], Batch [210/938], Loss: 1.1399641036987305\n",
      "Train: Epoch [2], Batch [211/938], Loss: 0.9945640563964844\n",
      "Train: Epoch [2], Batch [212/938], Loss: 1.0780935287475586\n",
      "Train: Epoch [2], Batch [213/938], Loss: 1.0147594213485718\n",
      "Train: Epoch [2], Batch [214/938], Loss: 1.1410558223724365\n",
      "Train: Epoch [2], Batch [215/938], Loss: 0.9684301018714905\n",
      "Train: Epoch [2], Batch [216/938], Loss: 1.1234714984893799\n",
      "Train: Epoch [2], Batch [217/938], Loss: 1.2041699886322021\n",
      "Train: Epoch [2], Batch [218/938], Loss: 1.051081895828247\n",
      "Train: Epoch [2], Batch [219/938], Loss: 1.010562539100647\n",
      "Train: Epoch [2], Batch [220/938], Loss: 1.0108013153076172\n",
      "Train: Epoch [2], Batch [221/938], Loss: 1.0123496055603027\n",
      "Train: Epoch [2], Batch [222/938], Loss: 0.976600706577301\n",
      "Train: Epoch [2], Batch [223/938], Loss: 1.0169012546539307\n",
      "Train: Epoch [2], Batch [224/938], Loss: 1.092059850692749\n",
      "Train: Epoch [2], Batch [225/938], Loss: 0.9791268706321716\n",
      "Train: Epoch [2], Batch [226/938], Loss: 1.2396461963653564\n",
      "Train: Epoch [2], Batch [227/938], Loss: 1.180734395980835\n",
      "Train: Epoch [2], Batch [228/938], Loss: 0.9580379724502563\n",
      "Train: Epoch [2], Batch [229/938], Loss: 1.0341860055923462\n",
      "Train: Epoch [2], Batch [230/938], Loss: 1.0988832712173462\n",
      "Train: Epoch [2], Batch [231/938], Loss: 1.1523855924606323\n",
      "Train: Epoch [2], Batch [232/938], Loss: 1.0162843465805054\n",
      "Train: Epoch [2], Batch [233/938], Loss: 1.1405022144317627\n",
      "Train: Epoch [2], Batch [234/938], Loss: 0.9487903118133545\n",
      "Train: Epoch [2], Batch [235/938], Loss: 1.042750597000122\n",
      "Train: Epoch [2], Batch [236/938], Loss: 1.1009712219238281\n",
      "Train: Epoch [2], Batch [237/938], Loss: 1.0455551147460938\n",
      "Train: Epoch [2], Batch [238/938], Loss: 1.0350794792175293\n",
      "Train: Epoch [2], Batch [239/938], Loss: 0.9991012811660767\n",
      "Train: Epoch [2], Batch [240/938], Loss: 1.0734899044036865\n",
      "Train: Epoch [2], Batch [241/938], Loss: 1.0703587532043457\n",
      "Train: Epoch [2], Batch [242/938], Loss: 1.034474492073059\n",
      "Train: Epoch [2], Batch [243/938], Loss: 1.033158779144287\n",
      "Train: Epoch [2], Batch [244/938], Loss: 1.1023893356323242\n",
      "Train: Epoch [2], Batch [245/938], Loss: 1.1066150665283203\n",
      "Train: Epoch [2], Batch [246/938], Loss: 1.139710783958435\n",
      "Train: Epoch [2], Batch [247/938], Loss: 1.079909324645996\n",
      "Train: Epoch [2], Batch [248/938], Loss: 0.999244213104248\n",
      "Train: Epoch [2], Batch [249/938], Loss: 1.0474541187286377\n",
      "Train: Epoch [2], Batch [250/938], Loss: 1.0358259677886963\n",
      "Train: Epoch [2], Batch [251/938], Loss: 0.9777300357818604\n",
      "Train: Epoch [2], Batch [252/938], Loss: 1.2720279693603516\n",
      "Train: Epoch [2], Batch [253/938], Loss: 0.9715102314949036\n",
      "Train: Epoch [2], Batch [254/938], Loss: 1.1223387718200684\n",
      "Train: Epoch [2], Batch [255/938], Loss: 1.02016282081604\n",
      "Train: Epoch [2], Batch [256/938], Loss: 1.0107614994049072\n",
      "Train: Epoch [2], Batch [257/938], Loss: 0.9379611015319824\n",
      "Train: Epoch [2], Batch [258/938], Loss: 0.9831433892250061\n",
      "Train: Epoch [2], Batch [259/938], Loss: 0.9567548036575317\n",
      "Train: Epoch [2], Batch [260/938], Loss: 1.0277979373931885\n",
      "Train: Epoch [2], Batch [261/938], Loss: 1.134589672088623\n",
      "Train: Epoch [2], Batch [262/938], Loss: 0.9103306531906128\n",
      "Train: Epoch [2], Batch [263/938], Loss: 1.1370947360992432\n",
      "Train: Epoch [2], Batch [264/938], Loss: 1.2133185863494873\n",
      "Train: Epoch [2], Batch [265/938], Loss: 0.9596716165542603\n",
      "Train: Epoch [2], Batch [266/938], Loss: 1.0072674751281738\n",
      "Train: Epoch [2], Batch [267/938], Loss: 1.0629708766937256\n",
      "Train: Epoch [2], Batch [268/938], Loss: 0.941208004951477\n",
      "Train: Epoch [2], Batch [269/938], Loss: 0.9854756593704224\n",
      "Train: Epoch [2], Batch [270/938], Loss: 0.9312992691993713\n",
      "Train: Epoch [2], Batch [271/938], Loss: 1.1418006420135498\n",
      "Train: Epoch [2], Batch [272/938], Loss: 0.939755916595459\n",
      "Train: Epoch [2], Batch [273/938], Loss: 1.04689621925354\n",
      "Train: Epoch [2], Batch [274/938], Loss: 0.9414730072021484\n",
      "Train: Epoch [2], Batch [275/938], Loss: 0.8763142228126526\n",
      "Train: Epoch [2], Batch [276/938], Loss: 1.0870118141174316\n",
      "Train: Epoch [2], Batch [277/938], Loss: 1.093017339706421\n",
      "Train: Epoch [2], Batch [278/938], Loss: 1.0559979677200317\n",
      "Train: Epoch [2], Batch [279/938], Loss: 0.9108926057815552\n",
      "Train: Epoch [2], Batch [280/938], Loss: 0.8715466260910034\n",
      "Train: Epoch [2], Batch [281/938], Loss: 0.8429200649261475\n",
      "Train: Epoch [2], Batch [282/938], Loss: 0.9272943735122681\n",
      "Train: Epoch [2], Batch [283/938], Loss: 1.046861171722412\n",
      "Train: Epoch [2], Batch [284/938], Loss: 0.8668655753135681\n",
      "Train: Epoch [2], Batch [285/938], Loss: 1.0405001640319824\n",
      "Train: Epoch [2], Batch [286/938], Loss: 1.048387050628662\n",
      "Train: Epoch [2], Batch [287/938], Loss: 0.9470080733299255\n",
      "Train: Epoch [2], Batch [288/938], Loss: 0.9636638164520264\n",
      "Train: Epoch [2], Batch [289/938], Loss: 0.9407042264938354\n",
      "Train: Epoch [2], Batch [290/938], Loss: 1.0383580923080444\n",
      "Train: Epoch [2], Batch [291/938], Loss: 0.8109361529350281\n",
      "Train: Epoch [2], Batch [292/938], Loss: 0.9534754753112793\n",
      "Train: Epoch [2], Batch [293/938], Loss: 0.8796802759170532\n",
      "Train: Epoch [2], Batch [294/938], Loss: 1.0707826614379883\n",
      "Train: Epoch [2], Batch [295/938], Loss: 1.2040222883224487\n",
      "Train: Epoch [2], Batch [296/938], Loss: 1.0826902389526367\n",
      "Train: Epoch [2], Batch [297/938], Loss: 0.9705881476402283\n",
      "Train: Epoch [2], Batch [298/938], Loss: 1.006638765335083\n",
      "Train: Epoch [2], Batch [299/938], Loss: 0.8442885875701904\n",
      "Train: Epoch [2], Batch [300/938], Loss: 0.7728949189186096\n",
      "Train: Epoch [2], Batch [301/938], Loss: 0.9668377637863159\n",
      "Train: Epoch [2], Batch [302/938], Loss: 1.0581461191177368\n",
      "Train: Epoch [2], Batch [303/938], Loss: 0.9936745166778564\n",
      "Train: Epoch [2], Batch [304/938], Loss: 0.9059008359909058\n",
      "Train: Epoch [2], Batch [305/938], Loss: 0.9581880569458008\n",
      "Train: Epoch [2], Batch [306/938], Loss: 1.0971665382385254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [307/938], Loss: 0.8803888559341431\n",
      "Train: Epoch [2], Batch [308/938], Loss: 0.975086510181427\n",
      "Train: Epoch [2], Batch [309/938], Loss: 0.8191405534744263\n",
      "Train: Epoch [2], Batch [310/938], Loss: 1.0011332035064697\n",
      "Train: Epoch [2], Batch [311/938], Loss: 1.0324456691741943\n",
      "Train: Epoch [2], Batch [312/938], Loss: 1.056957483291626\n",
      "Train: Epoch [2], Batch [313/938], Loss: 1.1232527494430542\n",
      "Train: Epoch [2], Batch [314/938], Loss: 1.0747618675231934\n",
      "Train: Epoch [2], Batch [315/938], Loss: 0.8974438905715942\n",
      "Train: Epoch [2], Batch [316/938], Loss: 0.945821225643158\n",
      "Train: Epoch [2], Batch [317/938], Loss: 0.8449810147285461\n",
      "Train: Epoch [2], Batch [318/938], Loss: 1.0894265174865723\n",
      "Train: Epoch [2], Batch [319/938], Loss: 0.9706758856773376\n",
      "Train: Epoch [2], Batch [320/938], Loss: 0.9921145439147949\n",
      "Train: Epoch [2], Batch [321/938], Loss: 0.9046674370765686\n",
      "Train: Epoch [2], Batch [322/938], Loss: 0.9161220192909241\n",
      "Train: Epoch [2], Batch [323/938], Loss: 0.9726946353912354\n",
      "Train: Epoch [2], Batch [324/938], Loss: 1.0561130046844482\n",
      "Train: Epoch [2], Batch [325/938], Loss: 1.0428996086120605\n",
      "Train: Epoch [2], Batch [326/938], Loss: 1.156270146369934\n",
      "Train: Epoch [2], Batch [327/938], Loss: 0.9203696250915527\n",
      "Train: Epoch [2], Batch [328/938], Loss: 1.082961916923523\n",
      "Train: Epoch [2], Batch [329/938], Loss: 1.0850602388381958\n",
      "Train: Epoch [2], Batch [330/938], Loss: 0.9846922755241394\n",
      "Train: Epoch [2], Batch [331/938], Loss: 0.9042768478393555\n",
      "Train: Epoch [2], Batch [332/938], Loss: 0.9257858991622925\n",
      "Train: Epoch [2], Batch [333/938], Loss: 1.0032671689987183\n",
      "Train: Epoch [2], Batch [334/938], Loss: 0.973739504814148\n",
      "Train: Epoch [2], Batch [335/938], Loss: 0.8962841629981995\n",
      "Train: Epoch [2], Batch [336/938], Loss: 0.872445821762085\n",
      "Train: Epoch [2], Batch [337/938], Loss: 0.9335778951644897\n",
      "Train: Epoch [2], Batch [338/938], Loss: 1.0158840417861938\n",
      "Train: Epoch [2], Batch [339/938], Loss: 0.829319179058075\n",
      "Train: Epoch [2], Batch [340/938], Loss: 0.9837965965270996\n",
      "Train: Epoch [2], Batch [341/938], Loss: 1.135660171508789\n",
      "Train: Epoch [2], Batch [342/938], Loss: 0.9625287652015686\n",
      "Train: Epoch [2], Batch [343/938], Loss: 0.9934700727462769\n",
      "Train: Epoch [2], Batch [344/938], Loss: 1.078363060951233\n",
      "Train: Epoch [2], Batch [345/938], Loss: 1.0073285102844238\n",
      "Train: Epoch [2], Batch [346/938], Loss: 1.151197910308838\n",
      "Train: Epoch [2], Batch [347/938], Loss: 1.0157313346862793\n",
      "Train: Epoch [2], Batch [348/938], Loss: 0.9569994807243347\n",
      "Train: Epoch [2], Batch [349/938], Loss: 0.9328330755233765\n",
      "Train: Epoch [2], Batch [350/938], Loss: 0.9747254848480225\n",
      "Train: Epoch [2], Batch [351/938], Loss: 0.9800291657447815\n",
      "Train: Epoch [2], Batch [352/938], Loss: 0.986594557762146\n",
      "Train: Epoch [2], Batch [353/938], Loss: 0.9407439231872559\n",
      "Train: Epoch [2], Batch [354/938], Loss: 0.9393390417098999\n",
      "Train: Epoch [2], Batch [355/938], Loss: 0.9574143290519714\n",
      "Train: Epoch [2], Batch [356/938], Loss: 0.9547398686408997\n",
      "Train: Epoch [2], Batch [357/938], Loss: 0.901818037033081\n",
      "Train: Epoch [2], Batch [358/938], Loss: 1.2191922664642334\n",
      "Train: Epoch [2], Batch [359/938], Loss: 1.1507997512817383\n",
      "Train: Epoch [2], Batch [360/938], Loss: 1.0296369791030884\n",
      "Train: Epoch [2], Batch [361/938], Loss: 0.9659833908081055\n",
      "Train: Epoch [2], Batch [362/938], Loss: 0.9636131525039673\n",
      "Train: Epoch [2], Batch [363/938], Loss: 0.925541877746582\n",
      "Train: Epoch [2], Batch [364/938], Loss: 0.8927322030067444\n",
      "Train: Epoch [2], Batch [365/938], Loss: 1.0006303787231445\n",
      "Train: Epoch [2], Batch [366/938], Loss: 1.0547816753387451\n",
      "Train: Epoch [2], Batch [367/938], Loss: 1.0742111206054688\n",
      "Train: Epoch [2], Batch [368/938], Loss: 1.0143831968307495\n",
      "Train: Epoch [2], Batch [369/938], Loss: 1.06937575340271\n",
      "Train: Epoch [2], Batch [370/938], Loss: 1.041483759880066\n",
      "Train: Epoch [2], Batch [371/938], Loss: 1.0007398128509521\n",
      "Train: Epoch [2], Batch [372/938], Loss: 1.0610239505767822\n",
      "Train: Epoch [2], Batch [373/938], Loss: 1.0719935894012451\n",
      "Train: Epoch [2], Batch [374/938], Loss: 1.1407052278518677\n",
      "Train: Epoch [2], Batch [375/938], Loss: 0.9146600961685181\n",
      "Train: Epoch [2], Batch [376/938], Loss: 0.8858732581138611\n",
      "Train: Epoch [2], Batch [377/938], Loss: 0.922715961933136\n",
      "Train: Epoch [2], Batch [378/938], Loss: 0.903873085975647\n",
      "Train: Epoch [2], Batch [379/938], Loss: 0.8664741516113281\n",
      "Train: Epoch [2], Batch [380/938], Loss: 1.0068745613098145\n",
      "Train: Epoch [2], Batch [381/938], Loss: 1.113767147064209\n",
      "Train: Epoch [2], Batch [382/938], Loss: 1.0918481349945068\n",
      "Train: Epoch [2], Batch [383/938], Loss: 0.8140619993209839\n",
      "Train: Epoch [2], Batch [384/938], Loss: 1.2173097133636475\n",
      "Train: Epoch [2], Batch [385/938], Loss: 0.9973402619361877\n",
      "Train: Epoch [2], Batch [386/938], Loss: 0.949680507183075\n",
      "Train: Epoch [2], Batch [387/938], Loss: 0.902725100517273\n",
      "Train: Epoch [2], Batch [388/938], Loss: 1.1166324615478516\n",
      "Train: Epoch [2], Batch [389/938], Loss: 1.0423924922943115\n",
      "Train: Epoch [2], Batch [390/938], Loss: 0.8265340924263\n",
      "Train: Epoch [2], Batch [391/938], Loss: 0.8808296918869019\n",
      "Train: Epoch [2], Batch [392/938], Loss: 1.068371057510376\n",
      "Train: Epoch [2], Batch [393/938], Loss: 0.8600086569786072\n",
      "Train: Epoch [2], Batch [394/938], Loss: 0.824271023273468\n",
      "Train: Epoch [2], Batch [395/938], Loss: 1.0557383298873901\n",
      "Train: Epoch [2], Batch [396/938], Loss: 0.9497566819190979\n",
      "Train: Epoch [2], Batch [397/938], Loss: 0.9303645491600037\n",
      "Train: Epoch [2], Batch [398/938], Loss: 1.0329687595367432\n",
      "Train: Epoch [2], Batch [399/938], Loss: 1.0908476114273071\n",
      "Train: Epoch [2], Batch [400/938], Loss: 1.0299972295761108\n",
      "Train: Epoch [2], Batch [401/938], Loss: 0.9155378341674805\n",
      "Train: Epoch [2], Batch [402/938], Loss: 0.9930731058120728\n",
      "Train: Epoch [2], Batch [403/938], Loss: 1.0010963678359985\n",
      "Train: Epoch [2], Batch [404/938], Loss: 0.9077964425086975\n",
      "Train: Epoch [2], Batch [405/938], Loss: 0.8838291168212891\n",
      "Train: Epoch [2], Batch [406/938], Loss: 0.9545699954032898\n",
      "Train: Epoch [2], Batch [407/938], Loss: 0.9213827252388\n",
      "Train: Epoch [2], Batch [408/938], Loss: 0.8751215934753418\n",
      "Train: Epoch [2], Batch [409/938], Loss: 0.960667610168457\n",
      "Train: Epoch [2], Batch [410/938], Loss: 0.9084332585334778\n",
      "Train: Epoch [2], Batch [411/938], Loss: 0.9157860279083252\n",
      "Train: Epoch [2], Batch [412/938], Loss: 0.9583492279052734\n",
      "Train: Epoch [2], Batch [413/938], Loss: 0.9260925054550171\n",
      "Train: Epoch [2], Batch [414/938], Loss: 0.9937832355499268\n",
      "Train: Epoch [2], Batch [415/938], Loss: 0.8898309469223022\n",
      "Train: Epoch [2], Batch [416/938], Loss: 0.9487936496734619\n",
      "Train: Epoch [2], Batch [417/938], Loss: 0.86493319272995\n",
      "Train: Epoch [2], Batch [418/938], Loss: 1.086529016494751\n",
      "Train: Epoch [2], Batch [419/938], Loss: 0.9592546820640564\n",
      "Train: Epoch [2], Batch [420/938], Loss: 0.9796212315559387\n",
      "Train: Epoch [2], Batch [421/938], Loss: 1.0117201805114746\n",
      "Train: Epoch [2], Batch [422/938], Loss: 1.0738000869750977\n",
      "Train: Epoch [2], Batch [423/938], Loss: 0.8764848113059998\n",
      "Train: Epoch [2], Batch [424/938], Loss: 0.9449173212051392\n",
      "Train: Epoch [2], Batch [425/938], Loss: 0.9985338449478149\n",
      "Train: Epoch [2], Batch [426/938], Loss: 0.8757220506668091\n",
      "Train: Epoch [2], Batch [427/938], Loss: 0.9158833026885986\n",
      "Train: Epoch [2], Batch [428/938], Loss: 1.1676114797592163\n",
      "Train: Epoch [2], Batch [429/938], Loss: 0.8233845233917236\n",
      "Train: Epoch [2], Batch [430/938], Loss: 0.8577839136123657\n",
      "Train: Epoch [2], Batch [431/938], Loss: 0.8401355147361755\n",
      "Train: Epoch [2], Batch [432/938], Loss: 0.8999066948890686\n",
      "Train: Epoch [2], Batch [433/938], Loss: 0.8128044605255127\n",
      "Train: Epoch [2], Batch [434/938], Loss: 1.2510933876037598\n",
      "Train: Epoch [2], Batch [435/938], Loss: 0.974919319152832\n",
      "Train: Epoch [2], Batch [436/938], Loss: 1.0136284828186035\n",
      "Train: Epoch [2], Batch [437/938], Loss: 0.9262207746505737\n",
      "Train: Epoch [2], Batch [438/938], Loss: 0.8457598686218262\n",
      "Train: Epoch [2], Batch [439/938], Loss: 0.9205822944641113\n",
      "Train: Epoch [2], Batch [440/938], Loss: 0.9889782071113586\n",
      "Train: Epoch [2], Batch [441/938], Loss: 1.0497827529907227\n",
      "Train: Epoch [2], Batch [442/938], Loss: 0.8868967294692993\n",
      "Train: Epoch [2], Batch [443/938], Loss: 0.834141731262207\n",
      "Train: Epoch [2], Batch [444/938], Loss: 0.9683643579483032\n",
      "Train: Epoch [2], Batch [445/938], Loss: 1.0539348125457764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [446/938], Loss: 0.9448440074920654\n",
      "Train: Epoch [2], Batch [447/938], Loss: 1.0134265422821045\n",
      "Train: Epoch [2], Batch [448/938], Loss: 0.8764525651931763\n",
      "Train: Epoch [2], Batch [449/938], Loss: 0.9231092929840088\n",
      "Train: Epoch [2], Batch [450/938], Loss: 0.9675180912017822\n",
      "Train: Epoch [2], Batch [451/938], Loss: 0.867139995098114\n",
      "Train: Epoch [2], Batch [452/938], Loss: 0.9710951447486877\n",
      "Train: Epoch [2], Batch [453/938], Loss: 1.0533347129821777\n",
      "Train: Epoch [2], Batch [454/938], Loss: 1.008735179901123\n",
      "Train: Epoch [2], Batch [455/938], Loss: 0.8329343795776367\n",
      "Train: Epoch [2], Batch [456/938], Loss: 0.7982492446899414\n",
      "Train: Epoch [2], Batch [457/938], Loss: 0.9038776159286499\n",
      "Train: Epoch [2], Batch [458/938], Loss: 0.9221121668815613\n",
      "Train: Epoch [2], Batch [459/938], Loss: 0.9899665713310242\n",
      "Train: Epoch [2], Batch [460/938], Loss: 1.1588627099990845\n",
      "Train: Epoch [2], Batch [461/938], Loss: 0.9652829170227051\n",
      "Train: Epoch [2], Batch [462/938], Loss: 1.1854875087738037\n",
      "Train: Epoch [2], Batch [463/938], Loss: 1.114690899848938\n",
      "Train: Epoch [2], Batch [464/938], Loss: 1.1404857635498047\n",
      "Train: Epoch [2], Batch [465/938], Loss: 0.8541615009307861\n",
      "Train: Epoch [2], Batch [466/938], Loss: 1.0060771703720093\n",
      "Train: Epoch [2], Batch [467/938], Loss: 1.017401933670044\n",
      "Train: Epoch [2], Batch [468/938], Loss: 0.871870219707489\n",
      "Train: Epoch [2], Batch [469/938], Loss: 1.0175812244415283\n",
      "Train: Epoch [2], Batch [470/938], Loss: 0.9308246374130249\n",
      "Train: Epoch [2], Batch [471/938], Loss: 0.9543994665145874\n",
      "Train: Epoch [2], Batch [472/938], Loss: 1.1230765581130981\n",
      "Train: Epoch [2], Batch [473/938], Loss: 0.939814567565918\n",
      "Train: Epoch [2], Batch [474/938], Loss: 0.8781079649925232\n",
      "Train: Epoch [2], Batch [475/938], Loss: 0.88850998878479\n",
      "Train: Epoch [2], Batch [476/938], Loss: 1.0199834108352661\n",
      "Train: Epoch [2], Batch [477/938], Loss: 0.9788094162940979\n",
      "Train: Epoch [2], Batch [478/938], Loss: 0.9579988718032837\n",
      "Train: Epoch [2], Batch [479/938], Loss: 0.9142048358917236\n",
      "Train: Epoch [2], Batch [480/938], Loss: 1.03743314743042\n",
      "Train: Epoch [2], Batch [481/938], Loss: 0.9692244529724121\n",
      "Train: Epoch [2], Batch [482/938], Loss: 0.9990239143371582\n",
      "Train: Epoch [2], Batch [483/938], Loss: 0.9065538644790649\n",
      "Train: Epoch [2], Batch [484/938], Loss: 1.177805781364441\n",
      "Train: Epoch [2], Batch [485/938], Loss: 0.9162324070930481\n",
      "Train: Epoch [2], Batch [486/938], Loss: 0.9517430067062378\n",
      "Train: Epoch [2], Batch [487/938], Loss: 0.8714638948440552\n",
      "Train: Epoch [2], Batch [488/938], Loss: 1.0008809566497803\n",
      "Train: Epoch [2], Batch [489/938], Loss: 1.1057497262954712\n",
      "Train: Epoch [2], Batch [490/938], Loss: 0.7641698122024536\n",
      "Train: Epoch [2], Batch [491/938], Loss: 0.89121013879776\n",
      "Train: Epoch [2], Batch [492/938], Loss: 0.971378743648529\n",
      "Train: Epoch [2], Batch [493/938], Loss: 0.9951340556144714\n",
      "Train: Epoch [2], Batch [494/938], Loss: 1.0244040489196777\n",
      "Train: Epoch [2], Batch [495/938], Loss: 1.0852704048156738\n",
      "Train: Epoch [2], Batch [496/938], Loss: 0.8629312515258789\n",
      "Train: Epoch [2], Batch [497/938], Loss: 0.8962894082069397\n",
      "Train: Epoch [2], Batch [498/938], Loss: 1.0684585571289062\n",
      "Train: Epoch [2], Batch [499/938], Loss: 0.926793098449707\n",
      "Train: Epoch [2], Batch [500/938], Loss: 0.8042608499526978\n",
      "Train: Epoch [2], Batch [501/938], Loss: 0.8832026720046997\n",
      "Train: Epoch [2], Batch [502/938], Loss: 0.8973075151443481\n",
      "Train: Epoch [2], Batch [503/938], Loss: 0.8687620162963867\n",
      "Train: Epoch [2], Batch [504/938], Loss: 0.9915040731430054\n",
      "Train: Epoch [2], Batch [505/938], Loss: 1.0106184482574463\n",
      "Train: Epoch [2], Batch [506/938], Loss: 0.9076541662216187\n",
      "Train: Epoch [2], Batch [507/938], Loss: 0.9854796528816223\n",
      "Train: Epoch [2], Batch [508/938], Loss: 0.7642092108726501\n",
      "Train: Epoch [2], Batch [509/938], Loss: 1.2173815965652466\n",
      "Train: Epoch [2], Batch [510/938], Loss: 0.7965505123138428\n",
      "Train: Epoch [2], Batch [511/938], Loss: 1.0007166862487793\n",
      "Train: Epoch [2], Batch [512/938], Loss: 1.0486236810684204\n",
      "Train: Epoch [2], Batch [513/938], Loss: 0.8769721984863281\n",
      "Train: Epoch [2], Batch [514/938], Loss: 0.9826942682266235\n",
      "Train: Epoch [2], Batch [515/938], Loss: 1.0816256999969482\n",
      "Train: Epoch [2], Batch [516/938], Loss: 1.2511703968048096\n",
      "Train: Epoch [2], Batch [517/938], Loss: 0.848499596118927\n",
      "Train: Epoch [2], Batch [518/938], Loss: 0.9986875653266907\n",
      "Train: Epoch [2], Batch [519/938], Loss: 0.8702476024627686\n",
      "Train: Epoch [2], Batch [520/938], Loss: 0.9854726195335388\n",
      "Train: Epoch [2], Batch [521/938], Loss: 0.9900578856468201\n",
      "Train: Epoch [2], Batch [522/938], Loss: 0.8815649747848511\n",
      "Train: Epoch [2], Batch [523/938], Loss: 1.0368866920471191\n",
      "Train: Epoch [2], Batch [524/938], Loss: 1.1234041452407837\n",
      "Train: Epoch [2], Batch [525/938], Loss: 0.9876434206962585\n",
      "Train: Epoch [2], Batch [526/938], Loss: 0.8591027855873108\n",
      "Train: Epoch [2], Batch [527/938], Loss: 0.9506931304931641\n",
      "Train: Epoch [2], Batch [528/938], Loss: 1.0119223594665527\n",
      "Train: Epoch [2], Batch [529/938], Loss: 1.038109302520752\n",
      "Train: Epoch [2], Batch [530/938], Loss: 1.127090573310852\n",
      "Train: Epoch [2], Batch [531/938], Loss: 0.8841425776481628\n",
      "Train: Epoch [2], Batch [532/938], Loss: 0.9144341945648193\n",
      "Train: Epoch [2], Batch [533/938], Loss: 0.9248896241188049\n",
      "Train: Epoch [2], Batch [534/938], Loss: 0.8772778511047363\n",
      "Train: Epoch [2], Batch [535/938], Loss: 0.8873531818389893\n",
      "Train: Epoch [2], Batch [536/938], Loss: 1.0701690912246704\n",
      "Train: Epoch [2], Batch [537/938], Loss: 0.833054780960083\n",
      "Train: Epoch [2], Batch [538/938], Loss: 0.9980741143226624\n",
      "Train: Epoch [2], Batch [539/938], Loss: 0.8317197561264038\n",
      "Train: Epoch [2], Batch [540/938], Loss: 0.8728182315826416\n",
      "Train: Epoch [2], Batch [541/938], Loss: 0.8328561186790466\n",
      "Train: Epoch [2], Batch [542/938], Loss: 0.9182000160217285\n",
      "Train: Epoch [2], Batch [543/938], Loss: 0.8408302068710327\n",
      "Train: Epoch [2], Batch [544/938], Loss: 1.0981683731079102\n",
      "Train: Epoch [2], Batch [545/938], Loss: 1.1151024103164673\n",
      "Train: Epoch [2], Batch [546/938], Loss: 0.9828146696090698\n",
      "Train: Epoch [2], Batch [547/938], Loss: 1.1720387935638428\n",
      "Train: Epoch [2], Batch [548/938], Loss: 0.8583028316497803\n",
      "Train: Epoch [2], Batch [549/938], Loss: 0.8410066366195679\n",
      "Train: Epoch [2], Batch [550/938], Loss: 0.8250179290771484\n",
      "Train: Epoch [2], Batch [551/938], Loss: 0.9022559523582458\n",
      "Train: Epoch [2], Batch [552/938], Loss: 0.751413106918335\n",
      "Train: Epoch [2], Batch [553/938], Loss: 0.9827166795730591\n",
      "Train: Epoch [2], Batch [554/938], Loss: 0.9813075065612793\n",
      "Train: Epoch [2], Batch [555/938], Loss: 0.9201959371566772\n",
      "Train: Epoch [2], Batch [556/938], Loss: 0.8834711313247681\n",
      "Train: Epoch [2], Batch [557/938], Loss: 1.1519315242767334\n",
      "Train: Epoch [2], Batch [558/938], Loss: 1.0236952304840088\n",
      "Train: Epoch [2], Batch [559/938], Loss: 0.7703198194503784\n",
      "Train: Epoch [2], Batch [560/938], Loss: 1.1299715042114258\n",
      "Train: Epoch [2], Batch [561/938], Loss: 0.9324434399604797\n",
      "Train: Epoch [2], Batch [562/938], Loss: 0.8811553716659546\n",
      "Train: Epoch [2], Batch [563/938], Loss: 0.9145981073379517\n",
      "Train: Epoch [2], Batch [564/938], Loss: 1.0067694187164307\n",
      "Train: Epoch [2], Batch [565/938], Loss: 0.9571828842163086\n",
      "Train: Epoch [2], Batch [566/938], Loss: 0.9724065661430359\n",
      "Train: Epoch [2], Batch [567/938], Loss: 1.0934741497039795\n",
      "Train: Epoch [2], Batch [568/938], Loss: 0.8849669694900513\n",
      "Train: Epoch [2], Batch [569/938], Loss: 0.9075444936752319\n",
      "Train: Epoch [2], Batch [570/938], Loss: 1.1080743074417114\n",
      "Train: Epoch [2], Batch [571/938], Loss: 0.8684430122375488\n",
      "Train: Epoch [2], Batch [572/938], Loss: 0.8057882189750671\n",
      "Train: Epoch [2], Batch [573/938], Loss: 0.7981923818588257\n",
      "Train: Epoch [2], Batch [574/938], Loss: 0.9996505975723267\n",
      "Train: Epoch [2], Batch [575/938], Loss: 0.8675674796104431\n",
      "Train: Epoch [2], Batch [576/938], Loss: 0.8805373907089233\n",
      "Train: Epoch [2], Batch [577/938], Loss: 1.226989984512329\n",
      "Train: Epoch [2], Batch [578/938], Loss: 0.923607349395752\n",
      "Train: Epoch [2], Batch [579/938], Loss: 0.9341657161712646\n",
      "Train: Epoch [2], Batch [580/938], Loss: 0.9660531282424927\n",
      "Train: Epoch [2], Batch [581/938], Loss: 1.2363970279693604\n",
      "Train: Epoch [2], Batch [582/938], Loss: 0.8909658193588257\n",
      "Train: Epoch [2], Batch [583/938], Loss: 0.8240015506744385\n",
      "Train: Epoch [2], Batch [584/938], Loss: 0.8675717711448669\n",
      "Train: Epoch [2], Batch [585/938], Loss: 0.8054611682891846\n",
      "Train: Epoch [2], Batch [586/938], Loss: 0.93849778175354\n",
      "Train: Epoch [2], Batch [587/938], Loss: 0.8017237782478333\n",
      "Train: Epoch [2], Batch [588/938], Loss: 0.8617600202560425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [589/938], Loss: 0.9132596254348755\n",
      "Train: Epoch [2], Batch [590/938], Loss: 0.760248064994812\n",
      "Train: Epoch [2], Batch [591/938], Loss: 0.8679866790771484\n",
      "Train: Epoch [2], Batch [592/938], Loss: 0.9916009902954102\n",
      "Train: Epoch [2], Batch [593/938], Loss: 0.8666902780532837\n",
      "Train: Epoch [2], Batch [594/938], Loss: 0.8414490222930908\n",
      "Train: Epoch [2], Batch [595/938], Loss: 0.8790454268455505\n",
      "Train: Epoch [2], Batch [596/938], Loss: 0.8816637992858887\n",
      "Train: Epoch [2], Batch [597/938], Loss: 0.9559834599494934\n",
      "Train: Epoch [2], Batch [598/938], Loss: 0.7892661094665527\n",
      "Train: Epoch [2], Batch [599/938], Loss: 0.9182717204093933\n",
      "Train: Epoch [2], Batch [600/938], Loss: 1.0179558992385864\n",
      "Train: Epoch [2], Batch [601/938], Loss: 1.0822474956512451\n",
      "Train: Epoch [2], Batch [602/938], Loss: 0.8962185978889465\n",
      "Train: Epoch [2], Batch [603/938], Loss: 1.1021716594696045\n",
      "Train: Epoch [2], Batch [604/938], Loss: 0.7934964895248413\n",
      "Train: Epoch [2], Batch [605/938], Loss: 1.4199533462524414\n",
      "Train: Epoch [2], Batch [606/938], Loss: 0.8986088037490845\n",
      "Train: Epoch [2], Batch [607/938], Loss: 0.7109209895133972\n",
      "Train: Epoch [2], Batch [608/938], Loss: 0.9383028745651245\n",
      "Train: Epoch [2], Batch [609/938], Loss: 1.033279538154602\n",
      "Train: Epoch [2], Batch [610/938], Loss: 1.0194923877716064\n",
      "Train: Epoch [2], Batch [611/938], Loss: 1.2338132858276367\n",
      "Train: Epoch [2], Batch [612/938], Loss: 0.8755767345428467\n",
      "Train: Epoch [2], Batch [613/938], Loss: 1.061837911605835\n",
      "Train: Epoch [2], Batch [614/938], Loss: 0.8189803957939148\n",
      "Train: Epoch [2], Batch [615/938], Loss: 0.8842163681983948\n",
      "Train: Epoch [2], Batch [616/938], Loss: 0.7822519540786743\n",
      "Train: Epoch [2], Batch [617/938], Loss: 1.1057204008102417\n",
      "Train: Epoch [2], Batch [618/938], Loss: 0.7961314916610718\n",
      "Train: Epoch [2], Batch [619/938], Loss: 0.8752856850624084\n",
      "Train: Epoch [2], Batch [620/938], Loss: 0.9213322401046753\n",
      "Train: Epoch [2], Batch [621/938], Loss: 0.8478502631187439\n",
      "Train: Epoch [2], Batch [622/938], Loss: 1.0411003828048706\n",
      "Train: Epoch [2], Batch [623/938], Loss: 0.8493822813034058\n",
      "Train: Epoch [2], Batch [624/938], Loss: 0.815332293510437\n",
      "Train: Epoch [2], Batch [625/938], Loss: 0.8813686370849609\n",
      "Train: Epoch [2], Batch [626/938], Loss: 0.9556481838226318\n",
      "Train: Epoch [2], Batch [627/938], Loss: 1.013594388961792\n",
      "Train: Epoch [2], Batch [628/938], Loss: 0.8725579977035522\n",
      "Train: Epoch [2], Batch [629/938], Loss: 0.9432575702667236\n",
      "Train: Epoch [2], Batch [630/938], Loss: 0.9601926803588867\n",
      "Train: Epoch [2], Batch [631/938], Loss: 1.0238596200942993\n",
      "Train: Epoch [2], Batch [632/938], Loss: 0.9291205406188965\n",
      "Train: Epoch [2], Batch [633/938], Loss: 0.8570669889450073\n",
      "Train: Epoch [2], Batch [634/938], Loss: 1.1962798833847046\n",
      "Train: Epoch [2], Batch [635/938], Loss: 0.9743327498435974\n",
      "Train: Epoch [2], Batch [636/938], Loss: 0.7722663879394531\n",
      "Train: Epoch [2], Batch [637/938], Loss: 0.8570486307144165\n",
      "Train: Epoch [2], Batch [638/938], Loss: 0.8403783440589905\n",
      "Train: Epoch [2], Batch [639/938], Loss: 1.1712472438812256\n",
      "Train: Epoch [2], Batch [640/938], Loss: 0.9067780375480652\n",
      "Train: Epoch [2], Batch [641/938], Loss: 0.9348406195640564\n",
      "Train: Epoch [2], Batch [642/938], Loss: 1.0369226932525635\n",
      "Train: Epoch [2], Batch [643/938], Loss: 0.7857992053031921\n",
      "Train: Epoch [2], Batch [644/938], Loss: 0.8305634260177612\n",
      "Train: Epoch [2], Batch [645/938], Loss: 0.9260925650596619\n",
      "Train: Epoch [2], Batch [646/938], Loss: 0.9440645575523376\n",
      "Train: Epoch [2], Batch [647/938], Loss: 1.0966391563415527\n",
      "Train: Epoch [2], Batch [648/938], Loss: 0.9535130262374878\n",
      "Train: Epoch [2], Batch [649/938], Loss: 0.9804195165634155\n",
      "Train: Epoch [2], Batch [650/938], Loss: 0.9391248226165771\n",
      "Train: Epoch [2], Batch [651/938], Loss: 0.9349640011787415\n",
      "Train: Epoch [2], Batch [652/938], Loss: 0.9589866399765015\n",
      "Train: Epoch [2], Batch [653/938], Loss: 0.8574511408805847\n",
      "Train: Epoch [2], Batch [654/938], Loss: 0.8880525827407837\n",
      "Train: Epoch [2], Batch [655/938], Loss: 0.8792044520378113\n",
      "Train: Epoch [2], Batch [656/938], Loss: 0.8156819343566895\n",
      "Train: Epoch [2], Batch [657/938], Loss: 1.0023930072784424\n",
      "Train: Epoch [2], Batch [658/938], Loss: 0.9607222080230713\n",
      "Train: Epoch [2], Batch [659/938], Loss: 1.0251424312591553\n",
      "Train: Epoch [2], Batch [660/938], Loss: 0.9794436097145081\n",
      "Train: Epoch [2], Batch [661/938], Loss: 0.7149245142936707\n",
      "Train: Epoch [2], Batch [662/938], Loss: 0.9360921382904053\n",
      "Train: Epoch [2], Batch [663/938], Loss: 1.1276391744613647\n",
      "Train: Epoch [2], Batch [664/938], Loss: 0.7617117762565613\n",
      "Train: Epoch [2], Batch [665/938], Loss: 1.1249313354492188\n",
      "Train: Epoch [2], Batch [666/938], Loss: 0.8884847164154053\n",
      "Train: Epoch [2], Batch [667/938], Loss: 1.038541316986084\n",
      "Train: Epoch [2], Batch [668/938], Loss: 0.939425528049469\n",
      "Train: Epoch [2], Batch [669/938], Loss: 0.9680207967758179\n",
      "Train: Epoch [2], Batch [670/938], Loss: 0.8256059885025024\n",
      "Train: Epoch [2], Batch [671/938], Loss: 0.8615074157714844\n",
      "Train: Epoch [2], Batch [672/938], Loss: 0.9197028875350952\n",
      "Train: Epoch [2], Batch [673/938], Loss: 0.8804302215576172\n",
      "Train: Epoch [2], Batch [674/938], Loss: 0.9223418235778809\n",
      "Train: Epoch [2], Batch [675/938], Loss: 0.9983773231506348\n",
      "Train: Epoch [2], Batch [676/938], Loss: 1.112962245941162\n",
      "Train: Epoch [2], Batch [677/938], Loss: 1.056731104850769\n",
      "Train: Epoch [2], Batch [678/938], Loss: 1.0947721004486084\n",
      "Train: Epoch [2], Batch [679/938], Loss: 0.87680983543396\n",
      "Train: Epoch [2], Batch [680/938], Loss: 0.6925691962242126\n",
      "Train: Epoch [2], Batch [681/938], Loss: 0.8159381151199341\n",
      "Train: Epoch [2], Batch [682/938], Loss: 0.9144001603126526\n",
      "Train: Epoch [2], Batch [683/938], Loss: 0.8531782627105713\n",
      "Train: Epoch [2], Batch [684/938], Loss: 0.806079089641571\n",
      "Train: Epoch [2], Batch [685/938], Loss: 0.9606175422668457\n",
      "Train: Epoch [2], Batch [686/938], Loss: 0.8945817947387695\n",
      "Train: Epoch [2], Batch [687/938], Loss: 0.8897175192832947\n",
      "Train: Epoch [2], Batch [688/938], Loss: 0.8838533163070679\n",
      "Train: Epoch [2], Batch [689/938], Loss: 0.7961190938949585\n",
      "Train: Epoch [2], Batch [690/938], Loss: 0.8377683162689209\n",
      "Train: Epoch [2], Batch [691/938], Loss: 0.7144908905029297\n",
      "Train: Epoch [2], Batch [692/938], Loss: 0.8556506633758545\n",
      "Train: Epoch [2], Batch [693/938], Loss: 0.7599920034408569\n",
      "Train: Epoch [2], Batch [694/938], Loss: 0.9130857586860657\n",
      "Train: Epoch [2], Batch [695/938], Loss: 0.7914096117019653\n",
      "Train: Epoch [2], Batch [696/938], Loss: 0.8972312808036804\n",
      "Train: Epoch [2], Batch [697/938], Loss: 1.114643931388855\n",
      "Train: Epoch [2], Batch [698/938], Loss: 0.8597185611724854\n",
      "Train: Epoch [2], Batch [699/938], Loss: 0.9697271585464478\n",
      "Train: Epoch [2], Batch [700/938], Loss: 1.0183676481246948\n",
      "Train: Epoch [2], Batch [701/938], Loss: 0.9184017777442932\n",
      "Train: Epoch [2], Batch [702/938], Loss: 1.040278673171997\n",
      "Train: Epoch [2], Batch [703/938], Loss: 0.8239137530326843\n",
      "Train: Epoch [2], Batch [704/938], Loss: 0.8906941413879395\n",
      "Train: Epoch [2], Batch [705/938], Loss: 1.0409736633300781\n",
      "Train: Epoch [2], Batch [706/938], Loss: 0.9124119877815247\n",
      "Train: Epoch [2], Batch [707/938], Loss: 0.9311312437057495\n",
      "Train: Epoch [2], Batch [708/938], Loss: 0.8592153787612915\n",
      "Train: Epoch [2], Batch [709/938], Loss: 0.8087970018386841\n",
      "Train: Epoch [2], Batch [710/938], Loss: 1.018986463546753\n",
      "Train: Epoch [2], Batch [711/938], Loss: 0.8711458444595337\n",
      "Train: Epoch [2], Batch [712/938], Loss: 1.0873212814331055\n",
      "Train: Epoch [2], Batch [713/938], Loss: 0.9748082160949707\n",
      "Train: Epoch [2], Batch [714/938], Loss: 0.8156118392944336\n",
      "Train: Epoch [2], Batch [715/938], Loss: 0.8022933602333069\n",
      "Train: Epoch [2], Batch [716/938], Loss: 0.9131051898002625\n",
      "Train: Epoch [2], Batch [717/938], Loss: 0.8417955636978149\n",
      "Train: Epoch [2], Batch [718/938], Loss: 1.1652119159698486\n",
      "Train: Epoch [2], Batch [719/938], Loss: 1.0258009433746338\n",
      "Train: Epoch [2], Batch [720/938], Loss: 0.7736809253692627\n",
      "Train: Epoch [2], Batch [721/938], Loss: 0.8932081460952759\n",
      "Train: Epoch [2], Batch [722/938], Loss: 1.0095024108886719\n",
      "Train: Epoch [2], Batch [723/938], Loss: 0.7206658124923706\n",
      "Train: Epoch [2], Batch [724/938], Loss: 0.7844294309616089\n",
      "Train: Epoch [2], Batch [725/938], Loss: 0.8621747493743896\n",
      "Train: Epoch [2], Batch [726/938], Loss: 0.9265373945236206\n",
      "Train: Epoch [2], Batch [727/938], Loss: 0.8317552208900452\n",
      "Train: Epoch [2], Batch [728/938], Loss: 0.9047262072563171\n",
      "Train: Epoch [2], Batch [729/938], Loss: 0.9048880934715271\n",
      "Train: Epoch [2], Batch [730/938], Loss: 0.7017650604248047\n",
      "Train: Epoch [2], Batch [731/938], Loss: 1.0827374458312988\n",
      "Train: Epoch [2], Batch [732/938], Loss: 0.8687448501586914\n",
      "Train: Epoch [2], Batch [733/938], Loss: 1.221618890762329\n",
      "Train: Epoch [2], Batch [734/938], Loss: 0.8483164310455322\n",
      "Train: Epoch [2], Batch [735/938], Loss: 0.7961742877960205\n",
      "Train: Epoch [2], Batch [736/938], Loss: 0.8719687461853027\n",
      "Train: Epoch [2], Batch [737/938], Loss: 0.7612622380256653\n",
      "Train: Epoch [2], Batch [738/938], Loss: 0.9141750931739807\n",
      "Train: Epoch [2], Batch [739/938], Loss: 1.03696870803833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [740/938], Loss: 0.9189320802688599\n",
      "Train: Epoch [2], Batch [741/938], Loss: 0.9301939010620117\n",
      "Train: Epoch [2], Batch [742/938], Loss: 0.7996474504470825\n",
      "Train: Epoch [2], Batch [743/938], Loss: 1.0340627431869507\n",
      "Train: Epoch [2], Batch [744/938], Loss: 0.8140745162963867\n",
      "Train: Epoch [2], Batch [745/938], Loss: 1.0400714874267578\n",
      "Train: Epoch [2], Batch [746/938], Loss: 0.9022281169891357\n",
      "Train: Epoch [2], Batch [747/938], Loss: 0.8732665181159973\n",
      "Train: Epoch [2], Batch [748/938], Loss: 0.7190778255462646\n",
      "Train: Epoch [2], Batch [749/938], Loss: 0.9590165615081787\n",
      "Train: Epoch [2], Batch [750/938], Loss: 0.7658933997154236\n",
      "Train: Epoch [2], Batch [751/938], Loss: 0.7954838275909424\n",
      "Train: Epoch [2], Batch [752/938], Loss: 0.8942500352859497\n",
      "Train: Epoch [2], Batch [753/938], Loss: 0.9124525189399719\n",
      "Train: Epoch [2], Batch [754/938], Loss: 0.9252775311470032\n",
      "Train: Epoch [2], Batch [755/938], Loss: 0.931875467300415\n",
      "Train: Epoch [2], Batch [756/938], Loss: 1.1403801441192627\n",
      "Train: Epoch [2], Batch [757/938], Loss: 0.9674716591835022\n",
      "Train: Epoch [2], Batch [758/938], Loss: 0.9402427077293396\n",
      "Train: Epoch [2], Batch [759/938], Loss: 0.8210257887840271\n",
      "Train: Epoch [2], Batch [760/938], Loss: 0.728645920753479\n",
      "Train: Epoch [2], Batch [761/938], Loss: 0.773848295211792\n",
      "Train: Epoch [2], Batch [762/938], Loss: 0.9361475110054016\n",
      "Train: Epoch [2], Batch [763/938], Loss: 0.82177734375\n",
      "Train: Epoch [2], Batch [764/938], Loss: 0.8078172206878662\n",
      "Train: Epoch [2], Batch [765/938], Loss: 0.8634029030799866\n",
      "Train: Epoch [2], Batch [766/938], Loss: 0.9170753955841064\n",
      "Train: Epoch [2], Batch [767/938], Loss: 1.0886129140853882\n",
      "Train: Epoch [2], Batch [768/938], Loss: 0.867012083530426\n",
      "Train: Epoch [2], Batch [769/938], Loss: 1.0578227043151855\n",
      "Train: Epoch [2], Batch [770/938], Loss: 0.8230047821998596\n",
      "Train: Epoch [2], Batch [771/938], Loss: 0.8998878002166748\n",
      "Train: Epoch [2], Batch [772/938], Loss: 1.0476040840148926\n",
      "Train: Epoch [2], Batch [773/938], Loss: 0.8526003360748291\n",
      "Train: Epoch [2], Batch [774/938], Loss: 0.9571773409843445\n",
      "Train: Epoch [2], Batch [775/938], Loss: 1.0475775003433228\n",
      "Train: Epoch [2], Batch [776/938], Loss: 0.8814699053764343\n",
      "Train: Epoch [2], Batch [777/938], Loss: 0.9416681528091431\n",
      "Train: Epoch [2], Batch [778/938], Loss: 0.8441574573516846\n",
      "Train: Epoch [2], Batch [779/938], Loss: 0.8874595165252686\n",
      "Train: Epoch [2], Batch [780/938], Loss: 0.8373143672943115\n",
      "Train: Epoch [2], Batch [781/938], Loss: 0.8908582925796509\n",
      "Train: Epoch [2], Batch [782/938], Loss: 0.8300468921661377\n",
      "Train: Epoch [2], Batch [783/938], Loss: 0.9666826725006104\n",
      "Train: Epoch [2], Batch [784/938], Loss: 0.9656702280044556\n",
      "Train: Epoch [2], Batch [785/938], Loss: 1.0072283744812012\n",
      "Train: Epoch [2], Batch [786/938], Loss: 0.8528634309768677\n",
      "Train: Epoch [2], Batch [787/938], Loss: 0.8767926692962646\n",
      "Train: Epoch [2], Batch [788/938], Loss: 0.8831601142883301\n",
      "Train: Epoch [2], Batch [789/938], Loss: 0.94577556848526\n",
      "Train: Epoch [2], Batch [790/938], Loss: 0.8480192422866821\n",
      "Train: Epoch [2], Batch [791/938], Loss: 0.816335916519165\n",
      "Train: Epoch [2], Batch [792/938], Loss: 0.8810368180274963\n",
      "Train: Epoch [2], Batch [793/938], Loss: 0.9171115159988403\n",
      "Train: Epoch [2], Batch [794/938], Loss: 0.8245962858200073\n",
      "Train: Epoch [2], Batch [795/938], Loss: 1.0226722955703735\n",
      "Train: Epoch [2], Batch [796/938], Loss: 0.8753088116645813\n",
      "Train: Epoch [2], Batch [797/938], Loss: 1.1235179901123047\n",
      "Train: Epoch [2], Batch [798/938], Loss: 1.0223184823989868\n",
      "Train: Epoch [2], Batch [799/938], Loss: 0.8363164663314819\n",
      "Train: Epoch [2], Batch [800/938], Loss: 0.9494743943214417\n",
      "Train: Epoch [2], Batch [801/938], Loss: 1.1137473583221436\n",
      "Train: Epoch [2], Batch [802/938], Loss: 0.9930052161216736\n",
      "Train: Epoch [2], Batch [803/938], Loss: 0.8846992254257202\n",
      "Train: Epoch [2], Batch [804/938], Loss: 0.7697130441665649\n",
      "Train: Epoch [2], Batch [805/938], Loss: 0.9184261560440063\n",
      "Train: Epoch [2], Batch [806/938], Loss: 1.0405898094177246\n",
      "Train: Epoch [2], Batch [807/938], Loss: 0.8732812404632568\n",
      "Train: Epoch [2], Batch [808/938], Loss: 0.9188092350959778\n",
      "Train: Epoch [2], Batch [809/938], Loss: 0.8107579350471497\n",
      "Train: Epoch [2], Batch [810/938], Loss: 0.7946963906288147\n",
      "Train: Epoch [2], Batch [811/938], Loss: 0.851004958152771\n",
      "Train: Epoch [2], Batch [812/938], Loss: 0.8629387617111206\n",
      "Train: Epoch [2], Batch [813/938], Loss: 1.1219829320907593\n",
      "Train: Epoch [2], Batch [814/938], Loss: 0.8903441429138184\n",
      "Train: Epoch [2], Batch [815/938], Loss: 0.8081180453300476\n",
      "Train: Epoch [2], Batch [816/938], Loss: 0.8870408535003662\n",
      "Train: Epoch [2], Batch [817/938], Loss: 0.8911446928977966\n",
      "Train: Epoch [2], Batch [818/938], Loss: 0.9511812925338745\n",
      "Train: Epoch [2], Batch [819/938], Loss: 1.1657536029815674\n",
      "Train: Epoch [2], Batch [820/938], Loss: 0.9496709704399109\n",
      "Train: Epoch [2], Batch [821/938], Loss: 0.8658058047294617\n",
      "Train: Epoch [2], Batch [822/938], Loss: 0.7757353782653809\n",
      "Train: Epoch [2], Batch [823/938], Loss: 0.7068710327148438\n",
      "Train: Epoch [2], Batch [824/938], Loss: 0.9653167128562927\n",
      "Train: Epoch [2], Batch [825/938], Loss: 1.063542366027832\n",
      "Train: Epoch [2], Batch [826/938], Loss: 0.9163203239440918\n",
      "Train: Epoch [2], Batch [827/938], Loss: 0.8137177228927612\n",
      "Train: Epoch [2], Batch [828/938], Loss: 0.7786571979522705\n",
      "Train: Epoch [2], Batch [829/938], Loss: 0.9775713682174683\n",
      "Train: Epoch [2], Batch [830/938], Loss: 0.9178706407546997\n",
      "Train: Epoch [2], Batch [831/938], Loss: 0.8940102458000183\n",
      "Train: Epoch [2], Batch [832/938], Loss: 0.9448349475860596\n",
      "Train: Epoch [2], Batch [833/938], Loss: 0.8067796230316162\n",
      "Train: Epoch [2], Batch [834/938], Loss: 0.9996929168701172\n",
      "Train: Epoch [2], Batch [835/938], Loss: 0.8904115557670593\n",
      "Train: Epoch [2], Batch [836/938], Loss: 1.1247344017028809\n",
      "Train: Epoch [2], Batch [837/938], Loss: 0.9790920615196228\n",
      "Train: Epoch [2], Batch [838/938], Loss: 1.0480422973632812\n",
      "Train: Epoch [2], Batch [839/938], Loss: 0.9452463388442993\n",
      "Train: Epoch [2], Batch [840/938], Loss: 1.0018259286880493\n",
      "Train: Epoch [2], Batch [841/938], Loss: 0.8532454371452332\n",
      "Train: Epoch [2], Batch [842/938], Loss: 0.8823291063308716\n",
      "Train: Epoch [2], Batch [843/938], Loss: 0.8771277666091919\n",
      "Train: Epoch [2], Batch [844/938], Loss: 1.0205917358398438\n",
      "Train: Epoch [2], Batch [845/938], Loss: 0.7099165916442871\n",
      "Train: Epoch [2], Batch [846/938], Loss: 0.8090785145759583\n",
      "Train: Epoch [2], Batch [847/938], Loss: 1.0195411443710327\n",
      "Train: Epoch [2], Batch [848/938], Loss: 0.8036322593688965\n",
      "Train: Epoch [2], Batch [849/938], Loss: 0.7316169738769531\n",
      "Train: Epoch [2], Batch [850/938], Loss: 1.2692197561264038\n",
      "Train: Epoch [2], Batch [851/938], Loss: 0.796578049659729\n",
      "Train: Epoch [2], Batch [852/938], Loss: 0.7506071329116821\n",
      "Train: Epoch [2], Batch [853/938], Loss: 0.7062473297119141\n",
      "Train: Epoch [2], Batch [854/938], Loss: 0.7230943441390991\n",
      "Train: Epoch [2], Batch [855/938], Loss: 0.9978477954864502\n",
      "Train: Epoch [2], Batch [856/938], Loss: 0.9868168830871582\n",
      "Train: Epoch [2], Batch [857/938], Loss: 0.6766667366027832\n",
      "Train: Epoch [2], Batch [858/938], Loss: 0.8305890560150146\n",
      "Train: Epoch [2], Batch [859/938], Loss: 1.1746231317520142\n",
      "Train: Epoch [2], Batch [860/938], Loss: 0.9280291795730591\n",
      "Train: Epoch [2], Batch [861/938], Loss: 0.8540952801704407\n",
      "Train: Epoch [2], Batch [862/938], Loss: 0.9602307677268982\n",
      "Train: Epoch [2], Batch [863/938], Loss: 0.8330413103103638\n",
      "Train: Epoch [2], Batch [864/938], Loss: 0.903994083404541\n",
      "Train: Epoch [2], Batch [865/938], Loss: 1.2011430263519287\n",
      "Train: Epoch [2], Batch [866/938], Loss: 0.8970094919204712\n",
      "Train: Epoch [2], Batch [867/938], Loss: 1.0877920389175415\n",
      "Train: Epoch [2], Batch [868/938], Loss: 0.8015221357345581\n",
      "Train: Epoch [2], Batch [869/938], Loss: 1.044936180114746\n",
      "Train: Epoch [2], Batch [870/938], Loss: 0.780164361000061\n",
      "Train: Epoch [2], Batch [871/938], Loss: 0.8484780788421631\n",
      "Train: Epoch [2], Batch [872/938], Loss: 0.8071787357330322\n",
      "Train: Epoch [2], Batch [873/938], Loss: 0.8888547420501709\n",
      "Train: Epoch [2], Batch [874/938], Loss: 0.9836646318435669\n",
      "Train: Epoch [2], Batch [875/938], Loss: 0.9329748749732971\n",
      "Train: Epoch [2], Batch [876/938], Loss: 1.1572855710983276\n",
      "Train: Epoch [2], Batch [877/938], Loss: 0.9433255195617676\n",
      "Train: Epoch [2], Batch [878/938], Loss: 0.9374003410339355\n",
      "Train: Epoch [2], Batch [879/938], Loss: 0.8779271841049194\n",
      "Train: Epoch [2], Batch [880/938], Loss: 0.8098070621490479\n",
      "Train: Epoch [2], Batch [881/938], Loss: 0.7627770900726318\n",
      "Train: Epoch [2], Batch [882/938], Loss: 1.1271737813949585\n",
      "Train: Epoch [2], Batch [883/938], Loss: 0.7927214503288269\n",
      "Train: Epoch [2], Batch [884/938], Loss: 0.9444960355758667\n",
      "Train: Epoch [2], Batch [885/938], Loss: 0.8744812607765198\n",
      "Train: Epoch [2], Batch [886/938], Loss: 1.0017170906066895\n",
      "Train: Epoch [2], Batch [887/938], Loss: 0.8202909231185913\n",
      "Train: Epoch [2], Batch [888/938], Loss: 0.8472414016723633\n",
      "Train: Epoch [2], Batch [889/938], Loss: 0.7319402694702148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [2], Batch [890/938], Loss: 0.6872996091842651\n",
      "Train: Epoch [2], Batch [891/938], Loss: 0.8809874057769775\n",
      "Train: Epoch [2], Batch [892/938], Loss: 1.2933865785598755\n",
      "Train: Epoch [2], Batch [893/938], Loss: 0.9481216669082642\n",
      "Train: Epoch [2], Batch [894/938], Loss: 0.8206741809844971\n",
      "Train: Epoch [2], Batch [895/938], Loss: 0.7979438304901123\n",
      "Train: Epoch [2], Batch [896/938], Loss: 1.144883394241333\n",
      "Train: Epoch [2], Batch [897/938], Loss: 0.877571702003479\n",
      "Train: Epoch [2], Batch [898/938], Loss: 0.7682361602783203\n",
      "Train: Epoch [2], Batch [899/938], Loss: 1.07816743850708\n",
      "Train: Epoch [2], Batch [900/938], Loss: 0.8325350284576416\n",
      "Train: Epoch [2], Batch [901/938], Loss: 0.6749709248542786\n",
      "Train: Epoch [2], Batch [902/938], Loss: 0.6840922832489014\n",
      "Train: Epoch [2], Batch [903/938], Loss: 0.7979124784469604\n",
      "Train: Epoch [2], Batch [904/938], Loss: 0.9469215869903564\n",
      "Train: Epoch [2], Batch [905/938], Loss: 0.9787079691886902\n",
      "Train: Epoch [2], Batch [906/938], Loss: 1.062608242034912\n",
      "Train: Epoch [2], Batch [907/938], Loss: 1.0178860425949097\n",
      "Train: Epoch [2], Batch [908/938], Loss: 0.8275094032287598\n",
      "Train: Epoch [2], Batch [909/938], Loss: 0.8290882110595703\n",
      "Train: Epoch [2], Batch [910/938], Loss: 0.793390154838562\n",
      "Train: Epoch [2], Batch [911/938], Loss: 0.9441497325897217\n",
      "Train: Epoch [2], Batch [912/938], Loss: 0.9314477443695068\n",
      "Train: Epoch [2], Batch [913/938], Loss: 0.9034425020217896\n",
      "Train: Epoch [2], Batch [914/938], Loss: 0.9456465244293213\n",
      "Train: Epoch [2], Batch [915/938], Loss: 0.6603246927261353\n",
      "Train: Epoch [2], Batch [916/938], Loss: 0.878103494644165\n",
      "Train: Epoch [2], Batch [917/938], Loss: 0.854124128818512\n",
      "Train: Epoch [2], Batch [918/938], Loss: 0.8929803371429443\n",
      "Train: Epoch [2], Batch [919/938], Loss: 0.7402185797691345\n",
      "Train: Epoch [2], Batch [920/938], Loss: 0.8004671931266785\n",
      "Train: Epoch [2], Batch [921/938], Loss: 0.5931841731071472\n",
      "Train: Epoch [2], Batch [922/938], Loss: 1.006137728691101\n",
      "Train: Epoch [2], Batch [923/938], Loss: 0.6651952266693115\n",
      "Train: Epoch [2], Batch [924/938], Loss: 0.6114871501922607\n",
      "Train: Epoch [2], Batch [925/938], Loss: 0.749395489692688\n",
      "Train: Epoch [2], Batch [926/938], Loss: 0.7876279950141907\n",
      "Train: Epoch [2], Batch [927/938], Loss: 0.9910831451416016\n",
      "Train: Epoch [2], Batch [928/938], Loss: 0.7644490003585815\n",
      "Train: Epoch [2], Batch [929/938], Loss: 0.7707843780517578\n",
      "Train: Epoch [2], Batch [930/938], Loss: 1.2742332220077515\n",
      "Train: Epoch [2], Batch [931/938], Loss: 0.7286497354507446\n",
      "Train: Epoch [2], Batch [932/938], Loss: 0.9077970385551453\n",
      "Train: Epoch [2], Batch [933/938], Loss: 0.6898692846298218\n",
      "Train: Epoch [2], Batch [934/938], Loss: 0.7124712467193604\n",
      "Train: Epoch [2], Batch [935/938], Loss: 0.8432855606079102\n",
      "Train: Epoch [2], Batch [936/938], Loss: 0.796737551689148\n",
      "Train: Epoch [2], Batch [937/938], Loss: 0.8521617650985718\n",
      "Train: Epoch [2], Batch [938/938], Loss: 0.7742274403572083\n",
      "Accuracy of train set: 0.618\n",
      "Validation: Epoch [2], Batch [1/938], Loss: 1.0307945013046265\n",
      "Validation: Epoch [2], Batch [2/938], Loss: 0.7555228471755981\n",
      "Validation: Epoch [2], Batch [3/938], Loss: 0.9983220100402832\n",
      "Validation: Epoch [2], Batch [4/938], Loss: 0.8685652017593384\n",
      "Validation: Epoch [2], Batch [5/938], Loss: 1.021161437034607\n",
      "Validation: Epoch [2], Batch [6/938], Loss: 0.9420297145843506\n",
      "Validation: Epoch [2], Batch [7/938], Loss: 0.9175224304199219\n",
      "Validation: Epoch [2], Batch [8/938], Loss: 0.780638575553894\n",
      "Validation: Epoch [2], Batch [9/938], Loss: 0.6955065727233887\n",
      "Validation: Epoch [2], Batch [10/938], Loss: 0.9620580673217773\n",
      "Validation: Epoch [2], Batch [11/938], Loss: 0.8674121499061584\n",
      "Validation: Epoch [2], Batch [12/938], Loss: 0.9577265977859497\n",
      "Validation: Epoch [2], Batch [13/938], Loss: 0.9124569296836853\n",
      "Validation: Epoch [2], Batch [14/938], Loss: 1.0604088306427002\n",
      "Validation: Epoch [2], Batch [15/938], Loss: 0.8487354516983032\n",
      "Validation: Epoch [2], Batch [16/938], Loss: 0.9100885391235352\n",
      "Validation: Epoch [2], Batch [17/938], Loss: 0.8477487564086914\n",
      "Validation: Epoch [2], Batch [18/938], Loss: 0.9287821054458618\n",
      "Validation: Epoch [2], Batch [19/938], Loss: 0.7470638155937195\n",
      "Validation: Epoch [2], Batch [20/938], Loss: 0.8649803400039673\n",
      "Validation: Epoch [2], Batch [21/938], Loss: 0.9004867076873779\n",
      "Validation: Epoch [2], Batch [22/938], Loss: 0.7466673851013184\n",
      "Validation: Epoch [2], Batch [23/938], Loss: 0.562746524810791\n",
      "Validation: Epoch [2], Batch [24/938], Loss: 0.8774922490119934\n",
      "Validation: Epoch [2], Batch [25/938], Loss: 0.7835782766342163\n",
      "Validation: Epoch [2], Batch [26/938], Loss: 0.9895601868629456\n",
      "Validation: Epoch [2], Batch [27/938], Loss: 0.7768893837928772\n",
      "Validation: Epoch [2], Batch [28/938], Loss: 0.9429236054420471\n",
      "Validation: Epoch [2], Batch [29/938], Loss: 1.0785927772521973\n",
      "Validation: Epoch [2], Batch [30/938], Loss: 0.8664425611495972\n",
      "Validation: Epoch [2], Batch [31/938], Loss: 0.8500494956970215\n",
      "Validation: Epoch [2], Batch [32/938], Loss: 0.8739422559738159\n",
      "Validation: Epoch [2], Batch [33/938], Loss: 0.973006010055542\n",
      "Validation: Epoch [2], Batch [34/938], Loss: 0.7433919906616211\n",
      "Validation: Epoch [2], Batch [35/938], Loss: 0.9138470888137817\n",
      "Validation: Epoch [2], Batch [36/938], Loss: 0.7948347330093384\n",
      "Validation: Epoch [2], Batch [37/938], Loss: 1.0161769390106201\n",
      "Validation: Epoch [2], Batch [38/938], Loss: 0.6515602469444275\n",
      "Validation: Epoch [2], Batch [39/938], Loss: 0.9859777092933655\n",
      "Validation: Epoch [2], Batch [40/938], Loss: 0.7137162089347839\n",
      "Validation: Epoch [2], Batch [41/938], Loss: 0.8758671283721924\n",
      "Validation: Epoch [2], Batch [42/938], Loss: 0.9574929475784302\n",
      "Validation: Epoch [2], Batch [43/938], Loss: 0.8732312917709351\n",
      "Validation: Epoch [2], Batch [44/938], Loss: 0.9380693435668945\n",
      "Validation: Epoch [2], Batch [45/938], Loss: 0.916334331035614\n",
      "Validation: Epoch [2], Batch [46/938], Loss: 0.664297342300415\n",
      "Validation: Epoch [2], Batch [47/938], Loss: 0.6586272716522217\n",
      "Validation: Epoch [2], Batch [48/938], Loss: 0.858198881149292\n",
      "Validation: Epoch [2], Batch [49/938], Loss: 0.9877645373344421\n",
      "Validation: Epoch [2], Batch [50/938], Loss: 0.8370067477226257\n",
      "Validation: Epoch [2], Batch [51/938], Loss: 0.8354811072349548\n",
      "Validation: Epoch [2], Batch [52/938], Loss: 1.1769275665283203\n",
      "Validation: Epoch [2], Batch [53/938], Loss: 0.7536108493804932\n",
      "Validation: Epoch [2], Batch [54/938], Loss: 0.9197878241539001\n",
      "Validation: Epoch [2], Batch [55/938], Loss: 0.8823341131210327\n",
      "Validation: Epoch [2], Batch [56/938], Loss: 0.8664857149124146\n",
      "Validation: Epoch [2], Batch [57/938], Loss: 0.902411699295044\n",
      "Validation: Epoch [2], Batch [58/938], Loss: 0.8798849582672119\n",
      "Validation: Epoch [2], Batch [59/938], Loss: 0.8393028974533081\n",
      "Validation: Epoch [2], Batch [60/938], Loss: 0.8513473272323608\n",
      "Validation: Epoch [2], Batch [61/938], Loss: 0.9372280836105347\n",
      "Validation: Epoch [2], Batch [62/938], Loss: 0.8084347248077393\n",
      "Validation: Epoch [2], Batch [63/938], Loss: 1.0544111728668213\n",
      "Validation: Epoch [2], Batch [64/938], Loss: 0.7162008285522461\n",
      "Validation: Epoch [2], Batch [65/938], Loss: 0.850042462348938\n",
      "Validation: Epoch [2], Batch [66/938], Loss: 0.8214061260223389\n",
      "Validation: Epoch [2], Batch [67/938], Loss: 0.8294833898544312\n",
      "Validation: Epoch [2], Batch [68/938], Loss: 0.7274466753005981\n",
      "Validation: Epoch [2], Batch [69/938], Loss: 0.9930669665336609\n",
      "Validation: Epoch [2], Batch [70/938], Loss: 0.8605469465255737\n",
      "Validation: Epoch [2], Batch [71/938], Loss: 0.9437379240989685\n",
      "Validation: Epoch [2], Batch [72/938], Loss: 0.7302349805831909\n",
      "Validation: Epoch [2], Batch [73/938], Loss: 0.7592123746871948\n",
      "Validation: Epoch [2], Batch [74/938], Loss: 0.8575845956802368\n",
      "Validation: Epoch [2], Batch [75/938], Loss: 0.9198275804519653\n",
      "Validation: Epoch [2], Batch [76/938], Loss: 1.0934317111968994\n",
      "Validation: Epoch [2], Batch [77/938], Loss: 0.801219642162323\n",
      "Validation: Epoch [2], Batch [78/938], Loss: 0.9370888471603394\n",
      "Validation: Epoch [2], Batch [79/938], Loss: 0.7663477659225464\n",
      "Validation: Epoch [2], Batch [80/938], Loss: 1.2966300249099731\n",
      "Validation: Epoch [2], Batch [81/938], Loss: 0.9285529851913452\n",
      "Validation: Epoch [2], Batch [82/938], Loss: 0.9342431426048279\n",
      "Validation: Epoch [2], Batch [83/938], Loss: 0.8834636807441711\n",
      "Validation: Epoch [2], Batch [84/938], Loss: 0.942215085029602\n",
      "Validation: Epoch [2], Batch [85/938], Loss: 1.0328459739685059\n",
      "Validation: Epoch [2], Batch [86/938], Loss: 0.9573177099227905\n",
      "Validation: Epoch [2], Batch [87/938], Loss: 0.7440533638000488\n",
      "Validation: Epoch [2], Batch [88/938], Loss: 1.2659120559692383\n",
      "Validation: Epoch [2], Batch [89/938], Loss: 0.8948419094085693\n",
      "Validation: Epoch [2], Batch [90/938], Loss: 0.871821403503418\n",
      "Validation: Epoch [2], Batch [91/938], Loss: 1.0126547813415527\n",
      "Validation: Epoch [2], Batch [92/938], Loss: 0.8379473686218262\n",
      "Validation: Epoch [2], Batch [93/938], Loss: 0.8579356670379639\n",
      "Validation: Epoch [2], Batch [94/938], Loss: 0.8895812630653381\n",
      "Validation: Epoch [2], Batch [95/938], Loss: 0.7443272471427917\n",
      "Validation: Epoch [2], Batch [96/938], Loss: 0.8116371631622314\n",
      "Validation: Epoch [2], Batch [97/938], Loss: 0.7612006664276123\n",
      "Validation: Epoch [2], Batch [98/938], Loss: 1.099799633026123\n",
      "Validation: Epoch [2], Batch [99/938], Loss: 0.8583313822746277\n",
      "Validation: Epoch [2], Batch [100/938], Loss: 0.9411714673042297\n",
      "Validation: Epoch [2], Batch [101/938], Loss: 0.9953225255012512\n",
      "Validation: Epoch [2], Batch [102/938], Loss: 0.9997003674507141\n",
      "Validation: Epoch [2], Batch [103/938], Loss: 0.7656772136688232\n",
      "Validation: Epoch [2], Batch [104/938], Loss: 0.8782666921615601\n",
      "Validation: Epoch [2], Batch [105/938], Loss: 1.219746470451355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [106/938], Loss: 0.7036913633346558\n",
      "Validation: Epoch [2], Batch [107/938], Loss: 0.8534324169158936\n",
      "Validation: Epoch [2], Batch [108/938], Loss: 0.8500981330871582\n",
      "Validation: Epoch [2], Batch [109/938], Loss: 1.0571202039718628\n",
      "Validation: Epoch [2], Batch [110/938], Loss: 0.9957704544067383\n",
      "Validation: Epoch [2], Batch [111/938], Loss: 0.8085882067680359\n",
      "Validation: Epoch [2], Batch [112/938], Loss: 0.8648684620857239\n",
      "Validation: Epoch [2], Batch [113/938], Loss: 0.8436692953109741\n",
      "Validation: Epoch [2], Batch [114/938], Loss: 0.7983918190002441\n",
      "Validation: Epoch [2], Batch [115/938], Loss: 0.8326457738876343\n",
      "Validation: Epoch [2], Batch [116/938], Loss: 0.7837518453598022\n",
      "Validation: Epoch [2], Batch [117/938], Loss: 0.891423761844635\n",
      "Validation: Epoch [2], Batch [118/938], Loss: 0.9057013988494873\n",
      "Validation: Epoch [2], Batch [119/938], Loss: 0.9109190702438354\n",
      "Validation: Epoch [2], Batch [120/938], Loss: 0.8733394145965576\n",
      "Validation: Epoch [2], Batch [121/938], Loss: 0.9330016374588013\n",
      "Validation: Epoch [2], Batch [122/938], Loss: 0.9429758787155151\n",
      "Validation: Epoch [2], Batch [123/938], Loss: 0.8699613809585571\n",
      "Validation: Epoch [2], Batch [124/938], Loss: 0.8009222745895386\n",
      "Validation: Epoch [2], Batch [125/938], Loss: 0.8478853106498718\n",
      "Validation: Epoch [2], Batch [126/938], Loss: 0.9065430760383606\n",
      "Validation: Epoch [2], Batch [127/938], Loss: 0.8704341650009155\n",
      "Validation: Epoch [2], Batch [128/938], Loss: 0.8180233836174011\n",
      "Validation: Epoch [2], Batch [129/938], Loss: 1.030924916267395\n",
      "Validation: Epoch [2], Batch [130/938], Loss: 0.9554135203361511\n",
      "Validation: Epoch [2], Batch [131/938], Loss: 0.9205682277679443\n",
      "Validation: Epoch [2], Batch [132/938], Loss: 0.9264493584632874\n",
      "Validation: Epoch [2], Batch [133/938], Loss: 1.0371402502059937\n",
      "Validation: Epoch [2], Batch [134/938], Loss: 1.095638632774353\n",
      "Validation: Epoch [2], Batch [135/938], Loss: 0.8323012590408325\n",
      "Validation: Epoch [2], Batch [136/938], Loss: 0.7148914337158203\n",
      "Validation: Epoch [2], Batch [137/938], Loss: 0.7973328828811646\n",
      "Validation: Epoch [2], Batch [138/938], Loss: 1.1042885780334473\n",
      "Validation: Epoch [2], Batch [139/938], Loss: 0.7820447683334351\n",
      "Validation: Epoch [2], Batch [140/938], Loss: 0.8905178904533386\n",
      "Validation: Epoch [2], Batch [141/938], Loss: 0.7700014114379883\n",
      "Validation: Epoch [2], Batch [142/938], Loss: 1.0473065376281738\n",
      "Validation: Epoch [2], Batch [143/938], Loss: 0.9280607104301453\n",
      "Validation: Epoch [2], Batch [144/938], Loss: 0.9269489049911499\n",
      "Validation: Epoch [2], Batch [145/938], Loss: 0.8325334191322327\n",
      "Validation: Epoch [2], Batch [146/938], Loss: 0.6786113977432251\n",
      "Validation: Epoch [2], Batch [147/938], Loss: 0.8251724243164062\n",
      "Validation: Epoch [2], Batch [148/938], Loss: 0.8409161567687988\n",
      "Validation: Epoch [2], Batch [149/938], Loss: 0.9450326561927795\n",
      "Validation: Epoch [2], Batch [150/938], Loss: 0.9344019889831543\n",
      "Validation: Epoch [2], Batch [151/938], Loss: 0.8123695850372314\n",
      "Validation: Epoch [2], Batch [152/938], Loss: 1.0629104375839233\n",
      "Validation: Epoch [2], Batch [153/938], Loss: 0.9239062666893005\n",
      "Validation: Epoch [2], Batch [154/938], Loss: 0.8193337917327881\n",
      "Validation: Epoch [2], Batch [155/938], Loss: 0.8169506788253784\n",
      "Validation: Epoch [2], Batch [156/938], Loss: 0.7257199883460999\n",
      "Validation: Epoch [2], Batch [157/938], Loss: 0.7753486037254333\n",
      "Validation: Epoch [2], Batch [158/938], Loss: 0.8738391399383545\n",
      "Validation: Epoch [2], Batch [159/938], Loss: 1.0229073762893677\n",
      "Validation: Epoch [2], Batch [160/938], Loss: 1.0357050895690918\n",
      "Validation: Epoch [2], Batch [161/938], Loss: 0.6697874069213867\n",
      "Validation: Epoch [2], Batch [162/938], Loss: 0.7451832294464111\n",
      "Validation: Epoch [2], Batch [163/938], Loss: 0.8223277926445007\n",
      "Validation: Epoch [2], Batch [164/938], Loss: 0.8442720174789429\n",
      "Validation: Epoch [2], Batch [165/938], Loss: 0.8617454767227173\n",
      "Validation: Epoch [2], Batch [166/938], Loss: 0.8187187314033508\n",
      "Validation: Epoch [2], Batch [167/938], Loss: 0.9239615201950073\n",
      "Validation: Epoch [2], Batch [168/938], Loss: 0.7714499235153198\n",
      "Validation: Epoch [2], Batch [169/938], Loss: 0.9603285193443298\n",
      "Validation: Epoch [2], Batch [170/938], Loss: 0.8487476110458374\n",
      "Validation: Epoch [2], Batch [171/938], Loss: 0.9395751357078552\n",
      "Validation: Epoch [2], Batch [172/938], Loss: 0.9490169286727905\n",
      "Validation: Epoch [2], Batch [173/938], Loss: 0.8325660228729248\n",
      "Validation: Epoch [2], Batch [174/938], Loss: 0.9675619006156921\n",
      "Validation: Epoch [2], Batch [175/938], Loss: 0.90129154920578\n",
      "Validation: Epoch [2], Batch [176/938], Loss: 0.7783856391906738\n",
      "Validation: Epoch [2], Batch [177/938], Loss: 1.029484748840332\n",
      "Validation: Epoch [2], Batch [178/938], Loss: 0.8555599451065063\n",
      "Validation: Epoch [2], Batch [179/938], Loss: 0.8149545788764954\n",
      "Validation: Epoch [2], Batch [180/938], Loss: 0.9316846132278442\n",
      "Validation: Epoch [2], Batch [181/938], Loss: 0.790975034236908\n",
      "Validation: Epoch [2], Batch [182/938], Loss: 0.926432192325592\n",
      "Validation: Epoch [2], Batch [183/938], Loss: 1.0835249423980713\n",
      "Validation: Epoch [2], Batch [184/938], Loss: 0.8328632116317749\n",
      "Validation: Epoch [2], Batch [185/938], Loss: 1.0101432800292969\n",
      "Validation: Epoch [2], Batch [186/938], Loss: 0.9195760488510132\n",
      "Validation: Epoch [2], Batch [187/938], Loss: 0.8579550981521606\n",
      "Validation: Epoch [2], Batch [188/938], Loss: 0.9212280511856079\n",
      "Validation: Epoch [2], Batch [189/938], Loss: 0.7936335206031799\n",
      "Validation: Epoch [2], Batch [190/938], Loss: 0.9859306812286377\n",
      "Validation: Epoch [2], Batch [191/938], Loss: 1.0010533332824707\n",
      "Validation: Epoch [2], Batch [192/938], Loss: 0.8644273281097412\n",
      "Validation: Epoch [2], Batch [193/938], Loss: 0.8396775722503662\n",
      "Validation: Epoch [2], Batch [194/938], Loss: 1.0653969049453735\n",
      "Validation: Epoch [2], Batch [195/938], Loss: 0.9510446786880493\n",
      "Validation: Epoch [2], Batch [196/938], Loss: 1.0130729675292969\n",
      "Validation: Epoch [2], Batch [197/938], Loss: 0.9218372106552124\n",
      "Validation: Epoch [2], Batch [198/938], Loss: 0.7940740585327148\n",
      "Validation: Epoch [2], Batch [199/938], Loss: 0.9430119395256042\n",
      "Validation: Epoch [2], Batch [200/938], Loss: 0.9018924236297607\n",
      "Validation: Epoch [2], Batch [201/938], Loss: 0.8814346790313721\n",
      "Validation: Epoch [2], Batch [202/938], Loss: 0.8931276798248291\n",
      "Validation: Epoch [2], Batch [203/938], Loss: 0.8342218399047852\n",
      "Validation: Epoch [2], Batch [204/938], Loss: 1.1203409433364868\n",
      "Validation: Epoch [2], Batch [205/938], Loss: 0.9714124202728271\n",
      "Validation: Epoch [2], Batch [206/938], Loss: 0.9314766526222229\n",
      "Validation: Epoch [2], Batch [207/938], Loss: 0.9441237449645996\n",
      "Validation: Epoch [2], Batch [208/938], Loss: 1.0272631645202637\n",
      "Validation: Epoch [2], Batch [209/938], Loss: 0.7164940237998962\n",
      "Validation: Epoch [2], Batch [210/938], Loss: 0.8786837458610535\n",
      "Validation: Epoch [2], Batch [211/938], Loss: 0.8842277526855469\n",
      "Validation: Epoch [2], Batch [212/938], Loss: 0.7076364755630493\n",
      "Validation: Epoch [2], Batch [213/938], Loss: 0.8754181861877441\n",
      "Validation: Epoch [2], Batch [214/938], Loss: 0.8862823843955994\n",
      "Validation: Epoch [2], Batch [215/938], Loss: 0.8663649559020996\n",
      "Validation: Epoch [2], Batch [216/938], Loss: 0.869073748588562\n",
      "Validation: Epoch [2], Batch [217/938], Loss: 1.023693323135376\n",
      "Validation: Epoch [2], Batch [218/938], Loss: 0.8488373756408691\n",
      "Validation: Epoch [2], Batch [219/938], Loss: 0.8726884126663208\n",
      "Validation: Epoch [2], Batch [220/938], Loss: 0.9445101022720337\n",
      "Validation: Epoch [2], Batch [221/938], Loss: 0.8893828392028809\n",
      "Validation: Epoch [2], Batch [222/938], Loss: 1.0483886003494263\n",
      "Validation: Epoch [2], Batch [223/938], Loss: 0.7432131767272949\n",
      "Validation: Epoch [2], Batch [224/938], Loss: 0.8576490879058838\n",
      "Validation: Epoch [2], Batch [225/938], Loss: 0.8099051713943481\n",
      "Validation: Epoch [2], Batch [226/938], Loss: 0.87249755859375\n",
      "Validation: Epoch [2], Batch [227/938], Loss: 0.9414048790931702\n",
      "Validation: Epoch [2], Batch [228/938], Loss: 0.7813982367515564\n",
      "Validation: Epoch [2], Batch [229/938], Loss: 0.8260324001312256\n",
      "Validation: Epoch [2], Batch [230/938], Loss: 0.8330933451652527\n",
      "Validation: Epoch [2], Batch [231/938], Loss: 0.7758841514587402\n",
      "Validation: Epoch [2], Batch [232/938], Loss: 0.9783579111099243\n",
      "Validation: Epoch [2], Batch [233/938], Loss: 0.7494395971298218\n",
      "Validation: Epoch [2], Batch [234/938], Loss: 0.9032387137413025\n",
      "Validation: Epoch [2], Batch [235/938], Loss: 0.8106392621994019\n",
      "Validation: Epoch [2], Batch [236/938], Loss: 0.7460271120071411\n",
      "Validation: Epoch [2], Batch [237/938], Loss: 1.0637500286102295\n",
      "Validation: Epoch [2], Batch [238/938], Loss: 0.7896213531494141\n",
      "Validation: Epoch [2], Batch [239/938], Loss: 0.9383280277252197\n",
      "Validation: Epoch [2], Batch [240/938], Loss: 0.86103755235672\n",
      "Validation: Epoch [2], Batch [241/938], Loss: 1.0171183347702026\n",
      "Validation: Epoch [2], Batch [242/938], Loss: 0.7750612497329712\n",
      "Validation: Epoch [2], Batch [243/938], Loss: 0.7458845376968384\n",
      "Validation: Epoch [2], Batch [244/938], Loss: 0.9243845343589783\n",
      "Validation: Epoch [2], Batch [245/938], Loss: 0.7020465135574341\n",
      "Validation: Epoch [2], Batch [246/938], Loss: 1.0763416290283203\n",
      "Validation: Epoch [2], Batch [247/938], Loss: 0.9798358678817749\n",
      "Validation: Epoch [2], Batch [248/938], Loss: 0.7784658670425415\n",
      "Validation: Epoch [2], Batch [249/938], Loss: 0.8194438219070435\n",
      "Validation: Epoch [2], Batch [250/938], Loss: 0.9969782829284668\n",
      "Validation: Epoch [2], Batch [251/938], Loss: 1.0667195320129395\n",
      "Validation: Epoch [2], Batch [252/938], Loss: 0.9040574431419373\n",
      "Validation: Epoch [2], Batch [253/938], Loss: 0.7688077688217163\n",
      "Validation: Epoch [2], Batch [254/938], Loss: 0.826664388179779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [255/938], Loss: 0.9653710722923279\n",
      "Validation: Epoch [2], Batch [256/938], Loss: 0.6872263550758362\n",
      "Validation: Epoch [2], Batch [257/938], Loss: 0.7849835157394409\n",
      "Validation: Epoch [2], Batch [258/938], Loss: 0.7517027258872986\n",
      "Validation: Epoch [2], Batch [259/938], Loss: 0.9365242123603821\n",
      "Validation: Epoch [2], Batch [260/938], Loss: 1.0453355312347412\n",
      "Validation: Epoch [2], Batch [261/938], Loss: 0.8453023433685303\n",
      "Validation: Epoch [2], Batch [262/938], Loss: 0.897537112236023\n",
      "Validation: Epoch [2], Batch [263/938], Loss: 0.9414039850234985\n",
      "Validation: Epoch [2], Batch [264/938], Loss: 1.0921108722686768\n",
      "Validation: Epoch [2], Batch [265/938], Loss: 0.927350640296936\n",
      "Validation: Epoch [2], Batch [266/938], Loss: 0.7867875099182129\n",
      "Validation: Epoch [2], Batch [267/938], Loss: 0.9341297149658203\n",
      "Validation: Epoch [2], Batch [268/938], Loss: 0.9201105833053589\n",
      "Validation: Epoch [2], Batch [269/938], Loss: 0.8792767524719238\n",
      "Validation: Epoch [2], Batch [270/938], Loss: 0.806174099445343\n",
      "Validation: Epoch [2], Batch [271/938], Loss: 0.7428936958312988\n",
      "Validation: Epoch [2], Batch [272/938], Loss: 0.8143678903579712\n",
      "Validation: Epoch [2], Batch [273/938], Loss: 0.823968231678009\n",
      "Validation: Epoch [2], Batch [274/938], Loss: 0.766586184501648\n",
      "Validation: Epoch [2], Batch [275/938], Loss: 0.9369479417800903\n",
      "Validation: Epoch [2], Batch [276/938], Loss: 0.7911500930786133\n",
      "Validation: Epoch [2], Batch [277/938], Loss: 0.8503562211990356\n",
      "Validation: Epoch [2], Batch [278/938], Loss: 0.8924128413200378\n",
      "Validation: Epoch [2], Batch [279/938], Loss: 0.9944491982460022\n",
      "Validation: Epoch [2], Batch [280/938], Loss: 0.787545919418335\n",
      "Validation: Epoch [2], Batch [281/938], Loss: 0.8502321243286133\n",
      "Validation: Epoch [2], Batch [282/938], Loss: 0.7581846714019775\n",
      "Validation: Epoch [2], Batch [283/938], Loss: 0.8611506223678589\n",
      "Validation: Epoch [2], Batch [284/938], Loss: 1.0112855434417725\n",
      "Validation: Epoch [2], Batch [285/938], Loss: 1.1121633052825928\n",
      "Validation: Epoch [2], Batch [286/938], Loss: 0.9490737915039062\n",
      "Validation: Epoch [2], Batch [287/938], Loss: 0.7033854722976685\n",
      "Validation: Epoch [2], Batch [288/938], Loss: 0.8132576942443848\n",
      "Validation: Epoch [2], Batch [289/938], Loss: 0.9608738422393799\n",
      "Validation: Epoch [2], Batch [290/938], Loss: 0.8362156748771667\n",
      "Validation: Epoch [2], Batch [291/938], Loss: 0.8121554851531982\n",
      "Validation: Epoch [2], Batch [292/938], Loss: 0.8146792650222778\n",
      "Validation: Epoch [2], Batch [293/938], Loss: 1.0087432861328125\n",
      "Validation: Epoch [2], Batch [294/938], Loss: 0.8468973636627197\n",
      "Validation: Epoch [2], Batch [295/938], Loss: 0.8037952184677124\n",
      "Validation: Epoch [2], Batch [296/938], Loss: 0.911865770816803\n",
      "Validation: Epoch [2], Batch [297/938], Loss: 1.0151562690734863\n",
      "Validation: Epoch [2], Batch [298/938], Loss: 0.9630091786384583\n",
      "Validation: Epoch [2], Batch [299/938], Loss: 0.7083144187927246\n",
      "Validation: Epoch [2], Batch [300/938], Loss: 0.9134950637817383\n",
      "Validation: Epoch [2], Batch [301/938], Loss: 0.5955740809440613\n",
      "Validation: Epoch [2], Batch [302/938], Loss: 1.0105831623077393\n",
      "Validation: Epoch [2], Batch [303/938], Loss: 0.8492916822433472\n",
      "Validation: Epoch [2], Batch [304/938], Loss: 0.785956621170044\n",
      "Validation: Epoch [2], Batch [305/938], Loss: 0.8361191749572754\n",
      "Validation: Epoch [2], Batch [306/938], Loss: 0.9511924386024475\n",
      "Validation: Epoch [2], Batch [307/938], Loss: 0.6637730002403259\n",
      "Validation: Epoch [2], Batch [308/938], Loss: 0.9861520528793335\n",
      "Validation: Epoch [2], Batch [309/938], Loss: 0.7955865859985352\n",
      "Validation: Epoch [2], Batch [310/938], Loss: 0.8865635395050049\n",
      "Validation: Epoch [2], Batch [311/938], Loss: 0.7352480888366699\n",
      "Validation: Epoch [2], Batch [312/938], Loss: 1.0773940086364746\n",
      "Validation: Epoch [2], Batch [313/938], Loss: 0.9309837222099304\n",
      "Validation: Epoch [2], Batch [314/938], Loss: 0.8725798726081848\n",
      "Validation: Epoch [2], Batch [315/938], Loss: 0.8295010924339294\n",
      "Validation: Epoch [2], Batch [316/938], Loss: 0.9643955826759338\n",
      "Validation: Epoch [2], Batch [317/938], Loss: 0.800947368144989\n",
      "Validation: Epoch [2], Batch [318/938], Loss: 0.8884099721908569\n",
      "Validation: Epoch [2], Batch [319/938], Loss: 0.8415718674659729\n",
      "Validation: Epoch [2], Batch [320/938], Loss: 0.8474680185317993\n",
      "Validation: Epoch [2], Batch [321/938], Loss: 0.7902509570121765\n",
      "Validation: Epoch [2], Batch [322/938], Loss: 1.0153523683547974\n",
      "Validation: Epoch [2], Batch [323/938], Loss: 0.7864556312561035\n",
      "Validation: Epoch [2], Batch [324/938], Loss: 0.9154201745986938\n",
      "Validation: Epoch [2], Batch [325/938], Loss: 0.9316413998603821\n",
      "Validation: Epoch [2], Batch [326/938], Loss: 0.8099024295806885\n",
      "Validation: Epoch [2], Batch [327/938], Loss: 0.7136615514755249\n",
      "Validation: Epoch [2], Batch [328/938], Loss: 1.199007511138916\n",
      "Validation: Epoch [2], Batch [329/938], Loss: 0.868270993232727\n",
      "Validation: Epoch [2], Batch [330/938], Loss: 0.7751569747924805\n",
      "Validation: Epoch [2], Batch [331/938], Loss: 0.9969335794448853\n",
      "Validation: Epoch [2], Batch [332/938], Loss: 1.4906327724456787\n",
      "Validation: Epoch [2], Batch [333/938], Loss: 0.8364465236663818\n",
      "Validation: Epoch [2], Batch [334/938], Loss: 0.8242043256759644\n",
      "Validation: Epoch [2], Batch [335/938], Loss: 0.7853893637657166\n",
      "Validation: Epoch [2], Batch [336/938], Loss: 0.9661862850189209\n",
      "Validation: Epoch [2], Batch [337/938], Loss: 0.8947887420654297\n",
      "Validation: Epoch [2], Batch [338/938], Loss: 0.7172175645828247\n",
      "Validation: Epoch [2], Batch [339/938], Loss: 0.913485050201416\n",
      "Validation: Epoch [2], Batch [340/938], Loss: 0.9008274674415588\n",
      "Validation: Epoch [2], Batch [341/938], Loss: 0.9315630197525024\n",
      "Validation: Epoch [2], Batch [342/938], Loss: 0.950765073299408\n",
      "Validation: Epoch [2], Batch [343/938], Loss: 0.9721022844314575\n",
      "Validation: Epoch [2], Batch [344/938], Loss: 0.8862177133560181\n",
      "Validation: Epoch [2], Batch [345/938], Loss: 0.9007291197776794\n",
      "Validation: Epoch [2], Batch [346/938], Loss: 0.7901132106781006\n",
      "Validation: Epoch [2], Batch [347/938], Loss: 0.9744464159011841\n",
      "Validation: Epoch [2], Batch [348/938], Loss: 0.9704897999763489\n",
      "Validation: Epoch [2], Batch [349/938], Loss: 1.1618974208831787\n",
      "Validation: Epoch [2], Batch [350/938], Loss: 0.9262629151344299\n",
      "Validation: Epoch [2], Batch [351/938], Loss: 0.9536846876144409\n",
      "Validation: Epoch [2], Batch [352/938], Loss: 0.8492352366447449\n",
      "Validation: Epoch [2], Batch [353/938], Loss: 0.7256357669830322\n",
      "Validation: Epoch [2], Batch [354/938], Loss: 1.0102722644805908\n",
      "Validation: Epoch [2], Batch [355/938], Loss: 0.8374120593070984\n",
      "Validation: Epoch [2], Batch [356/938], Loss: 0.805486798286438\n",
      "Validation: Epoch [2], Batch [357/938], Loss: 0.7910598516464233\n",
      "Validation: Epoch [2], Batch [358/938], Loss: 0.8339323401451111\n",
      "Validation: Epoch [2], Batch [359/938], Loss: 0.8154025673866272\n",
      "Validation: Epoch [2], Batch [360/938], Loss: 1.0951473712921143\n",
      "Validation: Epoch [2], Batch [361/938], Loss: 0.9795687198638916\n",
      "Validation: Epoch [2], Batch [362/938], Loss: 0.817908525466919\n",
      "Validation: Epoch [2], Batch [363/938], Loss: 0.8581539392471313\n",
      "Validation: Epoch [2], Batch [364/938], Loss: 0.9140779376029968\n",
      "Validation: Epoch [2], Batch [365/938], Loss: 0.8741967678070068\n",
      "Validation: Epoch [2], Batch [366/938], Loss: 0.9437543749809265\n",
      "Validation: Epoch [2], Batch [367/938], Loss: 0.9693933725357056\n",
      "Validation: Epoch [2], Batch [368/938], Loss: 0.9446927905082703\n",
      "Validation: Epoch [2], Batch [369/938], Loss: 0.9967722296714783\n",
      "Validation: Epoch [2], Batch [370/938], Loss: 0.6427006721496582\n",
      "Validation: Epoch [2], Batch [371/938], Loss: 1.0546820163726807\n",
      "Validation: Epoch [2], Batch [372/938], Loss: 1.0153348445892334\n",
      "Validation: Epoch [2], Batch [373/938], Loss: 1.0537941455841064\n",
      "Validation: Epoch [2], Batch [374/938], Loss: 0.9378119707107544\n",
      "Validation: Epoch [2], Batch [375/938], Loss: 0.9399343132972717\n",
      "Validation: Epoch [2], Batch [376/938], Loss: 0.9558467268943787\n",
      "Validation: Epoch [2], Batch [377/938], Loss: 0.9113935828208923\n",
      "Validation: Epoch [2], Batch [378/938], Loss: 0.7305389642715454\n",
      "Validation: Epoch [2], Batch [379/938], Loss: 0.7891973853111267\n",
      "Validation: Epoch [2], Batch [380/938], Loss: 0.946048378944397\n",
      "Validation: Epoch [2], Batch [381/938], Loss: 0.9196683168411255\n",
      "Validation: Epoch [2], Batch [382/938], Loss: 0.8801488876342773\n",
      "Validation: Epoch [2], Batch [383/938], Loss: 0.8528447151184082\n",
      "Validation: Epoch [2], Batch [384/938], Loss: 0.719228982925415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [385/938], Loss: 1.0584510564804077\n",
      "Validation: Epoch [2], Batch [386/938], Loss: 0.975452184677124\n",
      "Validation: Epoch [2], Batch [387/938], Loss: 0.9817358255386353\n",
      "Validation: Epoch [2], Batch [388/938], Loss: 0.8273714780807495\n",
      "Validation: Epoch [2], Batch [389/938], Loss: 0.7848889231681824\n",
      "Validation: Epoch [2], Batch [390/938], Loss: 0.69530189037323\n",
      "Validation: Epoch [2], Batch [391/938], Loss: 0.9068142175674438\n",
      "Validation: Epoch [2], Batch [392/938], Loss: 0.9286700487136841\n",
      "Validation: Epoch [2], Batch [393/938], Loss: 0.8745839595794678\n",
      "Validation: Epoch [2], Batch [394/938], Loss: 0.5902679562568665\n",
      "Validation: Epoch [2], Batch [395/938], Loss: 0.8540072441101074\n",
      "Validation: Epoch [2], Batch [396/938], Loss: 1.0165480375289917\n",
      "Validation: Epoch [2], Batch [397/938], Loss: 0.9703294634819031\n",
      "Validation: Epoch [2], Batch [398/938], Loss: 0.8378710746765137\n",
      "Validation: Epoch [2], Batch [399/938], Loss: 0.9351595640182495\n",
      "Validation: Epoch [2], Batch [400/938], Loss: 0.9242192506790161\n",
      "Validation: Epoch [2], Batch [401/938], Loss: 0.8305904269218445\n",
      "Validation: Epoch [2], Batch [402/938], Loss: 0.7373809218406677\n",
      "Validation: Epoch [2], Batch [403/938], Loss: 0.8628105521202087\n",
      "Validation: Epoch [2], Batch [404/938], Loss: 0.7279629707336426\n",
      "Validation: Epoch [2], Batch [405/938], Loss: 0.9457724094390869\n",
      "Validation: Epoch [2], Batch [406/938], Loss: 0.868249773979187\n",
      "Validation: Epoch [2], Batch [407/938], Loss: 0.8061448931694031\n",
      "Validation: Epoch [2], Batch [408/938], Loss: 1.1308743953704834\n",
      "Validation: Epoch [2], Batch [409/938], Loss: 0.738439679145813\n",
      "Validation: Epoch [2], Batch [410/938], Loss: 0.9307544231414795\n",
      "Validation: Epoch [2], Batch [411/938], Loss: 0.9110807180404663\n",
      "Validation: Epoch [2], Batch [412/938], Loss: 0.7918285131454468\n",
      "Validation: Epoch [2], Batch [413/938], Loss: 0.9056681990623474\n",
      "Validation: Epoch [2], Batch [414/938], Loss: 0.8482934832572937\n",
      "Validation: Epoch [2], Batch [415/938], Loss: 0.9666650295257568\n",
      "Validation: Epoch [2], Batch [416/938], Loss: 1.076918125152588\n",
      "Validation: Epoch [2], Batch [417/938], Loss: 0.6845272779464722\n",
      "Validation: Epoch [2], Batch [418/938], Loss: 0.9601331353187561\n",
      "Validation: Epoch [2], Batch [419/938], Loss: 0.9899219274520874\n",
      "Validation: Epoch [2], Batch [420/938], Loss: 1.167940378189087\n",
      "Validation: Epoch [2], Batch [421/938], Loss: 0.8931114077568054\n",
      "Validation: Epoch [2], Batch [422/938], Loss: 0.704799473285675\n",
      "Validation: Epoch [2], Batch [423/938], Loss: 0.8762757778167725\n",
      "Validation: Epoch [2], Batch [424/938], Loss: 0.70938640832901\n",
      "Validation: Epoch [2], Batch [425/938], Loss: 0.7834711670875549\n",
      "Validation: Epoch [2], Batch [426/938], Loss: 0.879170298576355\n",
      "Validation: Epoch [2], Batch [427/938], Loss: 0.8586404323577881\n",
      "Validation: Epoch [2], Batch [428/938], Loss: 1.1656076908111572\n",
      "Validation: Epoch [2], Batch [429/938], Loss: 0.933272123336792\n",
      "Validation: Epoch [2], Batch [430/938], Loss: 0.9681446552276611\n",
      "Validation: Epoch [2], Batch [431/938], Loss: 0.8189643621444702\n",
      "Validation: Epoch [2], Batch [432/938], Loss: 0.9232423901557922\n",
      "Validation: Epoch [2], Batch [433/938], Loss: 0.9462810158729553\n",
      "Validation: Epoch [2], Batch [434/938], Loss: 0.6984536647796631\n",
      "Validation: Epoch [2], Batch [435/938], Loss: 1.0335808992385864\n",
      "Validation: Epoch [2], Batch [436/938], Loss: 0.9572662115097046\n",
      "Validation: Epoch [2], Batch [437/938], Loss: 0.7663660645484924\n",
      "Validation: Epoch [2], Batch [438/938], Loss: 0.8682899475097656\n",
      "Validation: Epoch [2], Batch [439/938], Loss: 0.9564955234527588\n",
      "Validation: Epoch [2], Batch [440/938], Loss: 0.9273925423622131\n",
      "Validation: Epoch [2], Batch [441/938], Loss: 0.8759015202522278\n",
      "Validation: Epoch [2], Batch [442/938], Loss: 0.76177978515625\n",
      "Validation: Epoch [2], Batch [443/938], Loss: 0.9954953193664551\n",
      "Validation: Epoch [2], Batch [444/938], Loss: 0.7988576889038086\n",
      "Validation: Epoch [2], Batch [445/938], Loss: 0.8926776647567749\n",
      "Validation: Epoch [2], Batch [446/938], Loss: 0.7786816358566284\n",
      "Validation: Epoch [2], Batch [447/938], Loss: 0.9396883845329285\n",
      "Validation: Epoch [2], Batch [448/938], Loss: 0.9957665205001831\n",
      "Validation: Epoch [2], Batch [449/938], Loss: 1.0401532649993896\n",
      "Validation: Epoch [2], Batch [450/938], Loss: 0.760565996170044\n",
      "Validation: Epoch [2], Batch [451/938], Loss: 0.9653866291046143\n",
      "Validation: Epoch [2], Batch [452/938], Loss: 0.6862888336181641\n",
      "Validation: Epoch [2], Batch [453/938], Loss: 1.0248692035675049\n",
      "Validation: Epoch [2], Batch [454/938], Loss: 0.9457390904426575\n",
      "Validation: Epoch [2], Batch [455/938], Loss: 1.1359703540802002\n",
      "Validation: Epoch [2], Batch [456/938], Loss: 0.8775769472122192\n",
      "Validation: Epoch [2], Batch [457/938], Loss: 0.7817923426628113\n",
      "Validation: Epoch [2], Batch [458/938], Loss: 0.8761314153671265\n",
      "Validation: Epoch [2], Batch [459/938], Loss: 0.9465301036834717\n",
      "Validation: Epoch [2], Batch [460/938], Loss: 0.876471757888794\n",
      "Validation: Epoch [2], Batch [461/938], Loss: 0.8737395405769348\n",
      "Validation: Epoch [2], Batch [462/938], Loss: 0.764273464679718\n",
      "Validation: Epoch [2], Batch [463/938], Loss: 0.848808228969574\n",
      "Validation: Epoch [2], Batch [464/938], Loss: 0.9175203442573547\n",
      "Validation: Epoch [2], Batch [465/938], Loss: 0.7502681016921997\n",
      "Validation: Epoch [2], Batch [466/938], Loss: 1.0034153461456299\n",
      "Validation: Epoch [2], Batch [467/938], Loss: 0.7145054340362549\n",
      "Validation: Epoch [2], Batch [468/938], Loss: 0.9421684741973877\n",
      "Validation: Epoch [2], Batch [469/938], Loss: 0.7770925164222717\n",
      "Validation: Epoch [2], Batch [470/938], Loss: 0.7456350326538086\n",
      "Validation: Epoch [2], Batch [471/938], Loss: 0.9072577357292175\n",
      "Validation: Epoch [2], Batch [472/938], Loss: 0.8814685344696045\n",
      "Validation: Epoch [2], Batch [473/938], Loss: 0.819809079170227\n",
      "Validation: Epoch [2], Batch [474/938], Loss: 0.8698066473007202\n",
      "Validation: Epoch [2], Batch [475/938], Loss: 0.840070366859436\n",
      "Validation: Epoch [2], Batch [476/938], Loss: 0.8844343423843384\n",
      "Validation: Epoch [2], Batch [477/938], Loss: 0.8688718676567078\n",
      "Validation: Epoch [2], Batch [478/938], Loss: 0.9301068782806396\n",
      "Validation: Epoch [2], Batch [479/938], Loss: 0.8465366363525391\n",
      "Validation: Epoch [2], Batch [480/938], Loss: 0.8975082635879517\n",
      "Validation: Epoch [2], Batch [481/938], Loss: 0.838797390460968\n",
      "Validation: Epoch [2], Batch [482/938], Loss: 0.7652633786201477\n",
      "Validation: Epoch [2], Batch [483/938], Loss: 0.8641003370285034\n",
      "Validation: Epoch [2], Batch [484/938], Loss: 0.7682274580001831\n",
      "Validation: Epoch [2], Batch [485/938], Loss: 1.060989260673523\n",
      "Validation: Epoch [2], Batch [486/938], Loss: 0.9724284410476685\n",
      "Validation: Epoch [2], Batch [487/938], Loss: 1.0067049264907837\n",
      "Validation: Epoch [2], Batch [488/938], Loss: 0.982471764087677\n",
      "Validation: Epoch [2], Batch [489/938], Loss: 0.9357013702392578\n",
      "Validation: Epoch [2], Batch [490/938], Loss: 0.7873479723930359\n",
      "Validation: Epoch [2], Batch [491/938], Loss: 1.0540270805358887\n",
      "Validation: Epoch [2], Batch [492/938], Loss: 0.9178640842437744\n",
      "Validation: Epoch [2], Batch [493/938], Loss: 0.912849485874176\n",
      "Validation: Epoch [2], Batch [494/938], Loss: 0.8534250855445862\n",
      "Validation: Epoch [2], Batch [495/938], Loss: 1.2088813781738281\n",
      "Validation: Epoch [2], Batch [496/938], Loss: 1.1304914951324463\n",
      "Validation: Epoch [2], Batch [497/938], Loss: 0.8385199308395386\n",
      "Validation: Epoch [2], Batch [498/938], Loss: 0.8031803965568542\n",
      "Validation: Epoch [2], Batch [499/938], Loss: 0.7906661033630371\n",
      "Validation: Epoch [2], Batch [500/938], Loss: 0.801462709903717\n",
      "Validation: Epoch [2], Batch [501/938], Loss: 0.8154441118240356\n",
      "Validation: Epoch [2], Batch [502/938], Loss: 0.7054964900016785\n",
      "Validation: Epoch [2], Batch [503/938], Loss: 0.7242003083229065\n",
      "Validation: Epoch [2], Batch [504/938], Loss: 0.869847297668457\n",
      "Validation: Epoch [2], Batch [505/938], Loss: 0.8419748544692993\n",
      "Validation: Epoch [2], Batch [506/938], Loss: 0.748211681842804\n",
      "Validation: Epoch [2], Batch [507/938], Loss: 0.8647980093955994\n",
      "Validation: Epoch [2], Batch [508/938], Loss: 0.843988299369812\n",
      "Validation: Epoch [2], Batch [509/938], Loss: 0.9234384894371033\n",
      "Validation: Epoch [2], Batch [510/938], Loss: 0.8026067018508911\n",
      "Validation: Epoch [2], Batch [511/938], Loss: 0.8700695633888245\n",
      "Validation: Epoch [2], Batch [512/938], Loss: 0.7175241708755493\n",
      "Validation: Epoch [2], Batch [513/938], Loss: 0.8861963748931885\n",
      "Validation: Epoch [2], Batch [514/938], Loss: 0.9273998737335205\n",
      "Validation: Epoch [2], Batch [515/938], Loss: 0.6982614994049072\n",
      "Validation: Epoch [2], Batch [516/938], Loss: 1.0646908283233643\n",
      "Validation: Epoch [2], Batch [517/938], Loss: 0.8717401027679443\n",
      "Validation: Epoch [2], Batch [518/938], Loss: 0.7770661115646362\n",
      "Validation: Epoch [2], Batch [519/938], Loss: 0.8015228509902954\n",
      "Validation: Epoch [2], Batch [520/938], Loss: 0.9184278249740601\n",
      "Validation: Epoch [2], Batch [521/938], Loss: 0.7826870679855347\n",
      "Validation: Epoch [2], Batch [522/938], Loss: 0.8826500177383423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [523/938], Loss: 0.889779806137085\n",
      "Validation: Epoch [2], Batch [524/938], Loss: 0.8814346790313721\n",
      "Validation: Epoch [2], Batch [525/938], Loss: 0.9053831696510315\n",
      "Validation: Epoch [2], Batch [526/938], Loss: 0.9125964641571045\n",
      "Validation: Epoch [2], Batch [527/938], Loss: 0.8596147298812866\n",
      "Validation: Epoch [2], Batch [528/938], Loss: 0.8368262052536011\n",
      "Validation: Epoch [2], Batch [529/938], Loss: 0.866766631603241\n",
      "Validation: Epoch [2], Batch [530/938], Loss: 0.8646225929260254\n",
      "Validation: Epoch [2], Batch [531/938], Loss: 0.8023781180381775\n",
      "Validation: Epoch [2], Batch [532/938], Loss: 0.9013371467590332\n",
      "Validation: Epoch [2], Batch [533/938], Loss: 0.8274608850479126\n",
      "Validation: Epoch [2], Batch [534/938], Loss: 0.7650536298751831\n",
      "Validation: Epoch [2], Batch [535/938], Loss: 1.1917821168899536\n",
      "Validation: Epoch [2], Batch [536/938], Loss: 0.9123654365539551\n",
      "Validation: Epoch [2], Batch [537/938], Loss: 1.1147351264953613\n",
      "Validation: Epoch [2], Batch [538/938], Loss: 0.9431546926498413\n",
      "Validation: Epoch [2], Batch [539/938], Loss: 0.7299932241439819\n",
      "Validation: Epoch [2], Batch [540/938], Loss: 0.9647963047027588\n",
      "Validation: Epoch [2], Batch [541/938], Loss: 0.7628674507141113\n",
      "Validation: Epoch [2], Batch [542/938], Loss: 0.7138147354125977\n",
      "Validation: Epoch [2], Batch [543/938], Loss: 0.8200891017913818\n",
      "Validation: Epoch [2], Batch [544/938], Loss: 0.7108744978904724\n",
      "Validation: Epoch [2], Batch [545/938], Loss: 0.9795033931732178\n",
      "Validation: Epoch [2], Batch [546/938], Loss: 0.8529542088508606\n",
      "Validation: Epoch [2], Batch [547/938], Loss: 0.7610899209976196\n",
      "Validation: Epoch [2], Batch [548/938], Loss: 0.8908966779708862\n",
      "Validation: Epoch [2], Batch [549/938], Loss: 0.8248261213302612\n",
      "Validation: Epoch [2], Batch [550/938], Loss: 0.9591138362884521\n",
      "Validation: Epoch [2], Batch [551/938], Loss: 0.7329146862030029\n",
      "Validation: Epoch [2], Batch [552/938], Loss: 0.678141713142395\n",
      "Validation: Epoch [2], Batch [553/938], Loss: 0.8798828125\n",
      "Validation: Epoch [2], Batch [554/938], Loss: 0.6780992746353149\n",
      "Validation: Epoch [2], Batch [555/938], Loss: 0.8926510214805603\n",
      "Validation: Epoch [2], Batch [556/938], Loss: 0.7641509771347046\n",
      "Validation: Epoch [2], Batch [557/938], Loss: 1.3917250633239746\n",
      "Validation: Epoch [2], Batch [558/938], Loss: 0.7601391077041626\n",
      "Validation: Epoch [2], Batch [559/938], Loss: 0.9666320085525513\n",
      "Validation: Epoch [2], Batch [560/938], Loss: 0.8073097467422485\n",
      "Validation: Epoch [2], Batch [561/938], Loss: 0.7870143055915833\n",
      "Validation: Epoch [2], Batch [562/938], Loss: 0.832469642162323\n",
      "Validation: Epoch [2], Batch [563/938], Loss: 0.7457985877990723\n",
      "Validation: Epoch [2], Batch [564/938], Loss: 0.7539007663726807\n",
      "Validation: Epoch [2], Batch [565/938], Loss: 0.7758616209030151\n",
      "Validation: Epoch [2], Batch [566/938], Loss: 1.041159987449646\n",
      "Validation: Epoch [2], Batch [567/938], Loss: 0.9368927478790283\n",
      "Validation: Epoch [2], Batch [568/938], Loss: 0.8213609457015991\n",
      "Validation: Epoch [2], Batch [569/938], Loss: 0.7464702129364014\n",
      "Validation: Epoch [2], Batch [570/938], Loss: 0.7811285257339478\n",
      "Validation: Epoch [2], Batch [571/938], Loss: 0.9825336933135986\n",
      "Validation: Epoch [2], Batch [572/938], Loss: 0.723145067691803\n",
      "Validation: Epoch [2], Batch [573/938], Loss: 0.7850828170776367\n",
      "Validation: Epoch [2], Batch [574/938], Loss: 1.0869827270507812\n",
      "Validation: Epoch [2], Batch [575/938], Loss: 0.8171527981758118\n",
      "Validation: Epoch [2], Batch [576/938], Loss: 0.8609039187431335\n",
      "Validation: Epoch [2], Batch [577/938], Loss: 0.902431070804596\n",
      "Validation: Epoch [2], Batch [578/938], Loss: 0.9614800214767456\n",
      "Validation: Epoch [2], Batch [579/938], Loss: 0.7124772071838379\n",
      "Validation: Epoch [2], Batch [580/938], Loss: 0.902614951133728\n",
      "Validation: Epoch [2], Batch [581/938], Loss: 0.823227047920227\n",
      "Validation: Epoch [2], Batch [582/938], Loss: 0.8120760917663574\n",
      "Validation: Epoch [2], Batch [583/938], Loss: 0.735439121723175\n",
      "Validation: Epoch [2], Batch [584/938], Loss: 0.8587377071380615\n",
      "Validation: Epoch [2], Batch [585/938], Loss: 0.8924141526222229\n",
      "Validation: Epoch [2], Batch [586/938], Loss: 0.8495526313781738\n",
      "Validation: Epoch [2], Batch [587/938], Loss: 0.6211316585540771\n",
      "Validation: Epoch [2], Batch [588/938], Loss: 0.7883139848709106\n",
      "Validation: Epoch [2], Batch [589/938], Loss: 0.7182989120483398\n",
      "Validation: Epoch [2], Batch [590/938], Loss: 1.040700078010559\n",
      "Validation: Epoch [2], Batch [591/938], Loss: 0.5746078491210938\n",
      "Validation: Epoch [2], Batch [592/938], Loss: 0.9566073417663574\n",
      "Validation: Epoch [2], Batch [593/938], Loss: 0.6909506916999817\n",
      "Validation: Epoch [2], Batch [594/938], Loss: 1.2024849653244019\n",
      "Validation: Epoch [2], Batch [595/938], Loss: 0.7988606691360474\n",
      "Validation: Epoch [2], Batch [596/938], Loss: 0.8792808055877686\n",
      "Validation: Epoch [2], Batch [597/938], Loss: 0.8976088166236877\n",
      "Validation: Epoch [2], Batch [598/938], Loss: 0.8359145522117615\n",
      "Validation: Epoch [2], Batch [599/938], Loss: 0.8354055285453796\n",
      "Validation: Epoch [2], Batch [600/938], Loss: 0.634507954120636\n",
      "Validation: Epoch [2], Batch [601/938], Loss: 1.088947057723999\n",
      "Validation: Epoch [2], Batch [602/938], Loss: 0.6348139047622681\n",
      "Validation: Epoch [2], Batch [603/938], Loss: 0.879907488822937\n",
      "Validation: Epoch [2], Batch [604/938], Loss: 0.9216434359550476\n",
      "Validation: Epoch [2], Batch [605/938], Loss: 0.8219947814941406\n",
      "Validation: Epoch [2], Batch [606/938], Loss: 0.9904903173446655\n",
      "Validation: Epoch [2], Batch [607/938], Loss: 0.804356575012207\n",
      "Validation: Epoch [2], Batch [608/938], Loss: 0.7399183511734009\n",
      "Validation: Epoch [2], Batch [609/938], Loss: 0.7555301189422607\n",
      "Validation: Epoch [2], Batch [610/938], Loss: 0.8198974132537842\n",
      "Validation: Epoch [2], Batch [611/938], Loss: 0.8823545575141907\n",
      "Validation: Epoch [2], Batch [612/938], Loss: 0.9249012470245361\n",
      "Validation: Epoch [2], Batch [613/938], Loss: 0.8408697843551636\n",
      "Validation: Epoch [2], Batch [614/938], Loss: 0.9661983251571655\n",
      "Validation: Epoch [2], Batch [615/938], Loss: 0.7878561019897461\n",
      "Validation: Epoch [2], Batch [616/938], Loss: 0.9230115413665771\n",
      "Validation: Epoch [2], Batch [617/938], Loss: 0.7499915957450867\n",
      "Validation: Epoch [2], Batch [618/938], Loss: 0.7515712976455688\n",
      "Validation: Epoch [2], Batch [619/938], Loss: 0.8905971646308899\n",
      "Validation: Epoch [2], Batch [620/938], Loss: 0.9457310438156128\n",
      "Validation: Epoch [2], Batch [621/938], Loss: 1.0338597297668457\n",
      "Validation: Epoch [2], Batch [622/938], Loss: 0.8105025887489319\n",
      "Validation: Epoch [2], Batch [623/938], Loss: 0.8485491275787354\n",
      "Validation: Epoch [2], Batch [624/938], Loss: 0.7978019714355469\n",
      "Validation: Epoch [2], Batch [625/938], Loss: 0.7305548191070557\n",
      "Validation: Epoch [2], Batch [626/938], Loss: 0.8667598366737366\n",
      "Validation: Epoch [2], Batch [627/938], Loss: 1.0276908874511719\n",
      "Validation: Epoch [2], Batch [628/938], Loss: 0.8300955891609192\n",
      "Validation: Epoch [2], Batch [629/938], Loss: 0.9224941730499268\n",
      "Validation: Epoch [2], Batch [630/938], Loss: 1.0866339206695557\n",
      "Validation: Epoch [2], Batch [631/938], Loss: 0.7416324615478516\n",
      "Validation: Epoch [2], Batch [632/938], Loss: 0.9652148485183716\n",
      "Validation: Epoch [2], Batch [633/938], Loss: 1.0425148010253906\n",
      "Validation: Epoch [2], Batch [634/938], Loss: 0.9356877207756042\n",
      "Validation: Epoch [2], Batch [635/938], Loss: 0.8109837770462036\n",
      "Validation: Epoch [2], Batch [636/938], Loss: 0.5859600305557251\n",
      "Validation: Epoch [2], Batch [637/938], Loss: 0.7679550647735596\n",
      "Validation: Epoch [2], Batch [638/938], Loss: 0.7263028621673584\n",
      "Validation: Epoch [2], Batch [639/938], Loss: 0.849738597869873\n",
      "Validation: Epoch [2], Batch [640/938], Loss: 0.8314456939697266\n",
      "Validation: Epoch [2], Batch [641/938], Loss: 0.8955411911010742\n",
      "Validation: Epoch [2], Batch [642/938], Loss: 0.9695535898208618\n",
      "Validation: Epoch [2], Batch [643/938], Loss: 0.9636434316635132\n",
      "Validation: Epoch [2], Batch [644/938], Loss: 0.7411545515060425\n",
      "Validation: Epoch [2], Batch [645/938], Loss: 1.0475785732269287\n",
      "Validation: Epoch [2], Batch [646/938], Loss: 0.7420504689216614\n",
      "Validation: Epoch [2], Batch [647/938], Loss: 0.7568621635437012\n",
      "Validation: Epoch [2], Batch [648/938], Loss: 0.9057013988494873\n",
      "Validation: Epoch [2], Batch [649/938], Loss: 1.0759882926940918\n",
      "Validation: Epoch [2], Batch [650/938], Loss: 1.0432053804397583\n",
      "Validation: Epoch [2], Batch [651/938], Loss: 0.8912448287010193\n",
      "Validation: Epoch [2], Batch [652/938], Loss: 0.9652220010757446\n",
      "Validation: Epoch [2], Batch [653/938], Loss: 0.8224185109138489\n",
      "Validation: Epoch [2], Batch [654/938], Loss: 0.7378202676773071\n",
      "Validation: Epoch [2], Batch [655/938], Loss: 0.8780438899993896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [656/938], Loss: 0.9439617395401001\n",
      "Validation: Epoch [2], Batch [657/938], Loss: 0.78291916847229\n",
      "Validation: Epoch [2], Batch [658/938], Loss: 0.86271071434021\n",
      "Validation: Epoch [2], Batch [659/938], Loss: 0.8400818109512329\n",
      "Validation: Epoch [2], Batch [660/938], Loss: 0.9574517011642456\n",
      "Validation: Epoch [2], Batch [661/938], Loss: 1.0489360094070435\n",
      "Validation: Epoch [2], Batch [662/938], Loss: 0.8728392720222473\n",
      "Validation: Epoch [2], Batch [663/938], Loss: 1.074668049812317\n",
      "Validation: Epoch [2], Batch [664/938], Loss: 0.9882386326789856\n",
      "Validation: Epoch [2], Batch [665/938], Loss: 0.8060774803161621\n",
      "Validation: Epoch [2], Batch [666/938], Loss: 1.091637372970581\n",
      "Validation: Epoch [2], Batch [667/938], Loss: 0.9090402126312256\n",
      "Validation: Epoch [2], Batch [668/938], Loss: 0.8802798390388489\n",
      "Validation: Epoch [2], Batch [669/938], Loss: 0.9251176714897156\n",
      "Validation: Epoch [2], Batch [670/938], Loss: 0.7455841302871704\n",
      "Validation: Epoch [2], Batch [671/938], Loss: 0.6969375610351562\n",
      "Validation: Epoch [2], Batch [672/938], Loss: 0.7732459306716919\n",
      "Validation: Epoch [2], Batch [673/938], Loss: 0.8920115232467651\n",
      "Validation: Epoch [2], Batch [674/938], Loss: 0.7729275226593018\n",
      "Validation: Epoch [2], Batch [675/938], Loss: 0.6971347332000732\n",
      "Validation: Epoch [2], Batch [676/938], Loss: 0.8531410694122314\n",
      "Validation: Epoch [2], Batch [677/938], Loss: 1.0197025537490845\n",
      "Validation: Epoch [2], Batch [678/938], Loss: 0.8342755436897278\n",
      "Validation: Epoch [2], Batch [679/938], Loss: 0.8499473929405212\n",
      "Validation: Epoch [2], Batch [680/938], Loss: 1.0678834915161133\n",
      "Validation: Epoch [2], Batch [681/938], Loss: 0.7668555974960327\n",
      "Validation: Epoch [2], Batch [682/938], Loss: 0.6808186173439026\n",
      "Validation: Epoch [2], Batch [683/938], Loss: 0.8021470308303833\n",
      "Validation: Epoch [2], Batch [684/938], Loss: 0.6384627819061279\n",
      "Validation: Epoch [2], Batch [685/938], Loss: 0.8032232522964478\n",
      "Validation: Epoch [2], Batch [686/938], Loss: 0.8729835748672485\n",
      "Validation: Epoch [2], Batch [687/938], Loss: 0.9738897681236267\n",
      "Validation: Epoch [2], Batch [688/938], Loss: 0.6635172367095947\n",
      "Validation: Epoch [2], Batch [689/938], Loss: 0.9367057681083679\n",
      "Validation: Epoch [2], Batch [690/938], Loss: 0.9397227168083191\n",
      "Validation: Epoch [2], Batch [691/938], Loss: 0.7766255140304565\n",
      "Validation: Epoch [2], Batch [692/938], Loss: 0.6861701607704163\n",
      "Validation: Epoch [2], Batch [693/938], Loss: 0.8463513255119324\n",
      "Validation: Epoch [2], Batch [694/938], Loss: 0.8350616097450256\n",
      "Validation: Epoch [2], Batch [695/938], Loss: 0.9202451705932617\n",
      "Validation: Epoch [2], Batch [696/938], Loss: 0.8441062569618225\n",
      "Validation: Epoch [2], Batch [697/938], Loss: 0.793624758720398\n",
      "Validation: Epoch [2], Batch [698/938], Loss: 1.0050668716430664\n",
      "Validation: Epoch [2], Batch [699/938], Loss: 0.9115878343582153\n",
      "Validation: Epoch [2], Batch [700/938], Loss: 0.7065271735191345\n",
      "Validation: Epoch [2], Batch [701/938], Loss: 0.7418360710144043\n",
      "Validation: Epoch [2], Batch [702/938], Loss: 0.943023681640625\n",
      "Validation: Epoch [2], Batch [703/938], Loss: 0.9517759680747986\n",
      "Validation: Epoch [2], Batch [704/938], Loss: 0.7871769666671753\n",
      "Validation: Epoch [2], Batch [705/938], Loss: 0.8885455131530762\n",
      "Validation: Epoch [2], Batch [706/938], Loss: 0.7261707782745361\n",
      "Validation: Epoch [2], Batch [707/938], Loss: 0.7904986143112183\n",
      "Validation: Epoch [2], Batch [708/938], Loss: 0.8211999535560608\n",
      "Validation: Epoch [2], Batch [709/938], Loss: 0.683079719543457\n",
      "Validation: Epoch [2], Batch [710/938], Loss: 0.824927806854248\n",
      "Validation: Epoch [2], Batch [711/938], Loss: 0.8305048942565918\n",
      "Validation: Epoch [2], Batch [712/938], Loss: 0.7725895047187805\n",
      "Validation: Epoch [2], Batch [713/938], Loss: 0.887244701385498\n",
      "Validation: Epoch [2], Batch [714/938], Loss: 1.1595516204833984\n",
      "Validation: Epoch [2], Batch [715/938], Loss: 0.818751335144043\n",
      "Validation: Epoch [2], Batch [716/938], Loss: 0.7575873136520386\n",
      "Validation: Epoch [2], Batch [717/938], Loss: 0.9434789419174194\n",
      "Validation: Epoch [2], Batch [718/938], Loss: 0.7598522305488586\n",
      "Validation: Epoch [2], Batch [719/938], Loss: 0.7778217792510986\n",
      "Validation: Epoch [2], Batch [720/938], Loss: 0.8192989230155945\n",
      "Validation: Epoch [2], Batch [721/938], Loss: 0.7300209403038025\n",
      "Validation: Epoch [2], Batch [722/938], Loss: 0.8159681558609009\n",
      "Validation: Epoch [2], Batch [723/938], Loss: 0.7540043592453003\n",
      "Validation: Epoch [2], Batch [724/938], Loss: 0.8171621561050415\n",
      "Validation: Epoch [2], Batch [725/938], Loss: 1.0398234128952026\n",
      "Validation: Epoch [2], Batch [726/938], Loss: 0.7197573184967041\n",
      "Validation: Epoch [2], Batch [727/938], Loss: 1.0777671337127686\n",
      "Validation: Epoch [2], Batch [728/938], Loss: 0.8435577154159546\n",
      "Validation: Epoch [2], Batch [729/938], Loss: 0.7426297664642334\n",
      "Validation: Epoch [2], Batch [730/938], Loss: 0.7573654055595398\n",
      "Validation: Epoch [2], Batch [731/938], Loss: 1.4277286529541016\n",
      "Validation: Epoch [2], Batch [732/938], Loss: 0.7070522904396057\n",
      "Validation: Epoch [2], Batch [733/938], Loss: 0.7179552316665649\n",
      "Validation: Epoch [2], Batch [734/938], Loss: 0.8746081590652466\n",
      "Validation: Epoch [2], Batch [735/938], Loss: 0.7285221815109253\n",
      "Validation: Epoch [2], Batch [736/938], Loss: 0.9251313805580139\n",
      "Validation: Epoch [2], Batch [737/938], Loss: 0.8227958679199219\n",
      "Validation: Epoch [2], Batch [738/938], Loss: 0.8075551986694336\n",
      "Validation: Epoch [2], Batch [739/938], Loss: 1.0908050537109375\n",
      "Validation: Epoch [2], Batch [740/938], Loss: 0.9386060237884521\n",
      "Validation: Epoch [2], Batch [741/938], Loss: 0.951438307762146\n",
      "Validation: Epoch [2], Batch [742/938], Loss: 1.0109927654266357\n",
      "Validation: Epoch [2], Batch [743/938], Loss: 0.7831646203994751\n",
      "Validation: Epoch [2], Batch [744/938], Loss: 1.0006163120269775\n",
      "Validation: Epoch [2], Batch [745/938], Loss: 1.1198606491088867\n",
      "Validation: Epoch [2], Batch [746/938], Loss: 0.8418815732002258\n",
      "Validation: Epoch [2], Batch [747/938], Loss: 0.9454595446586609\n",
      "Validation: Epoch [2], Batch [748/938], Loss: 0.977486252784729\n",
      "Validation: Epoch [2], Batch [749/938], Loss: 0.8444052338600159\n",
      "Validation: Epoch [2], Batch [750/938], Loss: 0.773385763168335\n",
      "Validation: Epoch [2], Batch [751/938], Loss: 0.7647501230239868\n",
      "Validation: Epoch [2], Batch [752/938], Loss: 0.7326737642288208\n",
      "Validation: Epoch [2], Batch [753/938], Loss: 0.8852066993713379\n",
      "Validation: Epoch [2], Batch [754/938], Loss: 0.7454970479011536\n",
      "Validation: Epoch [2], Batch [755/938], Loss: 0.849533200263977\n",
      "Validation: Epoch [2], Batch [756/938], Loss: 0.9693233966827393\n",
      "Validation: Epoch [2], Batch [757/938], Loss: 0.9257677793502808\n",
      "Validation: Epoch [2], Batch [758/938], Loss: 0.8838473558425903\n",
      "Validation: Epoch [2], Batch [759/938], Loss: 0.8477201461791992\n",
      "Validation: Epoch [2], Batch [760/938], Loss: 0.8524823188781738\n",
      "Validation: Epoch [2], Batch [761/938], Loss: 0.7247850894927979\n",
      "Validation: Epoch [2], Batch [762/938], Loss: 0.7163028120994568\n",
      "Validation: Epoch [2], Batch [763/938], Loss: 0.9930980205535889\n",
      "Validation: Epoch [2], Batch [764/938], Loss: 0.9172840118408203\n",
      "Validation: Epoch [2], Batch [765/938], Loss: 0.8508111834526062\n",
      "Validation: Epoch [2], Batch [766/938], Loss: 0.8478991389274597\n",
      "Validation: Epoch [2], Batch [767/938], Loss: 0.7989802360534668\n",
      "Validation: Epoch [2], Batch [768/938], Loss: 0.8140106201171875\n",
      "Validation: Epoch [2], Batch [769/938], Loss: 0.8131769299507141\n",
      "Validation: Epoch [2], Batch [770/938], Loss: 0.9221959710121155\n",
      "Validation: Epoch [2], Batch [771/938], Loss: 0.8584059476852417\n",
      "Validation: Epoch [2], Batch [772/938], Loss: 0.8177844285964966\n",
      "Validation: Epoch [2], Batch [773/938], Loss: 1.0464413166046143\n",
      "Validation: Epoch [2], Batch [774/938], Loss: 0.6878252625465393\n",
      "Validation: Epoch [2], Batch [775/938], Loss: 0.9300075769424438\n",
      "Validation: Epoch [2], Batch [776/938], Loss: 0.8041621446609497\n",
      "Validation: Epoch [2], Batch [777/938], Loss: 0.8239657878875732\n",
      "Validation: Epoch [2], Batch [778/938], Loss: 0.8660329580307007\n",
      "Validation: Epoch [2], Batch [779/938], Loss: 0.9812800884246826\n",
      "Validation: Epoch [2], Batch [780/938], Loss: 0.8793689012527466\n",
      "Validation: Epoch [2], Batch [781/938], Loss: 1.0117648839950562\n",
      "Validation: Epoch [2], Batch [782/938], Loss: 0.9276641607284546\n",
      "Validation: Epoch [2], Batch [783/938], Loss: 0.8282493352890015\n",
      "Validation: Epoch [2], Batch [784/938], Loss: 0.9502710103988647\n",
      "Validation: Epoch [2], Batch [785/938], Loss: 0.8216097354888916\n",
      "Validation: Epoch [2], Batch [786/938], Loss: 0.7699567079544067\n",
      "Validation: Epoch [2], Batch [787/938], Loss: 0.8354977369308472\n",
      "Validation: Epoch [2], Batch [788/938], Loss: 0.9087642431259155\n",
      "Validation: Epoch [2], Batch [789/938], Loss: 0.7180477380752563\n",
      "Validation: Epoch [2], Batch [790/938], Loss: 0.8806569576263428\n",
      "Validation: Epoch [2], Batch [791/938], Loss: 0.9081342816352844\n",
      "Validation: Epoch [2], Batch [792/938], Loss: 1.032325029373169\n",
      "Validation: Epoch [2], Batch [793/938], Loss: 0.7437189221382141\n",
      "Validation: Epoch [2], Batch [794/938], Loss: 0.7574496269226074\n",
      "Validation: Epoch [2], Batch [795/938], Loss: 0.7809619903564453\n",
      "Validation: Epoch [2], Batch [796/938], Loss: 0.8147250413894653\n",
      "Validation: Epoch [2], Batch [797/938], Loss: 0.9165341854095459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [798/938], Loss: 1.1079214811325073\n",
      "Validation: Epoch [2], Batch [799/938], Loss: 0.8312549591064453\n",
      "Validation: Epoch [2], Batch [800/938], Loss: 0.9436407089233398\n",
      "Validation: Epoch [2], Batch [801/938], Loss: 1.0361535549163818\n",
      "Validation: Epoch [2], Batch [802/938], Loss: 0.9147771000862122\n",
      "Validation: Epoch [2], Batch [803/938], Loss: 0.8775756359100342\n",
      "Validation: Epoch [2], Batch [804/938], Loss: 0.8307971954345703\n",
      "Validation: Epoch [2], Batch [805/938], Loss: 0.7630924582481384\n",
      "Validation: Epoch [2], Batch [806/938], Loss: 0.8999845385551453\n",
      "Validation: Epoch [2], Batch [807/938], Loss: 0.8842586278915405\n",
      "Validation: Epoch [2], Batch [808/938], Loss: 0.8111788630485535\n",
      "Validation: Epoch [2], Batch [809/938], Loss: 1.1016969680786133\n",
      "Validation: Epoch [2], Batch [810/938], Loss: 0.7238950729370117\n",
      "Validation: Epoch [2], Batch [811/938], Loss: 0.8901292085647583\n",
      "Validation: Epoch [2], Batch [812/938], Loss: 0.9298920035362244\n",
      "Validation: Epoch [2], Batch [813/938], Loss: 0.9492319822311401\n",
      "Validation: Epoch [2], Batch [814/938], Loss: 0.8713909387588501\n",
      "Validation: Epoch [2], Batch [815/938], Loss: 0.6457130312919617\n",
      "Validation: Epoch [2], Batch [816/938], Loss: 1.1602131128311157\n",
      "Validation: Epoch [2], Batch [817/938], Loss: 0.8498132228851318\n",
      "Validation: Epoch [2], Batch [818/938], Loss: 0.9356828927993774\n",
      "Validation: Epoch [2], Batch [819/938], Loss: 0.8254694938659668\n",
      "Validation: Epoch [2], Batch [820/938], Loss: 0.8632280826568604\n",
      "Validation: Epoch [2], Batch [821/938], Loss: 0.8259521722793579\n",
      "Validation: Epoch [2], Batch [822/938], Loss: 1.0717498064041138\n",
      "Validation: Epoch [2], Batch [823/938], Loss: 0.9589775204658508\n",
      "Validation: Epoch [2], Batch [824/938], Loss: 0.6965100765228271\n",
      "Validation: Epoch [2], Batch [825/938], Loss: 0.8890063762664795\n",
      "Validation: Epoch [2], Batch [826/938], Loss: 0.8469202518463135\n",
      "Validation: Epoch [2], Batch [827/938], Loss: 0.8819137811660767\n",
      "Validation: Epoch [2], Batch [828/938], Loss: 0.809926450252533\n",
      "Validation: Epoch [2], Batch [829/938], Loss: 0.6159324645996094\n",
      "Validation: Epoch [2], Batch [830/938], Loss: 1.0899667739868164\n",
      "Validation: Epoch [2], Batch [831/938], Loss: 0.7514785528182983\n",
      "Validation: Epoch [2], Batch [832/938], Loss: 1.0028014183044434\n",
      "Validation: Epoch [2], Batch [833/938], Loss: 0.9327292442321777\n",
      "Validation: Epoch [2], Batch [834/938], Loss: 0.853873610496521\n",
      "Validation: Epoch [2], Batch [835/938], Loss: 0.9310954213142395\n",
      "Validation: Epoch [2], Batch [836/938], Loss: 0.9463282823562622\n",
      "Validation: Epoch [2], Batch [837/938], Loss: 0.7517240643501282\n",
      "Validation: Epoch [2], Batch [838/938], Loss: 1.0125926733016968\n",
      "Validation: Epoch [2], Batch [839/938], Loss: 0.915858268737793\n",
      "Validation: Epoch [2], Batch [840/938], Loss: 0.8409523963928223\n",
      "Validation: Epoch [2], Batch [841/938], Loss: 0.8657559156417847\n",
      "Validation: Epoch [2], Batch [842/938], Loss: 0.7628617286682129\n",
      "Validation: Epoch [2], Batch [843/938], Loss: 1.059826135635376\n",
      "Validation: Epoch [2], Batch [844/938], Loss: 0.7974607944488525\n",
      "Validation: Epoch [2], Batch [845/938], Loss: 0.9906063079833984\n",
      "Validation: Epoch [2], Batch [846/938], Loss: 0.6951045989990234\n",
      "Validation: Epoch [2], Batch [847/938], Loss: 0.9787060022354126\n",
      "Validation: Epoch [2], Batch [848/938], Loss: 0.8601808547973633\n",
      "Validation: Epoch [2], Batch [849/938], Loss: 0.9309051632881165\n",
      "Validation: Epoch [2], Batch [850/938], Loss: 1.0733461380004883\n",
      "Validation: Epoch [2], Batch [851/938], Loss: 0.9966496825218201\n",
      "Validation: Epoch [2], Batch [852/938], Loss: 0.8781286478042603\n",
      "Validation: Epoch [2], Batch [853/938], Loss: 0.7878342866897583\n",
      "Validation: Epoch [2], Batch [854/938], Loss: 0.7428053617477417\n",
      "Validation: Epoch [2], Batch [855/938], Loss: 0.8998977541923523\n",
      "Validation: Epoch [2], Batch [856/938], Loss: 0.7573160529136658\n",
      "Validation: Epoch [2], Batch [857/938], Loss: 0.9120293855667114\n",
      "Validation: Epoch [2], Batch [858/938], Loss: 0.6683765053749084\n",
      "Validation: Epoch [2], Batch [859/938], Loss: 0.949020504951477\n",
      "Validation: Epoch [2], Batch [860/938], Loss: 0.9296364784240723\n",
      "Validation: Epoch [2], Batch [861/938], Loss: 1.11113440990448\n",
      "Validation: Epoch [2], Batch [862/938], Loss: 0.6323230266571045\n",
      "Validation: Epoch [2], Batch [863/938], Loss: 0.717623233795166\n",
      "Validation: Epoch [2], Batch [864/938], Loss: 0.9470458030700684\n",
      "Validation: Epoch [2], Batch [865/938], Loss: 0.9165459275245667\n",
      "Validation: Epoch [2], Batch [866/938], Loss: 0.8280355930328369\n",
      "Validation: Epoch [2], Batch [867/938], Loss: 0.9875845909118652\n",
      "Validation: Epoch [2], Batch [868/938], Loss: 0.7653365135192871\n",
      "Validation: Epoch [2], Batch [869/938], Loss: 0.9839932918548584\n",
      "Validation: Epoch [2], Batch [870/938], Loss: 0.757003664970398\n",
      "Validation: Epoch [2], Batch [871/938], Loss: 1.0372283458709717\n",
      "Validation: Epoch [2], Batch [872/938], Loss: 0.887900710105896\n",
      "Validation: Epoch [2], Batch [873/938], Loss: 0.960540771484375\n",
      "Validation: Epoch [2], Batch [874/938], Loss: 0.9426315426826477\n",
      "Validation: Epoch [2], Batch [875/938], Loss: 0.9030243754386902\n",
      "Validation: Epoch [2], Batch [876/938], Loss: 0.8572161197662354\n",
      "Validation: Epoch [2], Batch [877/938], Loss: 0.8375976085662842\n",
      "Validation: Epoch [2], Batch [878/938], Loss: 0.9076499938964844\n",
      "Validation: Epoch [2], Batch [879/938], Loss: 0.8532818555831909\n",
      "Validation: Epoch [2], Batch [880/938], Loss: 0.923561692237854\n",
      "Validation: Epoch [2], Batch [881/938], Loss: 0.7319343090057373\n",
      "Validation: Epoch [2], Batch [882/938], Loss: 0.881148099899292\n",
      "Validation: Epoch [2], Batch [883/938], Loss: 0.8414911031723022\n",
      "Validation: Epoch [2], Batch [884/938], Loss: 0.8047024011611938\n",
      "Validation: Epoch [2], Batch [885/938], Loss: 0.9176064729690552\n",
      "Validation: Epoch [2], Batch [886/938], Loss: 1.0261082649230957\n",
      "Validation: Epoch [2], Batch [887/938], Loss: 0.9028527140617371\n",
      "Validation: Epoch [2], Batch [888/938], Loss: 0.827655553817749\n",
      "Validation: Epoch [2], Batch [889/938], Loss: 0.7586819529533386\n",
      "Validation: Epoch [2], Batch [890/938], Loss: 0.8586645126342773\n",
      "Validation: Epoch [2], Batch [891/938], Loss: 0.9563317894935608\n",
      "Validation: Epoch [2], Batch [892/938], Loss: 0.7914947271347046\n",
      "Validation: Epoch [2], Batch [893/938], Loss: 0.7531423568725586\n",
      "Validation: Epoch [2], Batch [894/938], Loss: 0.783103346824646\n",
      "Validation: Epoch [2], Batch [895/938], Loss: 0.9081066250801086\n",
      "Validation: Epoch [2], Batch [896/938], Loss: 0.7816618084907532\n",
      "Validation: Epoch [2], Batch [897/938], Loss: 0.8661787509918213\n",
      "Validation: Epoch [2], Batch [898/938], Loss: 0.7166677713394165\n",
      "Validation: Epoch [2], Batch [899/938], Loss: 1.2934350967407227\n",
      "Validation: Epoch [2], Batch [900/938], Loss: 0.8947457075119019\n",
      "Validation: Epoch [2], Batch [901/938], Loss: 0.8178837299346924\n",
      "Validation: Epoch [2], Batch [902/938], Loss: 0.8891611099243164\n",
      "Validation: Epoch [2], Batch [903/938], Loss: 0.8072929978370667\n",
      "Validation: Epoch [2], Batch [904/938], Loss: 1.1190096139907837\n",
      "Validation: Epoch [2], Batch [905/938], Loss: 0.9661359190940857\n",
      "Validation: Epoch [2], Batch [906/938], Loss: 0.794777512550354\n",
      "Validation: Epoch [2], Batch [907/938], Loss: 0.7001393437385559\n",
      "Validation: Epoch [2], Batch [908/938], Loss: 0.9500527381896973\n",
      "Validation: Epoch [2], Batch [909/938], Loss: 0.9274944067001343\n",
      "Validation: Epoch [2], Batch [910/938], Loss: 0.845694899559021\n",
      "Validation: Epoch [2], Batch [911/938], Loss: 0.8881576061248779\n",
      "Validation: Epoch [2], Batch [912/938], Loss: 0.7268319129943848\n",
      "Validation: Epoch [2], Batch [913/938], Loss: 0.8808072209358215\n",
      "Validation: Epoch [2], Batch [914/938], Loss: 0.9917620420455933\n",
      "Validation: Epoch [2], Batch [915/938], Loss: 1.1177583932876587\n",
      "Validation: Epoch [2], Batch [916/938], Loss: 0.7201045155525208\n",
      "Validation: Epoch [2], Batch [917/938], Loss: 0.7599111199378967\n",
      "Validation: Epoch [2], Batch [918/938], Loss: 0.7615163326263428\n",
      "Validation: Epoch [2], Batch [919/938], Loss: 0.822100043296814\n",
      "Validation: Epoch [2], Batch [920/938], Loss: 0.9686172008514404\n",
      "Validation: Epoch [2], Batch [921/938], Loss: 0.9061327576637268\n",
      "Validation: Epoch [2], Batch [922/938], Loss: 0.9212098121643066\n",
      "Validation: Epoch [2], Batch [923/938], Loss: 0.9480136632919312\n",
      "Validation: Epoch [2], Batch [924/938], Loss: 0.9076322913169861\n",
      "Validation: Epoch [2], Batch [925/938], Loss: 0.7323898077011108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [2], Batch [926/938], Loss: 1.0967979431152344\n",
      "Validation: Epoch [2], Batch [927/938], Loss: 0.8368813991546631\n",
      "Validation: Epoch [2], Batch [928/938], Loss: 0.8785095810890198\n",
      "Validation: Epoch [2], Batch [929/938], Loss: 0.6672642827033997\n",
      "Validation: Epoch [2], Batch [930/938], Loss: 0.783187985420227\n",
      "Validation: Epoch [2], Batch [931/938], Loss: 0.635405957698822\n",
      "Validation: Epoch [2], Batch [932/938], Loss: 0.7911393046379089\n",
      "Validation: Epoch [2], Batch [933/938], Loss: 0.8541662693023682\n",
      "Validation: Epoch [2], Batch [934/938], Loss: 0.8346599340438843\n",
      "Validation: Epoch [2], Batch [935/938], Loss: 1.052095651626587\n",
      "Validation: Epoch [2], Batch [936/938], Loss: 0.6508267521858215\n",
      "Validation: Epoch [2], Batch [937/938], Loss: 1.0136749744415283\n",
      "Validation: Epoch [2], Batch [938/938], Loss: 0.903886616230011\n",
      "Accuracy of test set: 0.6543333333333333\n",
      "Train: Epoch [3], Batch [1/938], Loss: 0.9510523676872253\n",
      "Train: Epoch [3], Batch [2/938], Loss: 1.1112371683120728\n",
      "Train: Epoch [3], Batch [3/938], Loss: 0.8718769550323486\n",
      "Train: Epoch [3], Batch [4/938], Loss: 0.9213336110115051\n",
      "Train: Epoch [3], Batch [5/938], Loss: 0.7809367179870605\n",
      "Train: Epoch [3], Batch [6/938], Loss: 0.8426142930984497\n",
      "Train: Epoch [3], Batch [7/938], Loss: 0.8175678849220276\n",
      "Train: Epoch [3], Batch [8/938], Loss: 0.9493405818939209\n",
      "Train: Epoch [3], Batch [9/938], Loss: 0.942664623260498\n",
      "Train: Epoch [3], Batch [10/938], Loss: 0.7042758464813232\n",
      "Train: Epoch [3], Batch [11/938], Loss: 0.8870978951454163\n",
      "Train: Epoch [3], Batch [12/938], Loss: 0.8993474245071411\n",
      "Train: Epoch [3], Batch [13/938], Loss: 0.8724650740623474\n",
      "Train: Epoch [3], Batch [14/938], Loss: 1.0091558694839478\n",
      "Train: Epoch [3], Batch [15/938], Loss: 0.7483814358711243\n",
      "Train: Epoch [3], Batch [16/938], Loss: 0.825779914855957\n",
      "Train: Epoch [3], Batch [17/938], Loss: 0.9635818004608154\n",
      "Train: Epoch [3], Batch [18/938], Loss: 0.8020856380462646\n",
      "Train: Epoch [3], Batch [19/938], Loss: 0.9717226028442383\n",
      "Train: Epoch [3], Batch [20/938], Loss: 0.7577860355377197\n",
      "Train: Epoch [3], Batch [21/938], Loss: 0.7868142127990723\n",
      "Train: Epoch [3], Batch [22/938], Loss: 0.7826456427574158\n",
      "Train: Epoch [3], Batch [23/938], Loss: 0.886266827583313\n",
      "Train: Epoch [3], Batch [24/938], Loss: 0.8309226036071777\n",
      "Train: Epoch [3], Batch [25/938], Loss: 0.9741126298904419\n",
      "Train: Epoch [3], Batch [26/938], Loss: 1.066860556602478\n",
      "Train: Epoch [3], Batch [27/938], Loss: 0.8847091794013977\n",
      "Train: Epoch [3], Batch [28/938], Loss: 0.8373560309410095\n",
      "Train: Epoch [3], Batch [29/938], Loss: 0.9323018789291382\n",
      "Train: Epoch [3], Batch [30/938], Loss: 0.8784036040306091\n",
      "Train: Epoch [3], Batch [31/938], Loss: 0.8026337623596191\n",
      "Train: Epoch [3], Batch [32/938], Loss: 0.7122718095779419\n",
      "Train: Epoch [3], Batch [33/938], Loss: 0.730675220489502\n",
      "Train: Epoch [3], Batch [34/938], Loss: 0.8018370866775513\n",
      "Train: Epoch [3], Batch [35/938], Loss: 0.7991995811462402\n",
      "Train: Epoch [3], Batch [36/938], Loss: 0.8160746097564697\n",
      "Train: Epoch [3], Batch [37/938], Loss: 0.7995845675468445\n",
      "Train: Epoch [3], Batch [38/938], Loss: 0.75058513879776\n",
      "Train: Epoch [3], Batch [39/938], Loss: 0.9685352444648743\n",
      "Train: Epoch [3], Batch [40/938], Loss: 0.8596593737602234\n",
      "Train: Epoch [3], Batch [41/938], Loss: 0.9500540494918823\n",
      "Train: Epoch [3], Batch [42/938], Loss: 1.0864747762680054\n",
      "Train: Epoch [3], Batch [43/938], Loss: 0.9559276103973389\n",
      "Train: Epoch [3], Batch [44/938], Loss: 0.6487855911254883\n",
      "Train: Epoch [3], Batch [45/938], Loss: 0.9663070440292358\n",
      "Train: Epoch [3], Batch [46/938], Loss: 0.8373277187347412\n",
      "Train: Epoch [3], Batch [47/938], Loss: 0.8904597759246826\n",
      "Train: Epoch [3], Batch [48/938], Loss: 0.8076506853103638\n",
      "Train: Epoch [3], Batch [49/938], Loss: 0.7759543061256409\n",
      "Train: Epoch [3], Batch [50/938], Loss: 0.7828608751296997\n",
      "Train: Epoch [3], Batch [51/938], Loss: 0.8403400778770447\n",
      "Train: Epoch [3], Batch [52/938], Loss: 0.7065109610557556\n",
      "Train: Epoch [3], Batch [53/938], Loss: 0.8087873458862305\n",
      "Train: Epoch [3], Batch [54/938], Loss: 0.9742807149887085\n",
      "Train: Epoch [3], Batch [55/938], Loss: 0.6417995691299438\n",
      "Train: Epoch [3], Batch [56/938], Loss: 0.9523017406463623\n",
      "Train: Epoch [3], Batch [57/938], Loss: 0.829409122467041\n",
      "Train: Epoch [3], Batch [58/938], Loss: 0.7770358920097351\n",
      "Train: Epoch [3], Batch [59/938], Loss: 0.8207323551177979\n",
      "Train: Epoch [3], Batch [60/938], Loss: 1.0211501121520996\n",
      "Train: Epoch [3], Batch [61/938], Loss: 0.9827376008033752\n",
      "Train: Epoch [3], Batch [62/938], Loss: 1.106101155281067\n",
      "Train: Epoch [3], Batch [63/938], Loss: 0.932532548904419\n",
      "Train: Epoch [3], Batch [64/938], Loss: 0.8328570127487183\n",
      "Train: Epoch [3], Batch [65/938], Loss: 1.2064950466156006\n",
      "Train: Epoch [3], Batch [66/938], Loss: 1.0354275703430176\n",
      "Train: Epoch [3], Batch [67/938], Loss: 0.8233976364135742\n",
      "Train: Epoch [3], Batch [68/938], Loss: 0.6936393976211548\n",
      "Train: Epoch [3], Batch [69/938], Loss: 0.7922827005386353\n",
      "Train: Epoch [3], Batch [70/938], Loss: 1.0540261268615723\n",
      "Train: Epoch [3], Batch [71/938], Loss: 0.7658719420433044\n",
      "Train: Epoch [3], Batch [72/938], Loss: 0.7284175157546997\n",
      "Train: Epoch [3], Batch [73/938], Loss: 0.7227504253387451\n",
      "Train: Epoch [3], Batch [74/938], Loss: 0.9022570252418518\n",
      "Train: Epoch [3], Batch [75/938], Loss: 0.8328742980957031\n",
      "Train: Epoch [3], Batch [76/938], Loss: 0.8276450037956238\n",
      "Train: Epoch [3], Batch [77/938], Loss: 0.9368271231651306\n",
      "Train: Epoch [3], Batch [78/938], Loss: 0.840624213218689\n",
      "Train: Epoch [3], Batch [79/938], Loss: 0.760075569152832\n",
      "Train: Epoch [3], Batch [80/938], Loss: 0.8335719108581543\n",
      "Train: Epoch [3], Batch [81/938], Loss: 0.7285805344581604\n",
      "Train: Epoch [3], Batch [82/938], Loss: 0.9859148263931274\n",
      "Train: Epoch [3], Batch [83/938], Loss: 0.9941300749778748\n",
      "Train: Epoch [3], Batch [84/938], Loss: 0.9998195171356201\n",
      "Train: Epoch [3], Batch [85/938], Loss: 1.1072027683258057\n",
      "Train: Epoch [3], Batch [86/938], Loss: 0.7435240149497986\n",
      "Train: Epoch [3], Batch [87/938], Loss: 0.8251638412475586\n",
      "Train: Epoch [3], Batch [88/938], Loss: 0.9233463406562805\n",
      "Train: Epoch [3], Batch [89/938], Loss: 0.7752156853675842\n",
      "Train: Epoch [3], Batch [90/938], Loss: 0.9834916591644287\n",
      "Train: Epoch [3], Batch [91/938], Loss: 0.9252375364303589\n",
      "Train: Epoch [3], Batch [92/938], Loss: 0.7134248614311218\n",
      "Train: Epoch [3], Batch [93/938], Loss: 0.8152208924293518\n",
      "Train: Epoch [3], Batch [94/938], Loss: 0.763567328453064\n",
      "Train: Epoch [3], Batch [95/938], Loss: 0.8194929361343384\n",
      "Train: Epoch [3], Batch [96/938], Loss: 0.7306245565414429\n",
      "Train: Epoch [3], Batch [97/938], Loss: 0.9432551860809326\n",
      "Train: Epoch [3], Batch [98/938], Loss: 0.9504638910293579\n",
      "Train: Epoch [3], Batch [99/938], Loss: 0.827048659324646\n",
      "Train: Epoch [3], Batch [100/938], Loss: 0.8377647399902344\n",
      "Train: Epoch [3], Batch [101/938], Loss: 0.8286433219909668\n",
      "Train: Epoch [3], Batch [102/938], Loss: 0.7866503000259399\n",
      "Train: Epoch [3], Batch [103/938], Loss: 0.9287108778953552\n",
      "Train: Epoch [3], Batch [104/938], Loss: 0.7141886949539185\n",
      "Train: Epoch [3], Batch [105/938], Loss: 0.9863570928573608\n",
      "Train: Epoch [3], Batch [106/938], Loss: 0.8384368419647217\n",
      "Train: Epoch [3], Batch [107/938], Loss: 0.9018045663833618\n",
      "Train: Epoch [3], Batch [108/938], Loss: 0.8170239329338074\n",
      "Train: Epoch [3], Batch [109/938], Loss: 1.0227768421173096\n",
      "Train: Epoch [3], Batch [110/938], Loss: 1.0262479782104492\n",
      "Train: Epoch [3], Batch [111/938], Loss: 1.2564549446105957\n",
      "Train: Epoch [3], Batch [112/938], Loss: 0.875058650970459\n",
      "Train: Epoch [3], Batch [113/938], Loss: 0.912501335144043\n",
      "Train: Epoch [3], Batch [114/938], Loss: 0.9565976858139038\n",
      "Train: Epoch [3], Batch [115/938], Loss: 0.8740772604942322\n",
      "Train: Epoch [3], Batch [116/938], Loss: 0.9746454358100891\n",
      "Train: Epoch [3], Batch [117/938], Loss: 0.8850492238998413\n",
      "Train: Epoch [3], Batch [118/938], Loss: 1.335568904876709\n",
      "Train: Epoch [3], Batch [119/938], Loss: 0.8464053869247437\n",
      "Train: Epoch [3], Batch [120/938], Loss: 0.8643726110458374\n",
      "Train: Epoch [3], Batch [121/938], Loss: 0.727799117565155\n",
      "Train: Epoch [3], Batch [122/938], Loss: 0.77298903465271\n",
      "Train: Epoch [3], Batch [123/938], Loss: 0.7027142643928528\n",
      "Train: Epoch [3], Batch [124/938], Loss: 0.8433048725128174\n",
      "Train: Epoch [3], Batch [125/938], Loss: 0.7615940570831299\n",
      "Train: Epoch [3], Batch [126/938], Loss: 0.9265694618225098\n",
      "Train: Epoch [3], Batch [127/938], Loss: 1.0173077583312988\n",
      "Train: Epoch [3], Batch [128/938], Loss: 0.8756675720214844\n",
      "Train: Epoch [3], Batch [129/938], Loss: 0.8153575658798218\n",
      "Train: Epoch [3], Batch [130/938], Loss: 0.8003809452056885\n",
      "Train: Epoch [3], Batch [131/938], Loss: 0.7870944142341614\n",
      "Train: Epoch [3], Batch [132/938], Loss: 0.7180510759353638\n",
      "Train: Epoch [3], Batch [133/938], Loss: 0.822563648223877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [3], Batch [134/938], Loss: 0.9518888592720032\n",
      "Train: Epoch [3], Batch [135/938], Loss: 0.7946008443832397\n",
      "Train: Epoch [3], Batch [136/938], Loss: 0.8790225982666016\n",
      "Train: Epoch [3], Batch [137/938], Loss: 0.7246686220169067\n",
      "Train: Epoch [3], Batch [138/938], Loss: 0.9421043992042542\n",
      "Train: Epoch [3], Batch [139/938], Loss: 0.9441806674003601\n",
      "Train: Epoch [3], Batch [140/938], Loss: 0.6320539712905884\n",
      "Train: Epoch [3], Batch [141/938], Loss: 0.6313246488571167\n",
      "Train: Epoch [3], Batch [142/938], Loss: 1.0465455055236816\n",
      "Train: Epoch [3], Batch [143/938], Loss: 0.9290467500686646\n",
      "Train: Epoch [3], Batch [144/938], Loss: 0.6765079498291016\n",
      "Train: Epoch [3], Batch [145/938], Loss: 0.7152724266052246\n",
      "Train: Epoch [3], Batch [146/938], Loss: 0.9901756644248962\n",
      "Train: Epoch [3], Batch [147/938], Loss: 1.0663714408874512\n",
      "Train: Epoch [3], Batch [148/938], Loss: 0.8779680132865906\n",
      "Train: Epoch [3], Batch [149/938], Loss: 0.7902138233184814\n",
      "Train: Epoch [3], Batch [150/938], Loss: 0.7135157585144043\n",
      "Train: Epoch [3], Batch [151/938], Loss: 0.7956303954124451\n",
      "Train: Epoch [3], Batch [152/938], Loss: 0.9768581390380859\n",
      "Train: Epoch [3], Batch [153/938], Loss: 0.7790985107421875\n",
      "Train: Epoch [3], Batch [154/938], Loss: 1.0273135900497437\n",
      "Train: Epoch [3], Batch [155/938], Loss: 0.8276220560073853\n",
      "Train: Epoch [3], Batch [156/938], Loss: 0.7609947919845581\n",
      "Train: Epoch [3], Batch [157/938], Loss: 0.8984238505363464\n",
      "Train: Epoch [3], Batch [158/938], Loss: 0.8954429626464844\n",
      "Train: Epoch [3], Batch [159/938], Loss: 0.8153671026229858\n",
      "Train: Epoch [3], Batch [160/938], Loss: 0.6690201163291931\n",
      "Train: Epoch [3], Batch [161/938], Loss: 0.7865690588951111\n",
      "Train: Epoch [3], Batch [162/938], Loss: 0.9246762990951538\n",
      "Train: Epoch [3], Batch [163/938], Loss: 0.981896162033081\n",
      "Train: Epoch [3], Batch [164/938], Loss: 0.8673093318939209\n",
      "Train: Epoch [3], Batch [165/938], Loss: 0.7174986004829407\n",
      "Train: Epoch [3], Batch [166/938], Loss: 0.9157543182373047\n",
      "Train: Epoch [3], Batch [167/938], Loss: 0.7331978678703308\n",
      "Train: Epoch [3], Batch [168/938], Loss: 1.0034677982330322\n",
      "Train: Epoch [3], Batch [169/938], Loss: 0.9439112544059753\n",
      "Train: Epoch [3], Batch [170/938], Loss: 0.737897515296936\n",
      "Train: Epoch [3], Batch [171/938], Loss: 0.7953951358795166\n",
      "Train: Epoch [3], Batch [172/938], Loss: 0.7686816453933716\n",
      "Train: Epoch [3], Batch [173/938], Loss: 0.7357645630836487\n",
      "Train: Epoch [3], Batch [174/938], Loss: 0.8311638832092285\n",
      "Train: Epoch [3], Batch [175/938], Loss: 0.9882893562316895\n",
      "Train: Epoch [3], Batch [176/938], Loss: 0.8158800005912781\n",
      "Train: Epoch [3], Batch [177/938], Loss: 0.9938355684280396\n",
      "Train: Epoch [3], Batch [178/938], Loss: 0.8031046390533447\n",
      "Train: Epoch [3], Batch [179/938], Loss: 0.8618160486221313\n",
      "Train: Epoch [3], Batch [180/938], Loss: 0.8269222974777222\n",
      "Train: Epoch [3], Batch [181/938], Loss: 0.7870551347732544\n",
      "Train: Epoch [3], Batch [182/938], Loss: 0.8705470561981201\n",
      "Train: Epoch [3], Batch [183/938], Loss: 0.8471379280090332\n",
      "Train: Epoch [3], Batch [184/938], Loss: 0.8427112102508545\n",
      "Train: Epoch [3], Batch [185/938], Loss: 1.0268206596374512\n",
      "Train: Epoch [3], Batch [186/938], Loss: 0.6963454484939575\n",
      "Train: Epoch [3], Batch [187/938], Loss: 0.8524469137191772\n",
      "Train: Epoch [3], Batch [188/938], Loss: 0.8289648294448853\n",
      "Train: Epoch [3], Batch [189/938], Loss: 0.8419244885444641\n",
      "Train: Epoch [3], Batch [190/938], Loss: 0.8576341867446899\n",
      "Train: Epoch [3], Batch [191/938], Loss: 0.7947890758514404\n",
      "Train: Epoch [3], Batch [192/938], Loss: 0.8493221402168274\n",
      "Train: Epoch [3], Batch [193/938], Loss: 0.9558742642402649\n",
      "Train: Epoch [3], Batch [194/938], Loss: 0.640758752822876\n",
      "Train: Epoch [3], Batch [195/938], Loss: 0.9549798369407654\n",
      "Train: Epoch [3], Batch [196/938], Loss: 0.879372239112854\n",
      "Train: Epoch [3], Batch [197/938], Loss: 0.8632307052612305\n",
      "Train: Epoch [3], Batch [198/938], Loss: 0.619522213935852\n",
      "Train: Epoch [3], Batch [199/938], Loss: 0.7589259147644043\n",
      "Train: Epoch [3], Batch [200/938], Loss: 0.7953941822052002\n",
      "Train: Epoch [3], Batch [201/938], Loss: 0.8136603832244873\n",
      "Train: Epoch [3], Batch [202/938], Loss: 0.8124118447303772\n",
      "Train: Epoch [3], Batch [203/938], Loss: 1.0524559020996094\n",
      "Train: Epoch [3], Batch [204/938], Loss: 0.8716676235198975\n",
      "Train: Epoch [3], Batch [205/938], Loss: 0.8625408411026001\n",
      "Train: Epoch [3], Batch [206/938], Loss: 0.9154858589172363\n",
      "Train: Epoch [3], Batch [207/938], Loss: 1.0732223987579346\n",
      "Train: Epoch [3], Batch [208/938], Loss: 0.8950726985931396\n",
      "Train: Epoch [3], Batch [209/938], Loss: 0.7628817558288574\n",
      "Train: Epoch [3], Batch [210/938], Loss: 0.7637659311294556\n",
      "Train: Epoch [3], Batch [211/938], Loss: 0.8509295582771301\n",
      "Train: Epoch [3], Batch [212/938], Loss: 0.8109442591667175\n",
      "Train: Epoch [3], Batch [213/938], Loss: 0.9419546127319336\n",
      "Train: Epoch [3], Batch [214/938], Loss: 0.7778036594390869\n",
      "Train: Epoch [3], Batch [215/938], Loss: 0.7489704489707947\n",
      "Train: Epoch [3], Batch [216/938], Loss: 0.7817795276641846\n",
      "Train: Epoch [3], Batch [217/938], Loss: 0.6920561790466309\n",
      "Train: Epoch [3], Batch [218/938], Loss: 0.6579652428627014\n",
      "Train: Epoch [3], Batch [219/938], Loss: 0.8289344310760498\n",
      "Train: Epoch [3], Batch [220/938], Loss: 0.8908492922782898\n",
      "Train: Epoch [3], Batch [221/938], Loss: 0.8015186786651611\n",
      "Train: Epoch [3], Batch [222/938], Loss: 1.0172314643859863\n",
      "Train: Epoch [3], Batch [223/938], Loss: 0.9829874634742737\n",
      "Train: Epoch [3], Batch [224/938], Loss: 0.7576252222061157\n",
      "Train: Epoch [3], Batch [225/938], Loss: 0.9285208582878113\n",
      "Train: Epoch [3], Batch [226/938], Loss: 0.776053249835968\n",
      "Train: Epoch [3], Batch [227/938], Loss: 0.8621631860733032\n",
      "Train: Epoch [3], Batch [228/938], Loss: 0.8884145617485046\n",
      "Train: Epoch [3], Batch [229/938], Loss: 0.7438489198684692\n",
      "Train: Epoch [3], Batch [230/938], Loss: 0.7731660008430481\n",
      "Train: Epoch [3], Batch [231/938], Loss: 0.9184287786483765\n",
      "Train: Epoch [3], Batch [232/938], Loss: 0.6659500598907471\n",
      "Train: Epoch [3], Batch [233/938], Loss: 0.7671256065368652\n",
      "Train: Epoch [3], Batch [234/938], Loss: 0.99375319480896\n",
      "Train: Epoch [3], Batch [235/938], Loss: 0.8359537124633789\n",
      "Train: Epoch [3], Batch [236/938], Loss: 0.8870241641998291\n",
      "Train: Epoch [3], Batch [237/938], Loss: 0.7639100551605225\n",
      "Train: Epoch [3], Batch [238/938], Loss: 0.7292391657829285\n",
      "Train: Epoch [3], Batch [239/938], Loss: 0.7479077577590942\n",
      "Train: Epoch [3], Batch [240/938], Loss: 0.9659126996994019\n",
      "Train: Epoch [3], Batch [241/938], Loss: 0.8722184896469116\n",
      "Train: Epoch [3], Batch [242/938], Loss: 0.7046140432357788\n",
      "Train: Epoch [3], Batch [243/938], Loss: 0.9284747838973999\n",
      "Train: Epoch [3], Batch [244/938], Loss: 0.8936868906021118\n",
      "Train: Epoch [3], Batch [245/938], Loss: 0.9698395133018494\n",
      "Train: Epoch [3], Batch [246/938], Loss: 0.8714960813522339\n",
      "Train: Epoch [3], Batch [247/938], Loss: 0.9470654129981995\n",
      "Train: Epoch [3], Batch [248/938], Loss: 0.8793337345123291\n",
      "Train: Epoch [3], Batch [249/938], Loss: 0.9655745625495911\n",
      "Train: Epoch [3], Batch [250/938], Loss: 0.6575669646263123\n",
      "Train: Epoch [3], Batch [251/938], Loss: 0.7655028104782104\n",
      "Train: Epoch [3], Batch [252/938], Loss: 0.8253142833709717\n",
      "Train: Epoch [3], Batch [253/938], Loss: 1.049243450164795\n",
      "Train: Epoch [3], Batch [254/938], Loss: 0.9985821843147278\n",
      "Train: Epoch [3], Batch [255/938], Loss: 0.8795303106307983\n",
      "Train: Epoch [3], Batch [256/938], Loss: 0.9095990657806396\n",
      "Train: Epoch [3], Batch [257/938], Loss: 0.8346560001373291\n",
      "Train: Epoch [3], Batch [258/938], Loss: 1.0350010395050049\n",
      "Train: Epoch [3], Batch [259/938], Loss: 0.7164075374603271\n",
      "Train: Epoch [3], Batch [260/938], Loss: 0.9383561611175537\n",
      "Train: Epoch [3], Batch [261/938], Loss: 0.8623173832893372\n",
      "Train: Epoch [3], Batch [262/938], Loss: 0.9629290103912354\n",
      "Train: Epoch [3], Batch [263/938], Loss: 0.9253888726234436\n",
      "Train: Epoch [3], Batch [264/938], Loss: 0.7487604022026062\n",
      "Train: Epoch [3], Batch [265/938], Loss: 0.6938542127609253\n",
      "Train: Epoch [3], Batch [266/938], Loss: 0.8229589462280273\n",
      "Train: Epoch [3], Batch [267/938], Loss: 0.8497413992881775\n",
      "Train: Epoch [3], Batch [268/938], Loss: 0.821740984916687\n",
      "Train: Epoch [3], Batch [269/938], Loss: 1.0364584922790527\n",
      "Train: Epoch [3], Batch [270/938], Loss: 0.8139207363128662\n",
      "Train: Epoch [3], Batch [271/938], Loss: 0.6473985910415649\n",
      "Train: Epoch [3], Batch [272/938], Loss: 0.7883755564689636\n",
      "Train: Epoch [3], Batch [273/938], Loss: 0.8687951564788818\n",
      "Train: Epoch [3], Batch [274/938], Loss: 0.9855419993400574\n",
      "Train: Epoch [3], Batch [275/938], Loss: 0.8222558498382568\n",
      "Train: Epoch [3], Batch [276/938], Loss: 0.904059648513794\n",
      "Train: Epoch [3], Batch [277/938], Loss: 0.6881643533706665\n",
      "Train: Epoch [3], Batch [278/938], Loss: 0.9872825741767883\n",
      "Train: Epoch [3], Batch [279/938], Loss: 0.6608554124832153\n",
      "Train: Epoch [3], Batch [280/938], Loss: 0.8653425574302673\n",
      "Train: Epoch [3], Batch [281/938], Loss: 1.2221193313598633\n",
      "Train: Epoch [3], Batch [282/938], Loss: 0.7606580257415771\n",
      "Train: Epoch [3], Batch [283/938], Loss: 0.9621931910514832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [3], Batch [284/938], Loss: 1.2738447189331055\n",
      "Train: Epoch [3], Batch [285/938], Loss: 0.8159922361373901\n",
      "Train: Epoch [3], Batch [286/938], Loss: 0.650153398513794\n",
      "Train: Epoch [3], Batch [287/938], Loss: 0.7495133876800537\n",
      "Train: Epoch [3], Batch [288/938], Loss: 0.9166182279586792\n",
      "Train: Epoch [3], Batch [289/938], Loss: 1.1131701469421387\n",
      "Train: Epoch [3], Batch [290/938], Loss: 0.8655100464820862\n",
      "Train: Epoch [3], Batch [291/938], Loss: 0.9769766330718994\n",
      "Train: Epoch [3], Batch [292/938], Loss: 0.8059237003326416\n",
      "Train: Epoch [3], Batch [293/938], Loss: 0.8689254522323608\n",
      "Train: Epoch [3], Batch [294/938], Loss: 0.8074506521224976\n",
      "Train: Epoch [3], Batch [295/938], Loss: 0.8490430116653442\n",
      "Train: Epoch [3], Batch [296/938], Loss: 0.838747501373291\n",
      "Train: Epoch [3], Batch [297/938], Loss: 0.8563429117202759\n",
      "Train: Epoch [3], Batch [298/938], Loss: 0.8457668423652649\n",
      "Train: Epoch [3], Batch [299/938], Loss: 0.891903281211853\n",
      "Train: Epoch [3], Batch [300/938], Loss: 0.8339829444885254\n",
      "Train: Epoch [3], Batch [301/938], Loss: 1.0329582691192627\n",
      "Train: Epoch [3], Batch [302/938], Loss: 0.874257504940033\n",
      "Train: Epoch [3], Batch [303/938], Loss: 0.71783047914505\n",
      "Train: Epoch [3], Batch [304/938], Loss: 0.931586742401123\n",
      "Train: Epoch [3], Batch [305/938], Loss: 0.8189612030982971\n",
      "Train: Epoch [3], Batch [306/938], Loss: 0.7811282873153687\n",
      "Train: Epoch [3], Batch [307/938], Loss: 0.7260617017745972\n",
      "Train: Epoch [3], Batch [308/938], Loss: 1.1406223773956299\n",
      "Train: Epoch [3], Batch [309/938], Loss: 0.9370495080947876\n",
      "Train: Epoch [3], Batch [310/938], Loss: 0.9175523519515991\n",
      "Train: Epoch [3], Batch [311/938], Loss: 0.9936220645904541\n",
      "Train: Epoch [3], Batch [312/938], Loss: 0.9983025193214417\n",
      "Train: Epoch [3], Batch [313/938], Loss: 0.9171152710914612\n",
      "Train: Epoch [3], Batch [314/938], Loss: 0.8407564759254456\n",
      "Train: Epoch [3], Batch [315/938], Loss: 0.7971135377883911\n",
      "Train: Epoch [3], Batch [316/938], Loss: 0.761238694190979\n",
      "Train: Epoch [3], Batch [317/938], Loss: 1.0129929780960083\n",
      "Train: Epoch [3], Batch [318/938], Loss: 0.9352800846099854\n",
      "Train: Epoch [3], Batch [319/938], Loss: 0.7212342619895935\n",
      "Train: Epoch [3], Batch [320/938], Loss: 1.0087554454803467\n",
      "Train: Epoch [3], Batch [321/938], Loss: 0.8877776265144348\n",
      "Train: Epoch [3], Batch [322/938], Loss: 0.835312008857727\n",
      "Train: Epoch [3], Batch [323/938], Loss: 0.9615639448165894\n",
      "Train: Epoch [3], Batch [324/938], Loss: 0.8497856855392456\n",
      "Train: Epoch [3], Batch [325/938], Loss: 1.0735548734664917\n",
      "Train: Epoch [3], Batch [326/938], Loss: 0.7722859382629395\n",
      "Train: Epoch [3], Batch [327/938], Loss: 0.8050130009651184\n",
      "Train: Epoch [3], Batch [328/938], Loss: 0.8251851201057434\n",
      "Train: Epoch [3], Batch [329/938], Loss: 0.7783696055412292\n",
      "Train: Epoch [3], Batch [330/938], Loss: 0.8924071788787842\n",
      "Train: Epoch [3], Batch [331/938], Loss: 0.8212069869041443\n",
      "Train: Epoch [3], Batch [332/938], Loss: 0.8339762091636658\n",
      "Train: Epoch [3], Batch [333/938], Loss: 0.7280818223953247\n",
      "Train: Epoch [3], Batch [334/938], Loss: 0.6824743747711182\n",
      "Train: Epoch [3], Batch [335/938], Loss: 0.9147126078605652\n",
      "Train: Epoch [3], Batch [336/938], Loss: 0.6746684312820435\n",
      "Train: Epoch [3], Batch [337/938], Loss: 0.6913073658943176\n",
      "Train: Epoch [3], Batch [338/938], Loss: 0.8515392541885376\n",
      "Train: Epoch [3], Batch [339/938], Loss: 0.7062958478927612\n",
      "Train: Epoch [3], Batch [340/938], Loss: 0.8625505566596985\n",
      "Train: Epoch [3], Batch [341/938], Loss: 0.9271970391273499\n",
      "Train: Epoch [3], Batch [342/938], Loss: 0.8350192308425903\n",
      "Train: Epoch [3], Batch [343/938], Loss: 0.9216445684432983\n",
      "Train: Epoch [3], Batch [344/938], Loss: 0.9739698767662048\n",
      "Train: Epoch [3], Batch [345/938], Loss: 0.8810089230537415\n",
      "Train: Epoch [3], Batch [346/938], Loss: 0.7825837135314941\n",
      "Train: Epoch [3], Batch [347/938], Loss: 0.7695316076278687\n",
      "Train: Epoch [3], Batch [348/938], Loss: 0.857943058013916\n",
      "Train: Epoch [3], Batch [349/938], Loss: 0.8715838193893433\n",
      "Train: Epoch [3], Batch [350/938], Loss: 0.8959966897964478\n",
      "Train: Epoch [3], Batch [351/938], Loss: 0.6408839225769043\n",
      "Train: Epoch [3], Batch [352/938], Loss: 0.9447652101516724\n",
      "Train: Epoch [3], Batch [353/938], Loss: 0.8076411485671997\n",
      "Train: Epoch [3], Batch [354/938], Loss: 1.1761057376861572\n",
      "Train: Epoch [3], Batch [355/938], Loss: 0.958214521408081\n",
      "Train: Epoch [3], Batch [356/938], Loss: 0.681712806224823\n",
      "Train: Epoch [3], Batch [357/938], Loss: 0.8179608583450317\n",
      "Train: Epoch [3], Batch [358/938], Loss: 0.7752972841262817\n",
      "Train: Epoch [3], Batch [359/938], Loss: 0.7814930081367493\n",
      "Train: Epoch [3], Batch [360/938], Loss: 0.8912375569343567\n",
      "Train: Epoch [3], Batch [361/938], Loss: 0.8602593541145325\n",
      "Train: Epoch [3], Batch [362/938], Loss: 0.7695659399032593\n",
      "Train: Epoch [3], Batch [363/938], Loss: 0.7509558200836182\n",
      "Train: Epoch [3], Batch [364/938], Loss: 0.8692617416381836\n",
      "Train: Epoch [3], Batch [365/938], Loss: 0.8110690116882324\n",
      "Train: Epoch [3], Batch [366/938], Loss: 0.8308233022689819\n",
      "Train: Epoch [3], Batch [367/938], Loss: 0.6584993600845337\n",
      "Train: Epoch [3], Batch [368/938], Loss: 0.8489387631416321\n",
      "Train: Epoch [3], Batch [369/938], Loss: 0.8577181100845337\n",
      "Train: Epoch [3], Batch [370/938], Loss: 0.7458915114402771\n",
      "Train: Epoch [3], Batch [371/938], Loss: 0.9745451211929321\n",
      "Train: Epoch [3], Batch [372/938], Loss: 0.9070519208908081\n",
      "Train: Epoch [3], Batch [373/938], Loss: 1.1200979948043823\n",
      "Train: Epoch [3], Batch [374/938], Loss: 0.7539353370666504\n",
      "Train: Epoch [3], Batch [375/938], Loss: 1.1244614124298096\n",
      "Train: Epoch [3], Batch [376/938], Loss: 0.6671837568283081\n",
      "Train: Epoch [3], Batch [377/938], Loss: 0.8639156222343445\n",
      "Train: Epoch [3], Batch [378/938], Loss: 0.8165055513381958\n",
      "Train: Epoch [3], Batch [379/938], Loss: 0.7109829187393188\n",
      "Train: Epoch [3], Batch [380/938], Loss: 0.737111508846283\n",
      "Train: Epoch [3], Batch [381/938], Loss: 0.6899188756942749\n",
      "Train: Epoch [3], Batch [382/938], Loss: 1.1268248558044434\n",
      "Train: Epoch [3], Batch [383/938], Loss: 0.7466142177581787\n",
      "Train: Epoch [3], Batch [384/938], Loss: 0.6944621801376343\n",
      "Train: Epoch [3], Batch [385/938], Loss: 0.9079171419143677\n",
      "Train: Epoch [3], Batch [386/938], Loss: 0.7730839848518372\n",
      "Train: Epoch [3], Batch [387/938], Loss: 0.9733075499534607\n",
      "Train: Epoch [3], Batch [388/938], Loss: 0.8985232710838318\n",
      "Train: Epoch [3], Batch [389/938], Loss: 0.9833143949508667\n",
      "Train: Epoch [3], Batch [390/938], Loss: 0.7853858470916748\n",
      "Train: Epoch [3], Batch [391/938], Loss: 0.8316193222999573\n",
      "Train: Epoch [3], Batch [392/938], Loss: 0.8076948523521423\n",
      "Train: Epoch [3], Batch [393/938], Loss: 0.7390676736831665\n",
      "Train: Epoch [3], Batch [394/938], Loss: 1.1075599193572998\n",
      "Train: Epoch [3], Batch [395/938], Loss: 1.0516873598098755\n",
      "Train: Epoch [3], Batch [396/938], Loss: 0.6233469843864441\n",
      "Train: Epoch [3], Batch [397/938], Loss: 0.9720808267593384\n",
      "Train: Epoch [3], Batch [398/938], Loss: 0.7476116418838501\n",
      "Train: Epoch [3], Batch [399/938], Loss: 0.946679949760437\n",
      "Train: Epoch [3], Batch [400/938], Loss: 0.9448357820510864\n",
      "Train: Epoch [3], Batch [401/938], Loss: 0.6862174868583679\n",
      "Train: Epoch [3], Batch [402/938], Loss: 0.7962234020233154\n",
      "Train: Epoch [3], Batch [403/938], Loss: 0.7930933237075806\n",
      "Train: Epoch [3], Batch [404/938], Loss: 1.1334261894226074\n",
      "Train: Epoch [3], Batch [405/938], Loss: 1.0406430959701538\n",
      "Train: Epoch [3], Batch [406/938], Loss: 0.7506067752838135\n",
      "Train: Epoch [3], Batch [407/938], Loss: 0.9435260891914368\n",
      "Train: Epoch [3], Batch [408/938], Loss: 0.8119440078735352\n",
      "Train: Epoch [3], Batch [409/938], Loss: 0.5971759557723999\n",
      "Train: Epoch [3], Batch [410/938], Loss: 0.8425354957580566\n",
      "Train: Epoch [3], Batch [411/938], Loss: 0.996893584728241\n",
      "Train: Epoch [3], Batch [412/938], Loss: 0.844185471534729\n",
      "Train: Epoch [3], Batch [413/938], Loss: 0.6843371391296387\n",
      "Train: Epoch [3], Batch [414/938], Loss: 0.7537658214569092\n",
      "Train: Epoch [3], Batch [415/938], Loss: 0.7951947450637817\n",
      "Train: Epoch [3], Batch [416/938], Loss: 0.7396501302719116\n",
      "Train: Epoch [3], Batch [417/938], Loss: 0.9554486870765686\n",
      "Train: Epoch [3], Batch [418/938], Loss: 0.748784601688385\n",
      "Train: Epoch [3], Batch [419/938], Loss: 0.7290139198303223\n",
      "Train: Epoch [3], Batch [420/938], Loss: 0.9438611268997192\n",
      "Train: Epoch [3], Batch [421/938], Loss: 0.9832719564437866\n",
      "Train: Epoch [3], Batch [422/938], Loss: 0.8604454398155212\n",
      "Train: Epoch [3], Batch [423/938], Loss: 0.9759000539779663\n",
      "Train: Epoch [3], Batch [424/938], Loss: 0.8155658841133118\n",
      "Train: Epoch [3], Batch [425/938], Loss: 0.8647188544273376\n",
      "Train: Epoch [3], Batch [426/938], Loss: 0.8677409887313843\n",
      "Train: Epoch [3], Batch [427/938], Loss: 1.2303245067596436\n",
      "Train: Epoch [3], Batch [428/938], Loss: 1.048328161239624\n",
      "Train: Epoch [3], Batch [429/938], Loss: 0.8336515426635742\n",
      "Train: Epoch [3], Batch [430/938], Loss: 0.8853455781936646\n",
      "Train: Epoch [3], Batch [431/938], Loss: 0.732520341873169\n",
      "Train: Epoch [3], Batch [432/938], Loss: 0.7562954425811768\n",
      "Train: Epoch [3], Batch [433/938], Loss: 0.7542148232460022\n",
      "Train: Epoch [3], Batch [434/938], Loss: 1.1204149723052979\n",
      "Train: Epoch [3], Batch [435/938], Loss: 0.8869833946228027\n",
      "Train: Epoch [3], Batch [436/938], Loss: 0.7718407511711121\n",
      "Train: Epoch [3], Batch [437/938], Loss: 0.6674295663833618\n",
      "Train: Epoch [3], Batch [438/938], Loss: 0.7963516116142273\n",
      "Train: Epoch [3], Batch [439/938], Loss: 0.793038010597229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [3], Batch [440/938], Loss: 0.8468589782714844\n",
      "Train: Epoch [3], Batch [441/938], Loss: 0.880547285079956\n",
      "Train: Epoch [3], Batch [442/938], Loss: 0.9078852534294128\n",
      "Train: Epoch [3], Batch [443/938], Loss: 0.7503761649131775\n",
      "Train: Epoch [3], Batch [444/938], Loss: 0.6522766351699829\n",
      "Train: Epoch [3], Batch [445/938], Loss: 0.7765552401542664\n",
      "Train: Epoch [3], Batch [446/938], Loss: 0.7982578277587891\n",
      "Train: Epoch [3], Batch [447/938], Loss: 0.8753935694694519\n",
      "Train: Epoch [3], Batch [448/938], Loss: 0.7232532501220703\n",
      "Train: Epoch [3], Batch [449/938], Loss: 0.8511606454849243\n",
      "Train: Epoch [3], Batch [450/938], Loss: 0.9575840830802917\n",
      "Train: Epoch [3], Batch [451/938], Loss: 0.7065779566764832\n",
      "Train: Epoch [3], Batch [452/938], Loss: 0.6319241523742676\n",
      "Train: Epoch [3], Batch [453/938], Loss: 0.7663647532463074\n",
      "Train: Epoch [3], Batch [454/938], Loss: 0.7349421381950378\n",
      "Train: Epoch [3], Batch [455/938], Loss: 1.1640403270721436\n",
      "Train: Epoch [3], Batch [456/938], Loss: 0.7975530028343201\n",
      "Train: Epoch [3], Batch [457/938], Loss: 1.0076537132263184\n",
      "Train: Epoch [3], Batch [458/938], Loss: 0.7507226467132568\n",
      "Train: Epoch [3], Batch [459/938], Loss: 0.9036188125610352\n",
      "Train: Epoch [3], Batch [460/938], Loss: 0.8832669854164124\n",
      "Train: Epoch [3], Batch [461/938], Loss: 0.9419045448303223\n",
      "Train: Epoch [3], Batch [462/938], Loss: 0.8992941379547119\n",
      "Train: Epoch [3], Batch [463/938], Loss: 0.8563657999038696\n",
      "Train: Epoch [3], Batch [464/938], Loss: 0.6800162196159363\n",
      "Train: Epoch [3], Batch [465/938], Loss: 0.8928459882736206\n",
      "Train: Epoch [3], Batch [466/938], Loss: 0.9480722546577454\n",
      "Train: Epoch [3], Batch [467/938], Loss: 0.7249725461006165\n",
      "Train: Epoch [3], Batch [468/938], Loss: 0.8055135607719421\n",
      "Train: Epoch [3], Batch [469/938], Loss: 0.6878211498260498\n",
      "Train: Epoch [3], Batch [470/938], Loss: 1.160888433456421\n",
      "Train: Epoch [3], Batch [471/938], Loss: 0.916702389717102\n",
      "Train: Epoch [3], Batch [472/938], Loss: 0.8149407505989075\n",
      "Train: Epoch [3], Batch [473/938], Loss: 0.8872195482254028\n",
      "Train: Epoch [3], Batch [474/938], Loss: 0.7179394960403442\n",
      "Train: Epoch [3], Batch [475/938], Loss: 0.691781759262085\n",
      "Train: Epoch [3], Batch [476/938], Loss: 0.8734623193740845\n",
      "Train: Epoch [3], Batch [477/938], Loss: 0.7698730230331421\n",
      "Train: Epoch [3], Batch [478/938], Loss: 0.7924575805664062\n",
      "Train: Epoch [3], Batch [479/938], Loss: 0.715526819229126\n",
      "Train: Epoch [3], Batch [480/938], Loss: 0.9404407739639282\n",
      "Train: Epoch [3], Batch [481/938], Loss: 0.9118120670318604\n",
      "Train: Epoch [3], Batch [482/938], Loss: 0.7948724031448364\n",
      "Train: Epoch [3], Batch [483/938], Loss: 0.941664457321167\n",
      "Train: Epoch [3], Batch [484/938], Loss: 0.8564767837524414\n",
      "Train: Epoch [3], Batch [485/938], Loss: 0.8884000778198242\n",
      "Train: Epoch [3], Batch [486/938], Loss: 1.0302257537841797\n",
      "Train: Epoch [3], Batch [487/938], Loss: 0.7994763851165771\n",
      "Train: Epoch [3], Batch [488/938], Loss: 1.129085659980774\n",
      "Train: Epoch [3], Batch [489/938], Loss: 0.7057336568832397\n",
      "Train: Epoch [3], Batch [490/938], Loss: 0.7374556660652161\n",
      "Train: Epoch [3], Batch [491/938], Loss: 0.8295252323150635\n",
      "Train: Epoch [3], Batch [492/938], Loss: 0.8222860097885132\n",
      "Train: Epoch [3], Batch [493/938], Loss: 0.6344029903411865\n",
      "Train: Epoch [3], Batch [494/938], Loss: 0.9572510719299316\n",
      "Train: Epoch [3], Batch [495/938], Loss: 0.8032368421554565\n",
      "Train: Epoch [3], Batch [496/938], Loss: 1.0905245542526245\n",
      "Train: Epoch [3], Batch [497/938], Loss: 0.8243792057037354\n",
      "Train: Epoch [3], Batch [498/938], Loss: 0.8804348707199097\n",
      "Train: Epoch [3], Batch [499/938], Loss: 1.020672082901001\n",
      "Train: Epoch [3], Batch [500/938], Loss: 0.9965618848800659\n",
      "Train: Epoch [3], Batch [501/938], Loss: 0.9106885194778442\n",
      "Train: Epoch [3], Batch [502/938], Loss: 0.8157293200492859\n",
      "Train: Epoch [3], Batch [503/938], Loss: 0.7952942848205566\n",
      "Train: Epoch [3], Batch [504/938], Loss: 0.7528729438781738\n",
      "Train: Epoch [3], Batch [505/938], Loss: 1.0229182243347168\n",
      "Train: Epoch [3], Batch [506/938], Loss: 0.9366714954376221\n",
      "Train: Epoch [3], Batch [507/938], Loss: 0.8184316158294678\n",
      "Train: Epoch [3], Batch [508/938], Loss: 0.8572875261306763\n",
      "Train: Epoch [3], Batch [509/938], Loss: 0.851665735244751\n",
      "Train: Epoch [3], Batch [510/938], Loss: 0.8542298078536987\n",
      "Train: Epoch [3], Batch [511/938], Loss: 0.7652798891067505\n",
      "Train: Epoch [3], Batch [512/938], Loss: 0.7149531841278076\n",
      "Train: Epoch [3], Batch [513/938], Loss: 0.6401995420455933\n",
      "Train: Epoch [3], Batch [514/938], Loss: 0.7805472612380981\n",
      "Train: Epoch [3], Batch [515/938], Loss: 0.8159008026123047\n",
      "Train: Epoch [3], Batch [516/938], Loss: 0.8623262643814087\n",
      "Train: Epoch [3], Batch [517/938], Loss: 0.9213892221450806\n",
      "Train: Epoch [3], Batch [518/938], Loss: 0.7388896942138672\n",
      "Train: Epoch [3], Batch [519/938], Loss: 0.7692923545837402\n",
      "Train: Epoch [3], Batch [520/938], Loss: 0.7665199041366577\n",
      "Train: Epoch [3], Batch [521/938], Loss: 1.0463204383850098\n",
      "Train: Epoch [3], Batch [522/938], Loss: 0.6246925592422485\n",
      "Train: Epoch [3], Batch [523/938], Loss: 0.6208559274673462\n",
      "Train: Epoch [3], Batch [524/938], Loss: 0.7344437837600708\n",
      "Train: Epoch [3], Batch [525/938], Loss: 0.7266022562980652\n",
      "Train: Epoch [3], Batch [526/938], Loss: 0.8904886841773987\n",
      "Train: Epoch [3], Batch [527/938], Loss: 0.8617843389511108\n",
      "Train: Epoch [3], Batch [528/938], Loss: 0.8086241483688354\n",
      "Train: Epoch [3], Batch [529/938], Loss: 0.7091652154922485\n",
      "Train: Epoch [3], Batch [530/938], Loss: 0.7752014398574829\n",
      "Train: Epoch [3], Batch [531/938], Loss: 0.8483130931854248\n",
      "Train: Epoch [3], Batch [532/938], Loss: 0.7729874849319458\n",
      "Train: Epoch [3], Batch [533/938], Loss: 0.7248530387878418\n",
      "Train: Epoch [3], Batch [534/938], Loss: 0.9257749319076538\n",
      "Train: Epoch [3], Batch [535/938], Loss: 0.7774946093559265\n",
      "Train: Epoch [3], Batch [536/938], Loss: 0.9427666664123535\n",
      "Train: Epoch [3], Batch [537/938], Loss: 0.7520499229431152\n",
      "Train: Epoch [3], Batch [538/938], Loss: 0.9481469988822937\n",
      "Train: Epoch [3], Batch [539/938], Loss: 0.7467831373214722\n",
      "Train: Epoch [3], Batch [540/938], Loss: 0.7568329572677612\n",
      "Train: Epoch [3], Batch [541/938], Loss: 0.8154523372650146\n",
      "Train: Epoch [3], Batch [542/938], Loss: 0.8032476902008057\n",
      "Train: Epoch [3], Batch [543/938], Loss: 0.9317871332168579\n",
      "Train: Epoch [3], Batch [544/938], Loss: 0.8847528696060181\n",
      "Train: Epoch [3], Batch [545/938], Loss: 0.7507273554801941\n",
      "Train: Epoch [3], Batch [546/938], Loss: 0.656380295753479\n",
      "Train: Epoch [3], Batch [547/938], Loss: 0.679901659488678\n",
      "Train: Epoch [3], Batch [548/938], Loss: 0.7988007068634033\n",
      "Train: Epoch [3], Batch [549/938], Loss: 0.6786478757858276\n",
      "Train: Epoch [3], Batch [550/938], Loss: 0.7288370728492737\n",
      "Train: Epoch [3], Batch [551/938], Loss: 1.2664316892623901\n",
      "Train: Epoch [3], Batch [552/938], Loss: 0.9440684914588928\n",
      "Train: Epoch [3], Batch [553/938], Loss: 0.6574757099151611\n",
      "Train: Epoch [3], Batch [554/938], Loss: 0.7662513256072998\n",
      "Train: Epoch [3], Batch [555/938], Loss: 0.7273982763290405\n",
      "Train: Epoch [3], Batch [556/938], Loss: 0.6231472492218018\n",
      "Train: Epoch [3], Batch [557/938], Loss: 0.7183741331100464\n",
      "Train: Epoch [3], Batch [558/938], Loss: 0.7564800977706909\n",
      "Train: Epoch [3], Batch [559/938], Loss: 0.761824905872345\n",
      "Train: Epoch [3], Batch [560/938], Loss: 0.7087710499763489\n",
      "Train: Epoch [3], Batch [561/938], Loss: 0.7910013198852539\n",
      "Train: Epoch [3], Batch [562/938], Loss: 0.640571117401123\n",
      "Train: Epoch [3], Batch [563/938], Loss: 0.9380982518196106\n",
      "Train: Epoch [3], Batch [564/938], Loss: 0.7926511764526367\n",
      "Train: Epoch [3], Batch [565/938], Loss: 0.6940363645553589\n",
      "Train: Epoch [3], Batch [566/938], Loss: 0.7747030854225159\n",
      "Train: Epoch [3], Batch [567/938], Loss: 0.84178227186203\n",
      "Train: Epoch [3], Batch [568/938], Loss: 0.8094169497489929\n",
      "Train: Epoch [3], Batch [569/938], Loss: 0.7995144128799438\n",
      "Train: Epoch [3], Batch [570/938], Loss: 0.8385676145553589\n",
      "Train: Epoch [3], Batch [571/938], Loss: 1.2009472846984863\n",
      "Train: Epoch [3], Batch [572/938], Loss: 0.7177285552024841\n",
      "Train: Epoch [3], Batch [573/938], Loss: 1.217188835144043\n",
      "Train: Epoch [3], Batch [574/938], Loss: 0.9108262062072754\n",
      "Train: Epoch [3], Batch [575/938], Loss: 0.6328625679016113\n",
      "Train: Epoch [3], Batch [576/938], Loss: 0.7985204458236694\n",
      "Train: Epoch [3], Batch [577/938], Loss: 0.7216084003448486\n",
      "Train: Epoch [3], Batch [578/938], Loss: 0.7702551484107971\n",
      "Train: Epoch [3], Batch [579/938], Loss: 0.8740444779396057\n",
      "Train: Epoch [3], Batch [580/938], Loss: 0.6736005544662476\n",
      "Train: Epoch [3], Batch [581/938], Loss: 0.7997720837593079\n",
      "Train: Epoch [3], Batch [582/938], Loss: 0.6820648908615112\n",
      "Train: Epoch [3], Batch [583/938], Loss: 0.8183794021606445\n",
      "Train: Epoch [3], Batch [584/938], Loss: 0.9537633657455444\n",
      "Train: Epoch [3], Batch [585/938], Loss: 0.771812915802002\n",
      "Train: Epoch [3], Batch [586/938], Loss: 0.8549447059631348\n",
      "Train: Epoch [3], Batch [587/938], Loss: 0.7998493909835815\n",
      "Train: Epoch [3], Batch [588/938], Loss: 0.837986946105957\n",
      "Train: Epoch [3], Batch [589/938], Loss: 0.8620580434799194\n",
      "Train: Epoch [3], Batch [590/938], Loss: 0.869681179523468\n",
      "Train: Epoch [3], Batch [591/938], Loss: 0.9522222280502319\n",
      "Train: Epoch [3], Batch [592/938], Loss: 0.782579779624939\n",
      "Train: Epoch [3], Batch [593/938], Loss: 0.873014509677887\n",
      "Train: Epoch [3], Batch [594/938], Loss: 0.8366542458534241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [3], Batch [595/938], Loss: 0.771300196647644\n",
      "Train: Epoch [3], Batch [596/938], Loss: 0.8449847102165222\n",
      "Train: Epoch [3], Batch [597/938], Loss: 0.8523699045181274\n",
      "Train: Epoch [3], Batch [598/938], Loss: 0.833266019821167\n",
      "Train: Epoch [3], Batch [599/938], Loss: 0.9786597490310669\n",
      "Train: Epoch [3], Batch [600/938], Loss: 0.6649484634399414\n",
      "Train: Epoch [3], Batch [601/938], Loss: 0.9585720300674438\n",
      "Train: Epoch [3], Batch [602/938], Loss: 1.0314304828643799\n",
      "Train: Epoch [3], Batch [603/938], Loss: 0.8004796504974365\n",
      "Train: Epoch [3], Batch [604/938], Loss: 0.9522393941879272\n",
      "Train: Epoch [3], Batch [605/938], Loss: 0.6394777894020081\n",
      "Train: Epoch [3], Batch [606/938], Loss: 0.8401513695716858\n",
      "Train: Epoch [3], Batch [607/938], Loss: 0.7697686553001404\n",
      "Train: Epoch [3], Batch [608/938], Loss: 0.7315098643302917\n",
      "Train: Epoch [3], Batch [609/938], Loss: 0.632263720035553\n",
      "Train: Epoch [3], Batch [610/938], Loss: 0.8044949769973755\n",
      "Train: Epoch [3], Batch [611/938], Loss: 0.7019000053405762\n",
      "Train: Epoch [3], Batch [612/938], Loss: 0.9020158052444458\n",
      "Train: Epoch [3], Batch [613/938], Loss: 0.6937278509140015\n",
      "Train: Epoch [3], Batch [614/938], Loss: 0.8817073106765747\n",
      "Train: Epoch [3], Batch [615/938], Loss: 0.9710100889205933\n",
      "Train: Epoch [3], Batch [616/938], Loss: 0.8567932844161987\n",
      "Train: Epoch [3], Batch [617/938], Loss: 0.8731277585029602\n",
      "Train: Epoch [3], Batch [618/938], Loss: 0.703510046005249\n",
      "Train: Epoch [3], Batch [619/938], Loss: 1.1598033905029297\n",
      "Train: Epoch [3], Batch [620/938], Loss: 0.8111280798912048\n",
      "Train: Epoch [3], Batch [621/938], Loss: 1.171952724456787\n",
      "Train: Epoch [3], Batch [622/938], Loss: 0.8704179525375366\n",
      "Train: Epoch [3], Batch [623/938], Loss: 0.88567054271698\n",
      "Train: Epoch [3], Batch [624/938], Loss: 0.9558760523796082\n",
      "Train: Epoch [3], Batch [625/938], Loss: 0.769900381565094\n",
      "Train: Epoch [3], Batch [626/938], Loss: 0.7480494976043701\n",
      "Train: Epoch [3], Batch [627/938], Loss: 0.7558261156082153\n",
      "Train: Epoch [3], Batch [628/938], Loss: 0.864762008190155\n",
      "Train: Epoch [3], Batch [629/938], Loss: 0.6522136926651001\n",
      "Train: Epoch [3], Batch [630/938], Loss: 0.8206608891487122\n",
      "Train: Epoch [3], Batch [631/938], Loss: 0.7954941987991333\n",
      "Train: Epoch [3], Batch [632/938], Loss: 0.7108189463615417\n",
      "Train: Epoch [3], Batch [633/938], Loss: 0.8708270788192749\n",
      "Train: Epoch [3], Batch [634/938], Loss: 0.8375921249389648\n",
      "Train: Epoch [3], Batch [635/938], Loss: 1.1987662315368652\n",
      "Train: Epoch [3], Batch [636/938], Loss: 0.7043527364730835\n",
      "Train: Epoch [3], Batch [637/938], Loss: 0.7387720942497253\n",
      "Train: Epoch [3], Batch [638/938], Loss: 0.9016646146774292\n",
      "Train: Epoch [3], Batch [639/938], Loss: 0.7523658275604248\n",
      "Train: Epoch [3], Batch [640/938], Loss: 0.6014871597290039\n",
      "Train: Epoch [3], Batch [641/938], Loss: 1.0241972208023071\n",
      "Train: Epoch [3], Batch [642/938], Loss: 0.6111565828323364\n",
      "Train: Epoch [3], Batch [643/938], Loss: 0.7780241966247559\n",
      "Train: Epoch [3], Batch [644/938], Loss: 0.714394211769104\n",
      "Train: Epoch [3], Batch [645/938], Loss: 0.8267149329185486\n",
      "Train: Epoch [3], Batch [646/938], Loss: 0.8888741135597229\n",
      "Train: Epoch [3], Batch [647/938], Loss: 0.800934910774231\n",
      "Train: Epoch [3], Batch [648/938], Loss: 0.7169389724731445\n",
      "Train: Epoch [3], Batch [649/938], Loss: 0.887349009513855\n",
      "Train: Epoch [3], Batch [650/938], Loss: 0.7246445417404175\n",
      "Train: Epoch [3], Batch [651/938], Loss: 0.9367300271987915\n",
      "Train: Epoch [3], Batch [652/938], Loss: 0.9170669317245483\n",
      "Train: Epoch [3], Batch [653/938], Loss: 0.7599596977233887\n",
      "Train: Epoch [3], Batch [654/938], Loss: 0.7262452840805054\n",
      "Train: Epoch [3], Batch [655/938], Loss: 0.7678446173667908\n",
      "Train: Epoch [3], Batch [656/938], Loss: 0.8802294731140137\n",
      "Train: Epoch [3], Batch [657/938], Loss: 0.9785128831863403\n",
      "Train: Epoch [3], Batch [658/938], Loss: 1.0091583728790283\n",
      "Train: Epoch [3], Batch [659/938], Loss: 0.9071710109710693\n",
      "Train: Epoch [3], Batch [660/938], Loss: 0.8577629327774048\n",
      "Train: Epoch [3], Batch [661/938], Loss: 0.8111573457717896\n",
      "Train: Epoch [3], Batch [662/938], Loss: 0.7277318835258484\n",
      "Train: Epoch [3], Batch [663/938], Loss: 0.8073784112930298\n",
      "Train: Epoch [3], Batch [664/938], Loss: 0.8101140260696411\n",
      "Train: Epoch [3], Batch [665/938], Loss: 0.6360547542572021\n",
      "Train: Epoch [3], Batch [666/938], Loss: 0.8909358978271484\n",
      "Train: Epoch [3], Batch [667/938], Loss: 0.9748173356056213\n",
      "Train: Epoch [3], Batch [668/938], Loss: 1.035864233970642\n",
      "Train: Epoch [3], Batch [669/938], Loss: 0.9175939559936523\n",
      "Train: Epoch [3], Batch [670/938], Loss: 0.9756119251251221\n",
      "Train: Epoch [3], Batch [671/938], Loss: 0.799843430519104\n",
      "Train: Epoch [3], Batch [672/938], Loss: 0.8733914494514465\n",
      "Train: Epoch [3], Batch [673/938], Loss: 0.680586576461792\n",
      "Train: Epoch [3], Batch [674/938], Loss: 0.8711869120597839\n",
      "Train: Epoch [3], Batch [675/938], Loss: 1.032750129699707\n",
      "Train: Epoch [3], Batch [676/938], Loss: 0.8825362324714661\n",
      "Train: Epoch [3], Batch [677/938], Loss: 0.770189642906189\n",
      "Train: Epoch [3], Batch [678/938], Loss: 0.8758726119995117\n",
      "Train: Epoch [3], Batch [679/938], Loss: 0.7063560485839844\n",
      "Train: Epoch [3], Batch [680/938], Loss: 1.1883541345596313\n",
      "Train: Epoch [3], Batch [681/938], Loss: 0.8332652449607849\n",
      "Train: Epoch [3], Batch [682/938], Loss: 0.654499351978302\n",
      "Train: Epoch [3], Batch [683/938], Loss: 0.7717350125312805\n",
      "Train: Epoch [3], Batch [684/938], Loss: 0.7078694105148315\n",
      "Train: Epoch [3], Batch [685/938], Loss: 0.8938182592391968\n",
      "Train: Epoch [3], Batch [686/938], Loss: 0.8439284563064575\n",
      "Train: Epoch [3], Batch [687/938], Loss: 0.6853371262550354\n",
      "Train: Epoch [3], Batch [688/938], Loss: 0.7454085946083069\n",
      "Train: Epoch [3], Batch [689/938], Loss: 0.7548367977142334\n",
      "Train: Epoch [3], Batch [690/938], Loss: 1.0339897871017456\n",
      "Train: Epoch [3], Batch [691/938], Loss: 0.8833684921264648\n",
      "Train: Epoch [3], Batch [692/938], Loss: 0.9151583909988403\n",
      "Train: Epoch [3], Batch [693/938], Loss: 0.7882723212242126\n",
      "Train: Epoch [3], Batch [694/938], Loss: 0.8159728050231934\n",
      "Train: Epoch [3], Batch [695/938], Loss: 0.7810423374176025\n",
      "Train: Epoch [3], Batch [696/938], Loss: 0.81783127784729\n",
      "Train: Epoch [3], Batch [697/938], Loss: 0.7121928334236145\n",
      "Train: Epoch [3], Batch [698/938], Loss: 0.7889808416366577\n",
      "Train: Epoch [3], Batch [699/938], Loss: 1.1148601770401\n",
      "Train: Epoch [3], Batch [700/938], Loss: 0.7057207822799683\n",
      "Train: Epoch [3], Batch [701/938], Loss: 0.7816530466079712\n",
      "Train: Epoch [3], Batch [702/938], Loss: 0.674456000328064\n",
      "Train: Epoch [3], Batch [703/938], Loss: 0.7834780216217041\n",
      "Train: Epoch [3], Batch [704/938], Loss: 0.8199421763420105\n",
      "Train: Epoch [3], Batch [705/938], Loss: 0.8034246563911438\n",
      "Train: Epoch [3], Batch [706/938], Loss: 0.9333399534225464\n",
      "Train: Epoch [3], Batch [707/938], Loss: 0.6449549198150635\n",
      "Train: Epoch [3], Batch [708/938], Loss: 0.723772406578064\n",
      "Train: Epoch [3], Batch [709/938], Loss: 0.7588291764259338\n",
      "Train: Epoch [3], Batch [710/938], Loss: 0.7819637060165405\n",
      "Train: Epoch [3], Batch [711/938], Loss: 0.744326651096344\n",
      "Train: Epoch [3], Batch [712/938], Loss: 0.7270079851150513\n",
      "Train: Epoch [3], Batch [713/938], Loss: 0.8941848278045654\n",
      "Train: Epoch [3], Batch [714/938], Loss: 0.791110098361969\n",
      "Train: Epoch [3], Batch [715/938], Loss: 0.6339848041534424\n",
      "Train: Epoch [3], Batch [716/938], Loss: 0.9175224900245667\n",
      "Train: Epoch [3], Batch [717/938], Loss: 0.6903283596038818\n",
      "Train: Epoch [3], Batch [718/938], Loss: 0.9116910696029663\n",
      "Train: Epoch [3], Batch [719/938], Loss: 0.7877844572067261\n",
      "Train: Epoch [3], Batch [720/938], Loss: 0.7571068406105042\n",
      "Train: Epoch [3], Batch [721/938], Loss: 0.7543109655380249\n",
      "Train: Epoch [3], Batch [722/938], Loss: 0.8655186891555786\n",
      "Train: Epoch [3], Batch [723/938], Loss: 0.7696162462234497\n",
      "Train: Epoch [3], Batch [724/938], Loss: 0.6180005073547363\n",
      "Train: Epoch [3], Batch [725/938], Loss: 0.6856299042701721\n",
      "Train: Epoch [3], Batch [726/938], Loss: 0.7357770204544067\n",
      "Train: Epoch [3], Batch [727/938], Loss: 0.7177358269691467\n",
      "Train: Epoch [3], Batch [728/938], Loss: 0.7495841979980469\n",
      "Train: Epoch [3], Batch [729/938], Loss: 0.6444764137268066\n",
      "Train: Epoch [3], Batch [730/938], Loss: 0.7324291467666626\n",
      "Train: Epoch [3], Batch [731/938], Loss: 0.7185174822807312\n",
      "Train: Epoch [3], Batch [732/938], Loss: 0.7157383561134338\n",
      "Train: Epoch [3], Batch [733/938], Loss: 0.9456312656402588\n",
      "Train: Epoch [3], Batch [734/938], Loss: 0.8848148584365845\n",
      "Train: Epoch [3], Batch [735/938], Loss: 1.0061661005020142\n",
      "Train: Epoch [3], Batch [736/938], Loss: 1.0911095142364502\n",
      "Train: Epoch [3], Batch [737/938], Loss: 0.7216672897338867\n",
      "Train: Epoch [3], Batch [738/938], Loss: 0.9574628472328186\n",
      "Train: Epoch [3], Batch [739/938], Loss: 0.7944267988204956\n",
      "Train: Epoch [3], Batch [740/938], Loss: 0.8392907381057739\n",
      "Train: Epoch [3], Batch [741/938], Loss: 0.8398381471633911\n",
      "Train: Epoch [3], Batch [742/938], Loss: 0.8446151614189148\n",
      "Train: Epoch [3], Batch [743/938], Loss: 1.1302673816680908\n",
      "Train: Epoch [3], Batch [744/938], Loss: 0.7111924290657043\n",
      "Train: Epoch [3], Batch [745/938], Loss: 0.8400750160217285\n",
      "Train: Epoch [3], Batch [746/938], Loss: 1.0281847715377808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [3], Batch [747/938], Loss: 0.8984315395355225\n",
      "Train: Epoch [3], Batch [748/938], Loss: 0.7000850439071655\n",
      "Train: Epoch [3], Batch [749/938], Loss: 0.8324652910232544\n",
      "Train: Epoch [3], Batch [750/938], Loss: 0.8116657733917236\n",
      "Train: Epoch [3], Batch [751/938], Loss: 0.9292713403701782\n",
      "Train: Epoch [3], Batch [752/938], Loss: 0.9852886199951172\n",
      "Train: Epoch [3], Batch [753/938], Loss: 0.7446250915527344\n",
      "Train: Epoch [3], Batch [754/938], Loss: 0.743747353553772\n",
      "Train: Epoch [3], Batch [755/938], Loss: 0.8332333564758301\n",
      "Train: Epoch [3], Batch [756/938], Loss: 0.6828178763389587\n",
      "Train: Epoch [3], Batch [757/938], Loss: 0.649179220199585\n",
      "Train: Epoch [3], Batch [758/938], Loss: 1.0390515327453613\n",
      "Train: Epoch [3], Batch [759/938], Loss: 0.7780104875564575\n",
      "Train: Epoch [3], Batch [760/938], Loss: 0.8815193176269531\n",
      "Train: Epoch [3], Batch [761/938], Loss: 0.896550178527832\n",
      "Train: Epoch [3], Batch [762/938], Loss: 0.8473125696182251\n",
      "Train: Epoch [3], Batch [763/938], Loss: 0.8876917958259583\n",
      "Train: Epoch [3], Batch [764/938], Loss: 0.7180012464523315\n",
      "Train: Epoch [3], Batch [765/938], Loss: 0.8794084787368774\n",
      "Train: Epoch [3], Batch [766/938], Loss: 0.7725101709365845\n",
      "Train: Epoch [3], Batch [767/938], Loss: 0.7855578064918518\n",
      "Train: Epoch [3], Batch [768/938], Loss: 0.8547283411026001\n",
      "Train: Epoch [3], Batch [769/938], Loss: 0.7859994769096375\n",
      "Train: Epoch [3], Batch [770/938], Loss: 0.8279167413711548\n",
      "Train: Epoch [3], Batch [771/938], Loss: 0.8574004769325256\n",
      "Train: Epoch [3], Batch [772/938], Loss: 0.811521589756012\n",
      "Train: Epoch [3], Batch [773/938], Loss: 0.9463615417480469\n",
      "Train: Epoch [3], Batch [774/938], Loss: 0.8443925976753235\n",
      "Train: Epoch [3], Batch [775/938], Loss: 0.8002123832702637\n",
      "Train: Epoch [3], Batch [776/938], Loss: 0.7942142486572266\n",
      "Train: Epoch [3], Batch [777/938], Loss: 0.8078010082244873\n",
      "Train: Epoch [3], Batch [778/938], Loss: 0.8606665134429932\n",
      "Train: Epoch [3], Batch [779/938], Loss: 0.7799978852272034\n",
      "Train: Epoch [3], Batch [780/938], Loss: 0.7703363299369812\n",
      "Train: Epoch [3], Batch [781/938], Loss: 0.7779313921928406\n",
      "Train: Epoch [3], Batch [782/938], Loss: 0.7236467003822327\n",
      "Train: Epoch [3], Batch [783/938], Loss: 0.8513793349266052\n",
      "Train: Epoch [3], Batch [784/938], Loss: 0.7047973275184631\n",
      "Train: Epoch [3], Batch [785/938], Loss: 0.7689388990402222\n",
      "Train: Epoch [3], Batch [786/938], Loss: 0.8143140077590942\n",
      "Train: Epoch [3], Batch [787/938], Loss: 0.7015788555145264\n",
      "Train: Epoch [3], Batch [788/938], Loss: 0.7820639610290527\n",
      "Train: Epoch [3], Batch [789/938], Loss: 0.8742996454238892\n",
      "Train: Epoch [3], Batch [790/938], Loss: 0.8742574453353882\n",
      "Train: Epoch [3], Batch [791/938], Loss: 0.7351201176643372\n",
      "Train: Epoch [3], Batch [792/938], Loss: 0.747695803642273\n",
      "Train: Epoch [3], Batch [793/938], Loss: 0.7546989321708679\n",
      "Train: Epoch [3], Batch [794/938], Loss: 0.708197832107544\n",
      "Train: Epoch [3], Batch [795/938], Loss: 0.7726985216140747\n",
      "Train: Epoch [3], Batch [796/938], Loss: 0.8662823438644409\n",
      "Train: Epoch [3], Batch [797/938], Loss: 0.8120006322860718\n",
      "Train: Epoch [3], Batch [798/938], Loss: 0.7591710090637207\n",
      "Train: Epoch [3], Batch [799/938], Loss: 0.9510688781738281\n",
      "Train: Epoch [3], Batch [800/938], Loss: 1.0618789196014404\n",
      "Train: Epoch [3], Batch [801/938], Loss: 0.9018657207489014\n",
      "Train: Epoch [3], Batch [802/938], Loss: 0.9516047835350037\n",
      "Train: Epoch [3], Batch [803/938], Loss: 0.7157924175262451\n",
      "Train: Epoch [3], Batch [804/938], Loss: 0.7720024585723877\n",
      "Train: Epoch [3], Batch [805/938], Loss: 0.7367201447486877\n",
      "Train: Epoch [3], Batch [806/938], Loss: 0.748089075088501\n",
      "Train: Epoch [3], Batch [807/938], Loss: 0.7329221367835999\n",
      "Train: Epoch [3], Batch [808/938], Loss: 0.8270958662033081\n",
      "Train: Epoch [3], Batch [809/938], Loss: 0.953424334526062\n",
      "Train: Epoch [3], Batch [810/938], Loss: 1.0099973678588867\n",
      "Train: Epoch [3], Batch [811/938], Loss: 0.7693153619766235\n",
      "Train: Epoch [3], Batch [812/938], Loss: 0.7970269918441772\n",
      "Train: Epoch [3], Batch [813/938], Loss: 0.8407851457595825\n",
      "Train: Epoch [3], Batch [814/938], Loss: 0.8240247964859009\n",
      "Train: Epoch [3], Batch [815/938], Loss: 1.058016300201416\n",
      "Train: Epoch [3], Batch [816/938], Loss: 0.683054506778717\n",
      "Train: Epoch [3], Batch [817/938], Loss: 0.5441150665283203\n",
      "Train: Epoch [3], Batch [818/938], Loss: 0.7061952352523804\n",
      "Train: Epoch [3], Batch [819/938], Loss: 0.9257287979125977\n",
      "Train: Epoch [3], Batch [820/938], Loss: 0.8306570649147034\n",
      "Train: Epoch [3], Batch [821/938], Loss: 0.7812905311584473\n",
      "Train: Epoch [3], Batch [822/938], Loss: 0.7830222845077515\n",
      "Train: Epoch [3], Batch [823/938], Loss: 0.8455076217651367\n",
      "Train: Epoch [3], Batch [824/938], Loss: 0.8432389497756958\n",
      "Train: Epoch [3], Batch [825/938], Loss: 0.7095253467559814\n",
      "Train: Epoch [3], Batch [826/938], Loss: 0.760918378829956\n",
      "Train: Epoch [3], Batch [827/938], Loss: 0.9043303728103638\n",
      "Train: Epoch [3], Batch [828/938], Loss: 0.773125410079956\n",
      "Train: Epoch [3], Batch [829/938], Loss: 0.850469172000885\n",
      "Train: Epoch [3], Batch [830/938], Loss: 0.7321383953094482\n",
      "Train: Epoch [3], Batch [831/938], Loss: 0.9114331007003784\n",
      "Train: Epoch [3], Batch [832/938], Loss: 0.7935147881507874\n",
      "Train: Epoch [3], Batch [833/938], Loss: 0.8129268884658813\n",
      "Train: Epoch [3], Batch [834/938], Loss: 0.657241702079773\n",
      "Train: Epoch [3], Batch [835/938], Loss: 0.9164268970489502\n",
      "Train: Epoch [3], Batch [836/938], Loss: 0.8122406005859375\n",
      "Train: Epoch [3], Batch [837/938], Loss: 0.7497074604034424\n",
      "Train: Epoch [3], Batch [838/938], Loss: 0.9387354850769043\n",
      "Train: Epoch [3], Batch [839/938], Loss: 0.720157265663147\n",
      "Train: Epoch [3], Batch [840/938], Loss: 0.7270540595054626\n",
      "Train: Epoch [3], Batch [841/938], Loss: 0.770857036113739\n",
      "Train: Epoch [3], Batch [842/938], Loss: 0.8294574022293091\n",
      "Train: Epoch [3], Batch [843/938], Loss: 0.7857124209403992\n",
      "Train: Epoch [3], Batch [844/938], Loss: 0.842645525932312\n",
      "Train: Epoch [3], Batch [845/938], Loss: 0.9967314004898071\n",
      "Train: Epoch [3], Batch [846/938], Loss: 0.5623520016670227\n",
      "Train: Epoch [3], Batch [847/938], Loss: 0.7282674908638\n",
      "Train: Epoch [3], Batch [848/938], Loss: 0.7332425117492676\n",
      "Train: Epoch [3], Batch [849/938], Loss: 0.86212557554245\n",
      "Train: Epoch [3], Batch [850/938], Loss: 0.9570265412330627\n",
      "Train: Epoch [3], Batch [851/938], Loss: 0.8172548413276672\n",
      "Train: Epoch [3], Batch [852/938], Loss: 0.5747878551483154\n",
      "Train: Epoch [3], Batch [853/938], Loss: 0.8845974802970886\n",
      "Train: Epoch [3], Batch [854/938], Loss: 0.7918383479118347\n",
      "Train: Epoch [3], Batch [855/938], Loss: 0.6596728563308716\n",
      "Train: Epoch [3], Batch [856/938], Loss: 0.5843610763549805\n",
      "Train: Epoch [3], Batch [857/938], Loss: 0.9732300043106079\n",
      "Train: Epoch [3], Batch [858/938], Loss: 0.9209089875221252\n",
      "Train: Epoch [3], Batch [859/938], Loss: 0.9900557994842529\n",
      "Train: Epoch [3], Batch [860/938], Loss: 0.686378002166748\n",
      "Train: Epoch [3], Batch [861/938], Loss: 0.6693311333656311\n",
      "Train: Epoch [3], Batch [862/938], Loss: 0.7581523656845093\n",
      "Train: Epoch [3], Batch [863/938], Loss: 1.0389111042022705\n",
      "Train: Epoch [3], Batch [864/938], Loss: 0.9158499836921692\n",
      "Train: Epoch [3], Batch [865/938], Loss: 0.7745437026023865\n",
      "Train: Epoch [3], Batch [866/938], Loss: 0.9694873094558716\n",
      "Train: Epoch [3], Batch [867/938], Loss: 0.9051737785339355\n",
      "Train: Epoch [3], Batch [868/938], Loss: 0.8017395734786987\n",
      "Train: Epoch [3], Batch [869/938], Loss: 0.7150894403457642\n",
      "Train: Epoch [3], Batch [870/938], Loss: 0.7681487798690796\n",
      "Train: Epoch [3], Batch [871/938], Loss: 0.7974689602851868\n",
      "Train: Epoch [3], Batch [872/938], Loss: 0.8374631404876709\n",
      "Train: Epoch [3], Batch [873/938], Loss: 0.7623820304870605\n",
      "Train: Epoch [3], Batch [874/938], Loss: 0.9351409077644348\n",
      "Train: Epoch [3], Batch [875/938], Loss: 0.8326078057289124\n",
      "Train: Epoch [3], Batch [876/938], Loss: 1.0556375980377197\n",
      "Train: Epoch [3], Batch [877/938], Loss: 0.690571129322052\n",
      "Train: Epoch [3], Batch [878/938], Loss: 0.5123060941696167\n",
      "Train: Epoch [3], Batch [879/938], Loss: 0.7580505609512329\n",
      "Train: Epoch [3], Batch [880/938], Loss: 0.7351537346839905\n",
      "Train: Epoch [3], Batch [881/938], Loss: 0.7600705623626709\n",
      "Train: Epoch [3], Batch [882/938], Loss: 0.7793968915939331\n",
      "Train: Epoch [3], Batch [883/938], Loss: 0.745160698890686\n",
      "Train: Epoch [3], Batch [884/938], Loss: 0.9225530624389648\n",
      "Train: Epoch [3], Batch [885/938], Loss: 0.6961040496826172\n",
      "Train: Epoch [3], Batch [886/938], Loss: 0.7693246006965637\n",
      "Train: Epoch [3], Batch [887/938], Loss: 0.7537554502487183\n",
      "Train: Epoch [3], Batch [888/938], Loss: 0.5554251670837402\n",
      "Train: Epoch [3], Batch [889/938], Loss: 0.9437463879585266\n",
      "Train: Epoch [3], Batch [890/938], Loss: 0.7097594738006592\n",
      "Train: Epoch [3], Batch [891/938], Loss: 0.890798807144165\n",
      "Train: Epoch [3], Batch [892/938], Loss: 0.7612734436988831\n",
      "Train: Epoch [3], Batch [893/938], Loss: 0.8092435002326965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [3], Batch [894/938], Loss: 0.7677493095397949\n",
      "Train: Epoch [3], Batch [895/938], Loss: 0.8315694332122803\n",
      "Train: Epoch [3], Batch [896/938], Loss: 0.8119875192642212\n",
      "Train: Epoch [3], Batch [897/938], Loss: 0.840105414390564\n",
      "Train: Epoch [3], Batch [898/938], Loss: 0.7292872667312622\n",
      "Train: Epoch [3], Batch [899/938], Loss: 0.7814130783081055\n",
      "Train: Epoch [3], Batch [900/938], Loss: 0.6740775108337402\n",
      "Train: Epoch [3], Batch [901/938], Loss: 0.7386424541473389\n",
      "Train: Epoch [3], Batch [902/938], Loss: 0.9244499802589417\n",
      "Train: Epoch [3], Batch [903/938], Loss: 0.6792886853218079\n",
      "Train: Epoch [3], Batch [904/938], Loss: 0.6936205625534058\n",
      "Train: Epoch [3], Batch [905/938], Loss: 0.6384437084197998\n",
      "Train: Epoch [3], Batch [906/938], Loss: 1.06039559841156\n",
      "Train: Epoch [3], Batch [907/938], Loss: 0.8031706809997559\n",
      "Train: Epoch [3], Batch [908/938], Loss: 0.6629626750946045\n",
      "Train: Epoch [3], Batch [909/938], Loss: 0.86260586977005\n",
      "Train: Epoch [3], Batch [910/938], Loss: 0.7372156381607056\n",
      "Train: Epoch [3], Batch [911/938], Loss: 1.039174199104309\n",
      "Train: Epoch [3], Batch [912/938], Loss: 0.8704201579093933\n",
      "Train: Epoch [3], Batch [913/938], Loss: 0.7256562113761902\n",
      "Train: Epoch [3], Batch [914/938], Loss: 0.8271861672401428\n",
      "Train: Epoch [3], Batch [915/938], Loss: 0.6194295883178711\n",
      "Train: Epoch [3], Batch [916/938], Loss: 0.7451516389846802\n",
      "Train: Epoch [3], Batch [917/938], Loss: 0.853238582611084\n",
      "Train: Epoch [3], Batch [918/938], Loss: 0.8190006017684937\n",
      "Train: Epoch [3], Batch [919/938], Loss: 0.9180696606636047\n",
      "Train: Epoch [3], Batch [920/938], Loss: 0.7788975238800049\n",
      "Train: Epoch [3], Batch [921/938], Loss: 1.1124780178070068\n",
      "Train: Epoch [3], Batch [922/938], Loss: 0.6735272407531738\n",
      "Train: Epoch [3], Batch [923/938], Loss: 0.7003763914108276\n",
      "Train: Epoch [3], Batch [924/938], Loss: 0.7320554256439209\n",
      "Train: Epoch [3], Batch [925/938], Loss: 0.7349587082862854\n",
      "Train: Epoch [3], Batch [926/938], Loss: 0.8636891841888428\n",
      "Train: Epoch [3], Batch [927/938], Loss: 0.8877569437026978\n",
      "Train: Epoch [3], Batch [928/938], Loss: 0.8474544882774353\n",
      "Train: Epoch [3], Batch [929/938], Loss: 0.6926734447479248\n",
      "Train: Epoch [3], Batch [930/938], Loss: 0.9670824408531189\n",
      "Train: Epoch [3], Batch [931/938], Loss: 0.9792686700820923\n",
      "Train: Epoch [3], Batch [932/938], Loss: 0.9954802393913269\n",
      "Train: Epoch [3], Batch [933/938], Loss: 0.8174214959144592\n",
      "Train: Epoch [3], Batch [934/938], Loss: 0.750087559223175\n",
      "Train: Epoch [3], Batch [935/938], Loss: 0.7452817559242249\n",
      "Train: Epoch [3], Batch [936/938], Loss: 0.8435382843017578\n",
      "Train: Epoch [3], Batch [937/938], Loss: 0.6520450115203857\n",
      "Train: Epoch [3], Batch [938/938], Loss: 0.6560146808624268\n",
      "Accuracy of train set: 0.6785333333333333\n",
      "Validation: Epoch [3], Batch [1/938], Loss: 0.941633403301239\n",
      "Validation: Epoch [3], Batch [2/938], Loss: 0.6242008805274963\n",
      "Validation: Epoch [3], Batch [3/938], Loss: 0.8068426847457886\n",
      "Validation: Epoch [3], Batch [4/938], Loss: 0.7258694767951965\n",
      "Validation: Epoch [3], Batch [5/938], Loss: 0.7581453323364258\n",
      "Validation: Epoch [3], Batch [6/938], Loss: 0.9061340093612671\n",
      "Validation: Epoch [3], Batch [7/938], Loss: 0.7502121925354004\n",
      "Validation: Epoch [3], Batch [8/938], Loss: 0.8420213460922241\n",
      "Validation: Epoch [3], Batch [9/938], Loss: 0.8180776834487915\n",
      "Validation: Epoch [3], Batch [10/938], Loss: 0.7710889577865601\n",
      "Validation: Epoch [3], Batch [11/938], Loss: 0.8782631754875183\n",
      "Validation: Epoch [3], Batch [12/938], Loss: 0.9292364120483398\n",
      "Validation: Epoch [3], Batch [13/938], Loss: 0.9358516335487366\n",
      "Validation: Epoch [3], Batch [14/938], Loss: 0.8002966046333313\n",
      "Validation: Epoch [3], Batch [15/938], Loss: 0.7306725382804871\n",
      "Validation: Epoch [3], Batch [16/938], Loss: 0.8084389567375183\n",
      "Validation: Epoch [3], Batch [17/938], Loss: 0.7447013854980469\n",
      "Validation: Epoch [3], Batch [18/938], Loss: 0.820517897605896\n",
      "Validation: Epoch [3], Batch [19/938], Loss: 0.7392302751541138\n",
      "Validation: Epoch [3], Batch [20/938], Loss: 0.7829616069793701\n",
      "Validation: Epoch [3], Batch [21/938], Loss: 0.8923090696334839\n",
      "Validation: Epoch [3], Batch [22/938], Loss: 0.6546943187713623\n",
      "Validation: Epoch [3], Batch [23/938], Loss: 0.5617655515670776\n",
      "Validation: Epoch [3], Batch [24/938], Loss: 0.7694284319877625\n",
      "Validation: Epoch [3], Batch [25/938], Loss: 0.8306824564933777\n",
      "Validation: Epoch [3], Batch [26/938], Loss: 0.8365556597709656\n",
      "Validation: Epoch [3], Batch [27/938], Loss: 0.86821448802948\n",
      "Validation: Epoch [3], Batch [28/938], Loss: 0.9936642050743103\n",
      "Validation: Epoch [3], Batch [29/938], Loss: 0.7200713753700256\n",
      "Validation: Epoch [3], Batch [30/938], Loss: 0.979694664478302\n",
      "Validation: Epoch [3], Batch [31/938], Loss: 0.7129130363464355\n",
      "Validation: Epoch [3], Batch [32/938], Loss: 0.924568235874176\n",
      "Validation: Epoch [3], Batch [33/938], Loss: 0.7062702178955078\n",
      "Validation: Epoch [3], Batch [34/938], Loss: 0.6684321761131287\n",
      "Validation: Epoch [3], Batch [35/938], Loss: 0.7086701393127441\n",
      "Validation: Epoch [3], Batch [36/938], Loss: 0.7985788583755493\n",
      "Validation: Epoch [3], Batch [37/938], Loss: 0.7996412515640259\n",
      "Validation: Epoch [3], Batch [38/938], Loss: 0.8985097408294678\n",
      "Validation: Epoch [3], Batch [39/938], Loss: 0.8325458765029907\n",
      "Validation: Epoch [3], Batch [40/938], Loss: 0.6578421592712402\n",
      "Validation: Epoch [3], Batch [41/938], Loss: 0.7526118755340576\n",
      "Validation: Epoch [3], Batch [42/938], Loss: 0.6866876482963562\n",
      "Validation: Epoch [3], Batch [43/938], Loss: 0.7607226371765137\n",
      "Validation: Epoch [3], Batch [44/938], Loss: 0.7784273028373718\n",
      "Validation: Epoch [3], Batch [45/938], Loss: 0.8744146823883057\n",
      "Validation: Epoch [3], Batch [46/938], Loss: 0.9425073862075806\n",
      "Validation: Epoch [3], Batch [47/938], Loss: 0.6843258142471313\n",
      "Validation: Epoch [3], Batch [48/938], Loss: 0.9609355926513672\n",
      "Validation: Epoch [3], Batch [49/938], Loss: 0.6867603063583374\n",
      "Validation: Epoch [3], Batch [50/938], Loss: 0.6004528999328613\n",
      "Validation: Epoch [3], Batch [51/938], Loss: 0.6160070896148682\n",
      "Validation: Epoch [3], Batch [52/938], Loss: 0.7519617080688477\n",
      "Validation: Epoch [3], Batch [53/938], Loss: 0.7776411771774292\n",
      "Validation: Epoch [3], Batch [54/938], Loss: 0.9754777550697327\n",
      "Validation: Epoch [3], Batch [55/938], Loss: 0.8852453231811523\n",
      "Validation: Epoch [3], Batch [56/938], Loss: 0.7038726210594177\n",
      "Validation: Epoch [3], Batch [57/938], Loss: 0.8228662014007568\n",
      "Validation: Epoch [3], Batch [58/938], Loss: 0.8603450059890747\n",
      "Validation: Epoch [3], Batch [59/938], Loss: 0.9959983825683594\n",
      "Validation: Epoch [3], Batch [60/938], Loss: 0.7773150205612183\n",
      "Validation: Epoch [3], Batch [61/938], Loss: 0.7504080533981323\n",
      "Validation: Epoch [3], Batch [62/938], Loss: 0.8919448852539062\n",
      "Validation: Epoch [3], Batch [63/938], Loss: 0.7162560224533081\n",
      "Validation: Epoch [3], Batch [64/938], Loss: 0.8412947654724121\n",
      "Validation: Epoch [3], Batch [65/938], Loss: 0.8707559108734131\n",
      "Validation: Epoch [3], Batch [66/938], Loss: 0.8465894460678101\n",
      "Validation: Epoch [3], Batch [67/938], Loss: 0.7801287770271301\n",
      "Validation: Epoch [3], Batch [68/938], Loss: 0.9329509735107422\n",
      "Validation: Epoch [3], Batch [69/938], Loss: 0.8535930514335632\n",
      "Validation: Epoch [3], Batch [70/938], Loss: 0.7906466722488403\n",
      "Validation: Epoch [3], Batch [71/938], Loss: 0.9645061492919922\n",
      "Validation: Epoch [3], Batch [72/938], Loss: 0.7979488372802734\n",
      "Validation: Epoch [3], Batch [73/938], Loss: 1.0860693454742432\n",
      "Validation: Epoch [3], Batch [74/938], Loss: 0.8324236273765564\n",
      "Validation: Epoch [3], Batch [75/938], Loss: 0.7774097919464111\n",
      "Validation: Epoch [3], Batch [76/938], Loss: 0.7812374830245972\n",
      "Validation: Epoch [3], Batch [77/938], Loss: 0.8682050108909607\n",
      "Validation: Epoch [3], Batch [78/938], Loss: 0.8471436500549316\n",
      "Validation: Epoch [3], Batch [79/938], Loss: 0.7265175580978394\n",
      "Validation: Epoch [3], Batch [80/938], Loss: 0.9843101501464844\n",
      "Validation: Epoch [3], Batch [81/938], Loss: 0.728086531162262\n",
      "Validation: Epoch [3], Batch [82/938], Loss: 0.9887803792953491\n",
      "Validation: Epoch [3], Batch [83/938], Loss: 0.7786709070205688\n",
      "Validation: Epoch [3], Batch [84/938], Loss: 0.7198708653450012\n",
      "Validation: Epoch [3], Batch [85/938], Loss: 0.8103573322296143\n",
      "Validation: Epoch [3], Batch [86/938], Loss: 1.0543572902679443\n",
      "Validation: Epoch [3], Batch [87/938], Loss: 0.750619649887085\n",
      "Validation: Epoch [3], Batch [88/938], Loss: 0.8163824081420898\n",
      "Validation: Epoch [3], Batch [89/938], Loss: 0.6078281998634338\n",
      "Validation: Epoch [3], Batch [90/938], Loss: 0.912230908870697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [91/938], Loss: 0.7256221175193787\n",
      "Validation: Epoch [3], Batch [92/938], Loss: 0.9114438891410828\n",
      "Validation: Epoch [3], Batch [93/938], Loss: 0.7520458102226257\n",
      "Validation: Epoch [3], Batch [94/938], Loss: 0.651835560798645\n",
      "Validation: Epoch [3], Batch [95/938], Loss: 0.813571035861969\n",
      "Validation: Epoch [3], Batch [96/938], Loss: 0.7071353197097778\n",
      "Validation: Epoch [3], Batch [97/938], Loss: 0.7884472608566284\n",
      "Validation: Epoch [3], Batch [98/938], Loss: 0.6872872114181519\n",
      "Validation: Epoch [3], Batch [99/938], Loss: 0.8264146447181702\n",
      "Validation: Epoch [3], Batch [100/938], Loss: 0.6650823354721069\n",
      "Validation: Epoch [3], Batch [101/938], Loss: 0.8433730602264404\n",
      "Validation: Epoch [3], Batch [102/938], Loss: 0.9206112027168274\n",
      "Validation: Epoch [3], Batch [103/938], Loss: 0.8250187039375305\n",
      "Validation: Epoch [3], Batch [104/938], Loss: 0.8119919300079346\n",
      "Validation: Epoch [3], Batch [105/938], Loss: 0.6921002864837646\n",
      "Validation: Epoch [3], Batch [106/938], Loss: 0.8332157135009766\n",
      "Validation: Epoch [3], Batch [107/938], Loss: 0.9791781902313232\n",
      "Validation: Epoch [3], Batch [108/938], Loss: 0.7415446043014526\n",
      "Validation: Epoch [3], Batch [109/938], Loss: 0.9912419319152832\n",
      "Validation: Epoch [3], Batch [110/938], Loss: 0.9295480251312256\n",
      "Validation: Epoch [3], Batch [111/938], Loss: 0.9897446036338806\n",
      "Validation: Epoch [3], Batch [112/938], Loss: 0.8534984588623047\n",
      "Validation: Epoch [3], Batch [113/938], Loss: 0.7354456782341003\n",
      "Validation: Epoch [3], Batch [114/938], Loss: 0.8975915312767029\n",
      "Validation: Epoch [3], Batch [115/938], Loss: 0.6862859725952148\n",
      "Validation: Epoch [3], Batch [116/938], Loss: 0.9384051561355591\n",
      "Validation: Epoch [3], Batch [117/938], Loss: 1.2617623805999756\n",
      "Validation: Epoch [3], Batch [118/938], Loss: 0.6276904344558716\n",
      "Validation: Epoch [3], Batch [119/938], Loss: 1.1543209552764893\n",
      "Validation: Epoch [3], Batch [120/938], Loss: 0.9635610580444336\n",
      "Validation: Epoch [3], Batch [121/938], Loss: 0.7265684008598328\n",
      "Validation: Epoch [3], Batch [122/938], Loss: 0.6682503819465637\n",
      "Validation: Epoch [3], Batch [123/938], Loss: 0.915255606174469\n",
      "Validation: Epoch [3], Batch [124/938], Loss: 0.8465021252632141\n",
      "Validation: Epoch [3], Batch [125/938], Loss: 0.9178701639175415\n",
      "Validation: Epoch [3], Batch [126/938], Loss: 0.9877597093582153\n",
      "Validation: Epoch [3], Batch [127/938], Loss: 0.6725764274597168\n",
      "Validation: Epoch [3], Batch [128/938], Loss: 0.8345150947570801\n",
      "Validation: Epoch [3], Batch [129/938], Loss: 0.757189929485321\n",
      "Validation: Epoch [3], Batch [130/938], Loss: 0.6219872236251831\n",
      "Validation: Epoch [3], Batch [131/938], Loss: 0.8045692443847656\n",
      "Validation: Epoch [3], Batch [132/938], Loss: 0.7770226001739502\n",
      "Validation: Epoch [3], Batch [133/938], Loss: 0.5637874007225037\n",
      "Validation: Epoch [3], Batch [134/938], Loss: 0.8676310181617737\n",
      "Validation: Epoch [3], Batch [135/938], Loss: 0.9396488666534424\n",
      "Validation: Epoch [3], Batch [136/938], Loss: 1.0881714820861816\n",
      "Validation: Epoch [3], Batch [137/938], Loss: 0.8372551202774048\n",
      "Validation: Epoch [3], Batch [138/938], Loss: 0.7315484285354614\n",
      "Validation: Epoch [3], Batch [139/938], Loss: 0.6613844633102417\n",
      "Validation: Epoch [3], Batch [140/938], Loss: 0.8631967306137085\n",
      "Validation: Epoch [3], Batch [141/938], Loss: 0.6104692220687866\n",
      "Validation: Epoch [3], Batch [142/938], Loss: 0.92075115442276\n",
      "Validation: Epoch [3], Batch [143/938], Loss: 0.8867825269699097\n",
      "Validation: Epoch [3], Batch [144/938], Loss: 1.0768449306488037\n",
      "Validation: Epoch [3], Batch [145/938], Loss: 0.6887468099594116\n",
      "Validation: Epoch [3], Batch [146/938], Loss: 0.6845149397850037\n",
      "Validation: Epoch [3], Batch [147/938], Loss: 0.7245769500732422\n",
      "Validation: Epoch [3], Batch [148/938], Loss: 0.9256956577301025\n",
      "Validation: Epoch [3], Batch [149/938], Loss: 1.0348230600357056\n",
      "Validation: Epoch [3], Batch [150/938], Loss: 0.8335452079772949\n",
      "Validation: Epoch [3], Batch [151/938], Loss: 0.7312892079353333\n",
      "Validation: Epoch [3], Batch [152/938], Loss: 0.45798438787460327\n",
      "Validation: Epoch [3], Batch [153/938], Loss: 0.8304457068443298\n",
      "Validation: Epoch [3], Batch [154/938], Loss: 0.992988109588623\n",
      "Validation: Epoch [3], Batch [155/938], Loss: 0.7088569402694702\n",
      "Validation: Epoch [3], Batch [156/938], Loss: 0.7320564389228821\n",
      "Validation: Epoch [3], Batch [157/938], Loss: 0.7912118434906006\n",
      "Validation: Epoch [3], Batch [158/938], Loss: 0.6714315414428711\n",
      "Validation: Epoch [3], Batch [159/938], Loss: 0.7711392641067505\n",
      "Validation: Epoch [3], Batch [160/938], Loss: 0.7078533172607422\n",
      "Validation: Epoch [3], Batch [161/938], Loss: 0.7215264439582825\n",
      "Validation: Epoch [3], Batch [162/938], Loss: 0.8159252405166626\n",
      "Validation: Epoch [3], Batch [163/938], Loss: 0.7493631839752197\n",
      "Validation: Epoch [3], Batch [164/938], Loss: 0.8760194778442383\n",
      "Validation: Epoch [3], Batch [165/938], Loss: 0.8601744771003723\n",
      "Validation: Epoch [3], Batch [166/938], Loss: 0.7727135419845581\n",
      "Validation: Epoch [3], Batch [167/938], Loss: 0.6499887108802795\n",
      "Validation: Epoch [3], Batch [168/938], Loss: 0.7732061147689819\n",
      "Validation: Epoch [3], Batch [169/938], Loss: 0.8626551628112793\n",
      "Validation: Epoch [3], Batch [170/938], Loss: 0.961351752281189\n",
      "Validation: Epoch [3], Batch [171/938], Loss: 0.6516531705856323\n",
      "Validation: Epoch [3], Batch [172/938], Loss: 0.7618208527565002\n",
      "Validation: Epoch [3], Batch [173/938], Loss: 0.733393132686615\n",
      "Validation: Epoch [3], Batch [174/938], Loss: 0.9159786105155945\n",
      "Validation: Epoch [3], Batch [175/938], Loss: 0.7530871629714966\n",
      "Validation: Epoch [3], Batch [176/938], Loss: 0.7940006852149963\n",
      "Validation: Epoch [3], Batch [177/938], Loss: 0.9321653842926025\n",
      "Validation: Epoch [3], Batch [178/938], Loss: 0.7623392343521118\n",
      "Validation: Epoch [3], Batch [179/938], Loss: 0.933586061000824\n",
      "Validation: Epoch [3], Batch [180/938], Loss: 0.8770334720611572\n",
      "Validation: Epoch [3], Batch [181/938], Loss: 0.6972051858901978\n",
      "Validation: Epoch [3], Batch [182/938], Loss: 0.8089278936386108\n",
      "Validation: Epoch [3], Batch [183/938], Loss: 0.6346875429153442\n",
      "Validation: Epoch [3], Batch [184/938], Loss: 0.8215682506561279\n",
      "Validation: Epoch [3], Batch [185/938], Loss: 0.7110505104064941\n",
      "Validation: Epoch [3], Batch [186/938], Loss: 0.8939825892448425\n",
      "Validation: Epoch [3], Batch [187/938], Loss: 0.7165873646736145\n",
      "Validation: Epoch [3], Batch [188/938], Loss: 0.7615900039672852\n",
      "Validation: Epoch [3], Batch [189/938], Loss: 0.769477367401123\n",
      "Validation: Epoch [3], Batch [190/938], Loss: 0.5750274658203125\n",
      "Validation: Epoch [3], Batch [191/938], Loss: 0.7988384962081909\n",
      "Validation: Epoch [3], Batch [192/938], Loss: 0.6912130117416382\n",
      "Validation: Epoch [3], Batch [193/938], Loss: 0.7902395725250244\n",
      "Validation: Epoch [3], Batch [194/938], Loss: 0.8271598815917969\n",
      "Validation: Epoch [3], Batch [195/938], Loss: 0.6936951875686646\n",
      "Validation: Epoch [3], Batch [196/938], Loss: 0.806982696056366\n",
      "Validation: Epoch [3], Batch [197/938], Loss: 0.6641098856925964\n",
      "Validation: Epoch [3], Batch [198/938], Loss: 0.9780623912811279\n",
      "Validation: Epoch [3], Batch [199/938], Loss: 0.623958945274353\n",
      "Validation: Epoch [3], Batch [200/938], Loss: 0.8282346725463867\n",
      "Validation: Epoch [3], Batch [201/938], Loss: 1.0788581371307373\n",
      "Validation: Epoch [3], Batch [202/938], Loss: 0.7479298114776611\n",
      "Validation: Epoch [3], Batch [203/938], Loss: 0.6576063632965088\n",
      "Validation: Epoch [3], Batch [204/938], Loss: 0.6802307367324829\n",
      "Validation: Epoch [3], Batch [205/938], Loss: 0.6734384298324585\n",
      "Validation: Epoch [3], Batch [206/938], Loss: 0.9044511318206787\n",
      "Validation: Epoch [3], Batch [207/938], Loss: 0.7522364854812622\n",
      "Validation: Epoch [3], Batch [208/938], Loss: 0.733909010887146\n",
      "Validation: Epoch [3], Batch [209/938], Loss: 0.8722387552261353\n",
      "Validation: Epoch [3], Batch [210/938], Loss: 0.6172634363174438\n",
      "Validation: Epoch [3], Batch [211/938], Loss: 0.8717561364173889\n",
      "Validation: Epoch [3], Batch [212/938], Loss: 0.9226798415184021\n",
      "Validation: Epoch [3], Batch [213/938], Loss: 0.527253270149231\n",
      "Validation: Epoch [3], Batch [214/938], Loss: 0.5786827206611633\n",
      "Validation: Epoch [3], Batch [215/938], Loss: 0.6276702880859375\n",
      "Validation: Epoch [3], Batch [216/938], Loss: 0.6983224153518677\n",
      "Validation: Epoch [3], Batch [217/938], Loss: 0.7511647939682007\n",
      "Validation: Epoch [3], Batch [218/938], Loss: 0.8548959493637085\n",
      "Validation: Epoch [3], Batch [219/938], Loss: 0.7348290681838989\n",
      "Validation: Epoch [3], Batch [220/938], Loss: 0.74580979347229\n",
      "Validation: Epoch [3], Batch [221/938], Loss: 0.8188912868499756\n",
      "Validation: Epoch [3], Batch [222/938], Loss: 0.835213303565979\n",
      "Validation: Epoch [3], Batch [223/938], Loss: 0.6925216317176819\n",
      "Validation: Epoch [3], Batch [224/938], Loss: 0.7429397702217102\n",
      "Validation: Epoch [3], Batch [225/938], Loss: 1.1726806163787842\n",
      "Validation: Epoch [3], Batch [226/938], Loss: 0.6697792410850525\n",
      "Validation: Epoch [3], Batch [227/938], Loss: 0.7188663482666016\n",
      "Validation: Epoch [3], Batch [228/938], Loss: 0.6575236320495605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [229/938], Loss: 0.7442203760147095\n",
      "Validation: Epoch [3], Batch [230/938], Loss: 0.8410329222679138\n",
      "Validation: Epoch [3], Batch [231/938], Loss: 0.7486094236373901\n",
      "Validation: Epoch [3], Batch [232/938], Loss: 0.86997389793396\n",
      "Validation: Epoch [3], Batch [233/938], Loss: 0.7068684101104736\n",
      "Validation: Epoch [3], Batch [234/938], Loss: 0.6961585283279419\n",
      "Validation: Epoch [3], Batch [235/938], Loss: 0.8481521606445312\n",
      "Validation: Epoch [3], Batch [236/938], Loss: 0.6979143619537354\n",
      "Validation: Epoch [3], Batch [237/938], Loss: 0.8781872987747192\n",
      "Validation: Epoch [3], Batch [238/938], Loss: 0.690136194229126\n",
      "Validation: Epoch [3], Batch [239/938], Loss: 0.8741551637649536\n",
      "Validation: Epoch [3], Batch [240/938], Loss: 0.6311460733413696\n",
      "Validation: Epoch [3], Batch [241/938], Loss: 0.7536402940750122\n",
      "Validation: Epoch [3], Batch [242/938], Loss: 0.6320937275886536\n",
      "Validation: Epoch [3], Batch [243/938], Loss: 0.8738349080085754\n",
      "Validation: Epoch [3], Batch [244/938], Loss: 0.7408977746963501\n",
      "Validation: Epoch [3], Batch [245/938], Loss: 0.7189642190933228\n",
      "Validation: Epoch [3], Batch [246/938], Loss: 0.9905360341072083\n",
      "Validation: Epoch [3], Batch [247/938], Loss: 0.8911333084106445\n",
      "Validation: Epoch [3], Batch [248/938], Loss: 0.74357008934021\n",
      "Validation: Epoch [3], Batch [249/938], Loss: 0.6666738986968994\n",
      "Validation: Epoch [3], Batch [250/938], Loss: 0.9422807693481445\n",
      "Validation: Epoch [3], Batch [251/938], Loss: 0.99576735496521\n",
      "Validation: Epoch [3], Batch [252/938], Loss: 0.9207268953323364\n",
      "Validation: Epoch [3], Batch [253/938], Loss: 0.604202926158905\n",
      "Validation: Epoch [3], Batch [254/938], Loss: 0.7808549404144287\n",
      "Validation: Epoch [3], Batch [255/938], Loss: 1.0283794403076172\n",
      "Validation: Epoch [3], Batch [256/938], Loss: 0.8740590214729309\n",
      "Validation: Epoch [3], Batch [257/938], Loss: 0.8749469518661499\n",
      "Validation: Epoch [3], Batch [258/938], Loss: 0.7916315793991089\n",
      "Validation: Epoch [3], Batch [259/938], Loss: 0.7660195827484131\n",
      "Validation: Epoch [3], Batch [260/938], Loss: 0.6474116444587708\n",
      "Validation: Epoch [3], Batch [261/938], Loss: 0.8700618743896484\n",
      "Validation: Epoch [3], Batch [262/938], Loss: 0.6749120950698853\n",
      "Validation: Epoch [3], Batch [263/938], Loss: 0.7081778049468994\n",
      "Validation: Epoch [3], Batch [264/938], Loss: 0.908639669418335\n",
      "Validation: Epoch [3], Batch [265/938], Loss: 0.8239792585372925\n",
      "Validation: Epoch [3], Batch [266/938], Loss: 0.8242642879486084\n",
      "Validation: Epoch [3], Batch [267/938], Loss: 0.7408187985420227\n",
      "Validation: Epoch [3], Batch [268/938], Loss: 0.7316427826881409\n",
      "Validation: Epoch [3], Batch [269/938], Loss: 0.9447555541992188\n",
      "Validation: Epoch [3], Batch [270/938], Loss: 0.5734204649925232\n",
      "Validation: Epoch [3], Batch [271/938], Loss: 0.6851580142974854\n",
      "Validation: Epoch [3], Batch [272/938], Loss: 0.8913403749465942\n",
      "Validation: Epoch [3], Batch [273/938], Loss: 0.941123366355896\n",
      "Validation: Epoch [3], Batch [274/938], Loss: 0.8646533489227295\n",
      "Validation: Epoch [3], Batch [275/938], Loss: 0.7814644575119019\n",
      "Validation: Epoch [3], Batch [276/938], Loss: 0.7914135456085205\n",
      "Validation: Epoch [3], Batch [277/938], Loss: 0.6879045367240906\n",
      "Validation: Epoch [3], Batch [278/938], Loss: 0.948960542678833\n",
      "Validation: Epoch [3], Batch [279/938], Loss: 0.7187433242797852\n",
      "Validation: Epoch [3], Batch [280/938], Loss: 0.8683075904846191\n",
      "Validation: Epoch [3], Batch [281/938], Loss: 0.8752221465110779\n",
      "Validation: Epoch [3], Batch [282/938], Loss: 0.7198816537857056\n",
      "Validation: Epoch [3], Batch [283/938], Loss: 0.8540251851081848\n",
      "Validation: Epoch [3], Batch [284/938], Loss: 0.7818995714187622\n",
      "Validation: Epoch [3], Batch [285/938], Loss: 0.8459653258323669\n",
      "Validation: Epoch [3], Batch [286/938], Loss: 0.851984977722168\n",
      "Validation: Epoch [3], Batch [287/938], Loss: 0.6881456971168518\n",
      "Validation: Epoch [3], Batch [288/938], Loss: 0.8094627857208252\n",
      "Validation: Epoch [3], Batch [289/938], Loss: 0.8697164058685303\n",
      "Validation: Epoch [3], Batch [290/938], Loss: 0.775871992111206\n",
      "Validation: Epoch [3], Batch [291/938], Loss: 0.8569065928459167\n",
      "Validation: Epoch [3], Batch [292/938], Loss: 0.7411762475967407\n",
      "Validation: Epoch [3], Batch [293/938], Loss: 0.6809138059616089\n",
      "Validation: Epoch [3], Batch [294/938], Loss: 0.7317337393760681\n",
      "Validation: Epoch [3], Batch [295/938], Loss: 0.9143514633178711\n",
      "Validation: Epoch [3], Batch [296/938], Loss: 0.779789924621582\n",
      "Validation: Epoch [3], Batch [297/938], Loss: 0.8171319365501404\n",
      "Validation: Epoch [3], Batch [298/938], Loss: 0.7557295560836792\n",
      "Validation: Epoch [3], Batch [299/938], Loss: 0.6834850311279297\n",
      "Validation: Epoch [3], Batch [300/938], Loss: 0.5813643932342529\n",
      "Validation: Epoch [3], Batch [301/938], Loss: 0.8217700719833374\n",
      "Validation: Epoch [3], Batch [302/938], Loss: 0.8805727958679199\n",
      "Validation: Epoch [3], Batch [303/938], Loss: 0.7394812107086182\n",
      "Validation: Epoch [3], Batch [304/938], Loss: 0.9043023586273193\n",
      "Validation: Epoch [3], Batch [305/938], Loss: 0.729253888130188\n",
      "Validation: Epoch [3], Batch [306/938], Loss: 0.7585791349411011\n",
      "Validation: Epoch [3], Batch [307/938], Loss: 0.6880491971969604\n",
      "Validation: Epoch [3], Batch [308/938], Loss: 0.8889584541320801\n",
      "Validation: Epoch [3], Batch [309/938], Loss: 0.6572777032852173\n",
      "Validation: Epoch [3], Batch [310/938], Loss: 0.7925816774368286\n",
      "Validation: Epoch [3], Batch [311/938], Loss: 0.9161974191665649\n",
      "Validation: Epoch [3], Batch [312/938], Loss: 0.7516565322875977\n",
      "Validation: Epoch [3], Batch [313/938], Loss: 0.88698410987854\n",
      "Validation: Epoch [3], Batch [314/938], Loss: 0.7633225321769714\n",
      "Validation: Epoch [3], Batch [315/938], Loss: 0.7927545309066772\n",
      "Validation: Epoch [3], Batch [316/938], Loss: 0.8654196262359619\n",
      "Validation: Epoch [3], Batch [317/938], Loss: 0.7548779845237732\n",
      "Validation: Epoch [3], Batch [318/938], Loss: 0.7944271564483643\n",
      "Validation: Epoch [3], Batch [319/938], Loss: 0.834622323513031\n",
      "Validation: Epoch [3], Batch [320/938], Loss: 0.911896824836731\n",
      "Validation: Epoch [3], Batch [321/938], Loss: 0.934578537940979\n",
      "Validation: Epoch [3], Batch [322/938], Loss: 0.8300729393959045\n",
      "Validation: Epoch [3], Batch [323/938], Loss: 0.8322746753692627\n",
      "Validation: Epoch [3], Batch [324/938], Loss: 1.006589412689209\n",
      "Validation: Epoch [3], Batch [325/938], Loss: 0.6225577592849731\n",
      "Validation: Epoch [3], Batch [326/938], Loss: 0.9011491537094116\n",
      "Validation: Epoch [3], Batch [327/938], Loss: 1.1437671184539795\n",
      "Validation: Epoch [3], Batch [328/938], Loss: 0.8513553738594055\n",
      "Validation: Epoch [3], Batch [329/938], Loss: 0.7087623476982117\n",
      "Validation: Epoch [3], Batch [330/938], Loss: 0.9767873287200928\n",
      "Validation: Epoch [3], Batch [331/938], Loss: 0.7186028957366943\n",
      "Validation: Epoch [3], Batch [332/938], Loss: 0.6543006300926208\n",
      "Validation: Epoch [3], Batch [333/938], Loss: 0.6910085082054138\n",
      "Validation: Epoch [3], Batch [334/938], Loss: 0.8124551773071289\n",
      "Validation: Epoch [3], Batch [335/938], Loss: 0.7276638746261597\n",
      "Validation: Epoch [3], Batch [336/938], Loss: 0.803148090839386\n",
      "Validation: Epoch [3], Batch [337/938], Loss: 0.8371361494064331\n",
      "Validation: Epoch [3], Batch [338/938], Loss: 0.8194966912269592\n",
      "Validation: Epoch [3], Batch [339/938], Loss: 0.6304106116294861\n",
      "Validation: Epoch [3], Batch [340/938], Loss: 0.7045552730560303\n",
      "Validation: Epoch [3], Batch [341/938], Loss: 0.6344286799430847\n",
      "Validation: Epoch [3], Batch [342/938], Loss: 0.5999164581298828\n",
      "Validation: Epoch [3], Batch [343/938], Loss: 0.8564161658287048\n",
      "Validation: Epoch [3], Batch [344/938], Loss: 0.7333700656890869\n",
      "Validation: Epoch [3], Batch [345/938], Loss: 0.8922686576843262\n",
      "Validation: Epoch [3], Batch [346/938], Loss: 0.7011672854423523\n",
      "Validation: Epoch [3], Batch [347/938], Loss: 0.5733330845832825\n",
      "Validation: Epoch [3], Batch [348/938], Loss: 0.7204023599624634\n",
      "Validation: Epoch [3], Batch [349/938], Loss: 0.8884235620498657\n",
      "Validation: Epoch [3], Batch [350/938], Loss: 0.7093997597694397\n",
      "Validation: Epoch [3], Batch [351/938], Loss: 0.7381471395492554\n",
      "Validation: Epoch [3], Batch [352/938], Loss: 0.825842022895813\n",
      "Validation: Epoch [3], Batch [353/938], Loss: 0.5359991788864136\n",
      "Validation: Epoch [3], Batch [354/938], Loss: 0.7909100651741028\n",
      "Validation: Epoch [3], Batch [355/938], Loss: 1.2000788450241089\n",
      "Validation: Epoch [3], Batch [356/938], Loss: 0.746849000453949\n",
      "Validation: Epoch [3], Batch [357/938], Loss: 0.7223026752471924\n",
      "Validation: Epoch [3], Batch [358/938], Loss: 0.6946990489959717\n",
      "Validation: Epoch [3], Batch [359/938], Loss: 0.7496662139892578\n",
      "Validation: Epoch [3], Batch [360/938], Loss: 0.9396483302116394\n",
      "Validation: Epoch [3], Batch [361/938], Loss: 0.8258907794952393\n",
      "Validation: Epoch [3], Batch [362/938], Loss: 0.9027382135391235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [363/938], Loss: 0.630150556564331\n",
      "Validation: Epoch [3], Batch [364/938], Loss: 0.654792308807373\n",
      "Validation: Epoch [3], Batch [365/938], Loss: 1.0775654315948486\n",
      "Validation: Epoch [3], Batch [366/938], Loss: 0.8458746075630188\n",
      "Validation: Epoch [3], Batch [367/938], Loss: 0.8727619647979736\n",
      "Validation: Epoch [3], Batch [368/938], Loss: 0.7736643552780151\n",
      "Validation: Epoch [3], Batch [369/938], Loss: 0.7110985517501831\n",
      "Validation: Epoch [3], Batch [370/938], Loss: 0.668281078338623\n",
      "Validation: Epoch [3], Batch [371/938], Loss: 0.8986506462097168\n",
      "Validation: Epoch [3], Batch [372/938], Loss: 0.8839498162269592\n",
      "Validation: Epoch [3], Batch [373/938], Loss: 0.7851588726043701\n",
      "Validation: Epoch [3], Batch [374/938], Loss: 0.8976326584815979\n",
      "Validation: Epoch [3], Batch [375/938], Loss: 0.7496858835220337\n",
      "Validation: Epoch [3], Batch [376/938], Loss: 0.8600589036941528\n",
      "Validation: Epoch [3], Batch [377/938], Loss: 0.7931957840919495\n",
      "Validation: Epoch [3], Batch [378/938], Loss: 0.5968641042709351\n",
      "Validation: Epoch [3], Batch [379/938], Loss: 0.715023398399353\n",
      "Validation: Epoch [3], Batch [380/938], Loss: 0.750543475151062\n",
      "Validation: Epoch [3], Batch [381/938], Loss: 0.8383291363716125\n",
      "Validation: Epoch [3], Batch [382/938], Loss: 0.7317257523536682\n",
      "Validation: Epoch [3], Batch [383/938], Loss: 0.6786864995956421\n",
      "Validation: Epoch [3], Batch [384/938], Loss: 0.7822102904319763\n",
      "Validation: Epoch [3], Batch [385/938], Loss: 0.8944073915481567\n",
      "Validation: Epoch [3], Batch [386/938], Loss: 0.8369138240814209\n",
      "Validation: Epoch [3], Batch [387/938], Loss: 0.8572478890419006\n",
      "Validation: Epoch [3], Batch [388/938], Loss: 0.7591037154197693\n",
      "Validation: Epoch [3], Batch [389/938], Loss: 0.7207436561584473\n",
      "Validation: Epoch [3], Batch [390/938], Loss: 0.92425137758255\n",
      "Validation: Epoch [3], Batch [391/938], Loss: 0.8826403617858887\n",
      "Validation: Epoch [3], Batch [392/938], Loss: 0.776990532875061\n",
      "Validation: Epoch [3], Batch [393/938], Loss: 0.8777456879615784\n",
      "Validation: Epoch [3], Batch [394/938], Loss: 0.7021242380142212\n",
      "Validation: Epoch [3], Batch [395/938], Loss: 0.7499904632568359\n",
      "Validation: Epoch [3], Batch [396/938], Loss: 0.8792728185653687\n",
      "Validation: Epoch [3], Batch [397/938], Loss: 0.8483918309211731\n",
      "Validation: Epoch [3], Batch [398/938], Loss: 0.784250020980835\n",
      "Validation: Epoch [3], Batch [399/938], Loss: 0.6712751388549805\n",
      "Validation: Epoch [3], Batch [400/938], Loss: 0.769584059715271\n",
      "Validation: Epoch [3], Batch [401/938], Loss: 0.6752128601074219\n",
      "Validation: Epoch [3], Batch [402/938], Loss: 0.6655089855194092\n",
      "Validation: Epoch [3], Batch [403/938], Loss: 0.6936351656913757\n",
      "Validation: Epoch [3], Batch [404/938], Loss: 1.090429425239563\n",
      "Validation: Epoch [3], Batch [405/938], Loss: 0.707962155342102\n",
      "Validation: Epoch [3], Batch [406/938], Loss: 0.7607235908508301\n",
      "Validation: Epoch [3], Batch [407/938], Loss: 0.7334392070770264\n",
      "Validation: Epoch [3], Batch [408/938], Loss: 0.699635922908783\n",
      "Validation: Epoch [3], Batch [409/938], Loss: 0.8318856358528137\n",
      "Validation: Epoch [3], Batch [410/938], Loss: 0.8078453540802002\n",
      "Validation: Epoch [3], Batch [411/938], Loss: 0.9028520584106445\n",
      "Validation: Epoch [3], Batch [412/938], Loss: 0.7746655941009521\n",
      "Validation: Epoch [3], Batch [413/938], Loss: 0.9461210370063782\n",
      "Validation: Epoch [3], Batch [414/938], Loss: 1.0180294513702393\n",
      "Validation: Epoch [3], Batch [415/938], Loss: 1.0105358362197876\n",
      "Validation: Epoch [3], Batch [416/938], Loss: 0.7856663465499878\n",
      "Validation: Epoch [3], Batch [417/938], Loss: 0.7876558303833008\n",
      "Validation: Epoch [3], Batch [418/938], Loss: 0.8138312101364136\n",
      "Validation: Epoch [3], Batch [419/938], Loss: 0.8373132944107056\n",
      "Validation: Epoch [3], Batch [420/938], Loss: 0.7848178148269653\n",
      "Validation: Epoch [3], Batch [421/938], Loss: 0.6889880895614624\n",
      "Validation: Epoch [3], Batch [422/938], Loss: 0.7769301533699036\n",
      "Validation: Epoch [3], Batch [423/938], Loss: 0.6778273582458496\n",
      "Validation: Epoch [3], Batch [424/938], Loss: 0.7050127983093262\n",
      "Validation: Epoch [3], Batch [425/938], Loss: 0.7597123384475708\n",
      "Validation: Epoch [3], Batch [426/938], Loss: 0.900644063949585\n",
      "Validation: Epoch [3], Batch [427/938], Loss: 0.7202968001365662\n",
      "Validation: Epoch [3], Batch [428/938], Loss: 0.6819528341293335\n",
      "Validation: Epoch [3], Batch [429/938], Loss: 0.8718602657318115\n",
      "Validation: Epoch [3], Batch [430/938], Loss: 0.5514861345291138\n",
      "Validation: Epoch [3], Batch [431/938], Loss: 0.8437517881393433\n",
      "Validation: Epoch [3], Batch [432/938], Loss: 0.8298184871673584\n",
      "Validation: Epoch [3], Batch [433/938], Loss: 0.8339119553565979\n",
      "Validation: Epoch [3], Batch [434/938], Loss: 0.610619843006134\n",
      "Validation: Epoch [3], Batch [435/938], Loss: 1.0788501501083374\n",
      "Validation: Epoch [3], Batch [436/938], Loss: 0.9842473268508911\n",
      "Validation: Epoch [3], Batch [437/938], Loss: 0.8450238704681396\n",
      "Validation: Epoch [3], Batch [438/938], Loss: 0.6615135669708252\n",
      "Validation: Epoch [3], Batch [439/938], Loss: 0.9074832201004028\n",
      "Validation: Epoch [3], Batch [440/938], Loss: 0.7382220029830933\n",
      "Validation: Epoch [3], Batch [441/938], Loss: 0.8397715091705322\n",
      "Validation: Epoch [3], Batch [442/938], Loss: 0.8055793046951294\n",
      "Validation: Epoch [3], Batch [443/938], Loss: 0.7437700033187866\n",
      "Validation: Epoch [3], Batch [444/938], Loss: 0.8895514011383057\n",
      "Validation: Epoch [3], Batch [445/938], Loss: 0.5850448608398438\n",
      "Validation: Epoch [3], Batch [446/938], Loss: 0.8094527721405029\n",
      "Validation: Epoch [3], Batch [447/938], Loss: 0.6714571118354797\n",
      "Validation: Epoch [3], Batch [448/938], Loss: 0.744358241558075\n",
      "Validation: Epoch [3], Batch [449/938], Loss: 0.7529439926147461\n",
      "Validation: Epoch [3], Batch [450/938], Loss: 0.7731112241744995\n",
      "Validation: Epoch [3], Batch [451/938], Loss: 0.891711950302124\n",
      "Validation: Epoch [3], Batch [452/938], Loss: 0.7187792062759399\n",
      "Validation: Epoch [3], Batch [453/938], Loss: 0.9489854574203491\n",
      "Validation: Epoch [3], Batch [454/938], Loss: 0.7790396809577942\n",
      "Validation: Epoch [3], Batch [455/938], Loss: 0.750706672668457\n",
      "Validation: Epoch [3], Batch [456/938], Loss: 0.8551587462425232\n",
      "Validation: Epoch [3], Batch [457/938], Loss: 0.7542870044708252\n",
      "Validation: Epoch [3], Batch [458/938], Loss: 0.689480721950531\n",
      "Validation: Epoch [3], Batch [459/938], Loss: 0.8003866672515869\n",
      "Validation: Epoch [3], Batch [460/938], Loss: 0.9075101613998413\n",
      "Validation: Epoch [3], Batch [461/938], Loss: 0.8259928226470947\n",
      "Validation: Epoch [3], Batch [462/938], Loss: 0.7775746583938599\n",
      "Validation: Epoch [3], Batch [463/938], Loss: 0.7939724326133728\n",
      "Validation: Epoch [3], Batch [464/938], Loss: 0.8235787153244019\n",
      "Validation: Epoch [3], Batch [465/938], Loss: 0.8506619334220886\n",
      "Validation: Epoch [3], Batch [466/938], Loss: 0.802336573600769\n",
      "Validation: Epoch [3], Batch [467/938], Loss: 0.9009808301925659\n",
      "Validation: Epoch [3], Batch [468/938], Loss: 0.6886714696884155\n",
      "Validation: Epoch [3], Batch [469/938], Loss: 0.9659007787704468\n",
      "Validation: Epoch [3], Batch [470/938], Loss: 0.642101526260376\n",
      "Validation: Epoch [3], Batch [471/938], Loss: 0.832466185092926\n",
      "Validation: Epoch [3], Batch [472/938], Loss: 0.978564977645874\n",
      "Validation: Epoch [3], Batch [473/938], Loss: 0.8013938069343567\n",
      "Validation: Epoch [3], Batch [474/938], Loss: 0.8598144054412842\n",
      "Validation: Epoch [3], Batch [475/938], Loss: 0.7801573276519775\n",
      "Validation: Epoch [3], Batch [476/938], Loss: 0.711370587348938\n",
      "Validation: Epoch [3], Batch [477/938], Loss: 0.8782995343208313\n",
      "Validation: Epoch [3], Batch [478/938], Loss: 0.7327321767807007\n",
      "Validation: Epoch [3], Batch [479/938], Loss: 0.71794593334198\n",
      "Validation: Epoch [3], Batch [480/938], Loss: 0.7438980340957642\n",
      "Validation: Epoch [3], Batch [481/938], Loss: 0.9958513975143433\n",
      "Validation: Epoch [3], Batch [482/938], Loss: 0.6505922079086304\n",
      "Validation: Epoch [3], Batch [483/938], Loss: 1.224025011062622\n",
      "Validation: Epoch [3], Batch [484/938], Loss: 0.7833163738250732\n",
      "Validation: Epoch [3], Batch [485/938], Loss: 0.9088252186775208\n",
      "Validation: Epoch [3], Batch [486/938], Loss: 0.759324312210083\n",
      "Validation: Epoch [3], Batch [487/938], Loss: 0.9396465420722961\n",
      "Validation: Epoch [3], Batch [488/938], Loss: 0.6519672870635986\n",
      "Validation: Epoch [3], Batch [489/938], Loss: 0.8157380819320679\n",
      "Validation: Epoch [3], Batch [490/938], Loss: 0.6360353231430054\n",
      "Validation: Epoch [3], Batch [491/938], Loss: 0.9754401445388794\n",
      "Validation: Epoch [3], Batch [492/938], Loss: 0.8454619646072388\n",
      "Validation: Epoch [3], Batch [493/938], Loss: 0.7292784452438354\n",
      "Validation: Epoch [3], Batch [494/938], Loss: 0.6966260671615601\n",
      "Validation: Epoch [3], Batch [495/938], Loss: 0.6590712666511536\n",
      "Validation: Epoch [3], Batch [496/938], Loss: 0.7834397554397583\n",
      "Validation: Epoch [3], Batch [497/938], Loss: 0.7911161184310913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [498/938], Loss: 0.8524864912033081\n",
      "Validation: Epoch [3], Batch [499/938], Loss: 0.9614718556404114\n",
      "Validation: Epoch [3], Batch [500/938], Loss: 0.910416841506958\n",
      "Validation: Epoch [3], Batch [501/938], Loss: 0.7612692713737488\n",
      "Validation: Epoch [3], Batch [502/938], Loss: 0.8036561012268066\n",
      "Validation: Epoch [3], Batch [503/938], Loss: 0.9388946294784546\n",
      "Validation: Epoch [3], Batch [504/938], Loss: 0.8562277555465698\n",
      "Validation: Epoch [3], Batch [505/938], Loss: 0.8232372999191284\n",
      "Validation: Epoch [3], Batch [506/938], Loss: 0.6960508823394775\n",
      "Validation: Epoch [3], Batch [507/938], Loss: 0.6346197128295898\n",
      "Validation: Epoch [3], Batch [508/938], Loss: 0.6022680997848511\n",
      "Validation: Epoch [3], Batch [509/938], Loss: 0.9257932305335999\n",
      "Validation: Epoch [3], Batch [510/938], Loss: 1.1601003408432007\n",
      "Validation: Epoch [3], Batch [511/938], Loss: 0.9266096353530884\n",
      "Validation: Epoch [3], Batch [512/938], Loss: 0.8362982273101807\n",
      "Validation: Epoch [3], Batch [513/938], Loss: 0.7442983984947205\n",
      "Validation: Epoch [3], Batch [514/938], Loss: 0.7092100977897644\n",
      "Validation: Epoch [3], Batch [515/938], Loss: 0.7209838628768921\n",
      "Validation: Epoch [3], Batch [516/938], Loss: 0.7794714570045471\n",
      "Validation: Epoch [3], Batch [517/938], Loss: 0.8129910230636597\n",
      "Validation: Epoch [3], Batch [518/938], Loss: 0.6897925734519958\n",
      "Validation: Epoch [3], Batch [519/938], Loss: 0.7126846313476562\n",
      "Validation: Epoch [3], Batch [520/938], Loss: 1.0075427293777466\n",
      "Validation: Epoch [3], Batch [521/938], Loss: 0.6071525812149048\n",
      "Validation: Epoch [3], Batch [522/938], Loss: 0.7000634670257568\n",
      "Validation: Epoch [3], Batch [523/938], Loss: 0.963385820388794\n",
      "Validation: Epoch [3], Batch [524/938], Loss: 0.736688494682312\n",
      "Validation: Epoch [3], Batch [525/938], Loss: 0.7063037157058716\n",
      "Validation: Epoch [3], Batch [526/938], Loss: 0.7156659960746765\n",
      "Validation: Epoch [3], Batch [527/938], Loss: 0.858383297920227\n",
      "Validation: Epoch [3], Batch [528/938], Loss: 0.7750559449195862\n",
      "Validation: Epoch [3], Batch [529/938], Loss: 0.6689520478248596\n",
      "Validation: Epoch [3], Batch [530/938], Loss: 0.6466418504714966\n",
      "Validation: Epoch [3], Batch [531/938], Loss: 0.7941955327987671\n",
      "Validation: Epoch [3], Batch [532/938], Loss: 0.7574660778045654\n",
      "Validation: Epoch [3], Batch [533/938], Loss: 1.046301245689392\n",
      "Validation: Epoch [3], Batch [534/938], Loss: 0.8127762079238892\n",
      "Validation: Epoch [3], Batch [535/938], Loss: 0.7191966772079468\n",
      "Validation: Epoch [3], Batch [536/938], Loss: 0.9337841868400574\n",
      "Validation: Epoch [3], Batch [537/938], Loss: 0.8081207275390625\n",
      "Validation: Epoch [3], Batch [538/938], Loss: 0.6610226631164551\n",
      "Validation: Epoch [3], Batch [539/938], Loss: 0.6979819536209106\n",
      "Validation: Epoch [3], Batch [540/938], Loss: 0.8556369543075562\n",
      "Validation: Epoch [3], Batch [541/938], Loss: 0.7510067820549011\n",
      "Validation: Epoch [3], Batch [542/938], Loss: 0.8028704524040222\n",
      "Validation: Epoch [3], Batch [543/938], Loss: 0.880037784576416\n",
      "Validation: Epoch [3], Batch [544/938], Loss: 0.8795466423034668\n",
      "Validation: Epoch [3], Batch [545/938], Loss: 0.7613028287887573\n",
      "Validation: Epoch [3], Batch [546/938], Loss: 0.7859408855438232\n",
      "Validation: Epoch [3], Batch [547/938], Loss: 0.7656190991401672\n",
      "Validation: Epoch [3], Batch [548/938], Loss: 0.8349936008453369\n",
      "Validation: Epoch [3], Batch [549/938], Loss: 0.7121745944023132\n",
      "Validation: Epoch [3], Batch [550/938], Loss: 0.734678328037262\n",
      "Validation: Epoch [3], Batch [551/938], Loss: 0.694595456123352\n",
      "Validation: Epoch [3], Batch [552/938], Loss: 0.6845036745071411\n",
      "Validation: Epoch [3], Batch [553/938], Loss: 0.8084424734115601\n",
      "Validation: Epoch [3], Batch [554/938], Loss: 0.7081373929977417\n",
      "Validation: Epoch [3], Batch [555/938], Loss: 0.7215369939804077\n",
      "Validation: Epoch [3], Batch [556/938], Loss: 0.7857884168624878\n",
      "Validation: Epoch [3], Batch [557/938], Loss: 0.8436434268951416\n",
      "Validation: Epoch [3], Batch [558/938], Loss: 0.4877646565437317\n",
      "Validation: Epoch [3], Batch [559/938], Loss: 0.7345348596572876\n",
      "Validation: Epoch [3], Batch [560/938], Loss: 0.6237598657608032\n",
      "Validation: Epoch [3], Batch [561/938], Loss: 0.6529961824417114\n",
      "Validation: Epoch [3], Batch [562/938], Loss: 0.7647477388381958\n",
      "Validation: Epoch [3], Batch [563/938], Loss: 0.88077712059021\n",
      "Validation: Epoch [3], Batch [564/938], Loss: 0.7186151742935181\n",
      "Validation: Epoch [3], Batch [565/938], Loss: 0.8282442092895508\n",
      "Validation: Epoch [3], Batch [566/938], Loss: 0.7001171112060547\n",
      "Validation: Epoch [3], Batch [567/938], Loss: 0.7851328253746033\n",
      "Validation: Epoch [3], Batch [568/938], Loss: 0.8424028158187866\n",
      "Validation: Epoch [3], Batch [569/938], Loss: 0.6239573955535889\n",
      "Validation: Epoch [3], Batch [570/938], Loss: 0.7320721745491028\n",
      "Validation: Epoch [3], Batch [571/938], Loss: 0.8351083993911743\n",
      "Validation: Epoch [3], Batch [572/938], Loss: 0.7436193227767944\n",
      "Validation: Epoch [3], Batch [573/938], Loss: 0.8795522451400757\n",
      "Validation: Epoch [3], Batch [574/938], Loss: 0.8293363451957703\n",
      "Validation: Epoch [3], Batch [575/938], Loss: 0.7079055309295654\n",
      "Validation: Epoch [3], Batch [576/938], Loss: 0.8845175504684448\n",
      "Validation: Epoch [3], Batch [577/938], Loss: 0.8801631331443787\n",
      "Validation: Epoch [3], Batch [578/938], Loss: 0.8367552161216736\n",
      "Validation: Epoch [3], Batch [579/938], Loss: 0.7284566164016724\n",
      "Validation: Epoch [3], Batch [580/938], Loss: 0.8183612823486328\n",
      "Validation: Epoch [3], Batch [581/938], Loss: 0.8137669563293457\n",
      "Validation: Epoch [3], Batch [582/938], Loss: 0.7822319269180298\n",
      "Validation: Epoch [3], Batch [583/938], Loss: 0.7983537912368774\n",
      "Validation: Epoch [3], Batch [584/938], Loss: 0.6623879075050354\n",
      "Validation: Epoch [3], Batch [585/938], Loss: 0.7560518980026245\n",
      "Validation: Epoch [3], Batch [586/938], Loss: 0.7855383157730103\n",
      "Validation: Epoch [3], Batch [587/938], Loss: 0.7276000380516052\n",
      "Validation: Epoch [3], Batch [588/938], Loss: 0.593940258026123\n",
      "Validation: Epoch [3], Batch [589/938], Loss: 0.7157491445541382\n",
      "Validation: Epoch [3], Batch [590/938], Loss: 0.7644474506378174\n",
      "Validation: Epoch [3], Batch [591/938], Loss: 0.746281623840332\n",
      "Validation: Epoch [3], Batch [592/938], Loss: 0.9089435338973999\n",
      "Validation: Epoch [3], Batch [593/938], Loss: 0.78867506980896\n",
      "Validation: Epoch [3], Batch [594/938], Loss: 0.679108738899231\n",
      "Validation: Epoch [3], Batch [595/938], Loss: 0.8139356970787048\n",
      "Validation: Epoch [3], Batch [596/938], Loss: 0.6906275749206543\n",
      "Validation: Epoch [3], Batch [597/938], Loss: 0.7485777139663696\n",
      "Validation: Epoch [3], Batch [598/938], Loss: 0.8121150732040405\n",
      "Validation: Epoch [3], Batch [599/938], Loss: 0.8427178859710693\n",
      "Validation: Epoch [3], Batch [600/938], Loss: 0.7492086887359619\n",
      "Validation: Epoch [3], Batch [601/938], Loss: 0.7995867729187012\n",
      "Validation: Epoch [3], Batch [602/938], Loss: 0.7193392515182495\n",
      "Validation: Epoch [3], Batch [603/938], Loss: 0.6727800369262695\n",
      "Validation: Epoch [3], Batch [604/938], Loss: 0.6107013821601868\n",
      "Validation: Epoch [3], Batch [605/938], Loss: 0.7251383066177368\n",
      "Validation: Epoch [3], Batch [606/938], Loss: 0.7997276186943054\n",
      "Validation: Epoch [3], Batch [607/938], Loss: 1.0046969652175903\n",
      "Validation: Epoch [3], Batch [608/938], Loss: 0.8964834213256836\n",
      "Validation: Epoch [3], Batch [609/938], Loss: 0.8613044023513794\n",
      "Validation: Epoch [3], Batch [610/938], Loss: 0.7274723649024963\n",
      "Validation: Epoch [3], Batch [611/938], Loss: 0.8170607089996338\n",
      "Validation: Epoch [3], Batch [612/938], Loss: 0.760401725769043\n",
      "Validation: Epoch [3], Batch [613/938], Loss: 0.6955451965332031\n",
      "Validation: Epoch [3], Batch [614/938], Loss: 0.8179469108581543\n",
      "Validation: Epoch [3], Batch [615/938], Loss: 0.9162907600402832\n",
      "Validation: Epoch [3], Batch [616/938], Loss: 0.6662931442260742\n",
      "Validation: Epoch [3], Batch [617/938], Loss: 0.8895049691200256\n",
      "Validation: Epoch [3], Batch [618/938], Loss: 0.9298306703567505\n",
      "Validation: Epoch [3], Batch [619/938], Loss: 0.6891260147094727\n",
      "Validation: Epoch [3], Batch [620/938], Loss: 0.6669342517852783\n",
      "Validation: Epoch [3], Batch [621/938], Loss: 0.7068518996238708\n",
      "Validation: Epoch [3], Batch [622/938], Loss: 0.6906849145889282\n",
      "Validation: Epoch [3], Batch [623/938], Loss: 0.9986939430236816\n",
      "Validation: Epoch [3], Batch [624/938], Loss: 0.7746771574020386\n",
      "Validation: Epoch [3], Batch [625/938], Loss: 0.5822813510894775\n",
      "Validation: Epoch [3], Batch [626/938], Loss: 0.7917590737342834\n",
      "Validation: Epoch [3], Batch [627/938], Loss: 0.7453523874282837\n",
      "Validation: Epoch [3], Batch [628/938], Loss: 0.7544398307800293\n",
      "Validation: Epoch [3], Batch [629/938], Loss: 0.8824093341827393\n",
      "Validation: Epoch [3], Batch [630/938], Loss: 0.820042610168457\n",
      "Validation: Epoch [3], Batch [631/938], Loss: 0.8036043643951416\n",
      "Validation: Epoch [3], Batch [632/938], Loss: 0.8110203742980957\n",
      "Validation: Epoch [3], Batch [633/938], Loss: 0.785402774810791\n",
      "Validation: Epoch [3], Batch [634/938], Loss: 0.8211880922317505\n",
      "Validation: Epoch [3], Batch [635/938], Loss: 0.8559908866882324\n",
      "Validation: Epoch [3], Batch [636/938], Loss: 0.7868798971176147\n",
      "Validation: Epoch [3], Batch [637/938], Loss: 0.7146111130714417\n",
      "Validation: Epoch [3], Batch [638/938], Loss: 0.6932644248008728\n",
      "Validation: Epoch [3], Batch [639/938], Loss: 0.8594546914100647\n",
      "Validation: Epoch [3], Batch [640/938], Loss: 0.7287600040435791\n",
      "Validation: Epoch [3], Batch [641/938], Loss: 0.8969902992248535\n",
      "Validation: Epoch [3], Batch [642/938], Loss: 0.8247389197349548\n",
      "Validation: Epoch [3], Batch [643/938], Loss: 0.6760231852531433\n",
      "Validation: Epoch [3], Batch [644/938], Loss: 0.7456585764884949\n",
      "Validation: Epoch [3], Batch [645/938], Loss: 0.8794451951980591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [646/938], Loss: 0.6636402010917664\n",
      "Validation: Epoch [3], Batch [647/938], Loss: 0.8771872520446777\n",
      "Validation: Epoch [3], Batch [648/938], Loss: 0.9282597899436951\n",
      "Validation: Epoch [3], Batch [649/938], Loss: 0.960652232170105\n",
      "Validation: Epoch [3], Batch [650/938], Loss: 0.7951207160949707\n",
      "Validation: Epoch [3], Batch [651/938], Loss: 0.8124572038650513\n",
      "Validation: Epoch [3], Batch [652/938], Loss: 0.6777830719947815\n",
      "Validation: Epoch [3], Batch [653/938], Loss: 0.8841211199760437\n",
      "Validation: Epoch [3], Batch [654/938], Loss: 0.7876834273338318\n",
      "Validation: Epoch [3], Batch [655/938], Loss: 0.5639513731002808\n",
      "Validation: Epoch [3], Batch [656/938], Loss: 0.8313210010528564\n",
      "Validation: Epoch [3], Batch [657/938], Loss: 0.6978656649589539\n",
      "Validation: Epoch [3], Batch [658/938], Loss: 0.7673786878585815\n",
      "Validation: Epoch [3], Batch [659/938], Loss: 1.0975921154022217\n",
      "Validation: Epoch [3], Batch [660/938], Loss: 0.8871206641197205\n",
      "Validation: Epoch [3], Batch [661/938], Loss: 0.6923588514328003\n",
      "Validation: Epoch [3], Batch [662/938], Loss: 0.6821737885475159\n",
      "Validation: Epoch [3], Batch [663/938], Loss: 0.6900972127914429\n",
      "Validation: Epoch [3], Batch [664/938], Loss: 0.8199575543403625\n",
      "Validation: Epoch [3], Batch [665/938], Loss: 1.0565648078918457\n",
      "Validation: Epoch [3], Batch [666/938], Loss: 0.8553652763366699\n",
      "Validation: Epoch [3], Batch [667/938], Loss: 0.8236423134803772\n",
      "Validation: Epoch [3], Batch [668/938], Loss: 0.5567598938941956\n",
      "Validation: Epoch [3], Batch [669/938], Loss: 0.6817129850387573\n",
      "Validation: Epoch [3], Batch [670/938], Loss: 0.842154860496521\n",
      "Validation: Epoch [3], Batch [671/938], Loss: 0.7822868824005127\n",
      "Validation: Epoch [3], Batch [672/938], Loss: 0.742382824420929\n",
      "Validation: Epoch [3], Batch [673/938], Loss: 0.5963097810745239\n",
      "Validation: Epoch [3], Batch [674/938], Loss: 0.7917073965072632\n",
      "Validation: Epoch [3], Batch [675/938], Loss: 0.8525968790054321\n",
      "Validation: Epoch [3], Batch [676/938], Loss: 0.9384927749633789\n",
      "Validation: Epoch [3], Batch [677/938], Loss: 0.9920918345451355\n",
      "Validation: Epoch [3], Batch [678/938], Loss: 0.5554104447364807\n",
      "Validation: Epoch [3], Batch [679/938], Loss: 0.8259565830230713\n",
      "Validation: Epoch [3], Batch [680/938], Loss: 0.8838712573051453\n",
      "Validation: Epoch [3], Batch [681/938], Loss: 0.7200528979301453\n",
      "Validation: Epoch [3], Batch [682/938], Loss: 0.7049254179000854\n",
      "Validation: Epoch [3], Batch [683/938], Loss: 0.7374936938285828\n",
      "Validation: Epoch [3], Batch [684/938], Loss: 0.7356358170509338\n",
      "Validation: Epoch [3], Batch [685/938], Loss: 0.6152305603027344\n",
      "Validation: Epoch [3], Batch [686/938], Loss: 0.8659142851829529\n",
      "Validation: Epoch [3], Batch [687/938], Loss: 0.7251218557357788\n",
      "Validation: Epoch [3], Batch [688/938], Loss: 0.725483775138855\n",
      "Validation: Epoch [3], Batch [689/938], Loss: 0.7746219635009766\n",
      "Validation: Epoch [3], Batch [690/938], Loss: 0.6310536861419678\n",
      "Validation: Epoch [3], Batch [691/938], Loss: 0.7822213172912598\n",
      "Validation: Epoch [3], Batch [692/938], Loss: 0.8482617139816284\n",
      "Validation: Epoch [3], Batch [693/938], Loss: 0.8420006036758423\n",
      "Validation: Epoch [3], Batch [694/938], Loss: 0.8481450080871582\n",
      "Validation: Epoch [3], Batch [695/938], Loss: 0.8204537630081177\n",
      "Validation: Epoch [3], Batch [696/938], Loss: 0.804915189743042\n",
      "Validation: Epoch [3], Batch [697/938], Loss: 0.6806867718696594\n",
      "Validation: Epoch [3], Batch [698/938], Loss: 0.8518079519271851\n",
      "Validation: Epoch [3], Batch [699/938], Loss: 0.6279053688049316\n",
      "Validation: Epoch [3], Batch [700/938], Loss: 0.6717920899391174\n",
      "Validation: Epoch [3], Batch [701/938], Loss: 0.9362781047821045\n",
      "Validation: Epoch [3], Batch [702/938], Loss: 0.9999030828475952\n",
      "Validation: Epoch [3], Batch [703/938], Loss: 0.996688723564148\n",
      "Validation: Epoch [3], Batch [704/938], Loss: 0.6155329942703247\n",
      "Validation: Epoch [3], Batch [705/938], Loss: 0.7025555372238159\n",
      "Validation: Epoch [3], Batch [706/938], Loss: 0.8741190433502197\n",
      "Validation: Epoch [3], Batch [707/938], Loss: 0.7909559607505798\n",
      "Validation: Epoch [3], Batch [708/938], Loss: 0.9618546366691589\n",
      "Validation: Epoch [3], Batch [709/938], Loss: 0.8702534437179565\n",
      "Validation: Epoch [3], Batch [710/938], Loss: 0.8499970436096191\n",
      "Validation: Epoch [3], Batch [711/938], Loss: 0.8520979285240173\n",
      "Validation: Epoch [3], Batch [712/938], Loss: 0.9290650486946106\n",
      "Validation: Epoch [3], Batch [713/938], Loss: 0.8388258218765259\n",
      "Validation: Epoch [3], Batch [714/938], Loss: 0.826615035533905\n",
      "Validation: Epoch [3], Batch [715/938], Loss: 0.55797278881073\n",
      "Validation: Epoch [3], Batch [716/938], Loss: 0.8879876136779785\n",
      "Validation: Epoch [3], Batch [717/938], Loss: 0.8048641681671143\n",
      "Validation: Epoch [3], Batch [718/938], Loss: 0.6942861676216125\n",
      "Validation: Epoch [3], Batch [719/938], Loss: 0.7944962382316589\n",
      "Validation: Epoch [3], Batch [720/938], Loss: 0.709337055683136\n",
      "Validation: Epoch [3], Batch [721/938], Loss: 0.8831537365913391\n",
      "Validation: Epoch [3], Batch [722/938], Loss: 0.7285096645355225\n",
      "Validation: Epoch [3], Batch [723/938], Loss: 0.9038398861885071\n",
      "Validation: Epoch [3], Batch [724/938], Loss: 0.7388018369674683\n",
      "Validation: Epoch [3], Batch [725/938], Loss: 0.6910442113876343\n",
      "Validation: Epoch [3], Batch [726/938], Loss: 0.7709331512451172\n",
      "Validation: Epoch [3], Batch [727/938], Loss: 0.8600049018859863\n",
      "Validation: Epoch [3], Batch [728/938], Loss: 0.7811089754104614\n",
      "Validation: Epoch [3], Batch [729/938], Loss: 0.5640017986297607\n",
      "Validation: Epoch [3], Batch [730/938], Loss: 0.7900649309158325\n",
      "Validation: Epoch [3], Batch [731/938], Loss: 0.567481279373169\n",
      "Validation: Epoch [3], Batch [732/938], Loss: 0.7297449111938477\n",
      "Validation: Epoch [3], Batch [733/938], Loss: 0.938103199005127\n",
      "Validation: Epoch [3], Batch [734/938], Loss: 0.8046402931213379\n",
      "Validation: Epoch [3], Batch [735/938], Loss: 0.6912956237792969\n",
      "Validation: Epoch [3], Batch [736/938], Loss: 0.9564332962036133\n",
      "Validation: Epoch [3], Batch [737/938], Loss: 0.7603349685668945\n",
      "Validation: Epoch [3], Batch [738/938], Loss: 0.6450321674346924\n",
      "Validation: Epoch [3], Batch [739/938], Loss: 0.7985415458679199\n",
      "Validation: Epoch [3], Batch [740/938], Loss: 0.9313391447067261\n",
      "Validation: Epoch [3], Batch [741/938], Loss: 0.8314110040664673\n",
      "Validation: Epoch [3], Batch [742/938], Loss: 0.6924654245376587\n",
      "Validation: Epoch [3], Batch [743/938], Loss: 0.7508577108383179\n",
      "Validation: Epoch [3], Batch [744/938], Loss: 1.0636022090911865\n",
      "Validation: Epoch [3], Batch [745/938], Loss: 0.8218398094177246\n",
      "Validation: Epoch [3], Batch [746/938], Loss: 0.7423038482666016\n",
      "Validation: Epoch [3], Batch [747/938], Loss: 0.7443974018096924\n",
      "Validation: Epoch [3], Batch [748/938], Loss: 0.8961478471755981\n",
      "Validation: Epoch [3], Batch [749/938], Loss: 0.6757944226264954\n",
      "Validation: Epoch [3], Batch [750/938], Loss: 0.6009711623191833\n",
      "Validation: Epoch [3], Batch [751/938], Loss: 0.7093592882156372\n",
      "Validation: Epoch [3], Batch [752/938], Loss: 0.75379478931427\n",
      "Validation: Epoch [3], Batch [753/938], Loss: 0.7624804973602295\n",
      "Validation: Epoch [3], Batch [754/938], Loss: 0.7391055226325989\n",
      "Validation: Epoch [3], Batch [755/938], Loss: 0.8014545440673828\n",
      "Validation: Epoch [3], Batch [756/938], Loss: 0.7597267031669617\n",
      "Validation: Epoch [3], Batch [757/938], Loss: 0.7944582104682922\n",
      "Validation: Epoch [3], Batch [758/938], Loss: 0.8433577418327332\n",
      "Validation: Epoch [3], Batch [759/938], Loss: 1.0422008037567139\n",
      "Validation: Epoch [3], Batch [760/938], Loss: 0.8433499932289124\n",
      "Validation: Epoch [3], Batch [761/938], Loss: 0.8866284489631653\n",
      "Validation: Epoch [3], Batch [762/938], Loss: 0.5798728466033936\n",
      "Validation: Epoch [3], Batch [763/938], Loss: 0.8745048642158508\n",
      "Validation: Epoch [3], Batch [764/938], Loss: 0.6809006929397583\n",
      "Validation: Epoch [3], Batch [765/938], Loss: 0.9836896061897278\n",
      "Validation: Epoch [3], Batch [766/938], Loss: 0.8012863993644714\n",
      "Validation: Epoch [3], Batch [767/938], Loss: 0.9489164352416992\n",
      "Validation: Epoch [3], Batch [768/938], Loss: 0.8604008555412292\n",
      "Validation: Epoch [3], Batch [769/938], Loss: 0.7750195264816284\n",
      "Validation: Epoch [3], Batch [770/938], Loss: 0.8088138699531555\n",
      "Validation: Epoch [3], Batch [771/938], Loss: 0.9378584623336792\n",
      "Validation: Epoch [3], Batch [772/938], Loss: 0.7984316349029541\n",
      "Validation: Epoch [3], Batch [773/938], Loss: 0.5212072134017944\n",
      "Validation: Epoch [3], Batch [774/938], Loss: 0.7608866691589355\n",
      "Validation: Epoch [3], Batch [775/938], Loss: 0.6743773221969604\n",
      "Validation: Epoch [3], Batch [776/938], Loss: 0.7910504341125488\n",
      "Validation: Epoch [3], Batch [777/938], Loss: 1.09678053855896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [778/938], Loss: 0.7035260200500488\n",
      "Validation: Epoch [3], Batch [779/938], Loss: 0.8140046000480652\n",
      "Validation: Epoch [3], Batch [780/938], Loss: 0.7295932173728943\n",
      "Validation: Epoch [3], Batch [781/938], Loss: 0.5544660687446594\n",
      "Validation: Epoch [3], Batch [782/938], Loss: 0.7280049324035645\n",
      "Validation: Epoch [3], Batch [783/938], Loss: 0.6279710531234741\n",
      "Validation: Epoch [3], Batch [784/938], Loss: 0.6253424286842346\n",
      "Validation: Epoch [3], Batch [785/938], Loss: 0.5367988348007202\n",
      "Validation: Epoch [3], Batch [786/938], Loss: 0.8039765357971191\n",
      "Validation: Epoch [3], Batch [787/938], Loss: 0.7324323654174805\n",
      "Validation: Epoch [3], Batch [788/938], Loss: 0.7183020710945129\n",
      "Validation: Epoch [3], Batch [789/938], Loss: 0.744660496711731\n",
      "Validation: Epoch [3], Batch [790/938], Loss: 0.7956740260124207\n",
      "Validation: Epoch [3], Batch [791/938], Loss: 0.6989109516143799\n",
      "Validation: Epoch [3], Batch [792/938], Loss: 0.7664928436279297\n",
      "Validation: Epoch [3], Batch [793/938], Loss: 0.7025359869003296\n",
      "Validation: Epoch [3], Batch [794/938], Loss: 0.6826760172843933\n",
      "Validation: Epoch [3], Batch [795/938], Loss: 0.6597799062728882\n",
      "Validation: Epoch [3], Batch [796/938], Loss: 0.7796474099159241\n",
      "Validation: Epoch [3], Batch [797/938], Loss: 0.7319360971450806\n",
      "Validation: Epoch [3], Batch [798/938], Loss: 0.9539642333984375\n",
      "Validation: Epoch [3], Batch [799/938], Loss: 0.6641751527786255\n",
      "Validation: Epoch [3], Batch [800/938], Loss: 0.7107654809951782\n",
      "Validation: Epoch [3], Batch [801/938], Loss: 0.7504409551620483\n",
      "Validation: Epoch [3], Batch [802/938], Loss: 0.7317894101142883\n",
      "Validation: Epoch [3], Batch [803/938], Loss: 0.8081963062286377\n",
      "Validation: Epoch [3], Batch [804/938], Loss: 0.6699327230453491\n",
      "Validation: Epoch [3], Batch [805/938], Loss: 0.6739223003387451\n",
      "Validation: Epoch [3], Batch [806/938], Loss: 1.0187853574752808\n",
      "Validation: Epoch [3], Batch [807/938], Loss: 0.8474322557449341\n",
      "Validation: Epoch [3], Batch [808/938], Loss: 0.7324438691139221\n",
      "Validation: Epoch [3], Batch [809/938], Loss: 0.9740757942199707\n",
      "Validation: Epoch [3], Batch [810/938], Loss: 0.9560680389404297\n",
      "Validation: Epoch [3], Batch [811/938], Loss: 0.7247060537338257\n",
      "Validation: Epoch [3], Batch [812/938], Loss: 0.6036104559898376\n",
      "Validation: Epoch [3], Batch [813/938], Loss: 0.6828986406326294\n",
      "Validation: Epoch [3], Batch [814/938], Loss: 0.8116615414619446\n",
      "Validation: Epoch [3], Batch [815/938], Loss: 0.8119244575500488\n",
      "Validation: Epoch [3], Batch [816/938], Loss: 0.5766045451164246\n",
      "Validation: Epoch [3], Batch [817/938], Loss: 0.7442599534988403\n",
      "Validation: Epoch [3], Batch [818/938], Loss: 0.7323822379112244\n",
      "Validation: Epoch [3], Batch [819/938], Loss: 0.773921012878418\n",
      "Validation: Epoch [3], Batch [820/938], Loss: 1.0982215404510498\n",
      "Validation: Epoch [3], Batch [821/938], Loss: 0.9322845339775085\n",
      "Validation: Epoch [3], Batch [822/938], Loss: 0.6472512483596802\n",
      "Validation: Epoch [3], Batch [823/938], Loss: 0.9156044721603394\n",
      "Validation: Epoch [3], Batch [824/938], Loss: 0.925786018371582\n",
      "Validation: Epoch [3], Batch [825/938], Loss: 0.788201093673706\n",
      "Validation: Epoch [3], Batch [826/938], Loss: 0.7096717357635498\n",
      "Validation: Epoch [3], Batch [827/938], Loss: 0.817899763584137\n",
      "Validation: Epoch [3], Batch [828/938], Loss: 0.984218955039978\n",
      "Validation: Epoch [3], Batch [829/938], Loss: 0.802017331123352\n",
      "Validation: Epoch [3], Batch [830/938], Loss: 0.6746798157691956\n",
      "Validation: Epoch [3], Batch [831/938], Loss: 0.8738061189651489\n",
      "Validation: Epoch [3], Batch [832/938], Loss: 0.859270453453064\n",
      "Validation: Epoch [3], Batch [833/938], Loss: 0.8487724661827087\n",
      "Validation: Epoch [3], Batch [834/938], Loss: 0.7859156727790833\n",
      "Validation: Epoch [3], Batch [835/938], Loss: 0.6362546682357788\n",
      "Validation: Epoch [3], Batch [836/938], Loss: 0.763790488243103\n",
      "Validation: Epoch [3], Batch [837/938], Loss: 0.713375449180603\n",
      "Validation: Epoch [3], Batch [838/938], Loss: 0.8726568222045898\n",
      "Validation: Epoch [3], Batch [839/938], Loss: 0.9736971259117126\n",
      "Validation: Epoch [3], Batch [840/938], Loss: 0.8534666299819946\n",
      "Validation: Epoch [3], Batch [841/938], Loss: 0.8758969902992249\n",
      "Validation: Epoch [3], Batch [842/938], Loss: 0.9052613377571106\n",
      "Validation: Epoch [3], Batch [843/938], Loss: 0.7977360486984253\n",
      "Validation: Epoch [3], Batch [844/938], Loss: 0.6602612137794495\n",
      "Validation: Epoch [3], Batch [845/938], Loss: 0.7412058115005493\n",
      "Validation: Epoch [3], Batch [846/938], Loss: 0.7558741569519043\n",
      "Validation: Epoch [3], Batch [847/938], Loss: 0.9090372323989868\n",
      "Validation: Epoch [3], Batch [848/938], Loss: 0.8830373287200928\n",
      "Validation: Epoch [3], Batch [849/938], Loss: 0.6364620923995972\n",
      "Validation: Epoch [3], Batch [850/938], Loss: 0.7209107875823975\n",
      "Validation: Epoch [3], Batch [851/938], Loss: 0.6979900598526001\n",
      "Validation: Epoch [3], Batch [852/938], Loss: 1.035553216934204\n",
      "Validation: Epoch [3], Batch [853/938], Loss: 0.8061529397964478\n",
      "Validation: Epoch [3], Batch [854/938], Loss: 0.7739331722259521\n",
      "Validation: Epoch [3], Batch [855/938], Loss: 0.8038619160652161\n",
      "Validation: Epoch [3], Batch [856/938], Loss: 0.6883947253227234\n",
      "Validation: Epoch [3], Batch [857/938], Loss: 0.7414100766181946\n",
      "Validation: Epoch [3], Batch [858/938], Loss: 1.1086580753326416\n",
      "Validation: Epoch [3], Batch [859/938], Loss: 0.9368126392364502\n",
      "Validation: Epoch [3], Batch [860/938], Loss: 0.953804612159729\n",
      "Validation: Epoch [3], Batch [861/938], Loss: 0.9306230545043945\n",
      "Validation: Epoch [3], Batch [862/938], Loss: 0.9187332391738892\n",
      "Validation: Epoch [3], Batch [863/938], Loss: 0.5388235449790955\n",
      "Validation: Epoch [3], Batch [864/938], Loss: 0.7635880708694458\n",
      "Validation: Epoch [3], Batch [865/938], Loss: 0.8129231333732605\n",
      "Validation: Epoch [3], Batch [866/938], Loss: 0.8545636534690857\n",
      "Validation: Epoch [3], Batch [867/938], Loss: 0.7699686288833618\n",
      "Validation: Epoch [3], Batch [868/938], Loss: 0.7208861708641052\n",
      "Validation: Epoch [3], Batch [869/938], Loss: 1.0123320817947388\n",
      "Validation: Epoch [3], Batch [870/938], Loss: 0.953953742980957\n",
      "Validation: Epoch [3], Batch [871/938], Loss: 0.9865840077400208\n",
      "Validation: Epoch [3], Batch [872/938], Loss: 1.0236902236938477\n",
      "Validation: Epoch [3], Batch [873/938], Loss: 1.079686164855957\n",
      "Validation: Epoch [3], Batch [874/938], Loss: 0.6720829010009766\n",
      "Validation: Epoch [3], Batch [875/938], Loss: 0.7647721171379089\n",
      "Validation: Epoch [3], Batch [876/938], Loss: 0.7389780282974243\n",
      "Validation: Epoch [3], Batch [877/938], Loss: 0.8315365314483643\n",
      "Validation: Epoch [3], Batch [878/938], Loss: 0.807497501373291\n",
      "Validation: Epoch [3], Batch [879/938], Loss: 0.6735625863075256\n",
      "Validation: Epoch [3], Batch [880/938], Loss: 0.8512967824935913\n",
      "Validation: Epoch [3], Batch [881/938], Loss: 0.8125081062316895\n",
      "Validation: Epoch [3], Batch [882/938], Loss: 0.6618262529373169\n",
      "Validation: Epoch [3], Batch [883/938], Loss: 0.8006921410560608\n",
      "Validation: Epoch [3], Batch [884/938], Loss: 0.7770748138427734\n",
      "Validation: Epoch [3], Batch [885/938], Loss: 0.773829460144043\n",
      "Validation: Epoch [3], Batch [886/938], Loss: 0.9841364622116089\n",
      "Validation: Epoch [3], Batch [887/938], Loss: 0.6981098651885986\n",
      "Validation: Epoch [3], Batch [888/938], Loss: 0.6350120306015015\n",
      "Validation: Epoch [3], Batch [889/938], Loss: 0.7399291396141052\n",
      "Validation: Epoch [3], Batch [890/938], Loss: 0.73154217004776\n",
      "Validation: Epoch [3], Batch [891/938], Loss: 0.8565827012062073\n",
      "Validation: Epoch [3], Batch [892/938], Loss: 0.884361982345581\n",
      "Validation: Epoch [3], Batch [893/938], Loss: 0.801021933555603\n",
      "Validation: Epoch [3], Batch [894/938], Loss: 0.7384366989135742\n",
      "Validation: Epoch [3], Batch [895/938], Loss: 0.794364869594574\n",
      "Validation: Epoch [3], Batch [896/938], Loss: 0.7385647296905518\n",
      "Validation: Epoch [3], Batch [897/938], Loss: 0.8799996376037598\n",
      "Validation: Epoch [3], Batch [898/938], Loss: 0.6011422276496887\n",
      "Validation: Epoch [3], Batch [899/938], Loss: 0.9294219017028809\n",
      "Validation: Epoch [3], Batch [900/938], Loss: 0.7439086437225342\n",
      "Validation: Epoch [3], Batch [901/938], Loss: 0.6296077966690063\n",
      "Validation: Epoch [3], Batch [902/938], Loss: 0.8052558898925781\n",
      "Validation: Epoch [3], Batch [903/938], Loss: 0.6586562395095825\n",
      "Validation: Epoch [3], Batch [904/938], Loss: 0.687658429145813\n",
      "Validation: Epoch [3], Batch [905/938], Loss: 0.8185732364654541\n",
      "Validation: Epoch [3], Batch [906/938], Loss: 0.7209581136703491\n",
      "Validation: Epoch [3], Batch [907/938], Loss: 0.8352659940719604\n",
      "Validation: Epoch [3], Batch [908/938], Loss: 0.9227349758148193\n",
      "Validation: Epoch [3], Batch [909/938], Loss: 0.6880251169204712\n",
      "Validation: Epoch [3], Batch [910/938], Loss: 0.8244814872741699\n",
      "Validation: Epoch [3], Batch [911/938], Loss: 0.8166924118995667\n",
      "Validation: Epoch [3], Batch [912/938], Loss: 0.702885627746582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [3], Batch [913/938], Loss: 0.7214778661727905\n",
      "Validation: Epoch [3], Batch [914/938], Loss: 0.9866658449172974\n",
      "Validation: Epoch [3], Batch [915/938], Loss: 0.7740002274513245\n",
      "Validation: Epoch [3], Batch [916/938], Loss: 0.9254063367843628\n",
      "Validation: Epoch [3], Batch [917/938], Loss: 0.7829468250274658\n",
      "Validation: Epoch [3], Batch [918/938], Loss: 1.0186564922332764\n",
      "Validation: Epoch [3], Batch [919/938], Loss: 0.5772877931594849\n",
      "Validation: Epoch [3], Batch [920/938], Loss: 0.6904343366622925\n",
      "Validation: Epoch [3], Batch [921/938], Loss: 0.8178625106811523\n",
      "Validation: Epoch [3], Batch [922/938], Loss: 0.7728632688522339\n",
      "Validation: Epoch [3], Batch [923/938], Loss: 0.8566924929618835\n",
      "Validation: Epoch [3], Batch [924/938], Loss: 0.8964533805847168\n",
      "Validation: Epoch [3], Batch [925/938], Loss: 0.7727326154708862\n",
      "Validation: Epoch [3], Batch [926/938], Loss: 0.727435827255249\n",
      "Validation: Epoch [3], Batch [927/938], Loss: 0.8151925206184387\n",
      "Validation: Epoch [3], Batch [928/938], Loss: 0.8234459161758423\n",
      "Validation: Epoch [3], Batch [929/938], Loss: 0.6820323467254639\n",
      "Validation: Epoch [3], Batch [930/938], Loss: 0.7587677240371704\n",
      "Validation: Epoch [3], Batch [931/938], Loss: 0.6961337327957153\n",
      "Validation: Epoch [3], Batch [932/938], Loss: 1.0909781455993652\n",
      "Validation: Epoch [3], Batch [933/938], Loss: 0.7156195640563965\n",
      "Validation: Epoch [3], Batch [934/938], Loss: 0.7606314420700073\n",
      "Validation: Epoch [3], Batch [935/938], Loss: 0.7382113337516785\n",
      "Validation: Epoch [3], Batch [936/938], Loss: 0.8702887296676636\n",
      "Validation: Epoch [3], Batch [937/938], Loss: 0.8059058785438538\n",
      "Validation: Epoch [3], Batch [938/938], Loss: 0.5174586176872253\n",
      "Accuracy of test set: 0.7058166666666666\n",
      "Train: Epoch [4], Batch [1/938], Loss: 0.733475923538208\n",
      "Train: Epoch [4], Batch [2/938], Loss: 1.037983775138855\n",
      "Train: Epoch [4], Batch [3/938], Loss: 0.7602547407150269\n",
      "Train: Epoch [4], Batch [4/938], Loss: 0.9694038033485413\n",
      "Train: Epoch [4], Batch [5/938], Loss: 0.6411058902740479\n",
      "Train: Epoch [4], Batch [6/938], Loss: 0.605341911315918\n",
      "Train: Epoch [4], Batch [7/938], Loss: 0.8272581100463867\n",
      "Train: Epoch [4], Batch [8/938], Loss: 0.839907169342041\n",
      "Train: Epoch [4], Batch [9/938], Loss: 0.7781972289085388\n",
      "Train: Epoch [4], Batch [10/938], Loss: 0.7685137391090393\n",
      "Train: Epoch [4], Batch [11/938], Loss: 0.6926100254058838\n",
      "Train: Epoch [4], Batch [12/938], Loss: 0.8102815747261047\n",
      "Train: Epoch [4], Batch [13/938], Loss: 0.9356451034545898\n",
      "Train: Epoch [4], Batch [14/938], Loss: 0.8326628804206848\n",
      "Train: Epoch [4], Batch [15/938], Loss: 0.9926067590713501\n",
      "Train: Epoch [4], Batch [16/938], Loss: 1.023545265197754\n",
      "Train: Epoch [4], Batch [17/938], Loss: 0.7992702126502991\n",
      "Train: Epoch [4], Batch [18/938], Loss: 0.7839992046356201\n",
      "Train: Epoch [4], Batch [19/938], Loss: 0.8472415208816528\n",
      "Train: Epoch [4], Batch [20/938], Loss: 0.8040215969085693\n",
      "Train: Epoch [4], Batch [21/938], Loss: 0.7321022152900696\n",
      "Train: Epoch [4], Batch [22/938], Loss: 0.8483476042747498\n",
      "Train: Epoch [4], Batch [23/938], Loss: 0.6972469687461853\n",
      "Train: Epoch [4], Batch [24/938], Loss: 0.7359310984611511\n",
      "Train: Epoch [4], Batch [25/938], Loss: 0.6942822933197021\n",
      "Train: Epoch [4], Batch [26/938], Loss: 0.8559364080429077\n",
      "Train: Epoch [4], Batch [27/938], Loss: 0.8310939073562622\n",
      "Train: Epoch [4], Batch [28/938], Loss: 0.7349190711975098\n",
      "Train: Epoch [4], Batch [29/938], Loss: 0.7753885388374329\n",
      "Train: Epoch [4], Batch [30/938], Loss: 0.9664820432662964\n",
      "Train: Epoch [4], Batch [31/938], Loss: 0.9528733491897583\n",
      "Train: Epoch [4], Batch [32/938], Loss: 0.8575872182846069\n",
      "Train: Epoch [4], Batch [33/938], Loss: 0.7871569395065308\n",
      "Train: Epoch [4], Batch [34/938], Loss: 0.8715882897377014\n",
      "Train: Epoch [4], Batch [35/938], Loss: 0.7178362607955933\n",
      "Train: Epoch [4], Batch [36/938], Loss: 0.7438086271286011\n",
      "Train: Epoch [4], Batch [37/938], Loss: 0.7900155782699585\n",
      "Train: Epoch [4], Batch [38/938], Loss: 0.7539327144622803\n",
      "Train: Epoch [4], Batch [39/938], Loss: 0.6969479322433472\n",
      "Train: Epoch [4], Batch [40/938], Loss: 0.8332573771476746\n",
      "Train: Epoch [4], Batch [41/938], Loss: 0.8948942422866821\n",
      "Train: Epoch [4], Batch [42/938], Loss: 0.9916911125183105\n",
      "Train: Epoch [4], Batch [43/938], Loss: 0.705478310585022\n",
      "Train: Epoch [4], Batch [44/938], Loss: 0.636012077331543\n",
      "Train: Epoch [4], Batch [45/938], Loss: 0.7755652666091919\n",
      "Train: Epoch [4], Batch [46/938], Loss: 0.7053340077400208\n",
      "Train: Epoch [4], Batch [47/938], Loss: 0.711738109588623\n",
      "Train: Epoch [4], Batch [48/938], Loss: 0.8701847791671753\n",
      "Train: Epoch [4], Batch [49/938], Loss: 0.6586341857910156\n",
      "Train: Epoch [4], Batch [50/938], Loss: 0.6944506764411926\n",
      "Train: Epoch [4], Batch [51/938], Loss: 0.7531492710113525\n",
      "Train: Epoch [4], Batch [52/938], Loss: 0.7272214889526367\n",
      "Train: Epoch [4], Batch [53/938], Loss: 0.730911374092102\n",
      "Train: Epoch [4], Batch [54/938], Loss: 0.7014576196670532\n",
      "Train: Epoch [4], Batch [55/938], Loss: 0.6029496788978577\n",
      "Train: Epoch [4], Batch [56/938], Loss: 0.8502707481384277\n",
      "Train: Epoch [4], Batch [57/938], Loss: 0.7313259840011597\n",
      "Train: Epoch [4], Batch [58/938], Loss: 0.7520030736923218\n",
      "Train: Epoch [4], Batch [59/938], Loss: 0.7756775617599487\n",
      "Train: Epoch [4], Batch [60/938], Loss: 0.7482489347457886\n",
      "Train: Epoch [4], Batch [61/938], Loss: 0.9705555438995361\n",
      "Train: Epoch [4], Batch [62/938], Loss: 0.7388005256652832\n",
      "Train: Epoch [4], Batch [63/938], Loss: 1.0611047744750977\n",
      "Train: Epoch [4], Batch [64/938], Loss: 1.005458116531372\n",
      "Train: Epoch [4], Batch [65/938], Loss: 0.751049280166626\n",
      "Train: Epoch [4], Batch [66/938], Loss: 0.7273880839347839\n",
      "Train: Epoch [4], Batch [67/938], Loss: 0.8046744465827942\n",
      "Train: Epoch [4], Batch [68/938], Loss: 0.5901551246643066\n",
      "Train: Epoch [4], Batch [69/938], Loss: 1.08829665184021\n",
      "Train: Epoch [4], Batch [70/938], Loss: 0.924697756767273\n",
      "Train: Epoch [4], Batch [71/938], Loss: 0.8009308576583862\n",
      "Train: Epoch [4], Batch [72/938], Loss: 0.8397661447525024\n",
      "Train: Epoch [4], Batch [73/938], Loss: 0.9080168008804321\n",
      "Train: Epoch [4], Batch [74/938], Loss: 0.8832381963729858\n",
      "Train: Epoch [4], Batch [75/938], Loss: 0.6268317699432373\n",
      "Train: Epoch [4], Batch [76/938], Loss: 0.801798939704895\n",
      "Train: Epoch [4], Batch [77/938], Loss: 0.7644111514091492\n",
      "Train: Epoch [4], Batch [78/938], Loss: 0.6186436414718628\n",
      "Train: Epoch [4], Batch [79/938], Loss: 0.7581828236579895\n",
      "Train: Epoch [4], Batch [80/938], Loss: 0.8269203901290894\n",
      "Train: Epoch [4], Batch [81/938], Loss: 0.8359621167182922\n",
      "Train: Epoch [4], Batch [82/938], Loss: 0.6604846715927124\n",
      "Train: Epoch [4], Batch [83/938], Loss: 0.814355731010437\n",
      "Train: Epoch [4], Batch [84/938], Loss: 0.8398809432983398\n",
      "Train: Epoch [4], Batch [85/938], Loss: 0.7468315958976746\n",
      "Train: Epoch [4], Batch [86/938], Loss: 0.5879926681518555\n",
      "Train: Epoch [4], Batch [87/938], Loss: 0.971572756767273\n",
      "Train: Epoch [4], Batch [88/938], Loss: 0.8578776121139526\n",
      "Train: Epoch [4], Batch [89/938], Loss: 0.7365940809249878\n",
      "Train: Epoch [4], Batch [90/938], Loss: 0.6264171600341797\n",
      "Train: Epoch [4], Batch [91/938], Loss: 0.9937661290168762\n",
      "Train: Epoch [4], Batch [92/938], Loss: 0.7847951650619507\n",
      "Train: Epoch [4], Batch [93/938], Loss: 0.9556505084037781\n",
      "Train: Epoch [4], Batch [94/938], Loss: 0.8733835220336914\n",
      "Train: Epoch [4], Batch [95/938], Loss: 0.8535847663879395\n",
      "Train: Epoch [4], Batch [96/938], Loss: 0.6995210647583008\n",
      "Train: Epoch [4], Batch [97/938], Loss: 0.9740536212921143\n",
      "Train: Epoch [4], Batch [98/938], Loss: 0.7505391836166382\n",
      "Train: Epoch [4], Batch [99/938], Loss: 0.8925707936286926\n",
      "Train: Epoch [4], Batch [100/938], Loss: 0.927923321723938\n",
      "Train: Epoch [4], Batch [101/938], Loss: 0.8257898092269897\n",
      "Train: Epoch [4], Batch [102/938], Loss: 0.7197522521018982\n",
      "Train: Epoch [4], Batch [103/938], Loss: 0.9976187944412231\n",
      "Train: Epoch [4], Batch [104/938], Loss: 0.8309307098388672\n",
      "Train: Epoch [4], Batch [105/938], Loss: 0.8966307640075684\n",
      "Train: Epoch [4], Batch [106/938], Loss: 0.7634558081626892\n",
      "Train: Epoch [4], Batch [107/938], Loss: 0.7862939238548279\n",
      "Train: Epoch [4], Batch [108/938], Loss: 0.825896680355072\n",
      "Train: Epoch [4], Batch [109/938], Loss: 0.8447946310043335\n",
      "Train: Epoch [4], Batch [110/938], Loss: 0.7927230596542358\n",
      "Train: Epoch [4], Batch [111/938], Loss: 0.7448948621749878\n",
      "Train: Epoch [4], Batch [112/938], Loss: 0.9551400542259216\n",
      "Train: Epoch [4], Batch [113/938], Loss: 0.6002917289733887\n",
      "Train: Epoch [4], Batch [114/938], Loss: 0.87417072057724\n",
      "Train: Epoch [4], Batch [115/938], Loss: 0.7158116102218628\n",
      "Train: Epoch [4], Batch [116/938], Loss: 0.9245856404304504\n",
      "Train: Epoch [4], Batch [117/938], Loss: 0.9456878900527954\n",
      "Train: Epoch [4], Batch [118/938], Loss: 0.6887728571891785\n",
      "Train: Epoch [4], Batch [119/938], Loss: 0.9160731434822083\n",
      "Train: Epoch [4], Batch [120/938], Loss: 0.8299605846405029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [4], Batch [121/938], Loss: 1.2666585445404053\n",
      "Train: Epoch [4], Batch [122/938], Loss: 0.9030121564865112\n",
      "Train: Epoch [4], Batch [123/938], Loss: 0.649633526802063\n",
      "Train: Epoch [4], Batch [124/938], Loss: 0.6878950595855713\n",
      "Train: Epoch [4], Batch [125/938], Loss: 0.7813622951507568\n",
      "Train: Epoch [4], Batch [126/938], Loss: 0.8299145698547363\n",
      "Train: Epoch [4], Batch [127/938], Loss: 0.8082742691040039\n",
      "Train: Epoch [4], Batch [128/938], Loss: 1.0649601221084595\n",
      "Train: Epoch [4], Batch [129/938], Loss: 0.848096489906311\n",
      "Train: Epoch [4], Batch [130/938], Loss: 0.8139735460281372\n",
      "Train: Epoch [4], Batch [131/938], Loss: 0.7560561895370483\n",
      "Train: Epoch [4], Batch [132/938], Loss: 0.9734000563621521\n",
      "Train: Epoch [4], Batch [133/938], Loss: 0.9633212089538574\n",
      "Train: Epoch [4], Batch [134/938], Loss: 0.6944664716720581\n",
      "Train: Epoch [4], Batch [135/938], Loss: 0.7727754712104797\n",
      "Train: Epoch [4], Batch [136/938], Loss: 0.8194648623466492\n",
      "Train: Epoch [4], Batch [137/938], Loss: 0.9844974279403687\n",
      "Train: Epoch [4], Batch [138/938], Loss: 0.786186933517456\n",
      "Train: Epoch [4], Batch [139/938], Loss: 0.650915265083313\n",
      "Train: Epoch [4], Batch [140/938], Loss: 0.8675930500030518\n",
      "Train: Epoch [4], Batch [141/938], Loss: 0.6572743058204651\n",
      "Train: Epoch [4], Batch [142/938], Loss: 0.7072153687477112\n",
      "Train: Epoch [4], Batch [143/938], Loss: 0.8149845600128174\n",
      "Train: Epoch [4], Batch [144/938], Loss: 0.7871780395507812\n",
      "Train: Epoch [4], Batch [145/938], Loss: 0.7447262406349182\n",
      "Train: Epoch [4], Batch [146/938], Loss: 0.6436032056808472\n",
      "Train: Epoch [4], Batch [147/938], Loss: 0.8147851228713989\n",
      "Train: Epoch [4], Batch [148/938], Loss: 0.6785620450973511\n",
      "Train: Epoch [4], Batch [149/938], Loss: 0.7583794593811035\n",
      "Train: Epoch [4], Batch [150/938], Loss: 0.7520161867141724\n",
      "Train: Epoch [4], Batch [151/938], Loss: 0.6761385202407837\n",
      "Train: Epoch [4], Batch [152/938], Loss: 0.7241626977920532\n",
      "Train: Epoch [4], Batch [153/938], Loss: 0.7487832307815552\n",
      "Train: Epoch [4], Batch [154/938], Loss: 0.6296641826629639\n",
      "Train: Epoch [4], Batch [155/938], Loss: 0.8278025388717651\n",
      "Train: Epoch [4], Batch [156/938], Loss: 0.7837165594100952\n",
      "Train: Epoch [4], Batch [157/938], Loss: 0.9257385730743408\n",
      "Train: Epoch [4], Batch [158/938], Loss: 0.8230605125427246\n",
      "Train: Epoch [4], Batch [159/938], Loss: 0.8123115301132202\n",
      "Train: Epoch [4], Batch [160/938], Loss: 0.7819660902023315\n",
      "Train: Epoch [4], Batch [161/938], Loss: 0.9448657035827637\n",
      "Train: Epoch [4], Batch [162/938], Loss: 0.517935037612915\n",
      "Train: Epoch [4], Batch [163/938], Loss: 0.8312981128692627\n",
      "Train: Epoch [4], Batch [164/938], Loss: 0.7255082130432129\n",
      "Train: Epoch [4], Batch [165/938], Loss: 0.6857237815856934\n",
      "Train: Epoch [4], Batch [166/938], Loss: 0.7035666704177856\n",
      "Train: Epoch [4], Batch [167/938], Loss: 0.7566678524017334\n",
      "Train: Epoch [4], Batch [168/938], Loss: 0.7789744734764099\n",
      "Train: Epoch [4], Batch [169/938], Loss: 0.6562126874923706\n",
      "Train: Epoch [4], Batch [170/938], Loss: 0.5149649381637573\n",
      "Train: Epoch [4], Batch [171/938], Loss: 0.7711760997772217\n",
      "Train: Epoch [4], Batch [172/938], Loss: 0.7105474472045898\n",
      "Train: Epoch [4], Batch [173/938], Loss: 0.7724076509475708\n",
      "Train: Epoch [4], Batch [174/938], Loss: 0.7926989793777466\n",
      "Train: Epoch [4], Batch [175/938], Loss: 0.8340325951576233\n",
      "Train: Epoch [4], Batch [176/938], Loss: 0.7285054922103882\n",
      "Train: Epoch [4], Batch [177/938], Loss: 0.9395428895950317\n",
      "Train: Epoch [4], Batch [178/938], Loss: 0.6431686878204346\n",
      "Train: Epoch [4], Batch [179/938], Loss: 0.7756620645523071\n",
      "Train: Epoch [4], Batch [180/938], Loss: 0.6067672967910767\n",
      "Train: Epoch [4], Batch [181/938], Loss: 0.7169220447540283\n",
      "Train: Epoch [4], Batch [182/938], Loss: 0.7414299249649048\n",
      "Train: Epoch [4], Batch [183/938], Loss: 0.830772876739502\n",
      "Train: Epoch [4], Batch [184/938], Loss: 1.030855417251587\n",
      "Train: Epoch [4], Batch [185/938], Loss: 0.5543878674507141\n",
      "Train: Epoch [4], Batch [186/938], Loss: 0.9642847776412964\n",
      "Train: Epoch [4], Batch [187/938], Loss: 0.848953127861023\n",
      "Train: Epoch [4], Batch [188/938], Loss: 0.8791176080703735\n",
      "Train: Epoch [4], Batch [189/938], Loss: 0.7424948215484619\n",
      "Train: Epoch [4], Batch [190/938], Loss: 0.8708702921867371\n",
      "Train: Epoch [4], Batch [191/938], Loss: 0.6724944114685059\n",
      "Train: Epoch [4], Batch [192/938], Loss: 0.7973760962486267\n",
      "Train: Epoch [4], Batch [193/938], Loss: 0.5577843189239502\n",
      "Train: Epoch [4], Batch [194/938], Loss: 0.6914265155792236\n",
      "Train: Epoch [4], Batch [195/938], Loss: 0.8167758584022522\n",
      "Train: Epoch [4], Batch [196/938], Loss: 0.7796767950057983\n",
      "Train: Epoch [4], Batch [197/938], Loss: 0.8779274225234985\n",
      "Train: Epoch [4], Batch [198/938], Loss: 0.7152159810066223\n",
      "Train: Epoch [4], Batch [199/938], Loss: 0.6715313196182251\n",
      "Train: Epoch [4], Batch [200/938], Loss: 0.7042580842971802\n",
      "Train: Epoch [4], Batch [201/938], Loss: 0.6360070705413818\n",
      "Train: Epoch [4], Batch [202/938], Loss: 0.8003581762313843\n",
      "Train: Epoch [4], Batch [203/938], Loss: 0.8216006755828857\n",
      "Train: Epoch [4], Batch [204/938], Loss: 1.3529094457626343\n",
      "Train: Epoch [4], Batch [205/938], Loss: 0.9520726799964905\n",
      "Train: Epoch [4], Batch [206/938], Loss: 0.745008647441864\n",
      "Train: Epoch [4], Batch [207/938], Loss: 0.8664276599884033\n",
      "Train: Epoch [4], Batch [208/938], Loss: 0.9079416990280151\n",
      "Train: Epoch [4], Batch [209/938], Loss: 0.9028464555740356\n",
      "Train: Epoch [4], Batch [210/938], Loss: 0.8127046823501587\n",
      "Train: Epoch [4], Batch [211/938], Loss: 0.8559044599533081\n",
      "Train: Epoch [4], Batch [212/938], Loss: 0.758441686630249\n",
      "Train: Epoch [4], Batch [213/938], Loss: 0.6454278230667114\n",
      "Train: Epoch [4], Batch [214/938], Loss: 0.9579200148582458\n",
      "Train: Epoch [4], Batch [215/938], Loss: 0.6387745141983032\n",
      "Train: Epoch [4], Batch [216/938], Loss: 0.7780937552452087\n",
      "Train: Epoch [4], Batch [217/938], Loss: 0.8640869855880737\n",
      "Train: Epoch [4], Batch [218/938], Loss: 0.7350075244903564\n",
      "Train: Epoch [4], Batch [219/938], Loss: 0.674741268157959\n",
      "Train: Epoch [4], Batch [220/938], Loss: 0.8200968503952026\n",
      "Train: Epoch [4], Batch [221/938], Loss: 0.6210633516311646\n",
      "Train: Epoch [4], Batch [222/938], Loss: 0.6660745739936829\n",
      "Train: Epoch [4], Batch [223/938], Loss: 0.8669842481613159\n",
      "Train: Epoch [4], Batch [224/938], Loss: 0.6318978071212769\n",
      "Train: Epoch [4], Batch [225/938], Loss: 0.9303250312805176\n",
      "Train: Epoch [4], Batch [226/938], Loss: 0.7927289009094238\n",
      "Train: Epoch [4], Batch [227/938], Loss: 0.7186653017997742\n",
      "Train: Epoch [4], Batch [228/938], Loss: 0.9646144509315491\n",
      "Train: Epoch [4], Batch [229/938], Loss: 0.5655136704444885\n",
      "Train: Epoch [4], Batch [230/938], Loss: 0.8045797348022461\n",
      "Train: Epoch [4], Batch [231/938], Loss: 0.7905865907669067\n",
      "Train: Epoch [4], Batch [232/938], Loss: 0.8953343629837036\n",
      "Train: Epoch [4], Batch [233/938], Loss: 0.7302843332290649\n",
      "Train: Epoch [4], Batch [234/938], Loss: 0.946967601776123\n",
      "Train: Epoch [4], Batch [235/938], Loss: 0.9248084425926208\n",
      "Train: Epoch [4], Batch [236/938], Loss: 0.9874953031539917\n",
      "Train: Epoch [4], Batch [237/938], Loss: 0.8940528631210327\n",
      "Train: Epoch [4], Batch [238/938], Loss: 0.6945562362670898\n",
      "Train: Epoch [4], Batch [239/938], Loss: 0.7553280591964722\n",
      "Train: Epoch [4], Batch [240/938], Loss: 0.9472693204879761\n",
      "Train: Epoch [4], Batch [241/938], Loss: 0.8194612264633179\n",
      "Train: Epoch [4], Batch [242/938], Loss: 0.8408786654472351\n",
      "Train: Epoch [4], Batch [243/938], Loss: 0.8304476737976074\n",
      "Train: Epoch [4], Batch [244/938], Loss: 0.947493851184845\n",
      "Train: Epoch [4], Batch [245/938], Loss: 0.7548967599868774\n",
      "Train: Epoch [4], Batch [246/938], Loss: 0.9407911896705627\n",
      "Train: Epoch [4], Batch [247/938], Loss: 0.7999846935272217\n",
      "Train: Epoch [4], Batch [248/938], Loss: 0.6724134087562561\n",
      "Train: Epoch [4], Batch [249/938], Loss: 0.7648268938064575\n",
      "Train: Epoch [4], Batch [250/938], Loss: 0.7407088875770569\n",
      "Train: Epoch [4], Batch [251/938], Loss: 0.7012884020805359\n",
      "Train: Epoch [4], Batch [252/938], Loss: 0.8445540070533752\n",
      "Train: Epoch [4], Batch [253/938], Loss: 0.7995893955230713\n",
      "Train: Epoch [4], Batch [254/938], Loss: 0.7147781848907471\n",
      "Train: Epoch [4], Batch [255/938], Loss: 0.7287871241569519\n",
      "Train: Epoch [4], Batch [256/938], Loss: 0.7517505884170532\n",
      "Train: Epoch [4], Batch [257/938], Loss: 0.6840929388999939\n",
      "Train: Epoch [4], Batch [258/938], Loss: 0.7912838459014893\n",
      "Train: Epoch [4], Batch [259/938], Loss: 0.6182428598403931\n",
      "Train: Epoch [4], Batch [260/938], Loss: 0.8927826881408691\n",
      "Train: Epoch [4], Batch [261/938], Loss: 0.5456849932670593\n",
      "Train: Epoch [4], Batch [262/938], Loss: 0.6954528093338013\n",
      "Train: Epoch [4], Batch [263/938], Loss: 0.7275283336639404\n",
      "Train: Epoch [4], Batch [264/938], Loss: 0.8304487466812134\n",
      "Train: Epoch [4], Batch [265/938], Loss: 0.703852653503418\n",
      "Train: Epoch [4], Batch [266/938], Loss: 0.7356773614883423\n",
      "Train: Epoch [4], Batch [267/938], Loss: 0.6881975531578064\n",
      "Train: Epoch [4], Batch [268/938], Loss: 0.6156032085418701\n",
      "Train: Epoch [4], Batch [269/938], Loss: 0.7817136645317078\n",
      "Train: Epoch [4], Batch [270/938], Loss: 0.7859290838241577\n",
      "Train: Epoch [4], Batch [271/938], Loss: 0.746599555015564\n",
      "Train: Epoch [4], Batch [272/938], Loss: 0.8458856344223022\n",
      "Train: Epoch [4], Batch [273/938], Loss: 0.7205711007118225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [4], Batch [274/938], Loss: 0.8589807152748108\n",
      "Train: Epoch [4], Batch [275/938], Loss: 0.9078701734542847\n",
      "Train: Epoch [4], Batch [276/938], Loss: 0.7835263609886169\n",
      "Train: Epoch [4], Batch [277/938], Loss: 0.7858694791793823\n",
      "Train: Epoch [4], Batch [278/938], Loss: 0.7691771984100342\n",
      "Train: Epoch [4], Batch [279/938], Loss: 0.7534237504005432\n",
      "Train: Epoch [4], Batch [280/938], Loss: 0.7472120523452759\n",
      "Train: Epoch [4], Batch [281/938], Loss: 0.5939009785652161\n",
      "Train: Epoch [4], Batch [282/938], Loss: 0.692634105682373\n",
      "Train: Epoch [4], Batch [283/938], Loss: 0.6957242488861084\n",
      "Train: Epoch [4], Batch [284/938], Loss: 0.6819727420806885\n",
      "Train: Epoch [4], Batch [285/938], Loss: 0.6896197199821472\n",
      "Train: Epoch [4], Batch [286/938], Loss: 0.6873525381088257\n",
      "Train: Epoch [4], Batch [287/938], Loss: 0.7307978868484497\n",
      "Train: Epoch [4], Batch [288/938], Loss: 0.6357184648513794\n",
      "Train: Epoch [4], Batch [289/938], Loss: 0.8848353624343872\n",
      "Train: Epoch [4], Batch [290/938], Loss: 0.7888615727424622\n",
      "Train: Epoch [4], Batch [291/938], Loss: 0.6508007049560547\n",
      "Train: Epoch [4], Batch [292/938], Loss: 0.83478182554245\n",
      "Train: Epoch [4], Batch [293/938], Loss: 0.7778885364532471\n",
      "Train: Epoch [4], Batch [294/938], Loss: 0.8381341695785522\n",
      "Train: Epoch [4], Batch [295/938], Loss: 0.7356472015380859\n",
      "Train: Epoch [4], Batch [296/938], Loss: 0.5836560726165771\n",
      "Train: Epoch [4], Batch [297/938], Loss: 0.808896541595459\n",
      "Train: Epoch [4], Batch [298/938], Loss: 0.7522351145744324\n",
      "Train: Epoch [4], Batch [299/938], Loss: 0.7916803359985352\n",
      "Train: Epoch [4], Batch [300/938], Loss: 0.8376954793930054\n",
      "Train: Epoch [4], Batch [301/938], Loss: 0.7460708022117615\n",
      "Train: Epoch [4], Batch [302/938], Loss: 0.538402259349823\n",
      "Train: Epoch [4], Batch [303/938], Loss: 0.6804444193840027\n",
      "Train: Epoch [4], Batch [304/938], Loss: 0.7994675636291504\n",
      "Train: Epoch [4], Batch [305/938], Loss: 0.7405837178230286\n",
      "Train: Epoch [4], Batch [306/938], Loss: 0.8329629898071289\n",
      "Train: Epoch [4], Batch [307/938], Loss: 0.8023784160614014\n",
      "Train: Epoch [4], Batch [308/938], Loss: 0.9148964881896973\n",
      "Train: Epoch [4], Batch [309/938], Loss: 0.8109116554260254\n",
      "Train: Epoch [4], Batch [310/938], Loss: 0.6203528642654419\n",
      "Train: Epoch [4], Batch [311/938], Loss: 0.9665002822875977\n",
      "Train: Epoch [4], Batch [312/938], Loss: 0.8929067254066467\n",
      "Train: Epoch [4], Batch [313/938], Loss: 0.9876086711883545\n",
      "Train: Epoch [4], Batch [314/938], Loss: 0.8019927740097046\n",
      "Train: Epoch [4], Batch [315/938], Loss: 0.6504762172698975\n",
      "Train: Epoch [4], Batch [316/938], Loss: 0.9350741505622864\n",
      "Train: Epoch [4], Batch [317/938], Loss: 0.9236977100372314\n",
      "Train: Epoch [4], Batch [318/938], Loss: 0.8600896596908569\n",
      "Train: Epoch [4], Batch [319/938], Loss: 0.8684207797050476\n",
      "Train: Epoch [4], Batch [320/938], Loss: 0.6899989247322083\n",
      "Train: Epoch [4], Batch [321/938], Loss: 0.519186794757843\n",
      "Train: Epoch [4], Batch [322/938], Loss: 0.7586179971694946\n",
      "Train: Epoch [4], Batch [323/938], Loss: 0.8282196521759033\n",
      "Train: Epoch [4], Batch [324/938], Loss: 0.5504783391952515\n",
      "Train: Epoch [4], Batch [325/938], Loss: 0.5950202941894531\n",
      "Train: Epoch [4], Batch [326/938], Loss: 0.726710319519043\n",
      "Train: Epoch [4], Batch [327/938], Loss: 0.5859993696212769\n",
      "Train: Epoch [4], Batch [328/938], Loss: 0.7915787696838379\n",
      "Train: Epoch [4], Batch [329/938], Loss: 0.693389892578125\n",
      "Train: Epoch [4], Batch [330/938], Loss: 0.8976401090621948\n",
      "Train: Epoch [4], Batch [331/938], Loss: 0.8935722708702087\n",
      "Train: Epoch [4], Batch [332/938], Loss: 0.9568561911582947\n",
      "Train: Epoch [4], Batch [333/938], Loss: 0.7141920328140259\n",
      "Train: Epoch [4], Batch [334/938], Loss: 0.5406491756439209\n",
      "Train: Epoch [4], Batch [335/938], Loss: 1.060234785079956\n",
      "Train: Epoch [4], Batch [336/938], Loss: 0.6908968687057495\n",
      "Train: Epoch [4], Batch [337/938], Loss: 0.9276605844497681\n",
      "Train: Epoch [4], Batch [338/938], Loss: 0.6669937968254089\n",
      "Train: Epoch [4], Batch [339/938], Loss: 1.0300896167755127\n",
      "Train: Epoch [4], Batch [340/938], Loss: 0.5733178853988647\n",
      "Train: Epoch [4], Batch [341/938], Loss: 0.8548346757888794\n",
      "Train: Epoch [4], Batch [342/938], Loss: 0.7541831135749817\n",
      "Train: Epoch [4], Batch [343/938], Loss: 0.7395528554916382\n",
      "Train: Epoch [4], Batch [344/938], Loss: 0.8397907018661499\n",
      "Train: Epoch [4], Batch [345/938], Loss: 0.6583367586135864\n",
      "Train: Epoch [4], Batch [346/938], Loss: 0.6969853043556213\n",
      "Train: Epoch [4], Batch [347/938], Loss: 0.8579497933387756\n",
      "Train: Epoch [4], Batch [348/938], Loss: 0.6184414029121399\n",
      "Train: Epoch [4], Batch [349/938], Loss: 0.962131917476654\n",
      "Train: Epoch [4], Batch [350/938], Loss: 0.7808001637458801\n",
      "Train: Epoch [4], Batch [351/938], Loss: 0.8078072667121887\n",
      "Train: Epoch [4], Batch [352/938], Loss: 0.8088802695274353\n",
      "Train: Epoch [4], Batch [353/938], Loss: 0.854491114616394\n",
      "Train: Epoch [4], Batch [354/938], Loss: 0.7664421796798706\n",
      "Train: Epoch [4], Batch [355/938], Loss: 0.7834571599960327\n",
      "Train: Epoch [4], Batch [356/938], Loss: 0.6954681277275085\n",
      "Train: Epoch [4], Batch [357/938], Loss: 0.672339141368866\n",
      "Train: Epoch [4], Batch [358/938], Loss: 0.6311396360397339\n",
      "Train: Epoch [4], Batch [359/938], Loss: 0.9759203195571899\n",
      "Train: Epoch [4], Batch [360/938], Loss: 0.6790445446968079\n",
      "Train: Epoch [4], Batch [361/938], Loss: 0.8647556304931641\n",
      "Train: Epoch [4], Batch [362/938], Loss: 0.7824813723564148\n",
      "Train: Epoch [4], Batch [363/938], Loss: 0.9592386484146118\n",
      "Train: Epoch [4], Batch [364/938], Loss: 0.9275670051574707\n",
      "Train: Epoch [4], Batch [365/938], Loss: 0.6855978965759277\n",
      "Train: Epoch [4], Batch [366/938], Loss: 0.8892042636871338\n",
      "Train: Epoch [4], Batch [367/938], Loss: 0.7435907125473022\n",
      "Train: Epoch [4], Batch [368/938], Loss: 0.7244277000427246\n",
      "Train: Epoch [4], Batch [369/938], Loss: 0.8111573457717896\n",
      "Train: Epoch [4], Batch [370/938], Loss: 0.7196023464202881\n",
      "Train: Epoch [4], Batch [371/938], Loss: 0.7545419335365295\n",
      "Train: Epoch [4], Batch [372/938], Loss: 0.7280706167221069\n",
      "Train: Epoch [4], Batch [373/938], Loss: 0.735954999923706\n",
      "Train: Epoch [4], Batch [374/938], Loss: 0.46387240290641785\n",
      "Train: Epoch [4], Batch [375/938], Loss: 0.8524936437606812\n",
      "Train: Epoch [4], Batch [376/938], Loss: 0.9881923198699951\n",
      "Train: Epoch [4], Batch [377/938], Loss: 0.749377965927124\n",
      "Train: Epoch [4], Batch [378/938], Loss: 0.7227165699005127\n",
      "Train: Epoch [4], Batch [379/938], Loss: 0.7226104736328125\n",
      "Train: Epoch [4], Batch [380/938], Loss: 1.0232586860656738\n",
      "Train: Epoch [4], Batch [381/938], Loss: 0.8436092138290405\n",
      "Train: Epoch [4], Batch [382/938], Loss: 0.7163116931915283\n",
      "Train: Epoch [4], Batch [383/938], Loss: 0.6880600452423096\n",
      "Train: Epoch [4], Batch [384/938], Loss: 0.8226847648620605\n",
      "Train: Epoch [4], Batch [385/938], Loss: 0.7023442983627319\n",
      "Train: Epoch [4], Batch [386/938], Loss: 0.7341598272323608\n",
      "Train: Epoch [4], Batch [387/938], Loss: 0.7207919955253601\n",
      "Train: Epoch [4], Batch [388/938], Loss: 1.015096664428711\n",
      "Train: Epoch [4], Batch [389/938], Loss: 0.8649148941040039\n",
      "Train: Epoch [4], Batch [390/938], Loss: 0.727152407169342\n",
      "Train: Epoch [4], Batch [391/938], Loss: 0.6716005802154541\n",
      "Train: Epoch [4], Batch [392/938], Loss: 0.7210711240768433\n",
      "Train: Epoch [4], Batch [393/938], Loss: 0.6702141761779785\n",
      "Train: Epoch [4], Batch [394/938], Loss: 0.9333786368370056\n",
      "Train: Epoch [4], Batch [395/938], Loss: 0.7227300405502319\n",
      "Train: Epoch [4], Batch [396/938], Loss: 0.737808346748352\n",
      "Train: Epoch [4], Batch [397/938], Loss: 0.8057404160499573\n",
      "Train: Epoch [4], Batch [398/938], Loss: 0.8336518406867981\n",
      "Train: Epoch [4], Batch [399/938], Loss: 0.9220938682556152\n",
      "Train: Epoch [4], Batch [400/938], Loss: 0.6503424644470215\n",
      "Train: Epoch [4], Batch [401/938], Loss: 0.9890908002853394\n",
      "Train: Epoch [4], Batch [402/938], Loss: 0.7760430574417114\n",
      "Train: Epoch [4], Batch [403/938], Loss: 0.8409865498542786\n",
      "Train: Epoch [4], Batch [404/938], Loss: 0.766615629196167\n",
      "Train: Epoch [4], Batch [405/938], Loss: 0.5704323053359985\n",
      "Train: Epoch [4], Batch [406/938], Loss: 0.7467783689498901\n",
      "Train: Epoch [4], Batch [407/938], Loss: 0.6617775559425354\n",
      "Train: Epoch [4], Batch [408/938], Loss: 0.5934979319572449\n",
      "Train: Epoch [4], Batch [409/938], Loss: 0.6310694217681885\n",
      "Train: Epoch [4], Batch [410/938], Loss: 0.631746232509613\n",
      "Train: Epoch [4], Batch [411/938], Loss: 0.6842123866081238\n",
      "Train: Epoch [4], Batch [412/938], Loss: 0.7631679773330688\n",
      "Train: Epoch [4], Batch [413/938], Loss: 0.8607050180435181\n",
      "Train: Epoch [4], Batch [414/938], Loss: 0.8350855708122253\n",
      "Train: Epoch [4], Batch [415/938], Loss: 0.6115027666091919\n",
      "Train: Epoch [4], Batch [416/938], Loss: 1.117175579071045\n",
      "Train: Epoch [4], Batch [417/938], Loss: 0.6499433517456055\n",
      "Train: Epoch [4], Batch [418/938], Loss: 0.7636058330535889\n",
      "Train: Epoch [4], Batch [419/938], Loss: 0.7032097578048706\n",
      "Train: Epoch [4], Batch [420/938], Loss: 0.5227652192115784\n",
      "Train: Epoch [4], Batch [421/938], Loss: 0.670975923538208\n",
      "Train: Epoch [4], Batch [422/938], Loss: 0.751214325428009\n",
      "Train: Epoch [4], Batch [423/938], Loss: 0.6415201425552368\n",
      "Train: Epoch [4], Batch [424/938], Loss: 0.7320036888122559\n",
      "Train: Epoch [4], Batch [425/938], Loss: 0.7123553156852722\n",
      "Train: Epoch [4], Batch [426/938], Loss: 0.8242044448852539\n",
      "Train: Epoch [4], Batch [427/938], Loss: 0.755447268486023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [4], Batch [428/938], Loss: 0.8105090260505676\n",
      "Train: Epoch [4], Batch [429/938], Loss: 0.6934170722961426\n",
      "Train: Epoch [4], Batch [430/938], Loss: 0.6974532604217529\n",
      "Train: Epoch [4], Batch [431/938], Loss: 0.6583560705184937\n",
      "Train: Epoch [4], Batch [432/938], Loss: 0.7266990542411804\n",
      "Train: Epoch [4], Batch [433/938], Loss: 0.7445774078369141\n",
      "Train: Epoch [4], Batch [434/938], Loss: 0.5246989727020264\n",
      "Train: Epoch [4], Batch [435/938], Loss: 0.7468956708908081\n",
      "Train: Epoch [4], Batch [436/938], Loss: 0.6665939092636108\n",
      "Train: Epoch [4], Batch [437/938], Loss: 0.9819476008415222\n",
      "Train: Epoch [4], Batch [438/938], Loss: 0.7852306365966797\n",
      "Train: Epoch [4], Batch [439/938], Loss: 0.8505879640579224\n",
      "Train: Epoch [4], Batch [440/938], Loss: 0.6532174348831177\n",
      "Train: Epoch [4], Batch [441/938], Loss: 0.6970431804656982\n",
      "Train: Epoch [4], Batch [442/938], Loss: 0.7473678588867188\n",
      "Train: Epoch [4], Batch [443/938], Loss: 0.8533617258071899\n",
      "Train: Epoch [4], Batch [444/938], Loss: 0.9012980461120605\n",
      "Train: Epoch [4], Batch [445/938], Loss: 0.8379247188568115\n",
      "Train: Epoch [4], Batch [446/938], Loss: 0.6194645166397095\n",
      "Train: Epoch [4], Batch [447/938], Loss: 0.80296790599823\n",
      "Train: Epoch [4], Batch [448/938], Loss: 0.6912689208984375\n",
      "Train: Epoch [4], Batch [449/938], Loss: 0.7554020881652832\n",
      "Train: Epoch [4], Batch [450/938], Loss: 0.7606761455535889\n",
      "Train: Epoch [4], Batch [451/938], Loss: 0.548430323600769\n",
      "Train: Epoch [4], Batch [452/938], Loss: 0.7121752500534058\n",
      "Train: Epoch [4], Batch [453/938], Loss: 0.8451282978057861\n",
      "Train: Epoch [4], Batch [454/938], Loss: 0.5938437581062317\n",
      "Train: Epoch [4], Batch [455/938], Loss: 0.673491358757019\n",
      "Train: Epoch [4], Batch [456/938], Loss: 0.7330776453018188\n",
      "Train: Epoch [4], Batch [457/938], Loss: 0.627272367477417\n",
      "Train: Epoch [4], Batch [458/938], Loss: 0.7371037006378174\n",
      "Train: Epoch [4], Batch [459/938], Loss: 0.6282740831375122\n",
      "Train: Epoch [4], Batch [460/938], Loss: 0.656787633895874\n",
      "Train: Epoch [4], Batch [461/938], Loss: 0.8668367862701416\n",
      "Train: Epoch [4], Batch [462/938], Loss: 0.855498194694519\n",
      "Train: Epoch [4], Batch [463/938], Loss: 0.7157065868377686\n",
      "Train: Epoch [4], Batch [464/938], Loss: 0.7498431205749512\n",
      "Train: Epoch [4], Batch [465/938], Loss: 0.6370772123336792\n",
      "Train: Epoch [4], Batch [466/938], Loss: 0.5985572338104248\n",
      "Train: Epoch [4], Batch [467/938], Loss: 0.877260148525238\n",
      "Train: Epoch [4], Batch [468/938], Loss: 0.6118597984313965\n",
      "Train: Epoch [4], Batch [469/938], Loss: 0.6779698133468628\n",
      "Train: Epoch [4], Batch [470/938], Loss: 0.501440703868866\n",
      "Train: Epoch [4], Batch [471/938], Loss: 0.773847222328186\n",
      "Train: Epoch [4], Batch [472/938], Loss: 0.8108348250389099\n",
      "Train: Epoch [4], Batch [473/938], Loss: 0.6556328535079956\n",
      "Train: Epoch [4], Batch [474/938], Loss: 0.8014823198318481\n",
      "Train: Epoch [4], Batch [475/938], Loss: 0.6417291164398193\n",
      "Train: Epoch [4], Batch [476/938], Loss: 0.7849936485290527\n",
      "Train: Epoch [4], Batch [477/938], Loss: 0.8236327767372131\n",
      "Train: Epoch [4], Batch [478/938], Loss: 0.6456553936004639\n",
      "Train: Epoch [4], Batch [479/938], Loss: 0.8714841604232788\n",
      "Train: Epoch [4], Batch [480/938], Loss: 0.8042296171188354\n",
      "Train: Epoch [4], Batch [481/938], Loss: 0.7753706574440002\n",
      "Train: Epoch [4], Batch [482/938], Loss: 0.840010404586792\n",
      "Train: Epoch [4], Batch [483/938], Loss: 0.681437611579895\n",
      "Train: Epoch [4], Batch [484/938], Loss: 0.8047786951065063\n",
      "Train: Epoch [4], Batch [485/938], Loss: 0.8184261322021484\n",
      "Train: Epoch [4], Batch [486/938], Loss: 0.7391462326049805\n",
      "Train: Epoch [4], Batch [487/938], Loss: 0.7228618264198303\n",
      "Train: Epoch [4], Batch [488/938], Loss: 0.8270069360733032\n",
      "Train: Epoch [4], Batch [489/938], Loss: 0.6764634847640991\n",
      "Train: Epoch [4], Batch [490/938], Loss: 0.816272497177124\n",
      "Train: Epoch [4], Batch [491/938], Loss: 0.8792065382003784\n",
      "Train: Epoch [4], Batch [492/938], Loss: 0.8194591999053955\n",
      "Train: Epoch [4], Batch [493/938], Loss: 0.8793118000030518\n",
      "Train: Epoch [4], Batch [494/938], Loss: 0.8089245557785034\n",
      "Train: Epoch [4], Batch [495/938], Loss: 0.6846528649330139\n",
      "Train: Epoch [4], Batch [496/938], Loss: 0.695548415184021\n",
      "Train: Epoch [4], Batch [497/938], Loss: 0.778974711894989\n",
      "Train: Epoch [4], Batch [498/938], Loss: 0.7565068602561951\n",
      "Train: Epoch [4], Batch [499/938], Loss: 0.678624153137207\n",
      "Train: Epoch [4], Batch [500/938], Loss: 0.8895592093467712\n",
      "Train: Epoch [4], Batch [501/938], Loss: 0.7842053771018982\n",
      "Train: Epoch [4], Batch [502/938], Loss: 0.878312349319458\n",
      "Train: Epoch [4], Batch [503/938], Loss: 0.916297435760498\n",
      "Train: Epoch [4], Batch [504/938], Loss: 0.920580267906189\n",
      "Train: Epoch [4], Batch [505/938], Loss: 0.7132447957992554\n",
      "Train: Epoch [4], Batch [506/938], Loss: 0.8408920168876648\n",
      "Train: Epoch [4], Batch [507/938], Loss: 0.7971125841140747\n",
      "Train: Epoch [4], Batch [508/938], Loss: 0.7601192593574524\n",
      "Train: Epoch [4], Batch [509/938], Loss: 0.6170611381530762\n",
      "Train: Epoch [4], Batch [510/938], Loss: 0.9556158781051636\n",
      "Train: Epoch [4], Batch [511/938], Loss: 0.6898853182792664\n",
      "Train: Epoch [4], Batch [512/938], Loss: 1.1300501823425293\n",
      "Train: Epoch [4], Batch [513/938], Loss: 0.7548781633377075\n",
      "Train: Epoch [4], Batch [514/938], Loss: 0.8538196682929993\n",
      "Train: Epoch [4], Batch [515/938], Loss: 0.8386802673339844\n",
      "Train: Epoch [4], Batch [516/938], Loss: 0.728212296962738\n",
      "Train: Epoch [4], Batch [517/938], Loss: 0.6983428597450256\n",
      "Train: Epoch [4], Batch [518/938], Loss: 0.7965705394744873\n",
      "Train: Epoch [4], Batch [519/938], Loss: 0.6920943260192871\n",
      "Train: Epoch [4], Batch [520/938], Loss: 0.5994894504547119\n",
      "Train: Epoch [4], Batch [521/938], Loss: 0.5650217533111572\n",
      "Train: Epoch [4], Batch [522/938], Loss: 0.8286337852478027\n",
      "Train: Epoch [4], Batch [523/938], Loss: 0.8557173013687134\n",
      "Train: Epoch [4], Batch [524/938], Loss: 0.790888249874115\n",
      "Train: Epoch [4], Batch [525/938], Loss: 0.6302554607391357\n",
      "Train: Epoch [4], Batch [526/938], Loss: 0.7515944242477417\n",
      "Train: Epoch [4], Batch [527/938], Loss: 0.9008987545967102\n",
      "Train: Epoch [4], Batch [528/938], Loss: 0.7101560831069946\n",
      "Train: Epoch [4], Batch [529/938], Loss: 0.9911667108535767\n",
      "Train: Epoch [4], Batch [530/938], Loss: 0.7792806625366211\n",
      "Train: Epoch [4], Batch [531/938], Loss: 0.7545561790466309\n",
      "Train: Epoch [4], Batch [532/938], Loss: 0.8289730548858643\n",
      "Train: Epoch [4], Batch [533/938], Loss: 0.9317618012428284\n",
      "Train: Epoch [4], Batch [534/938], Loss: 0.8188521862030029\n",
      "Train: Epoch [4], Batch [535/938], Loss: 0.8677252531051636\n",
      "Train: Epoch [4], Batch [536/938], Loss: 0.7416514158248901\n",
      "Train: Epoch [4], Batch [537/938], Loss: 0.7510864734649658\n",
      "Train: Epoch [4], Batch [538/938], Loss: 0.8035150766372681\n",
      "Train: Epoch [4], Batch [539/938], Loss: 0.6835289001464844\n",
      "Train: Epoch [4], Batch [540/938], Loss: 0.6317113637924194\n",
      "Train: Epoch [4], Batch [541/938], Loss: 0.8127221465110779\n",
      "Train: Epoch [4], Batch [542/938], Loss: 0.7451950311660767\n",
      "Train: Epoch [4], Batch [543/938], Loss: 0.7005902528762817\n",
      "Train: Epoch [4], Batch [544/938], Loss: 0.7015970945358276\n",
      "Train: Epoch [4], Batch [545/938], Loss: 0.5413722395896912\n",
      "Train: Epoch [4], Batch [546/938], Loss: 0.7179606556892395\n",
      "Train: Epoch [4], Batch [547/938], Loss: 0.8081011772155762\n",
      "Train: Epoch [4], Batch [548/938], Loss: 0.656734049320221\n",
      "Train: Epoch [4], Batch [549/938], Loss: 0.7602798342704773\n",
      "Train: Epoch [4], Batch [550/938], Loss: 0.7099262475967407\n",
      "Train: Epoch [4], Batch [551/938], Loss: 0.8390151262283325\n",
      "Train: Epoch [4], Batch [552/938], Loss: 0.8204922676086426\n",
      "Train: Epoch [4], Batch [553/938], Loss: 0.7521803379058838\n",
      "Train: Epoch [4], Batch [554/938], Loss: 0.882380485534668\n",
      "Train: Epoch [4], Batch [555/938], Loss: 0.6286762952804565\n",
      "Train: Epoch [4], Batch [556/938], Loss: 0.6927540302276611\n",
      "Train: Epoch [4], Batch [557/938], Loss: 0.826977550983429\n",
      "Train: Epoch [4], Batch [558/938], Loss: 0.6780937910079956\n",
      "Train: Epoch [4], Batch [559/938], Loss: 0.7045626640319824\n",
      "Train: Epoch [4], Batch [560/938], Loss: 0.6169877052307129\n",
      "Train: Epoch [4], Batch [561/938], Loss: 0.6891317367553711\n",
      "Train: Epoch [4], Batch [562/938], Loss: 0.7912722229957581\n",
      "Train: Epoch [4], Batch [563/938], Loss: 0.6117087602615356\n",
      "Train: Epoch [4], Batch [564/938], Loss: 0.6281951069831848\n",
      "Train: Epoch [4], Batch [565/938], Loss: 0.6749611496925354\n",
      "Train: Epoch [4], Batch [566/938], Loss: 0.5791325569152832\n",
      "Train: Epoch [4], Batch [567/938], Loss: 0.784827470779419\n",
      "Train: Epoch [4], Batch [568/938], Loss: 0.6998211145401001\n",
      "Train: Epoch [4], Batch [569/938], Loss: 0.8874703645706177\n",
      "Train: Epoch [4], Batch [570/938], Loss: 0.8310728073120117\n",
      "Train: Epoch [4], Batch [571/938], Loss: 0.6113752126693726\n",
      "Train: Epoch [4], Batch [572/938], Loss: 0.713371992111206\n",
      "Train: Epoch [4], Batch [573/938], Loss: 0.8619509339332581\n",
      "Train: Epoch [4], Batch [574/938], Loss: 0.8065494298934937\n",
      "Train: Epoch [4], Batch [575/938], Loss: 0.7375310659408569\n",
      "Train: Epoch [4], Batch [576/938], Loss: 0.6540907025337219\n",
      "Train: Epoch [4], Batch [577/938], Loss: 0.9307385683059692\n",
      "Train: Epoch [4], Batch [578/938], Loss: 0.6645036339759827\n",
      "Train: Epoch [4], Batch [579/938], Loss: 0.9269648790359497\n",
      "Train: Epoch [4], Batch [580/938], Loss: 0.750194787979126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [4], Batch [581/938], Loss: 0.748223602771759\n",
      "Train: Epoch [4], Batch [582/938], Loss: 0.936604917049408\n",
      "Train: Epoch [4], Batch [583/938], Loss: 0.6116925477981567\n",
      "Train: Epoch [4], Batch [584/938], Loss: 1.0206493139266968\n",
      "Train: Epoch [4], Batch [585/938], Loss: 0.796630859375\n",
      "Train: Epoch [4], Batch [586/938], Loss: 0.6866129636764526\n",
      "Train: Epoch [4], Batch [587/938], Loss: 0.7097519636154175\n",
      "Train: Epoch [4], Batch [588/938], Loss: 0.8461757898330688\n",
      "Train: Epoch [4], Batch [589/938], Loss: 0.947611927986145\n",
      "Train: Epoch [4], Batch [590/938], Loss: 0.6600197553634644\n",
      "Train: Epoch [4], Batch [591/938], Loss: 0.608102560043335\n",
      "Train: Epoch [4], Batch [592/938], Loss: 0.7224199175834656\n",
      "Train: Epoch [4], Batch [593/938], Loss: 1.0445729494094849\n",
      "Train: Epoch [4], Batch [594/938], Loss: 0.8321744203567505\n",
      "Train: Epoch [4], Batch [595/938], Loss: 0.7167452573776245\n",
      "Train: Epoch [4], Batch [596/938], Loss: 0.8513569831848145\n",
      "Train: Epoch [4], Batch [597/938], Loss: 0.7556400895118713\n",
      "Train: Epoch [4], Batch [598/938], Loss: 0.858123779296875\n",
      "Train: Epoch [4], Batch [599/938], Loss: 0.7308499813079834\n",
      "Train: Epoch [4], Batch [600/938], Loss: 0.8299481868743896\n",
      "Train: Epoch [4], Batch [601/938], Loss: 0.7078244686126709\n",
      "Train: Epoch [4], Batch [602/938], Loss: 0.5974729061126709\n",
      "Train: Epoch [4], Batch [603/938], Loss: 0.831947922706604\n",
      "Train: Epoch [4], Batch [604/938], Loss: 0.7297104597091675\n",
      "Train: Epoch [4], Batch [605/938], Loss: 0.5316309928894043\n",
      "Train: Epoch [4], Batch [606/938], Loss: 0.7044720649719238\n",
      "Train: Epoch [4], Batch [607/938], Loss: 0.6799876689910889\n",
      "Train: Epoch [4], Batch [608/938], Loss: 0.8337994813919067\n",
      "Train: Epoch [4], Batch [609/938], Loss: 0.774442195892334\n",
      "Train: Epoch [4], Batch [610/938], Loss: 0.8275258541107178\n",
      "Train: Epoch [4], Batch [611/938], Loss: 1.119356632232666\n",
      "Train: Epoch [4], Batch [612/938], Loss: 0.804241955280304\n",
      "Train: Epoch [4], Batch [613/938], Loss: 0.8857272863388062\n",
      "Train: Epoch [4], Batch [614/938], Loss: 0.7712318897247314\n",
      "Train: Epoch [4], Batch [615/938], Loss: 0.7212985754013062\n",
      "Train: Epoch [4], Batch [616/938], Loss: 0.8418017625808716\n",
      "Train: Epoch [4], Batch [617/938], Loss: 0.6789210438728333\n",
      "Train: Epoch [4], Batch [618/938], Loss: 0.7204958200454712\n",
      "Train: Epoch [4], Batch [619/938], Loss: 0.5829263925552368\n",
      "Train: Epoch [4], Batch [620/938], Loss: 0.7301198244094849\n",
      "Train: Epoch [4], Batch [621/938], Loss: 0.8009011745452881\n",
      "Train: Epoch [4], Batch [622/938], Loss: 0.8561733961105347\n",
      "Train: Epoch [4], Batch [623/938], Loss: 0.8092772960662842\n",
      "Train: Epoch [4], Batch [624/938], Loss: 0.6654433012008667\n",
      "Train: Epoch [4], Batch [625/938], Loss: 0.6719146966934204\n",
      "Train: Epoch [4], Batch [626/938], Loss: 0.850631058216095\n",
      "Train: Epoch [4], Batch [627/938], Loss: 0.7541418075561523\n",
      "Train: Epoch [4], Batch [628/938], Loss: 0.8142406344413757\n",
      "Train: Epoch [4], Batch [629/938], Loss: 0.7503284215927124\n",
      "Train: Epoch [4], Batch [630/938], Loss: 0.6876981258392334\n",
      "Train: Epoch [4], Batch [631/938], Loss: 0.884919285774231\n",
      "Train: Epoch [4], Batch [632/938], Loss: 0.8418912887573242\n",
      "Train: Epoch [4], Batch [633/938], Loss: 0.6915581822395325\n",
      "Train: Epoch [4], Batch [634/938], Loss: 0.8166451454162598\n",
      "Train: Epoch [4], Batch [635/938], Loss: 0.9285848140716553\n",
      "Train: Epoch [4], Batch [636/938], Loss: 0.7444133758544922\n",
      "Train: Epoch [4], Batch [637/938], Loss: 0.6589548587799072\n",
      "Train: Epoch [4], Batch [638/938], Loss: 0.5877052545547485\n",
      "Train: Epoch [4], Batch [639/938], Loss: 0.7285614013671875\n",
      "Train: Epoch [4], Batch [640/938], Loss: 0.7854516506195068\n",
      "Train: Epoch [4], Batch [641/938], Loss: 0.8463081121444702\n",
      "Train: Epoch [4], Batch [642/938], Loss: 0.674324095249176\n",
      "Train: Epoch [4], Batch [643/938], Loss: 0.7683731317520142\n",
      "Train: Epoch [4], Batch [644/938], Loss: 0.7392289042472839\n",
      "Train: Epoch [4], Batch [645/938], Loss: 0.8066585659980774\n",
      "Train: Epoch [4], Batch [646/938], Loss: 0.7687944173812866\n",
      "Train: Epoch [4], Batch [647/938], Loss: 0.5643315315246582\n",
      "Train: Epoch [4], Batch [648/938], Loss: 0.938755989074707\n",
      "Train: Epoch [4], Batch [649/938], Loss: 0.7854059338569641\n",
      "Train: Epoch [4], Batch [650/938], Loss: 0.8340528011322021\n",
      "Train: Epoch [4], Batch [651/938], Loss: 0.6308004856109619\n",
      "Train: Epoch [4], Batch [652/938], Loss: 0.699881911277771\n",
      "Train: Epoch [4], Batch [653/938], Loss: 0.7666252851486206\n",
      "Train: Epoch [4], Batch [654/938], Loss: 0.7989271879196167\n",
      "Train: Epoch [4], Batch [655/938], Loss: 0.6039354205131531\n",
      "Train: Epoch [4], Batch [656/938], Loss: 0.7726137638092041\n",
      "Train: Epoch [4], Batch [657/938], Loss: 0.7956141829490662\n",
      "Train: Epoch [4], Batch [658/938], Loss: 0.7334251403808594\n",
      "Train: Epoch [4], Batch [659/938], Loss: 0.8832063674926758\n",
      "Train: Epoch [4], Batch [660/938], Loss: 0.5744632482528687\n",
      "Train: Epoch [4], Batch [661/938], Loss: 1.013251543045044\n",
      "Train: Epoch [4], Batch [662/938], Loss: 0.5371756553649902\n",
      "Train: Epoch [4], Batch [663/938], Loss: 0.6744480729103088\n",
      "Train: Epoch [4], Batch [664/938], Loss: 0.6198630332946777\n",
      "Train: Epoch [4], Batch [665/938], Loss: 0.6351272463798523\n",
      "Train: Epoch [4], Batch [666/938], Loss: 0.6969582438468933\n",
      "Train: Epoch [4], Batch [667/938], Loss: 0.7761926651000977\n",
      "Train: Epoch [4], Batch [668/938], Loss: 0.7335024476051331\n",
      "Train: Epoch [4], Batch [669/938], Loss: 0.6960488557815552\n",
      "Train: Epoch [4], Batch [670/938], Loss: 0.5771539211273193\n",
      "Train: Epoch [4], Batch [671/938], Loss: 0.7276840209960938\n",
      "Train: Epoch [4], Batch [672/938], Loss: 0.7463233470916748\n",
      "Train: Epoch [4], Batch [673/938], Loss: 0.6585599780082703\n",
      "Train: Epoch [4], Batch [674/938], Loss: 0.9527556896209717\n",
      "Train: Epoch [4], Batch [675/938], Loss: 0.7893356680870056\n",
      "Train: Epoch [4], Batch [676/938], Loss: 0.7440632581710815\n",
      "Train: Epoch [4], Batch [677/938], Loss: 0.797166109085083\n",
      "Train: Epoch [4], Batch [678/938], Loss: 0.8496203422546387\n",
      "Train: Epoch [4], Batch [679/938], Loss: 0.7973346710205078\n",
      "Train: Epoch [4], Batch [680/938], Loss: 0.8530120253562927\n",
      "Train: Epoch [4], Batch [681/938], Loss: 0.6432665586471558\n",
      "Train: Epoch [4], Batch [682/938], Loss: 0.7360552549362183\n",
      "Train: Epoch [4], Batch [683/938], Loss: 0.6444478631019592\n",
      "Train: Epoch [4], Batch [684/938], Loss: 0.878250241279602\n",
      "Train: Epoch [4], Batch [685/938], Loss: 0.6766992807388306\n",
      "Train: Epoch [4], Batch [686/938], Loss: 0.9592640399932861\n",
      "Train: Epoch [4], Batch [687/938], Loss: 0.567652702331543\n",
      "Train: Epoch [4], Batch [688/938], Loss: 0.7298173904418945\n",
      "Train: Epoch [4], Batch [689/938], Loss: 0.6595520377159119\n",
      "Train: Epoch [4], Batch [690/938], Loss: 0.7500171065330505\n",
      "Train: Epoch [4], Batch [691/938], Loss: 0.7249804139137268\n",
      "Train: Epoch [4], Batch [692/938], Loss: 0.7342159748077393\n",
      "Train: Epoch [4], Batch [693/938], Loss: 0.6271359920501709\n",
      "Train: Epoch [4], Batch [694/938], Loss: 0.8324155211448669\n",
      "Train: Epoch [4], Batch [695/938], Loss: 0.7625573873519897\n",
      "Train: Epoch [4], Batch [696/938], Loss: 0.689196765422821\n",
      "Train: Epoch [4], Batch [697/938], Loss: 0.8739006519317627\n",
      "Train: Epoch [4], Batch [698/938], Loss: 0.8216301202774048\n",
      "Train: Epoch [4], Batch [699/938], Loss: 0.6817241311073303\n",
      "Train: Epoch [4], Batch [700/938], Loss: 0.7585828304290771\n",
      "Train: Epoch [4], Batch [701/938], Loss: 0.6341565251350403\n",
      "Train: Epoch [4], Batch [702/938], Loss: 0.7485554814338684\n",
      "Train: Epoch [4], Batch [703/938], Loss: 0.6129799485206604\n",
      "Train: Epoch [4], Batch [704/938], Loss: 0.5310337543487549\n",
      "Train: Epoch [4], Batch [705/938], Loss: 0.6832375526428223\n",
      "Train: Epoch [4], Batch [706/938], Loss: 0.870296835899353\n",
      "Train: Epoch [4], Batch [707/938], Loss: 0.7591462135314941\n",
      "Train: Epoch [4], Batch [708/938], Loss: 0.8669302463531494\n",
      "Train: Epoch [4], Batch [709/938], Loss: 0.6964206099510193\n",
      "Train: Epoch [4], Batch [710/938], Loss: 0.9371016025543213\n",
      "Train: Epoch [4], Batch [711/938], Loss: 0.6805058121681213\n",
      "Train: Epoch [4], Batch [712/938], Loss: 0.645073652267456\n",
      "Train: Epoch [4], Batch [713/938], Loss: 0.6345640420913696\n",
      "Train: Epoch [4], Batch [714/938], Loss: 0.6233611106872559\n",
      "Train: Epoch [4], Batch [715/938], Loss: 0.6585757732391357\n",
      "Train: Epoch [4], Batch [716/938], Loss: 0.7869389057159424\n",
      "Train: Epoch [4], Batch [717/938], Loss: 0.8168526291847229\n",
      "Train: Epoch [4], Batch [718/938], Loss: 0.60296231508255\n",
      "Train: Epoch [4], Batch [719/938], Loss: 0.8383407592773438\n",
      "Train: Epoch [4], Batch [720/938], Loss: 0.9133678674697876\n",
      "Train: Epoch [4], Batch [721/938], Loss: 0.7527634501457214\n",
      "Train: Epoch [4], Batch [722/938], Loss: 0.7996697425842285\n",
      "Train: Epoch [4], Batch [723/938], Loss: 0.8663672804832458\n",
      "Train: Epoch [4], Batch [724/938], Loss: 0.5582817196846008\n",
      "Train: Epoch [4], Batch [725/938], Loss: 0.6954365372657776\n",
      "Train: Epoch [4], Batch [726/938], Loss: 0.867935836315155\n",
      "Train: Epoch [4], Batch [727/938], Loss: 0.8934388160705566\n",
      "Train: Epoch [4], Batch [728/938], Loss: 0.6041028499603271\n",
      "Train: Epoch [4], Batch [729/938], Loss: 0.6600916385650635\n",
      "Train: Epoch [4], Batch [730/938], Loss: 0.8297293782234192\n",
      "Train: Epoch [4], Batch [731/938], Loss: 0.830992579460144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [4], Batch [732/938], Loss: 0.62811279296875\n",
      "Train: Epoch [4], Batch [733/938], Loss: 0.9463316202163696\n",
      "Train: Epoch [4], Batch [734/938], Loss: 0.6709719896316528\n",
      "Train: Epoch [4], Batch [735/938], Loss: 0.6872334480285645\n",
      "Train: Epoch [4], Batch [736/938], Loss: 0.9688111543655396\n",
      "Train: Epoch [4], Batch [737/938], Loss: 0.7983617782592773\n",
      "Train: Epoch [4], Batch [738/938], Loss: 0.9074803590774536\n",
      "Train: Epoch [4], Batch [739/938], Loss: 0.9524222016334534\n",
      "Train: Epoch [4], Batch [740/938], Loss: 0.9365837574005127\n",
      "Train: Epoch [4], Batch [741/938], Loss: 0.6927517056465149\n",
      "Train: Epoch [4], Batch [742/938], Loss: 0.7487252950668335\n",
      "Train: Epoch [4], Batch [743/938], Loss: 0.8275688886642456\n",
      "Train: Epoch [4], Batch [744/938], Loss: 0.670378565788269\n",
      "Train: Epoch [4], Batch [745/938], Loss: 0.5879607796669006\n",
      "Train: Epoch [4], Batch [746/938], Loss: 0.6354771256446838\n",
      "Train: Epoch [4], Batch [747/938], Loss: 0.6315035223960876\n",
      "Train: Epoch [4], Batch [748/938], Loss: 0.8115754127502441\n",
      "Train: Epoch [4], Batch [749/938], Loss: 0.6323527693748474\n",
      "Train: Epoch [4], Batch [750/938], Loss: 0.5875580310821533\n",
      "Train: Epoch [4], Batch [751/938], Loss: 0.793735921382904\n",
      "Train: Epoch [4], Batch [752/938], Loss: 0.6593015789985657\n",
      "Train: Epoch [4], Batch [753/938], Loss: 0.701995849609375\n",
      "Train: Epoch [4], Batch [754/938], Loss: 0.7695215940475464\n",
      "Train: Epoch [4], Batch [755/938], Loss: 0.8417973518371582\n",
      "Train: Epoch [4], Batch [756/938], Loss: 0.7051316499710083\n",
      "Train: Epoch [4], Batch [757/938], Loss: 0.5821065902709961\n",
      "Train: Epoch [4], Batch [758/938], Loss: 0.8025223016738892\n",
      "Train: Epoch [4], Batch [759/938], Loss: 0.7176899909973145\n",
      "Train: Epoch [4], Batch [760/938], Loss: 0.6738681793212891\n",
      "Train: Epoch [4], Batch [761/938], Loss: 0.7397241592407227\n",
      "Train: Epoch [4], Batch [762/938], Loss: 0.665381908416748\n",
      "Train: Epoch [4], Batch [763/938], Loss: 0.6826027631759644\n",
      "Train: Epoch [4], Batch [764/938], Loss: 1.016563057899475\n",
      "Train: Epoch [4], Batch [765/938], Loss: 0.8222317695617676\n",
      "Train: Epoch [4], Batch [766/938], Loss: 1.0024150609970093\n",
      "Train: Epoch [4], Batch [767/938], Loss: 0.6802728176116943\n",
      "Train: Epoch [4], Batch [768/938], Loss: 0.7000479102134705\n",
      "Train: Epoch [4], Batch [769/938], Loss: 0.6748867034912109\n",
      "Train: Epoch [4], Batch [770/938], Loss: 0.6304646134376526\n",
      "Train: Epoch [4], Batch [771/938], Loss: 0.6384477615356445\n",
      "Train: Epoch [4], Batch [772/938], Loss: 0.6943527460098267\n",
      "Train: Epoch [4], Batch [773/938], Loss: 0.8606083393096924\n",
      "Train: Epoch [4], Batch [774/938], Loss: 0.7392973303794861\n",
      "Train: Epoch [4], Batch [775/938], Loss: 0.6681267619132996\n",
      "Train: Epoch [4], Batch [776/938], Loss: 0.6918637752532959\n",
      "Train: Epoch [4], Batch [777/938], Loss: 0.688004732131958\n",
      "Train: Epoch [4], Batch [778/938], Loss: 0.6130680441856384\n",
      "Train: Epoch [4], Batch [779/938], Loss: 0.6751497983932495\n",
      "Train: Epoch [4], Batch [780/938], Loss: 0.5543045997619629\n",
      "Train: Epoch [4], Batch [781/938], Loss: 0.8235349655151367\n",
      "Train: Epoch [4], Batch [782/938], Loss: 0.6569520235061646\n",
      "Train: Epoch [4], Batch [783/938], Loss: 0.7557885646820068\n",
      "Train: Epoch [4], Batch [784/938], Loss: 0.7387474775314331\n",
      "Train: Epoch [4], Batch [785/938], Loss: 0.7573856115341187\n",
      "Train: Epoch [4], Batch [786/938], Loss: 0.779910147190094\n",
      "Train: Epoch [4], Batch [787/938], Loss: 0.6785973906517029\n",
      "Train: Epoch [4], Batch [788/938], Loss: 0.705848217010498\n",
      "Train: Epoch [4], Batch [789/938], Loss: 0.6261372566223145\n",
      "Train: Epoch [4], Batch [790/938], Loss: 0.6906750202178955\n",
      "Train: Epoch [4], Batch [791/938], Loss: 0.6423197388648987\n",
      "Train: Epoch [4], Batch [792/938], Loss: 0.6279316544532776\n",
      "Train: Epoch [4], Batch [793/938], Loss: 0.4963952898979187\n",
      "Train: Epoch [4], Batch [794/938], Loss: 0.8784269094467163\n",
      "Train: Epoch [4], Batch [795/938], Loss: 0.7078237533569336\n",
      "Train: Epoch [4], Batch [796/938], Loss: 0.8247485160827637\n",
      "Train: Epoch [4], Batch [797/938], Loss: 0.8251277208328247\n",
      "Train: Epoch [4], Batch [798/938], Loss: 0.7922044992446899\n",
      "Train: Epoch [4], Batch [799/938], Loss: 0.8310909271240234\n",
      "Train: Epoch [4], Batch [800/938], Loss: 0.88173508644104\n",
      "Train: Epoch [4], Batch [801/938], Loss: 0.5976132750511169\n",
      "Train: Epoch [4], Batch [802/938], Loss: 0.8535865545272827\n",
      "Train: Epoch [4], Batch [803/938], Loss: 0.8206124305725098\n",
      "Train: Epoch [4], Batch [804/938], Loss: 0.8420437574386597\n",
      "Train: Epoch [4], Batch [805/938], Loss: 0.7332247495651245\n",
      "Train: Epoch [4], Batch [806/938], Loss: 0.6883561015129089\n",
      "Train: Epoch [4], Batch [807/938], Loss: 0.770572304725647\n",
      "Train: Epoch [4], Batch [808/938], Loss: 0.8336443901062012\n",
      "Train: Epoch [4], Batch [809/938], Loss: 0.6868016719818115\n",
      "Train: Epoch [4], Batch [810/938], Loss: 0.6417624950408936\n",
      "Train: Epoch [4], Batch [811/938], Loss: 0.7576335668563843\n",
      "Train: Epoch [4], Batch [812/938], Loss: 0.9387910962104797\n",
      "Train: Epoch [4], Batch [813/938], Loss: 0.626638650894165\n",
      "Train: Epoch [4], Batch [814/938], Loss: 0.7356694340705872\n",
      "Train: Epoch [4], Batch [815/938], Loss: 1.0197093486785889\n",
      "Train: Epoch [4], Batch [816/938], Loss: 0.9194062948226929\n",
      "Train: Epoch [4], Batch [817/938], Loss: 0.706891655921936\n",
      "Train: Epoch [4], Batch [818/938], Loss: 0.6854760646820068\n",
      "Train: Epoch [4], Batch [819/938], Loss: 0.525102972984314\n",
      "Train: Epoch [4], Batch [820/938], Loss: 0.6521730422973633\n",
      "Train: Epoch [4], Batch [821/938], Loss: 0.7641647458076477\n",
      "Train: Epoch [4], Batch [822/938], Loss: 0.770798921585083\n",
      "Train: Epoch [4], Batch [823/938], Loss: 0.8854216933250427\n",
      "Train: Epoch [4], Batch [824/938], Loss: 0.7146236896514893\n",
      "Train: Epoch [4], Batch [825/938], Loss: 0.854280948638916\n",
      "Train: Epoch [4], Batch [826/938], Loss: 0.6865127086639404\n",
      "Train: Epoch [4], Batch [827/938], Loss: 1.0973966121673584\n",
      "Train: Epoch [4], Batch [828/938], Loss: 0.7244371175765991\n",
      "Train: Epoch [4], Batch [829/938], Loss: 0.7539721727371216\n",
      "Train: Epoch [4], Batch [830/938], Loss: 0.7079228758811951\n",
      "Train: Epoch [4], Batch [831/938], Loss: 0.5305079817771912\n",
      "Train: Epoch [4], Batch [832/938], Loss: 0.8647910356521606\n",
      "Train: Epoch [4], Batch [833/938], Loss: 0.7049967050552368\n",
      "Train: Epoch [4], Batch [834/938], Loss: 0.6903523206710815\n",
      "Train: Epoch [4], Batch [835/938], Loss: 0.7858063578605652\n",
      "Train: Epoch [4], Batch [836/938], Loss: 0.8673644065856934\n",
      "Train: Epoch [4], Batch [837/938], Loss: 0.7520184516906738\n",
      "Train: Epoch [4], Batch [838/938], Loss: 0.6330540180206299\n",
      "Train: Epoch [4], Batch [839/938], Loss: 0.5481300950050354\n",
      "Train: Epoch [4], Batch [840/938], Loss: 0.8363813161849976\n",
      "Train: Epoch [4], Batch [841/938], Loss: 0.8071366548538208\n",
      "Train: Epoch [4], Batch [842/938], Loss: 0.7561166286468506\n",
      "Train: Epoch [4], Batch [843/938], Loss: 0.8063284158706665\n",
      "Train: Epoch [4], Batch [844/938], Loss: 0.8470892906188965\n",
      "Train: Epoch [4], Batch [845/938], Loss: 0.7797050476074219\n",
      "Train: Epoch [4], Batch [846/938], Loss: 0.7747097611427307\n",
      "Train: Epoch [4], Batch [847/938], Loss: 0.8856934309005737\n",
      "Train: Epoch [4], Batch [848/938], Loss: 0.7862986326217651\n",
      "Train: Epoch [4], Batch [849/938], Loss: 0.7885334491729736\n",
      "Train: Epoch [4], Batch [850/938], Loss: 0.8044065237045288\n",
      "Train: Epoch [4], Batch [851/938], Loss: 0.9545947313308716\n",
      "Train: Epoch [4], Batch [852/938], Loss: 0.6107123494148254\n",
      "Train: Epoch [4], Batch [853/938], Loss: 0.5973808765411377\n",
      "Train: Epoch [4], Batch [854/938], Loss: 0.8375222682952881\n",
      "Train: Epoch [4], Batch [855/938], Loss: 0.7595176100730896\n",
      "Train: Epoch [4], Batch [856/938], Loss: 0.8941527009010315\n",
      "Train: Epoch [4], Batch [857/938], Loss: 0.7346818447113037\n",
      "Train: Epoch [4], Batch [858/938], Loss: 0.8821156620979309\n",
      "Train: Epoch [4], Batch [859/938], Loss: 0.66096031665802\n",
      "Train: Epoch [4], Batch [860/938], Loss: 0.6062195301055908\n",
      "Train: Epoch [4], Batch [861/938], Loss: 0.5392976999282837\n",
      "Train: Epoch [4], Batch [862/938], Loss: 0.8432236909866333\n",
      "Train: Epoch [4], Batch [863/938], Loss: 0.7146948575973511\n",
      "Train: Epoch [4], Batch [864/938], Loss: 0.7178055047988892\n",
      "Train: Epoch [4], Batch [865/938], Loss: 0.7626158595085144\n",
      "Train: Epoch [4], Batch [866/938], Loss: 0.7794795036315918\n",
      "Train: Epoch [4], Batch [867/938], Loss: 0.9391078352928162\n",
      "Train: Epoch [4], Batch [868/938], Loss: 0.8118285536766052\n",
      "Train: Epoch [4], Batch [869/938], Loss: 0.4855833649635315\n",
      "Train: Epoch [4], Batch [870/938], Loss: 0.8470401167869568\n",
      "Train: Epoch [4], Batch [871/938], Loss: 0.8539189696311951\n",
      "Train: Epoch [4], Batch [872/938], Loss: 0.6669853329658508\n",
      "Train: Epoch [4], Batch [873/938], Loss: 0.731797456741333\n",
      "Train: Epoch [4], Batch [874/938], Loss: 0.8780272006988525\n",
      "Train: Epoch [4], Batch [875/938], Loss: 0.7041438817977905\n",
      "Train: Epoch [4], Batch [876/938], Loss: 0.672552227973938\n",
      "Train: Epoch [4], Batch [877/938], Loss: 0.5219022035598755\n",
      "Train: Epoch [4], Batch [878/938], Loss: 0.9189229011535645\n",
      "Train: Epoch [4], Batch [879/938], Loss: 0.8233570456504822\n",
      "Train: Epoch [4], Batch [880/938], Loss: 0.6271026730537415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [4], Batch [881/938], Loss: 0.6109975576400757\n",
      "Train: Epoch [4], Batch [882/938], Loss: 0.7800042629241943\n",
      "Train: Epoch [4], Batch [883/938], Loss: 0.6186721324920654\n",
      "Train: Epoch [4], Batch [884/938], Loss: 0.6662304401397705\n",
      "Train: Epoch [4], Batch [885/938], Loss: 0.6782329678535461\n",
      "Train: Epoch [4], Batch [886/938], Loss: 1.0085492134094238\n",
      "Train: Epoch [4], Batch [887/938], Loss: 0.7529006004333496\n",
      "Train: Epoch [4], Batch [888/938], Loss: 0.6896929740905762\n",
      "Train: Epoch [4], Batch [889/938], Loss: 0.7621258497238159\n",
      "Train: Epoch [4], Batch [890/938], Loss: 0.5353545546531677\n",
      "Train: Epoch [4], Batch [891/938], Loss: 0.644361138343811\n",
      "Train: Epoch [4], Batch [892/938], Loss: 0.892607569694519\n",
      "Train: Epoch [4], Batch [893/938], Loss: 0.6234089136123657\n",
      "Train: Epoch [4], Batch [894/938], Loss: 0.5781304836273193\n",
      "Train: Epoch [4], Batch [895/938], Loss: 0.783532977104187\n",
      "Train: Epoch [4], Batch [896/938], Loss: 0.7304751873016357\n",
      "Train: Epoch [4], Batch [897/938], Loss: 0.8594139814376831\n",
      "Train: Epoch [4], Batch [898/938], Loss: 0.8290423154830933\n",
      "Train: Epoch [4], Batch [899/938], Loss: 0.8059371709823608\n",
      "Train: Epoch [4], Batch [900/938], Loss: 0.8169172406196594\n",
      "Train: Epoch [4], Batch [901/938], Loss: 0.563827633857727\n",
      "Train: Epoch [4], Batch [902/938], Loss: 0.6343238949775696\n",
      "Train: Epoch [4], Batch [903/938], Loss: 0.6501522660255432\n",
      "Train: Epoch [4], Batch [904/938], Loss: 0.7808859348297119\n",
      "Train: Epoch [4], Batch [905/938], Loss: 0.6412151455879211\n",
      "Train: Epoch [4], Batch [906/938], Loss: 0.6477564573287964\n",
      "Train: Epoch [4], Batch [907/938], Loss: 0.7721412181854248\n",
      "Train: Epoch [4], Batch [908/938], Loss: 0.7082050442695618\n",
      "Train: Epoch [4], Batch [909/938], Loss: 0.6540284156799316\n",
      "Train: Epoch [4], Batch [910/938], Loss: 0.6323186159133911\n",
      "Train: Epoch [4], Batch [911/938], Loss: 0.7096874117851257\n",
      "Train: Epoch [4], Batch [912/938], Loss: 0.5914981365203857\n",
      "Train: Epoch [4], Batch [913/938], Loss: 0.6891300678253174\n",
      "Train: Epoch [4], Batch [914/938], Loss: 0.6953608989715576\n",
      "Train: Epoch [4], Batch [915/938], Loss: 0.745558500289917\n",
      "Train: Epoch [4], Batch [916/938], Loss: 0.7398595213890076\n",
      "Train: Epoch [4], Batch [917/938], Loss: 0.6601564884185791\n",
      "Train: Epoch [4], Batch [918/938], Loss: 0.6556516289710999\n",
      "Train: Epoch [4], Batch [919/938], Loss: 0.8227225542068481\n",
      "Train: Epoch [4], Batch [920/938], Loss: 0.8302890062332153\n",
      "Train: Epoch [4], Batch [921/938], Loss: 0.6155821681022644\n",
      "Train: Epoch [4], Batch [922/938], Loss: 0.6121011972427368\n",
      "Train: Epoch [4], Batch [923/938], Loss: 0.6599076986312866\n",
      "Train: Epoch [4], Batch [924/938], Loss: 0.7679017782211304\n",
      "Train: Epoch [4], Batch [925/938], Loss: 0.6684850454330444\n",
      "Train: Epoch [4], Batch [926/938], Loss: 0.7755752205848694\n",
      "Train: Epoch [4], Batch [927/938], Loss: 0.7035118341445923\n",
      "Train: Epoch [4], Batch [928/938], Loss: 0.7867650389671326\n",
      "Train: Epoch [4], Batch [929/938], Loss: 0.691122829914093\n",
      "Train: Epoch [4], Batch [930/938], Loss: 0.6926239132881165\n",
      "Train: Epoch [4], Batch [931/938], Loss: 0.7365279197692871\n",
      "Train: Epoch [4], Batch [932/938], Loss: 0.6394267082214355\n",
      "Train: Epoch [4], Batch [933/938], Loss: 0.7306047081947327\n",
      "Train: Epoch [4], Batch [934/938], Loss: 0.6157768964767456\n",
      "Train: Epoch [4], Batch [935/938], Loss: 0.8362342119216919\n",
      "Train: Epoch [4], Batch [936/938], Loss: 0.7052147388458252\n",
      "Train: Epoch [4], Batch [937/938], Loss: 0.7574547529220581\n",
      "Train: Epoch [4], Batch [938/938], Loss: 0.9300733804702759\n",
      "Accuracy of train set: 0.7196\n",
      "Validation: Epoch [4], Batch [1/938], Loss: 0.9157488346099854\n",
      "Validation: Epoch [4], Batch [2/938], Loss: 0.7241773009300232\n",
      "Validation: Epoch [4], Batch [3/938], Loss: 0.9593216776847839\n",
      "Validation: Epoch [4], Batch [4/938], Loss: 0.7460101842880249\n",
      "Validation: Epoch [4], Batch [5/938], Loss: 0.43144500255584717\n",
      "Validation: Epoch [4], Batch [6/938], Loss: 0.7190605401992798\n",
      "Validation: Epoch [4], Batch [7/938], Loss: 0.7489175796508789\n",
      "Validation: Epoch [4], Batch [8/938], Loss: 0.7395337820053101\n",
      "Validation: Epoch [4], Batch [9/938], Loss: 0.9804962873458862\n",
      "Validation: Epoch [4], Batch [10/938], Loss: 0.9458768367767334\n",
      "Validation: Epoch [4], Batch [11/938], Loss: 0.6800742149353027\n",
      "Validation: Epoch [4], Batch [12/938], Loss: 0.6730929613113403\n",
      "Validation: Epoch [4], Batch [13/938], Loss: 0.8650145530700684\n",
      "Validation: Epoch [4], Batch [14/938], Loss: 0.6723763942718506\n",
      "Validation: Epoch [4], Batch [15/938], Loss: 0.6801199316978455\n",
      "Validation: Epoch [4], Batch [16/938], Loss: 0.6274406313896179\n",
      "Validation: Epoch [4], Batch [17/938], Loss: 0.7588504552841187\n",
      "Validation: Epoch [4], Batch [18/938], Loss: 0.7080894708633423\n",
      "Validation: Epoch [4], Batch [19/938], Loss: 0.6382570266723633\n",
      "Validation: Epoch [4], Batch [20/938], Loss: 0.6824270486831665\n",
      "Validation: Epoch [4], Batch [21/938], Loss: 0.7300934791564941\n",
      "Validation: Epoch [4], Batch [22/938], Loss: 0.7270603179931641\n",
      "Validation: Epoch [4], Batch [23/938], Loss: 0.6731910705566406\n",
      "Validation: Epoch [4], Batch [24/938], Loss: 0.9450160264968872\n",
      "Validation: Epoch [4], Batch [25/938], Loss: 0.6529954671859741\n",
      "Validation: Epoch [4], Batch [26/938], Loss: 0.8803858757019043\n",
      "Validation: Epoch [4], Batch [27/938], Loss: 1.021597146987915\n",
      "Validation: Epoch [4], Batch [28/938], Loss: 0.7767016887664795\n",
      "Validation: Epoch [4], Batch [29/938], Loss: 0.7374876737594604\n",
      "Validation: Epoch [4], Batch [30/938], Loss: 0.7614381313323975\n",
      "Validation: Epoch [4], Batch [31/938], Loss: 0.6585862040519714\n",
      "Validation: Epoch [4], Batch [32/938], Loss: 0.6790708303451538\n",
      "Validation: Epoch [4], Batch [33/938], Loss: 0.8367337584495544\n",
      "Validation: Epoch [4], Batch [34/938], Loss: 0.6835227608680725\n",
      "Validation: Epoch [4], Batch [35/938], Loss: 0.9011196494102478\n",
      "Validation: Epoch [4], Batch [36/938], Loss: 0.8741910457611084\n",
      "Validation: Epoch [4], Batch [37/938], Loss: 0.745282769203186\n",
      "Validation: Epoch [4], Batch [38/938], Loss: 0.6634334325790405\n",
      "Validation: Epoch [4], Batch [39/938], Loss: 0.8412234783172607\n",
      "Validation: Epoch [4], Batch [40/938], Loss: 0.6989501714706421\n",
      "Validation: Epoch [4], Batch [41/938], Loss: 0.6361625790596008\n",
      "Validation: Epoch [4], Batch [42/938], Loss: 0.6318892240524292\n",
      "Validation: Epoch [4], Batch [43/938], Loss: 0.7857736945152283\n",
      "Validation: Epoch [4], Batch [44/938], Loss: 0.5743518471717834\n",
      "Validation: Epoch [4], Batch [45/938], Loss: 0.7447284460067749\n",
      "Validation: Epoch [4], Batch [46/938], Loss: 0.5652510523796082\n",
      "Validation: Epoch [4], Batch [47/938], Loss: 1.0095124244689941\n",
      "Validation: Epoch [4], Batch [48/938], Loss: 0.7234658598899841\n",
      "Validation: Epoch [4], Batch [49/938], Loss: 0.7448038458824158\n",
      "Validation: Epoch [4], Batch [50/938], Loss: 0.5872632265090942\n",
      "Validation: Epoch [4], Batch [51/938], Loss: 0.8550295829772949\n",
      "Validation: Epoch [4], Batch [52/938], Loss: 0.7343634963035583\n",
      "Validation: Epoch [4], Batch [53/938], Loss: 0.9761418104171753\n",
      "Validation: Epoch [4], Batch [54/938], Loss: 1.083386778831482\n",
      "Validation: Epoch [4], Batch [55/938], Loss: 0.7390836477279663\n",
      "Validation: Epoch [4], Batch [56/938], Loss: 0.747785210609436\n",
      "Validation: Epoch [4], Batch [57/938], Loss: 0.6868805885314941\n",
      "Validation: Epoch [4], Batch [58/938], Loss: 0.617638349533081\n",
      "Validation: Epoch [4], Batch [59/938], Loss: 0.5798704624176025\n",
      "Validation: Epoch [4], Batch [60/938], Loss: 0.6561955213546753\n",
      "Validation: Epoch [4], Batch [61/938], Loss: 0.8095861077308655\n",
      "Validation: Epoch [4], Batch [62/938], Loss: 0.9318393468856812\n",
      "Validation: Epoch [4], Batch [63/938], Loss: 0.8947745561599731\n",
      "Validation: Epoch [4], Batch [64/938], Loss: 0.6159820556640625\n",
      "Validation: Epoch [4], Batch [65/938], Loss: 0.6384752988815308\n",
      "Validation: Epoch [4], Batch [66/938], Loss: 0.7038141489028931\n",
      "Validation: Epoch [4], Batch [67/938], Loss: 0.6500931978225708\n",
      "Validation: Epoch [4], Batch [68/938], Loss: 0.7580548524856567\n",
      "Validation: Epoch [4], Batch [69/938], Loss: 0.9860388040542603\n",
      "Validation: Epoch [4], Batch [70/938], Loss: 0.8495181798934937\n",
      "Validation: Epoch [4], Batch [71/938], Loss: 0.7406128644943237\n",
      "Validation: Epoch [4], Batch [72/938], Loss: 0.552628219127655\n",
      "Validation: Epoch [4], Batch [73/938], Loss: 0.796930193901062\n",
      "Validation: Epoch [4], Batch [74/938], Loss: 0.6401723623275757\n",
      "Validation: Epoch [4], Batch [75/938], Loss: 0.5819975137710571\n",
      "Validation: Epoch [4], Batch [76/938], Loss: 0.6887874603271484\n",
      "Validation: Epoch [4], Batch [77/938], Loss: 0.763077974319458\n",
      "Validation: Epoch [4], Batch [78/938], Loss: 0.8237091302871704\n",
      "Validation: Epoch [4], Batch [79/938], Loss: 0.7562265396118164\n",
      "Validation: Epoch [4], Batch [80/938], Loss: 0.8320872783660889\n",
      "Validation: Epoch [4], Batch [81/938], Loss: 0.6463030576705933\n",
      "Validation: Epoch [4], Batch [82/938], Loss: 0.9855170845985413\n",
      "Validation: Epoch [4], Batch [83/938], Loss: 0.720458984375\n",
      "Validation: Epoch [4], Batch [84/938], Loss: 0.6083481311798096\n",
      "Validation: Epoch [4], Batch [85/938], Loss: 0.7228884696960449\n",
      "Validation: Epoch [4], Batch [86/938], Loss: 0.8928316235542297\n",
      "Validation: Epoch [4], Batch [87/938], Loss: 0.6099686622619629\n",
      "Validation: Epoch [4], Batch [88/938], Loss: 0.7084255218505859\n",
      "Validation: Epoch [4], Batch [89/938], Loss: 0.7600123882293701\n",
      "Validation: Epoch [4], Batch [90/938], Loss: 0.5931598544120789\n",
      "Validation: Epoch [4], Batch [91/938], Loss: 0.8319551944732666\n",
      "Validation: Epoch [4], Batch [92/938], Loss: 1.0015599727630615\n",
      "Validation: Epoch [4], Batch [93/938], Loss: 0.7425599098205566\n",
      "Validation: Epoch [4], Batch [94/938], Loss: 0.8633828163146973\n",
      "Validation: Epoch [4], Batch [95/938], Loss: 0.5779105424880981\n",
      "Validation: Epoch [4], Batch [96/938], Loss: 0.7026453018188477\n",
      "Validation: Epoch [4], Batch [97/938], Loss: 0.8117896318435669\n",
      "Validation: Epoch [4], Batch [98/938], Loss: 0.6881111860275269\n",
      "Validation: Epoch [4], Batch [99/938], Loss: 0.8127040863037109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [100/938], Loss: 0.6271265149116516\n",
      "Validation: Epoch [4], Batch [101/938], Loss: 0.9166011214256287\n",
      "Validation: Epoch [4], Batch [102/938], Loss: 0.7089583873748779\n",
      "Validation: Epoch [4], Batch [103/938], Loss: 0.4787987470626831\n",
      "Validation: Epoch [4], Batch [104/938], Loss: 0.9436065554618835\n",
      "Validation: Epoch [4], Batch [105/938], Loss: 1.0090010166168213\n",
      "Validation: Epoch [4], Batch [106/938], Loss: 0.8881052136421204\n",
      "Validation: Epoch [4], Batch [107/938], Loss: 0.94611656665802\n",
      "Validation: Epoch [4], Batch [108/938], Loss: 0.9261696338653564\n",
      "Validation: Epoch [4], Batch [109/938], Loss: 0.7361719012260437\n",
      "Validation: Epoch [4], Batch [110/938], Loss: 0.6998368501663208\n",
      "Validation: Epoch [4], Batch [111/938], Loss: 0.8779468536376953\n",
      "Validation: Epoch [4], Batch [112/938], Loss: 0.7687596082687378\n",
      "Validation: Epoch [4], Batch [113/938], Loss: 0.6185111999511719\n",
      "Validation: Epoch [4], Batch [114/938], Loss: 0.5896844863891602\n",
      "Validation: Epoch [4], Batch [115/938], Loss: 0.8440344929695129\n",
      "Validation: Epoch [4], Batch [116/938], Loss: 0.845156729221344\n",
      "Validation: Epoch [4], Batch [117/938], Loss: 0.5425427556037903\n",
      "Validation: Epoch [4], Batch [118/938], Loss: 0.7473331689834595\n",
      "Validation: Epoch [4], Batch [119/938], Loss: 0.7996953725814819\n",
      "Validation: Epoch [4], Batch [120/938], Loss: 0.8449634313583374\n",
      "Validation: Epoch [4], Batch [121/938], Loss: 0.8202915191650391\n",
      "Validation: Epoch [4], Batch [122/938], Loss: 0.5989658832550049\n",
      "Validation: Epoch [4], Batch [123/938], Loss: 0.6248642206192017\n",
      "Validation: Epoch [4], Batch [124/938], Loss: 0.7175000309944153\n",
      "Validation: Epoch [4], Batch [125/938], Loss: 0.8117638230323792\n",
      "Validation: Epoch [4], Batch [126/938], Loss: 0.8271740674972534\n",
      "Validation: Epoch [4], Batch [127/938], Loss: 0.5526584386825562\n",
      "Validation: Epoch [4], Batch [128/938], Loss: 0.49603235721588135\n",
      "Validation: Epoch [4], Batch [129/938], Loss: 0.9289599657058716\n",
      "Validation: Epoch [4], Batch [130/938], Loss: 0.7679986953735352\n",
      "Validation: Epoch [4], Batch [131/938], Loss: 0.5910065174102783\n",
      "Validation: Epoch [4], Batch [132/938], Loss: 0.7498561143875122\n",
      "Validation: Epoch [4], Batch [133/938], Loss: 0.921747088432312\n",
      "Validation: Epoch [4], Batch [134/938], Loss: 0.6801483631134033\n",
      "Validation: Epoch [4], Batch [135/938], Loss: 0.7497072815895081\n",
      "Validation: Epoch [4], Batch [136/938], Loss: 0.6917673349380493\n",
      "Validation: Epoch [4], Batch [137/938], Loss: 0.7848469018936157\n",
      "Validation: Epoch [4], Batch [138/938], Loss: 0.6753500699996948\n",
      "Validation: Epoch [4], Batch [139/938], Loss: 0.6010631322860718\n",
      "Validation: Epoch [4], Batch [140/938], Loss: 1.0446981191635132\n",
      "Validation: Epoch [4], Batch [141/938], Loss: 0.6513224840164185\n",
      "Validation: Epoch [4], Batch [142/938], Loss: 0.776053786277771\n",
      "Validation: Epoch [4], Batch [143/938], Loss: 0.8261728286743164\n",
      "Validation: Epoch [4], Batch [144/938], Loss: 0.8297297954559326\n",
      "Validation: Epoch [4], Batch [145/938], Loss: 0.7615134119987488\n",
      "Validation: Epoch [4], Batch [146/938], Loss: 0.7120692729949951\n",
      "Validation: Epoch [4], Batch [147/938], Loss: 0.8179503679275513\n",
      "Validation: Epoch [4], Batch [148/938], Loss: 0.698581874370575\n",
      "Validation: Epoch [4], Batch [149/938], Loss: 0.9152390956878662\n",
      "Validation: Epoch [4], Batch [150/938], Loss: 0.6713396310806274\n",
      "Validation: Epoch [4], Batch [151/938], Loss: 0.8737419843673706\n",
      "Validation: Epoch [4], Batch [152/938], Loss: 0.7191444635391235\n",
      "Validation: Epoch [4], Batch [153/938], Loss: 0.7200775742530823\n",
      "Validation: Epoch [4], Batch [154/938], Loss: 0.6049491167068481\n",
      "Validation: Epoch [4], Batch [155/938], Loss: 0.7467654943466187\n",
      "Validation: Epoch [4], Batch [156/938], Loss: 0.7577742338180542\n",
      "Validation: Epoch [4], Batch [157/938], Loss: 0.628670871257782\n",
      "Validation: Epoch [4], Batch [158/938], Loss: 0.5666428804397583\n",
      "Validation: Epoch [4], Batch [159/938], Loss: 0.732464075088501\n",
      "Validation: Epoch [4], Batch [160/938], Loss: 0.5109044313430786\n",
      "Validation: Epoch [4], Batch [161/938], Loss: 0.8867288827896118\n",
      "Validation: Epoch [4], Batch [162/938], Loss: 0.7227655649185181\n",
      "Validation: Epoch [4], Batch [163/938], Loss: 0.640129804611206\n",
      "Validation: Epoch [4], Batch [164/938], Loss: 0.9204762578010559\n",
      "Validation: Epoch [4], Batch [165/938], Loss: 0.8088082075119019\n",
      "Validation: Epoch [4], Batch [166/938], Loss: 0.7166091203689575\n",
      "Validation: Epoch [4], Batch [167/938], Loss: 0.6946411728858948\n",
      "Validation: Epoch [4], Batch [168/938], Loss: 0.6638687252998352\n",
      "Validation: Epoch [4], Batch [169/938], Loss: 1.0484360456466675\n",
      "Validation: Epoch [4], Batch [170/938], Loss: 0.8528305292129517\n",
      "Validation: Epoch [4], Batch [171/938], Loss: 0.9256752729415894\n",
      "Validation: Epoch [4], Batch [172/938], Loss: 0.7975908517837524\n",
      "Validation: Epoch [4], Batch [173/938], Loss: 0.7876367568969727\n",
      "Validation: Epoch [4], Batch [174/938], Loss: 0.6420012712478638\n",
      "Validation: Epoch [4], Batch [175/938], Loss: 0.7102813720703125\n",
      "Validation: Epoch [4], Batch [176/938], Loss: 0.9443572163581848\n",
      "Validation: Epoch [4], Batch [177/938], Loss: 0.5267208814620972\n",
      "Validation: Epoch [4], Batch [178/938], Loss: 0.6799782514572144\n",
      "Validation: Epoch [4], Batch [179/938], Loss: 0.8640152812004089\n",
      "Validation: Epoch [4], Batch [180/938], Loss: 1.1630040407180786\n",
      "Validation: Epoch [4], Batch [181/938], Loss: 0.7163622975349426\n",
      "Validation: Epoch [4], Batch [182/938], Loss: 0.6830506324768066\n",
      "Validation: Epoch [4], Batch [183/938], Loss: 0.7927627563476562\n",
      "Validation: Epoch [4], Batch [184/938], Loss: 1.0967965126037598\n",
      "Validation: Epoch [4], Batch [185/938], Loss: 0.8994206190109253\n",
      "Validation: Epoch [4], Batch [186/938], Loss: 0.6379289627075195\n",
      "Validation: Epoch [4], Batch [187/938], Loss: 0.7647765874862671\n",
      "Validation: Epoch [4], Batch [188/938], Loss: 0.7499321103096008\n",
      "Validation: Epoch [4], Batch [189/938], Loss: 0.8474761247634888\n",
      "Validation: Epoch [4], Batch [190/938], Loss: 0.6086841821670532\n",
      "Validation: Epoch [4], Batch [191/938], Loss: 0.6476916074752808\n",
      "Validation: Epoch [4], Batch [192/938], Loss: 0.7537204623222351\n",
      "Validation: Epoch [4], Batch [193/938], Loss: 0.892591118812561\n",
      "Validation: Epoch [4], Batch [194/938], Loss: 0.6404763460159302\n",
      "Validation: Epoch [4], Batch [195/938], Loss: 0.6615222692489624\n",
      "Validation: Epoch [4], Batch [196/938], Loss: 0.8400537967681885\n",
      "Validation: Epoch [4], Batch [197/938], Loss: 0.6603602766990662\n",
      "Validation: Epoch [4], Batch [198/938], Loss: 0.6975222826004028\n",
      "Validation: Epoch [4], Batch [199/938], Loss: 0.6935885548591614\n",
      "Validation: Epoch [4], Batch [200/938], Loss: 0.8095390796661377\n",
      "Validation: Epoch [4], Batch [201/938], Loss: 0.7740566730499268\n",
      "Validation: Epoch [4], Batch [202/938], Loss: 0.6520125865936279\n",
      "Validation: Epoch [4], Batch [203/938], Loss: 0.7259814739227295\n",
      "Validation: Epoch [4], Batch [204/938], Loss: 0.5939476490020752\n",
      "Validation: Epoch [4], Batch [205/938], Loss: 0.8854511380195618\n",
      "Validation: Epoch [4], Batch [206/938], Loss: 1.1357545852661133\n",
      "Validation: Epoch [4], Batch [207/938], Loss: 0.5945836305618286\n",
      "Validation: Epoch [4], Batch [208/938], Loss: 0.7975896596908569\n",
      "Validation: Epoch [4], Batch [209/938], Loss: 0.8631272912025452\n",
      "Validation: Epoch [4], Batch [210/938], Loss: 0.6906912922859192\n",
      "Validation: Epoch [4], Batch [211/938], Loss: 0.8354905843734741\n",
      "Validation: Epoch [4], Batch [212/938], Loss: 0.8094220757484436\n",
      "Validation: Epoch [4], Batch [213/938], Loss: 0.8276528120040894\n",
      "Validation: Epoch [4], Batch [214/938], Loss: 0.8347021341323853\n",
      "Validation: Epoch [4], Batch [215/938], Loss: 0.8593931198120117\n",
      "Validation: Epoch [4], Batch [216/938], Loss: 0.7564821839332581\n",
      "Validation: Epoch [4], Batch [217/938], Loss: 0.6486322283744812\n",
      "Validation: Epoch [4], Batch [218/938], Loss: 0.7246703505516052\n",
      "Validation: Epoch [4], Batch [219/938], Loss: 0.7319037914276123\n",
      "Validation: Epoch [4], Batch [220/938], Loss: 0.7919032573699951\n",
      "Validation: Epoch [4], Batch [221/938], Loss: 0.9481279850006104\n",
      "Validation: Epoch [4], Batch [222/938], Loss: 0.7542964220046997\n",
      "Validation: Epoch [4], Batch [223/938], Loss: 0.8705369234085083\n",
      "Validation: Epoch [4], Batch [224/938], Loss: 0.9774504899978638\n",
      "Validation: Epoch [4], Batch [225/938], Loss: 0.5892971754074097\n",
      "Validation: Epoch [4], Batch [226/938], Loss: 0.8709287047386169\n",
      "Validation: Epoch [4], Batch [227/938], Loss: 0.5222136974334717\n",
      "Validation: Epoch [4], Batch [228/938], Loss: 0.7903143167495728\n",
      "Validation: Epoch [4], Batch [229/938], Loss: 0.6641823649406433\n",
      "Validation: Epoch [4], Batch [230/938], Loss: 0.7400590777397156\n",
      "Validation: Epoch [4], Batch [231/938], Loss: 0.9098879098892212\n",
      "Validation: Epoch [4], Batch [232/938], Loss: 0.6914674043655396\n",
      "Validation: Epoch [4], Batch [233/938], Loss: 0.638306736946106\n",
      "Validation: Epoch [4], Batch [234/938], Loss: 0.8034015893936157\n",
      "Validation: Epoch [4], Batch [235/938], Loss: 0.7597423195838928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [236/938], Loss: 0.8171358108520508\n",
      "Validation: Epoch [4], Batch [237/938], Loss: 0.5951347351074219\n",
      "Validation: Epoch [4], Batch [238/938], Loss: 0.7986640334129333\n",
      "Validation: Epoch [4], Batch [239/938], Loss: 0.7042122483253479\n",
      "Validation: Epoch [4], Batch [240/938], Loss: 0.9528558254241943\n",
      "Validation: Epoch [4], Batch [241/938], Loss: 0.6687818765640259\n",
      "Validation: Epoch [4], Batch [242/938], Loss: 0.6594929695129395\n",
      "Validation: Epoch [4], Batch [243/938], Loss: 0.8179718255996704\n",
      "Validation: Epoch [4], Batch [244/938], Loss: 0.759425163269043\n",
      "Validation: Epoch [4], Batch [245/938], Loss: 0.6276683807373047\n",
      "Validation: Epoch [4], Batch [246/938], Loss: 0.7122832536697388\n",
      "Validation: Epoch [4], Batch [247/938], Loss: 0.5820525884628296\n",
      "Validation: Epoch [4], Batch [248/938], Loss: 0.7198548913002014\n",
      "Validation: Epoch [4], Batch [249/938], Loss: 0.6408379673957825\n",
      "Validation: Epoch [4], Batch [250/938], Loss: 1.1013988256454468\n",
      "Validation: Epoch [4], Batch [251/938], Loss: 0.5332812666893005\n",
      "Validation: Epoch [4], Batch [252/938], Loss: 0.9290345907211304\n",
      "Validation: Epoch [4], Batch [253/938], Loss: 0.7069047093391418\n",
      "Validation: Epoch [4], Batch [254/938], Loss: 0.6918869018554688\n",
      "Validation: Epoch [4], Batch [255/938], Loss: 0.6322727203369141\n",
      "Validation: Epoch [4], Batch [256/938], Loss: 0.7099987268447876\n",
      "Validation: Epoch [4], Batch [257/938], Loss: 0.8043605089187622\n",
      "Validation: Epoch [4], Batch [258/938], Loss: 0.7059796452522278\n",
      "Validation: Epoch [4], Batch [259/938], Loss: 1.0215322971343994\n",
      "Validation: Epoch [4], Batch [260/938], Loss: 1.0599424839019775\n",
      "Validation: Epoch [4], Batch [261/938], Loss: 0.8816198110580444\n",
      "Validation: Epoch [4], Batch [262/938], Loss: 0.8447754383087158\n",
      "Validation: Epoch [4], Batch [263/938], Loss: 0.9225686192512512\n",
      "Validation: Epoch [4], Batch [264/938], Loss: 0.6534864902496338\n",
      "Validation: Epoch [4], Batch [265/938], Loss: 0.8808528184890747\n",
      "Validation: Epoch [4], Batch [266/938], Loss: 0.7522222995758057\n",
      "Validation: Epoch [4], Batch [267/938], Loss: 0.7512651085853577\n",
      "Validation: Epoch [4], Batch [268/938], Loss: 0.6964066624641418\n",
      "Validation: Epoch [4], Batch [269/938], Loss: 0.5563156604766846\n",
      "Validation: Epoch [4], Batch [270/938], Loss: 0.6792132258415222\n",
      "Validation: Epoch [4], Batch [271/938], Loss: 0.6551276445388794\n",
      "Validation: Epoch [4], Batch [272/938], Loss: 0.7811775207519531\n",
      "Validation: Epoch [4], Batch [273/938], Loss: 0.77424556016922\n",
      "Validation: Epoch [4], Batch [274/938], Loss: 0.6384548544883728\n",
      "Validation: Epoch [4], Batch [275/938], Loss: 0.7636802196502686\n",
      "Validation: Epoch [4], Batch [276/938], Loss: 0.7600260972976685\n",
      "Validation: Epoch [4], Batch [277/938], Loss: 0.6314085721969604\n",
      "Validation: Epoch [4], Batch [278/938], Loss: 0.6427427530288696\n",
      "Validation: Epoch [4], Batch [279/938], Loss: 0.6440448760986328\n",
      "Validation: Epoch [4], Batch [280/938], Loss: 0.6629169583320618\n",
      "Validation: Epoch [4], Batch [281/938], Loss: 0.7122615575790405\n",
      "Validation: Epoch [4], Batch [282/938], Loss: 0.5426921844482422\n",
      "Validation: Epoch [4], Batch [283/938], Loss: 0.7298562526702881\n",
      "Validation: Epoch [4], Batch [284/938], Loss: 0.585336446762085\n",
      "Validation: Epoch [4], Batch [285/938], Loss: 0.736835777759552\n",
      "Validation: Epoch [4], Batch [286/938], Loss: 0.7096264362335205\n",
      "Validation: Epoch [4], Batch [287/938], Loss: 0.6031111478805542\n",
      "Validation: Epoch [4], Batch [288/938], Loss: 0.7555480599403381\n",
      "Validation: Epoch [4], Batch [289/938], Loss: 0.7461919188499451\n",
      "Validation: Epoch [4], Batch [290/938], Loss: 0.7925110459327698\n",
      "Validation: Epoch [4], Batch [291/938], Loss: 0.8447970747947693\n",
      "Validation: Epoch [4], Batch [292/938], Loss: 0.6632841229438782\n",
      "Validation: Epoch [4], Batch [293/938], Loss: 0.9078199863433838\n",
      "Validation: Epoch [4], Batch [294/938], Loss: 0.9387337565422058\n",
      "Validation: Epoch [4], Batch [295/938], Loss: 0.6803974509239197\n",
      "Validation: Epoch [4], Batch [296/938], Loss: 0.8029675483703613\n",
      "Validation: Epoch [4], Batch [297/938], Loss: 1.3071683645248413\n",
      "Validation: Epoch [4], Batch [298/938], Loss: 0.5880692005157471\n",
      "Validation: Epoch [4], Batch [299/938], Loss: 0.6754239797592163\n",
      "Validation: Epoch [4], Batch [300/938], Loss: 0.7364212274551392\n",
      "Validation: Epoch [4], Batch [301/938], Loss: 0.8286514282226562\n",
      "Validation: Epoch [4], Batch [302/938], Loss: 0.7439969778060913\n",
      "Validation: Epoch [4], Batch [303/938], Loss: 0.6496359705924988\n",
      "Validation: Epoch [4], Batch [304/938], Loss: 0.6033151149749756\n",
      "Validation: Epoch [4], Batch [305/938], Loss: 0.6917991638183594\n",
      "Validation: Epoch [4], Batch [306/938], Loss: 1.085193157196045\n",
      "Validation: Epoch [4], Batch [307/938], Loss: 0.682127833366394\n",
      "Validation: Epoch [4], Batch [308/938], Loss: 0.8998407125473022\n",
      "Validation: Epoch [4], Batch [309/938], Loss: 0.759892463684082\n",
      "Validation: Epoch [4], Batch [310/938], Loss: 0.8593612313270569\n",
      "Validation: Epoch [4], Batch [311/938], Loss: 0.8039036989212036\n",
      "Validation: Epoch [4], Batch [312/938], Loss: 0.6856011152267456\n",
      "Validation: Epoch [4], Batch [313/938], Loss: 0.8793550133705139\n",
      "Validation: Epoch [4], Batch [314/938], Loss: 0.725718080997467\n",
      "Validation: Epoch [4], Batch [315/938], Loss: 0.6879034042358398\n",
      "Validation: Epoch [4], Batch [316/938], Loss: 0.6233978271484375\n",
      "Validation: Epoch [4], Batch [317/938], Loss: 0.7830409407615662\n",
      "Validation: Epoch [4], Batch [318/938], Loss: 0.7554442286491394\n",
      "Validation: Epoch [4], Batch [319/938], Loss: 0.7093420028686523\n",
      "Validation: Epoch [4], Batch [320/938], Loss: 0.7616363763809204\n",
      "Validation: Epoch [4], Batch [321/938], Loss: 0.7572985887527466\n",
      "Validation: Epoch [4], Batch [322/938], Loss: 0.7961360216140747\n",
      "Validation: Epoch [4], Batch [323/938], Loss: 0.6747565269470215\n",
      "Validation: Epoch [4], Batch [324/938], Loss: 0.8069504499435425\n",
      "Validation: Epoch [4], Batch [325/938], Loss: 0.7841528654098511\n",
      "Validation: Epoch [4], Batch [326/938], Loss: 0.9045416116714478\n",
      "Validation: Epoch [4], Batch [327/938], Loss: 0.8676162362098694\n",
      "Validation: Epoch [4], Batch [328/938], Loss: 0.7210434079170227\n",
      "Validation: Epoch [4], Batch [329/938], Loss: 0.6749721765518188\n",
      "Validation: Epoch [4], Batch [330/938], Loss: 0.9460904598236084\n",
      "Validation: Epoch [4], Batch [331/938], Loss: 0.9047321677207947\n",
      "Validation: Epoch [4], Batch [332/938], Loss: 0.8200789093971252\n",
      "Validation: Epoch [4], Batch [333/938], Loss: 0.7847949266433716\n",
      "Validation: Epoch [4], Batch [334/938], Loss: 0.7126519083976746\n",
      "Validation: Epoch [4], Batch [335/938], Loss: 0.7166017889976501\n",
      "Validation: Epoch [4], Batch [336/938], Loss: 0.6462944746017456\n",
      "Validation: Epoch [4], Batch [337/938], Loss: 0.6690030694007874\n",
      "Validation: Epoch [4], Batch [338/938], Loss: 0.8366677761077881\n",
      "Validation: Epoch [4], Batch [339/938], Loss: 0.6487480401992798\n",
      "Validation: Epoch [4], Batch [340/938], Loss: 0.612749457359314\n",
      "Validation: Epoch [4], Batch [341/938], Loss: 0.6792960166931152\n",
      "Validation: Epoch [4], Batch [342/938], Loss: 0.695974588394165\n",
      "Validation: Epoch [4], Batch [343/938], Loss: 0.8427749276161194\n",
      "Validation: Epoch [4], Batch [344/938], Loss: 0.6760660409927368\n",
      "Validation: Epoch [4], Batch [345/938], Loss: 0.8847153186798096\n",
      "Validation: Epoch [4], Batch [346/938], Loss: 0.9201047420501709\n",
      "Validation: Epoch [4], Batch [347/938], Loss: 1.160275936126709\n",
      "Validation: Epoch [4], Batch [348/938], Loss: 0.9085979461669922\n",
      "Validation: Epoch [4], Batch [349/938], Loss: 0.8936522006988525\n",
      "Validation: Epoch [4], Batch [350/938], Loss: 0.8734538555145264\n",
      "Validation: Epoch [4], Batch [351/938], Loss: 0.680152177810669\n",
      "Validation: Epoch [4], Batch [352/938], Loss: 0.9861732721328735\n",
      "Validation: Epoch [4], Batch [353/938], Loss: 0.5974687933921814\n",
      "Validation: Epoch [4], Batch [354/938], Loss: 0.6924954652786255\n",
      "Validation: Epoch [4], Batch [355/938], Loss: 0.7946183681488037\n",
      "Validation: Epoch [4], Batch [356/938], Loss: 1.0148897171020508\n",
      "Validation: Epoch [4], Batch [357/938], Loss: 0.791215181350708\n",
      "Validation: Epoch [4], Batch [358/938], Loss: 0.7897148132324219\n",
      "Validation: Epoch [4], Batch [359/938], Loss: 0.8864693641662598\n",
      "Validation: Epoch [4], Batch [360/938], Loss: 0.7546678781509399\n",
      "Validation: Epoch [4], Batch [361/938], Loss: 0.7722599506378174\n",
      "Validation: Epoch [4], Batch [362/938], Loss: 0.9134534597396851\n",
      "Validation: Epoch [4], Batch [363/938], Loss: 0.9114986658096313\n",
      "Validation: Epoch [4], Batch [364/938], Loss: 0.6369283199310303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [365/938], Loss: 0.6460180878639221\n",
      "Validation: Epoch [4], Batch [366/938], Loss: 0.8305166959762573\n",
      "Validation: Epoch [4], Batch [367/938], Loss: 0.81629878282547\n",
      "Validation: Epoch [4], Batch [368/938], Loss: 0.676667332649231\n",
      "Validation: Epoch [4], Batch [369/938], Loss: 0.8959788680076599\n",
      "Validation: Epoch [4], Batch [370/938], Loss: 0.760596513748169\n",
      "Validation: Epoch [4], Batch [371/938], Loss: 0.5560085773468018\n",
      "Validation: Epoch [4], Batch [372/938], Loss: 0.6450660228729248\n",
      "Validation: Epoch [4], Batch [373/938], Loss: 0.6853071451187134\n",
      "Validation: Epoch [4], Batch [374/938], Loss: 0.8139927387237549\n",
      "Validation: Epoch [4], Batch [375/938], Loss: 0.5942643880844116\n",
      "Validation: Epoch [4], Batch [376/938], Loss: 0.7083467841148376\n",
      "Validation: Epoch [4], Batch [377/938], Loss: 0.6664508581161499\n",
      "Validation: Epoch [4], Batch [378/938], Loss: 0.815086841583252\n",
      "Validation: Epoch [4], Batch [379/938], Loss: 0.567070722579956\n",
      "Validation: Epoch [4], Batch [380/938], Loss: 0.8137162327766418\n",
      "Validation: Epoch [4], Batch [381/938], Loss: 0.9620055556297302\n",
      "Validation: Epoch [4], Batch [382/938], Loss: 0.9080193042755127\n",
      "Validation: Epoch [4], Batch [383/938], Loss: 0.6637241244316101\n",
      "Validation: Epoch [4], Batch [384/938], Loss: 0.8458300828933716\n",
      "Validation: Epoch [4], Batch [385/938], Loss: 0.7439700365066528\n",
      "Validation: Epoch [4], Batch [386/938], Loss: 0.9916580319404602\n",
      "Validation: Epoch [4], Batch [387/938], Loss: 0.7628631591796875\n",
      "Validation: Epoch [4], Batch [388/938], Loss: 0.6934511065483093\n",
      "Validation: Epoch [4], Batch [389/938], Loss: 0.7020665407180786\n",
      "Validation: Epoch [4], Batch [390/938], Loss: 0.6336212754249573\n",
      "Validation: Epoch [4], Batch [391/938], Loss: 0.7881990671157837\n",
      "Validation: Epoch [4], Batch [392/938], Loss: 0.6220594048500061\n",
      "Validation: Epoch [4], Batch [393/938], Loss: 0.8212758898735046\n",
      "Validation: Epoch [4], Batch [394/938], Loss: 0.896777868270874\n",
      "Validation: Epoch [4], Batch [395/938], Loss: 0.734355628490448\n",
      "Validation: Epoch [4], Batch [396/938], Loss: 0.6707108020782471\n",
      "Validation: Epoch [4], Batch [397/938], Loss: 0.7682265043258667\n",
      "Validation: Epoch [4], Batch [398/938], Loss: 0.911928117275238\n",
      "Validation: Epoch [4], Batch [399/938], Loss: 0.8456451892852783\n",
      "Validation: Epoch [4], Batch [400/938], Loss: 0.8351578712463379\n",
      "Validation: Epoch [4], Batch [401/938], Loss: 0.583740770816803\n",
      "Validation: Epoch [4], Batch [402/938], Loss: 0.7211029529571533\n",
      "Validation: Epoch [4], Batch [403/938], Loss: 0.6303771138191223\n",
      "Validation: Epoch [4], Batch [404/938], Loss: 0.7260040044784546\n",
      "Validation: Epoch [4], Batch [405/938], Loss: 0.6104947328567505\n",
      "Validation: Epoch [4], Batch [406/938], Loss: 0.7344104051589966\n",
      "Validation: Epoch [4], Batch [407/938], Loss: 0.5503984093666077\n",
      "Validation: Epoch [4], Batch [408/938], Loss: 0.7096637487411499\n",
      "Validation: Epoch [4], Batch [409/938], Loss: 0.81280517578125\n",
      "Validation: Epoch [4], Batch [410/938], Loss: 0.770038366317749\n",
      "Validation: Epoch [4], Batch [411/938], Loss: 0.8123376965522766\n",
      "Validation: Epoch [4], Batch [412/938], Loss: 0.6625241041183472\n",
      "Validation: Epoch [4], Batch [413/938], Loss: 0.664800763130188\n",
      "Validation: Epoch [4], Batch [414/938], Loss: 0.6317881345748901\n",
      "Validation: Epoch [4], Batch [415/938], Loss: 0.6809419393539429\n",
      "Validation: Epoch [4], Batch [416/938], Loss: 0.8619576692581177\n",
      "Validation: Epoch [4], Batch [417/938], Loss: 0.6443389654159546\n",
      "Validation: Epoch [4], Batch [418/938], Loss: 0.7170537710189819\n",
      "Validation: Epoch [4], Batch [419/938], Loss: 0.7203032970428467\n",
      "Validation: Epoch [4], Batch [420/938], Loss: 0.6584807634353638\n",
      "Validation: Epoch [4], Batch [421/938], Loss: 0.4369320869445801\n",
      "Validation: Epoch [4], Batch [422/938], Loss: 1.0821527242660522\n",
      "Validation: Epoch [4], Batch [423/938], Loss: 0.5984287858009338\n",
      "Validation: Epoch [4], Batch [424/938], Loss: 0.908902645111084\n",
      "Validation: Epoch [4], Batch [425/938], Loss: 0.6599313020706177\n",
      "Validation: Epoch [4], Batch [426/938], Loss: 0.8517855405807495\n",
      "Validation: Epoch [4], Batch [427/938], Loss: 0.7235314846038818\n",
      "Validation: Epoch [4], Batch [428/938], Loss: 0.5856168270111084\n",
      "Validation: Epoch [4], Batch [429/938], Loss: 0.8301366567611694\n",
      "Validation: Epoch [4], Batch [430/938], Loss: 0.7976524829864502\n",
      "Validation: Epoch [4], Batch [431/938], Loss: 0.8885457515716553\n",
      "Validation: Epoch [4], Batch [432/938], Loss: 0.9368067979812622\n",
      "Validation: Epoch [4], Batch [433/938], Loss: 0.7128098011016846\n",
      "Validation: Epoch [4], Batch [434/938], Loss: 0.711018443107605\n",
      "Validation: Epoch [4], Batch [435/938], Loss: 0.7476313710212708\n",
      "Validation: Epoch [4], Batch [436/938], Loss: 0.6852200627326965\n",
      "Validation: Epoch [4], Batch [437/938], Loss: 0.6949506998062134\n",
      "Validation: Epoch [4], Batch [438/938], Loss: 0.6650768518447876\n",
      "Validation: Epoch [4], Batch [439/938], Loss: 0.7976623177528381\n",
      "Validation: Epoch [4], Batch [440/938], Loss: 0.7169049978256226\n",
      "Validation: Epoch [4], Batch [441/938], Loss: 0.9774154424667358\n",
      "Validation: Epoch [4], Batch [442/938], Loss: 0.6626255512237549\n",
      "Validation: Epoch [4], Batch [443/938], Loss: 0.8636288046836853\n",
      "Validation: Epoch [4], Batch [444/938], Loss: 1.2158496379852295\n",
      "Validation: Epoch [4], Batch [445/938], Loss: 0.9760197997093201\n",
      "Validation: Epoch [4], Batch [446/938], Loss: 0.7560628652572632\n",
      "Validation: Epoch [4], Batch [447/938], Loss: 1.0444769859313965\n",
      "Validation: Epoch [4], Batch [448/938], Loss: 0.7939728498458862\n",
      "Validation: Epoch [4], Batch [449/938], Loss: 0.6144087314605713\n",
      "Validation: Epoch [4], Batch [450/938], Loss: 0.5661255121231079\n",
      "Validation: Epoch [4], Batch [451/938], Loss: 0.5508211851119995\n",
      "Validation: Epoch [4], Batch [452/938], Loss: 0.7334299087524414\n",
      "Validation: Epoch [4], Batch [453/938], Loss: 1.0851576328277588\n",
      "Validation: Epoch [4], Batch [454/938], Loss: 0.8574002981185913\n",
      "Validation: Epoch [4], Batch [455/938], Loss: 0.8208585977554321\n",
      "Validation: Epoch [4], Batch [456/938], Loss: 0.5765117406845093\n",
      "Validation: Epoch [4], Batch [457/938], Loss: 0.8474752306938171\n",
      "Validation: Epoch [4], Batch [458/938], Loss: 0.6646257638931274\n",
      "Validation: Epoch [4], Batch [459/938], Loss: 0.8754144906997681\n",
      "Validation: Epoch [4], Batch [460/938], Loss: 0.7912247180938721\n",
      "Validation: Epoch [4], Batch [461/938], Loss: 0.8669013977050781\n",
      "Validation: Epoch [4], Batch [462/938], Loss: 0.5727285146713257\n",
      "Validation: Epoch [4], Batch [463/938], Loss: 0.8372990489006042\n",
      "Validation: Epoch [4], Batch [464/938], Loss: 0.4796604514122009\n",
      "Validation: Epoch [4], Batch [465/938], Loss: 0.9599325060844421\n",
      "Validation: Epoch [4], Batch [466/938], Loss: 0.621530294418335\n",
      "Validation: Epoch [4], Batch [467/938], Loss: 0.8512371778488159\n",
      "Validation: Epoch [4], Batch [468/938], Loss: 0.6881470680236816\n",
      "Validation: Epoch [4], Batch [469/938], Loss: 0.5968625545501709\n",
      "Validation: Epoch [4], Batch [470/938], Loss: 0.7323230504989624\n",
      "Validation: Epoch [4], Batch [471/938], Loss: 0.7232867479324341\n",
      "Validation: Epoch [4], Batch [472/938], Loss: 0.5978110432624817\n",
      "Validation: Epoch [4], Batch [473/938], Loss: 0.9664223790168762\n",
      "Validation: Epoch [4], Batch [474/938], Loss: 0.7663702368736267\n",
      "Validation: Epoch [4], Batch [475/938], Loss: 0.9196812510490417\n",
      "Validation: Epoch [4], Batch [476/938], Loss: 0.8084691762924194\n",
      "Validation: Epoch [4], Batch [477/938], Loss: 0.7852994799613953\n",
      "Validation: Epoch [4], Batch [478/938], Loss: 0.7836467623710632\n",
      "Validation: Epoch [4], Batch [479/938], Loss: 0.5723656415939331\n",
      "Validation: Epoch [4], Batch [480/938], Loss: 0.7507448196411133\n",
      "Validation: Epoch [4], Batch [481/938], Loss: 0.6523306369781494\n",
      "Validation: Epoch [4], Batch [482/938], Loss: 0.8395633101463318\n",
      "Validation: Epoch [4], Batch [483/938], Loss: 0.8240252733230591\n",
      "Validation: Epoch [4], Batch [484/938], Loss: 0.6407479047775269\n",
      "Validation: Epoch [4], Batch [485/938], Loss: 0.7669997215270996\n",
      "Validation: Epoch [4], Batch [486/938], Loss: 0.8398961424827576\n",
      "Validation: Epoch [4], Batch [487/938], Loss: 0.6876447200775146\n",
      "Validation: Epoch [4], Batch [488/938], Loss: 0.746128261089325\n",
      "Validation: Epoch [4], Batch [489/938], Loss: 0.7760999798774719\n",
      "Validation: Epoch [4], Batch [490/938], Loss: 0.790251612663269\n",
      "Validation: Epoch [4], Batch [491/938], Loss: 0.8948245048522949\n",
      "Validation: Epoch [4], Batch [492/938], Loss: 0.7489069700241089\n",
      "Validation: Epoch [4], Batch [493/938], Loss: 0.8165039420127869\n",
      "Validation: Epoch [4], Batch [494/938], Loss: 0.6954751014709473\n",
      "Validation: Epoch [4], Batch [495/938], Loss: 0.8021238446235657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [496/938], Loss: 0.6946704983711243\n",
      "Validation: Epoch [4], Batch [497/938], Loss: 0.7925951480865479\n",
      "Validation: Epoch [4], Batch [498/938], Loss: 0.587019681930542\n",
      "Validation: Epoch [4], Batch [499/938], Loss: 0.6047234535217285\n",
      "Validation: Epoch [4], Batch [500/938], Loss: 0.8572088479995728\n",
      "Validation: Epoch [4], Batch [501/938], Loss: 0.5757045149803162\n",
      "Validation: Epoch [4], Batch [502/938], Loss: 0.7848449349403381\n",
      "Validation: Epoch [4], Batch [503/938], Loss: 0.6719890832901001\n",
      "Validation: Epoch [4], Batch [504/938], Loss: 0.758283257484436\n",
      "Validation: Epoch [4], Batch [505/938], Loss: 0.8398832082748413\n",
      "Validation: Epoch [4], Batch [506/938], Loss: 0.5879290103912354\n",
      "Validation: Epoch [4], Batch [507/938], Loss: 0.754615306854248\n",
      "Validation: Epoch [4], Batch [508/938], Loss: 0.6979387998580933\n",
      "Validation: Epoch [4], Batch [509/938], Loss: 0.6496033668518066\n",
      "Validation: Epoch [4], Batch [510/938], Loss: 0.6977893710136414\n",
      "Validation: Epoch [4], Batch [511/938], Loss: 0.8329405784606934\n",
      "Validation: Epoch [4], Batch [512/938], Loss: 0.8457121253013611\n",
      "Validation: Epoch [4], Batch [513/938], Loss: 0.6262024641036987\n",
      "Validation: Epoch [4], Batch [514/938], Loss: 0.9284195899963379\n",
      "Validation: Epoch [4], Batch [515/938], Loss: 0.7743474245071411\n",
      "Validation: Epoch [4], Batch [516/938], Loss: 0.7650072574615479\n",
      "Validation: Epoch [4], Batch [517/938], Loss: 0.8383464813232422\n",
      "Validation: Epoch [4], Batch [518/938], Loss: 0.8078526258468628\n",
      "Validation: Epoch [4], Batch [519/938], Loss: 0.8571025133132935\n",
      "Validation: Epoch [4], Batch [520/938], Loss: 0.8433218002319336\n",
      "Validation: Epoch [4], Batch [521/938], Loss: 0.7430881261825562\n",
      "Validation: Epoch [4], Batch [522/938], Loss: 0.6938034892082214\n",
      "Validation: Epoch [4], Batch [523/938], Loss: 0.8296921253204346\n",
      "Validation: Epoch [4], Batch [524/938], Loss: 0.7007828950881958\n",
      "Validation: Epoch [4], Batch [525/938], Loss: 0.9106994271278381\n",
      "Validation: Epoch [4], Batch [526/938], Loss: 0.746111273765564\n",
      "Validation: Epoch [4], Batch [527/938], Loss: 0.5982829332351685\n",
      "Validation: Epoch [4], Batch [528/938], Loss: 0.7844157218933105\n",
      "Validation: Epoch [4], Batch [529/938], Loss: 0.7113701105117798\n",
      "Validation: Epoch [4], Batch [530/938], Loss: 0.8764359354972839\n",
      "Validation: Epoch [4], Batch [531/938], Loss: 0.6239003539085388\n",
      "Validation: Epoch [4], Batch [532/938], Loss: 0.6770151257514954\n",
      "Validation: Epoch [4], Batch [533/938], Loss: 0.8193240761756897\n",
      "Validation: Epoch [4], Batch [534/938], Loss: 0.8960237503051758\n",
      "Validation: Epoch [4], Batch [535/938], Loss: 0.8015968799591064\n",
      "Validation: Epoch [4], Batch [536/938], Loss: 0.6683999300003052\n",
      "Validation: Epoch [4], Batch [537/938], Loss: 1.028367280960083\n",
      "Validation: Epoch [4], Batch [538/938], Loss: 0.7369126081466675\n",
      "Validation: Epoch [4], Batch [539/938], Loss: 0.6939716339111328\n",
      "Validation: Epoch [4], Batch [540/938], Loss: 0.6202948093414307\n",
      "Validation: Epoch [4], Batch [541/938], Loss: 0.7499696612358093\n",
      "Validation: Epoch [4], Batch [542/938], Loss: 0.9753848910331726\n",
      "Validation: Epoch [4], Batch [543/938], Loss: 0.6878182888031006\n",
      "Validation: Epoch [4], Batch [544/938], Loss: 0.7531834244728088\n",
      "Validation: Epoch [4], Batch [545/938], Loss: 0.8452407121658325\n",
      "Validation: Epoch [4], Batch [546/938], Loss: 0.7510274052619934\n",
      "Validation: Epoch [4], Batch [547/938], Loss: 0.8695783615112305\n",
      "Validation: Epoch [4], Batch [548/938], Loss: 0.6529344320297241\n",
      "Validation: Epoch [4], Batch [549/938], Loss: 0.6456993818283081\n",
      "Validation: Epoch [4], Batch [550/938], Loss: 0.9721693396568298\n",
      "Validation: Epoch [4], Batch [551/938], Loss: 0.9705618023872375\n",
      "Validation: Epoch [4], Batch [552/938], Loss: 0.9396215677261353\n",
      "Validation: Epoch [4], Batch [553/938], Loss: 0.6940903663635254\n",
      "Validation: Epoch [4], Batch [554/938], Loss: 0.6071096658706665\n",
      "Validation: Epoch [4], Batch [555/938], Loss: 0.7651787996292114\n",
      "Validation: Epoch [4], Batch [556/938], Loss: 0.764477014541626\n",
      "Validation: Epoch [4], Batch [557/938], Loss: 0.6368199586868286\n",
      "Validation: Epoch [4], Batch [558/938], Loss: 0.9021974802017212\n",
      "Validation: Epoch [4], Batch [559/938], Loss: 0.7899315357208252\n",
      "Validation: Epoch [4], Batch [560/938], Loss: 0.6903716921806335\n",
      "Validation: Epoch [4], Batch [561/938], Loss: 0.7393776178359985\n",
      "Validation: Epoch [4], Batch [562/938], Loss: 0.6048046946525574\n",
      "Validation: Epoch [4], Batch [563/938], Loss: 0.7455483675003052\n",
      "Validation: Epoch [4], Batch [564/938], Loss: 0.6796184778213501\n",
      "Validation: Epoch [4], Batch [565/938], Loss: 0.7339670658111572\n",
      "Validation: Epoch [4], Batch [566/938], Loss: 0.8012086153030396\n",
      "Validation: Epoch [4], Batch [567/938], Loss: 0.840068519115448\n",
      "Validation: Epoch [4], Batch [568/938], Loss: 0.7190825939178467\n",
      "Validation: Epoch [4], Batch [569/938], Loss: 0.8803743124008179\n",
      "Validation: Epoch [4], Batch [570/938], Loss: 0.5581934452056885\n",
      "Validation: Epoch [4], Batch [571/938], Loss: 0.7803911566734314\n",
      "Validation: Epoch [4], Batch [572/938], Loss: 0.6671476364135742\n",
      "Validation: Epoch [4], Batch [573/938], Loss: 0.8526260852813721\n",
      "Validation: Epoch [4], Batch [574/938], Loss: 0.6382887363433838\n",
      "Validation: Epoch [4], Batch [575/938], Loss: 0.7787857055664062\n",
      "Validation: Epoch [4], Batch [576/938], Loss: 0.8305822014808655\n",
      "Validation: Epoch [4], Batch [577/938], Loss: 0.9733240008354187\n",
      "Validation: Epoch [4], Batch [578/938], Loss: 0.8002583980560303\n",
      "Validation: Epoch [4], Batch [579/938], Loss: 0.7013146877288818\n",
      "Validation: Epoch [4], Batch [580/938], Loss: 0.7239157557487488\n",
      "Validation: Epoch [4], Batch [581/938], Loss: 0.9101710915565491\n",
      "Validation: Epoch [4], Batch [582/938], Loss: 0.6592456102371216\n",
      "Validation: Epoch [4], Batch [583/938], Loss: 0.7091594338417053\n",
      "Validation: Epoch [4], Batch [584/938], Loss: 0.9858940243721008\n",
      "Validation: Epoch [4], Batch [585/938], Loss: 1.0190304517745972\n",
      "Validation: Epoch [4], Batch [586/938], Loss: 1.0195183753967285\n",
      "Validation: Epoch [4], Batch [587/938], Loss: 0.9602921605110168\n",
      "Validation: Epoch [4], Batch [588/938], Loss: 0.6648966073989868\n",
      "Validation: Epoch [4], Batch [589/938], Loss: 0.8165057897567749\n",
      "Validation: Epoch [4], Batch [590/938], Loss: 0.771714448928833\n",
      "Validation: Epoch [4], Batch [591/938], Loss: 0.8489044308662415\n",
      "Validation: Epoch [4], Batch [592/938], Loss: 1.0027222633361816\n",
      "Validation: Epoch [4], Batch [593/938], Loss: 0.8227914571762085\n",
      "Validation: Epoch [4], Batch [594/938], Loss: 0.6630997061729431\n",
      "Validation: Epoch [4], Batch [595/938], Loss: 0.6780049800872803\n",
      "Validation: Epoch [4], Batch [596/938], Loss: 1.1296372413635254\n",
      "Validation: Epoch [4], Batch [597/938], Loss: 0.7377265691757202\n",
      "Validation: Epoch [4], Batch [598/938], Loss: 0.6733376979827881\n",
      "Validation: Epoch [4], Batch [599/938], Loss: 0.6890546083450317\n",
      "Validation: Epoch [4], Batch [600/938], Loss: 0.8733974695205688\n",
      "Validation: Epoch [4], Batch [601/938], Loss: 0.8357301354408264\n",
      "Validation: Epoch [4], Batch [602/938], Loss: 0.8929284811019897\n",
      "Validation: Epoch [4], Batch [603/938], Loss: 0.6629854440689087\n",
      "Validation: Epoch [4], Batch [604/938], Loss: 0.8853434324264526\n",
      "Validation: Epoch [4], Batch [605/938], Loss: 0.7226122617721558\n",
      "Validation: Epoch [4], Batch [606/938], Loss: 0.8881337642669678\n",
      "Validation: Epoch [4], Batch [607/938], Loss: 0.7183231115341187\n",
      "Validation: Epoch [4], Batch [608/938], Loss: 0.5834324359893799\n",
      "Validation: Epoch [4], Batch [609/938], Loss: 1.0052218437194824\n",
      "Validation: Epoch [4], Batch [610/938], Loss: 0.8734729290008545\n",
      "Validation: Epoch [4], Batch [611/938], Loss: 0.6881268620491028\n",
      "Validation: Epoch [4], Batch [612/938], Loss: 0.8076316118240356\n",
      "Validation: Epoch [4], Batch [613/938], Loss: 0.9080033302307129\n",
      "Validation: Epoch [4], Batch [614/938], Loss: 0.8379255533218384\n",
      "Validation: Epoch [4], Batch [615/938], Loss: 0.6052712798118591\n",
      "Validation: Epoch [4], Batch [616/938], Loss: 0.7425141334533691\n",
      "Validation: Epoch [4], Batch [617/938], Loss: 0.8580353260040283\n",
      "Validation: Epoch [4], Batch [618/938], Loss: 0.6920188665390015\n",
      "Validation: Epoch [4], Batch [619/938], Loss: 0.7401677370071411\n",
      "Validation: Epoch [4], Batch [620/938], Loss: 0.7285469770431519\n",
      "Validation: Epoch [4], Batch [621/938], Loss: 0.7589992880821228\n",
      "Validation: Epoch [4], Batch [622/938], Loss: 0.7814973592758179\n",
      "Validation: Epoch [4], Batch [623/938], Loss: 0.7054688930511475\n",
      "Validation: Epoch [4], Batch [624/938], Loss: 0.8981938362121582\n",
      "Validation: Epoch [4], Batch [625/938], Loss: 0.5942968130111694\n",
      "Validation: Epoch [4], Batch [626/938], Loss: 0.6346949338912964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [627/938], Loss: 0.7215689420700073\n",
      "Validation: Epoch [4], Batch [628/938], Loss: 0.9258109331130981\n",
      "Validation: Epoch [4], Batch [629/938], Loss: 0.8231686949729919\n",
      "Validation: Epoch [4], Batch [630/938], Loss: 0.9578490257263184\n",
      "Validation: Epoch [4], Batch [631/938], Loss: 0.6656006574630737\n",
      "Validation: Epoch [4], Batch [632/938], Loss: 0.6819345951080322\n",
      "Validation: Epoch [4], Batch [633/938], Loss: 0.8186860084533691\n",
      "Validation: Epoch [4], Batch [634/938], Loss: 0.6449056267738342\n",
      "Validation: Epoch [4], Batch [635/938], Loss: 0.7671203017234802\n",
      "Validation: Epoch [4], Batch [636/938], Loss: 0.7910703420639038\n",
      "Validation: Epoch [4], Batch [637/938], Loss: 0.7460787296295166\n",
      "Validation: Epoch [4], Batch [638/938], Loss: 0.7758004069328308\n",
      "Validation: Epoch [4], Batch [639/938], Loss: 0.8673424124717712\n",
      "Validation: Epoch [4], Batch [640/938], Loss: 0.558680534362793\n",
      "Validation: Epoch [4], Batch [641/938], Loss: 0.854995608329773\n",
      "Validation: Epoch [4], Batch [642/938], Loss: 0.6138969659805298\n",
      "Validation: Epoch [4], Batch [643/938], Loss: 0.6893378496170044\n",
      "Validation: Epoch [4], Batch [644/938], Loss: 0.7519921064376831\n",
      "Validation: Epoch [4], Batch [645/938], Loss: 0.9474561214447021\n",
      "Validation: Epoch [4], Batch [646/938], Loss: 0.7758622765541077\n",
      "Validation: Epoch [4], Batch [647/938], Loss: 0.8134644031524658\n",
      "Validation: Epoch [4], Batch [648/938], Loss: 0.963538646697998\n",
      "Validation: Epoch [4], Batch [649/938], Loss: 0.7482272386550903\n",
      "Validation: Epoch [4], Batch [650/938], Loss: 0.9911638498306274\n",
      "Validation: Epoch [4], Batch [651/938], Loss: 0.673816978931427\n",
      "Validation: Epoch [4], Batch [652/938], Loss: 1.0108240842819214\n",
      "Validation: Epoch [4], Batch [653/938], Loss: 0.7421882152557373\n",
      "Validation: Epoch [4], Batch [654/938], Loss: 0.6955176591873169\n",
      "Validation: Epoch [4], Batch [655/938], Loss: 0.796218991279602\n",
      "Validation: Epoch [4], Batch [656/938], Loss: 0.8631322979927063\n",
      "Validation: Epoch [4], Batch [657/938], Loss: 0.8935525417327881\n",
      "Validation: Epoch [4], Batch [658/938], Loss: 0.7609623074531555\n",
      "Validation: Epoch [4], Batch [659/938], Loss: 0.9325729608535767\n",
      "Validation: Epoch [4], Batch [660/938], Loss: 0.6643085479736328\n",
      "Validation: Epoch [4], Batch [661/938], Loss: 0.8601295948028564\n",
      "Validation: Epoch [4], Batch [662/938], Loss: 0.8447269797325134\n",
      "Validation: Epoch [4], Batch [663/938], Loss: 0.8466942310333252\n",
      "Validation: Epoch [4], Batch [664/938], Loss: 0.7265512943267822\n",
      "Validation: Epoch [4], Batch [665/938], Loss: 0.596860408782959\n",
      "Validation: Epoch [4], Batch [666/938], Loss: 0.7325859665870667\n",
      "Validation: Epoch [4], Batch [667/938], Loss: 1.0200188159942627\n",
      "Validation: Epoch [4], Batch [668/938], Loss: 0.7649867534637451\n",
      "Validation: Epoch [4], Batch [669/938], Loss: 0.7434980869293213\n",
      "Validation: Epoch [4], Batch [670/938], Loss: 0.8403019905090332\n",
      "Validation: Epoch [4], Batch [671/938], Loss: 0.7765084505081177\n",
      "Validation: Epoch [4], Batch [672/938], Loss: 0.7705907821655273\n",
      "Validation: Epoch [4], Batch [673/938], Loss: 0.6588453054428101\n",
      "Validation: Epoch [4], Batch [674/938], Loss: 0.9476235508918762\n",
      "Validation: Epoch [4], Batch [675/938], Loss: 0.6926678419113159\n",
      "Validation: Epoch [4], Batch [676/938], Loss: 0.8315768837928772\n",
      "Validation: Epoch [4], Batch [677/938], Loss: 0.69334876537323\n",
      "Validation: Epoch [4], Batch [678/938], Loss: 0.7971366047859192\n",
      "Validation: Epoch [4], Batch [679/938], Loss: 0.8133624792098999\n",
      "Validation: Epoch [4], Batch [680/938], Loss: 0.7775769829750061\n",
      "Validation: Epoch [4], Batch [681/938], Loss: 0.8082326650619507\n",
      "Validation: Epoch [4], Batch [682/938], Loss: 0.6477136611938477\n",
      "Validation: Epoch [4], Batch [683/938], Loss: 0.7087876200675964\n",
      "Validation: Epoch [4], Batch [684/938], Loss: 0.7910161018371582\n",
      "Validation: Epoch [4], Batch [685/938], Loss: 0.824137806892395\n",
      "Validation: Epoch [4], Batch [686/938], Loss: 0.6945695877075195\n",
      "Validation: Epoch [4], Batch [687/938], Loss: 0.7326459884643555\n",
      "Validation: Epoch [4], Batch [688/938], Loss: 0.7507151961326599\n",
      "Validation: Epoch [4], Batch [689/938], Loss: 0.5826629400253296\n",
      "Validation: Epoch [4], Batch [690/938], Loss: 0.7068328261375427\n",
      "Validation: Epoch [4], Batch [691/938], Loss: 0.6258123517036438\n",
      "Validation: Epoch [4], Batch [692/938], Loss: 0.8756927847862244\n",
      "Validation: Epoch [4], Batch [693/938], Loss: 0.7037441730499268\n",
      "Validation: Epoch [4], Batch [694/938], Loss: 0.7401354312896729\n",
      "Validation: Epoch [4], Batch [695/938], Loss: 0.6385986804962158\n",
      "Validation: Epoch [4], Batch [696/938], Loss: 0.7138583660125732\n",
      "Validation: Epoch [4], Batch [697/938], Loss: 0.7182950973510742\n",
      "Validation: Epoch [4], Batch [698/938], Loss: 0.6213924884796143\n",
      "Validation: Epoch [4], Batch [699/938], Loss: 0.7756867408752441\n",
      "Validation: Epoch [4], Batch [700/938], Loss: 0.7150657176971436\n",
      "Validation: Epoch [4], Batch [701/938], Loss: 0.7431188821792603\n",
      "Validation: Epoch [4], Batch [702/938], Loss: 0.6474047899246216\n",
      "Validation: Epoch [4], Batch [703/938], Loss: 0.8191049098968506\n",
      "Validation: Epoch [4], Batch [704/938], Loss: 0.7718068361282349\n",
      "Validation: Epoch [4], Batch [705/938], Loss: 0.9850641489028931\n",
      "Validation: Epoch [4], Batch [706/938], Loss: 0.7983115911483765\n",
      "Validation: Epoch [4], Batch [707/938], Loss: 0.9351476430892944\n",
      "Validation: Epoch [4], Batch [708/938], Loss: 0.6988998055458069\n",
      "Validation: Epoch [4], Batch [709/938], Loss: 0.5830233097076416\n",
      "Validation: Epoch [4], Batch [710/938], Loss: 0.7748534679412842\n",
      "Validation: Epoch [4], Batch [711/938], Loss: 0.8697772026062012\n",
      "Validation: Epoch [4], Batch [712/938], Loss: 0.6856760382652283\n",
      "Validation: Epoch [4], Batch [713/938], Loss: 0.8984931707382202\n",
      "Validation: Epoch [4], Batch [714/938], Loss: 0.6521510481834412\n",
      "Validation: Epoch [4], Batch [715/938], Loss: 1.0215927362442017\n",
      "Validation: Epoch [4], Batch [716/938], Loss: 0.7124303579330444\n",
      "Validation: Epoch [4], Batch [717/938], Loss: 0.6688768267631531\n",
      "Validation: Epoch [4], Batch [718/938], Loss: 0.9939327239990234\n",
      "Validation: Epoch [4], Batch [719/938], Loss: 0.5645014643669128\n",
      "Validation: Epoch [4], Batch [720/938], Loss: 0.8079210519790649\n",
      "Validation: Epoch [4], Batch [721/938], Loss: 0.7947992086410522\n",
      "Validation: Epoch [4], Batch [722/938], Loss: 0.7125204801559448\n",
      "Validation: Epoch [4], Batch [723/938], Loss: 0.7937315702438354\n",
      "Validation: Epoch [4], Batch [724/938], Loss: 0.9329202175140381\n",
      "Validation: Epoch [4], Batch [725/938], Loss: 0.7074309587478638\n",
      "Validation: Epoch [4], Batch [726/938], Loss: 0.6904236078262329\n",
      "Validation: Epoch [4], Batch [727/938], Loss: 0.751956582069397\n",
      "Validation: Epoch [4], Batch [728/938], Loss: 0.7523676753044128\n",
      "Validation: Epoch [4], Batch [729/938], Loss: 0.930762767791748\n",
      "Validation: Epoch [4], Batch [730/938], Loss: 0.7732750773429871\n",
      "Validation: Epoch [4], Batch [731/938], Loss: 0.7884085178375244\n",
      "Validation: Epoch [4], Batch [732/938], Loss: 0.6900663375854492\n",
      "Validation: Epoch [4], Batch [733/938], Loss: 0.9241051077842712\n",
      "Validation: Epoch [4], Batch [734/938], Loss: 1.0240510702133179\n",
      "Validation: Epoch [4], Batch [735/938], Loss: 0.6487505435943604\n",
      "Validation: Epoch [4], Batch [736/938], Loss: 0.7758905291557312\n",
      "Validation: Epoch [4], Batch [737/938], Loss: 0.6399340629577637\n",
      "Validation: Epoch [4], Batch [738/938], Loss: 0.695617139339447\n",
      "Validation: Epoch [4], Batch [739/938], Loss: 0.6906064748764038\n",
      "Validation: Epoch [4], Batch [740/938], Loss: 0.9000471234321594\n",
      "Validation: Epoch [4], Batch [741/938], Loss: 0.8930545449256897\n",
      "Validation: Epoch [4], Batch [742/938], Loss: 0.6973043084144592\n",
      "Validation: Epoch [4], Batch [743/938], Loss: 0.6320288181304932\n",
      "Validation: Epoch [4], Batch [744/938], Loss: 0.5820155143737793\n",
      "Validation: Epoch [4], Batch [745/938], Loss: 0.7526148557662964\n",
      "Validation: Epoch [4], Batch [746/938], Loss: 0.6581903100013733\n",
      "Validation: Epoch [4], Batch [747/938], Loss: 0.5680062770843506\n",
      "Validation: Epoch [4], Batch [748/938], Loss: 0.7992440462112427\n",
      "Validation: Epoch [4], Batch [749/938], Loss: 0.5683016777038574\n",
      "Validation: Epoch [4], Batch [750/938], Loss: 0.5368866324424744\n",
      "Validation: Epoch [4], Batch [751/938], Loss: 0.8314759135246277\n",
      "Validation: Epoch [4], Batch [752/938], Loss: 0.7202058434486389\n",
      "Validation: Epoch [4], Batch [753/938], Loss: 0.9685845971107483\n",
      "Validation: Epoch [4], Batch [754/938], Loss: 0.8422903418540955\n",
      "Validation: Epoch [4], Batch [755/938], Loss: 0.7180520296096802\n",
      "Validation: Epoch [4], Batch [756/938], Loss: 0.9124630689620972\n",
      "Validation: Epoch [4], Batch [757/938], Loss: 0.7284749746322632\n",
      "Validation: Epoch [4], Batch [758/938], Loss: 0.6708053946495056\n",
      "Validation: Epoch [4], Batch [759/938], Loss: 0.6672744154930115\n",
      "Validation: Epoch [4], Batch [760/938], Loss: 0.6543784737586975\n",
      "Validation: Epoch [4], Batch [761/938], Loss: 0.7491089105606079\n",
      "Validation: Epoch [4], Batch [762/938], Loss: 0.7030582427978516\n",
      "Validation: Epoch [4], Batch [763/938], Loss: 0.5267285108566284\n",
      "Validation: Epoch [4], Batch [764/938], Loss: 0.5802682042121887\n",
      "Validation: Epoch [4], Batch [765/938], Loss: 0.5929222106933594\n",
      "Validation: Epoch [4], Batch [766/938], Loss: 0.6922680139541626\n",
      "Validation: Epoch [4], Batch [767/938], Loss: 0.6392906904220581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [768/938], Loss: 0.6735266447067261\n",
      "Validation: Epoch [4], Batch [769/938], Loss: 0.8349132537841797\n",
      "Validation: Epoch [4], Batch [770/938], Loss: 0.9288471341133118\n",
      "Validation: Epoch [4], Batch [771/938], Loss: 0.9907864928245544\n",
      "Validation: Epoch [4], Batch [772/938], Loss: 0.9286096096038818\n",
      "Validation: Epoch [4], Batch [773/938], Loss: 0.7156012058258057\n",
      "Validation: Epoch [4], Batch [774/938], Loss: 0.7315348982810974\n",
      "Validation: Epoch [4], Batch [775/938], Loss: 0.6997219920158386\n",
      "Validation: Epoch [4], Batch [776/938], Loss: 0.7950397729873657\n",
      "Validation: Epoch [4], Batch [777/938], Loss: 1.0289690494537354\n",
      "Validation: Epoch [4], Batch [778/938], Loss: 0.6833655834197998\n",
      "Validation: Epoch [4], Batch [779/938], Loss: 0.6870756149291992\n",
      "Validation: Epoch [4], Batch [780/938], Loss: 0.6942718029022217\n",
      "Validation: Epoch [4], Batch [781/938], Loss: 0.727552056312561\n",
      "Validation: Epoch [4], Batch [782/938], Loss: 0.88582444190979\n",
      "Validation: Epoch [4], Batch [783/938], Loss: 0.9515273571014404\n",
      "Validation: Epoch [4], Batch [784/938], Loss: 0.8396728038787842\n",
      "Validation: Epoch [4], Batch [785/938], Loss: 0.8637186288833618\n",
      "Validation: Epoch [4], Batch [786/938], Loss: 0.6066572666168213\n",
      "Validation: Epoch [4], Batch [787/938], Loss: 0.6758192777633667\n",
      "Validation: Epoch [4], Batch [788/938], Loss: 0.5798531770706177\n",
      "Validation: Epoch [4], Batch [789/938], Loss: 0.8973571062088013\n",
      "Validation: Epoch [4], Batch [790/938], Loss: 0.8756749033927917\n",
      "Validation: Epoch [4], Batch [791/938], Loss: 0.7977943420410156\n",
      "Validation: Epoch [4], Batch [792/938], Loss: 0.7888689041137695\n",
      "Validation: Epoch [4], Batch [793/938], Loss: 0.7817981839179993\n",
      "Validation: Epoch [4], Batch [794/938], Loss: 0.6844719648361206\n",
      "Validation: Epoch [4], Batch [795/938], Loss: 0.984969973564148\n",
      "Validation: Epoch [4], Batch [796/938], Loss: 0.9886637330055237\n",
      "Validation: Epoch [4], Batch [797/938], Loss: 0.6051332354545593\n",
      "Validation: Epoch [4], Batch [798/938], Loss: 0.9148306846618652\n",
      "Validation: Epoch [4], Batch [799/938], Loss: 0.7490097880363464\n",
      "Validation: Epoch [4], Batch [800/938], Loss: 0.7590834498405457\n",
      "Validation: Epoch [4], Batch [801/938], Loss: 0.7787458896636963\n",
      "Validation: Epoch [4], Batch [802/938], Loss: 0.8009072542190552\n",
      "Validation: Epoch [4], Batch [803/938], Loss: 0.7785534858703613\n",
      "Validation: Epoch [4], Batch [804/938], Loss: 0.6341225504875183\n",
      "Validation: Epoch [4], Batch [805/938], Loss: 0.7318155169487\n",
      "Validation: Epoch [4], Batch [806/938], Loss: 0.7421896457672119\n",
      "Validation: Epoch [4], Batch [807/938], Loss: 0.8444045186042786\n",
      "Validation: Epoch [4], Batch [808/938], Loss: 0.6712313890457153\n",
      "Validation: Epoch [4], Batch [809/938], Loss: 0.7963000535964966\n",
      "Validation: Epoch [4], Batch [810/938], Loss: 0.6624553203582764\n",
      "Validation: Epoch [4], Batch [811/938], Loss: 0.8114002346992493\n",
      "Validation: Epoch [4], Batch [812/938], Loss: 0.642927885055542\n",
      "Validation: Epoch [4], Batch [813/938], Loss: 0.7741455435752869\n",
      "Validation: Epoch [4], Batch [814/938], Loss: 0.5369473695755005\n",
      "Validation: Epoch [4], Batch [815/938], Loss: 0.7240968942642212\n",
      "Validation: Epoch [4], Batch [816/938], Loss: 0.7874409556388855\n",
      "Validation: Epoch [4], Batch [817/938], Loss: 0.8717777729034424\n",
      "Validation: Epoch [4], Batch [818/938], Loss: 0.8588258028030396\n",
      "Validation: Epoch [4], Batch [819/938], Loss: 0.7507742047309875\n",
      "Validation: Epoch [4], Batch [820/938], Loss: 0.8747414350509644\n",
      "Validation: Epoch [4], Batch [821/938], Loss: 1.1987634897232056\n",
      "Validation: Epoch [4], Batch [822/938], Loss: 0.8857184648513794\n",
      "Validation: Epoch [4], Batch [823/938], Loss: 0.6199904680252075\n",
      "Validation: Epoch [4], Batch [824/938], Loss: 1.1316055059432983\n",
      "Validation: Epoch [4], Batch [825/938], Loss: 1.300734043121338\n",
      "Validation: Epoch [4], Batch [826/938], Loss: 0.7771585583686829\n",
      "Validation: Epoch [4], Batch [827/938], Loss: 0.8510399460792542\n",
      "Validation: Epoch [4], Batch [828/938], Loss: 0.6056808233261108\n",
      "Validation: Epoch [4], Batch [829/938], Loss: 0.7683037519454956\n",
      "Validation: Epoch [4], Batch [830/938], Loss: 0.820600152015686\n",
      "Validation: Epoch [4], Batch [831/938], Loss: 0.740515410900116\n",
      "Validation: Epoch [4], Batch [832/938], Loss: 0.95040363073349\n",
      "Validation: Epoch [4], Batch [833/938], Loss: 0.7157982587814331\n",
      "Validation: Epoch [4], Batch [834/938], Loss: 0.7844043374061584\n",
      "Validation: Epoch [4], Batch [835/938], Loss: 1.033033847808838\n",
      "Validation: Epoch [4], Batch [836/938], Loss: 0.6900997161865234\n",
      "Validation: Epoch [4], Batch [837/938], Loss: 0.75464928150177\n",
      "Validation: Epoch [4], Batch [838/938], Loss: 0.7581937909126282\n",
      "Validation: Epoch [4], Batch [839/938], Loss: 0.7878777980804443\n",
      "Validation: Epoch [4], Batch [840/938], Loss: 0.8096104860305786\n",
      "Validation: Epoch [4], Batch [841/938], Loss: 0.8679691553115845\n",
      "Validation: Epoch [4], Batch [842/938], Loss: 0.8389931917190552\n",
      "Validation: Epoch [4], Batch [843/938], Loss: 0.7117111682891846\n",
      "Validation: Epoch [4], Batch [844/938], Loss: 0.705693244934082\n",
      "Validation: Epoch [4], Batch [845/938], Loss: 0.7077842354774475\n",
      "Validation: Epoch [4], Batch [846/938], Loss: 0.687456488609314\n",
      "Validation: Epoch [4], Batch [847/938], Loss: 0.7713650465011597\n",
      "Validation: Epoch [4], Batch [848/938], Loss: 0.733050525188446\n",
      "Validation: Epoch [4], Batch [849/938], Loss: 0.7995695471763611\n",
      "Validation: Epoch [4], Batch [850/938], Loss: 0.8617044687271118\n",
      "Validation: Epoch [4], Batch [851/938], Loss: 0.6571866273880005\n",
      "Validation: Epoch [4], Batch [852/938], Loss: 0.6823303699493408\n",
      "Validation: Epoch [4], Batch [853/938], Loss: 0.7156794667243958\n",
      "Validation: Epoch [4], Batch [854/938], Loss: 0.754630446434021\n",
      "Validation: Epoch [4], Batch [855/938], Loss: 0.88206946849823\n",
      "Validation: Epoch [4], Batch [856/938], Loss: 0.6122919321060181\n",
      "Validation: Epoch [4], Batch [857/938], Loss: 0.7983857989311218\n",
      "Validation: Epoch [4], Batch [858/938], Loss: 0.7669377326965332\n",
      "Validation: Epoch [4], Batch [859/938], Loss: 0.5922451615333557\n",
      "Validation: Epoch [4], Batch [860/938], Loss: 0.7592963576316833\n",
      "Validation: Epoch [4], Batch [861/938], Loss: 0.8867199420928955\n",
      "Validation: Epoch [4], Batch [862/938], Loss: 0.645276665687561\n",
      "Validation: Epoch [4], Batch [863/938], Loss: 0.5445468425750732\n",
      "Validation: Epoch [4], Batch [864/938], Loss: 0.887495756149292\n",
      "Validation: Epoch [4], Batch [865/938], Loss: 0.8843270540237427\n",
      "Validation: Epoch [4], Batch [866/938], Loss: 0.8742271065711975\n",
      "Validation: Epoch [4], Batch [867/938], Loss: 0.7440794706344604\n",
      "Validation: Epoch [4], Batch [868/938], Loss: 0.6431884169578552\n",
      "Validation: Epoch [4], Batch [869/938], Loss: 0.8621057271957397\n",
      "Validation: Epoch [4], Batch [870/938], Loss: 0.6916877031326294\n",
      "Validation: Epoch [4], Batch [871/938], Loss: 0.6285050511360168\n",
      "Validation: Epoch [4], Batch [872/938], Loss: 0.6775104999542236\n",
      "Validation: Epoch [4], Batch [873/938], Loss: 0.8752794861793518\n",
      "Validation: Epoch [4], Batch [874/938], Loss: 0.6918380260467529\n",
      "Validation: Epoch [4], Batch [875/938], Loss: 0.8590075969696045\n",
      "Validation: Epoch [4], Batch [876/938], Loss: 0.8376452922821045\n",
      "Validation: Epoch [4], Batch [877/938], Loss: 0.845477819442749\n",
      "Validation: Epoch [4], Batch [878/938], Loss: 0.5464050769805908\n",
      "Validation: Epoch [4], Batch [879/938], Loss: 0.6649885177612305\n",
      "Validation: Epoch [4], Batch [880/938], Loss: 0.7567663788795471\n",
      "Validation: Epoch [4], Batch [881/938], Loss: 0.7505779266357422\n",
      "Validation: Epoch [4], Batch [882/938], Loss: 0.4930606484413147\n",
      "Validation: Epoch [4], Batch [883/938], Loss: 0.8753591775894165\n",
      "Validation: Epoch [4], Batch [884/938], Loss: 0.6996766924858093\n",
      "Validation: Epoch [4], Batch [885/938], Loss: 0.8225452899932861\n",
      "Validation: Epoch [4], Batch [886/938], Loss: 0.5995937585830688\n",
      "Validation: Epoch [4], Batch [887/938], Loss: 0.6860690712928772\n",
      "Validation: Epoch [4], Batch [888/938], Loss: 0.7169151902198792\n",
      "Validation: Epoch [4], Batch [889/938], Loss: 0.8168080449104309\n",
      "Validation: Epoch [4], Batch [890/938], Loss: 0.6980563402175903\n",
      "Validation: Epoch [4], Batch [891/938], Loss: 0.7706136703491211\n",
      "Validation: Epoch [4], Batch [892/938], Loss: 0.9417967200279236\n",
      "Validation: Epoch [4], Batch [893/938], Loss: 0.8454883098602295\n",
      "Validation: Epoch [4], Batch [894/938], Loss: 0.775232195854187\n",
      "Validation: Epoch [4], Batch [895/938], Loss: 0.5394397974014282\n",
      "Validation: Epoch [4], Batch [896/938], Loss: 0.8634775876998901\n",
      "Validation: Epoch [4], Batch [897/938], Loss: 0.8328845500946045\n",
      "Validation: Epoch [4], Batch [898/938], Loss: 0.8111163973808289\n",
      "Validation: Epoch [4], Batch [899/938], Loss: 0.7437971830368042\n",
      "Validation: Epoch [4], Batch [900/938], Loss: 0.8211230039596558\n",
      "Validation: Epoch [4], Batch [901/938], Loss: 0.7687403559684753\n",
      "Validation: Epoch [4], Batch [902/938], Loss: 0.6725553274154663\n",
      "Validation: Epoch [4], Batch [903/938], Loss: 0.603405237197876\n",
      "Validation: Epoch [4], Batch [904/938], Loss: 0.7333283424377441\n",
      "Validation: Epoch [4], Batch [905/938], Loss: 0.852821409702301\n",
      "Validation: Epoch [4], Batch [906/938], Loss: 0.7669050693511963\n",
      "Validation: Epoch [4], Batch [907/938], Loss: 0.6454061269760132\n",
      "Validation: Epoch [4], Batch [908/938], Loss: 0.875316858291626\n",
      "Validation: Epoch [4], Batch [909/938], Loss: 0.6521910429000854\n",
      "Validation: Epoch [4], Batch [910/938], Loss: 0.8041731715202332\n",
      "Validation: Epoch [4], Batch [911/938], Loss: 0.7556225061416626\n",
      "Validation: Epoch [4], Batch [912/938], Loss: 0.6135936975479126\n",
      "Validation: Epoch [4], Batch [913/938], Loss: 0.6649564504623413\n",
      "Validation: Epoch [4], Batch [914/938], Loss: 0.7538769245147705\n",
      "Validation: Epoch [4], Batch [915/938], Loss: 0.8703439235687256\n",
      "Validation: Epoch [4], Batch [916/938], Loss: 0.7895374298095703\n",
      "Validation: Epoch [4], Batch [917/938], Loss: 0.5510363578796387\n",
      "Validation: Epoch [4], Batch [918/938], Loss: 0.7825254201889038\n",
      "Validation: Epoch [4], Batch [919/938], Loss: 0.6878347992897034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [4], Batch [920/938], Loss: 0.6462995409965515\n",
      "Validation: Epoch [4], Batch [921/938], Loss: 0.6789491176605225\n",
      "Validation: Epoch [4], Batch [922/938], Loss: 0.6431130170822144\n",
      "Validation: Epoch [4], Batch [923/938], Loss: 0.7720906734466553\n",
      "Validation: Epoch [4], Batch [924/938], Loss: 0.4742814898490906\n",
      "Validation: Epoch [4], Batch [925/938], Loss: 0.7931239008903503\n",
      "Validation: Epoch [4], Batch [926/938], Loss: 0.7528269290924072\n",
      "Validation: Epoch [4], Batch [927/938], Loss: 0.8974313735961914\n",
      "Validation: Epoch [4], Batch [928/938], Loss: 0.5705933570861816\n",
      "Validation: Epoch [4], Batch [929/938], Loss: 0.762641429901123\n",
      "Validation: Epoch [4], Batch [930/938], Loss: 0.6924042105674744\n",
      "Validation: Epoch [4], Batch [931/938], Loss: 1.010084629058838\n",
      "Validation: Epoch [4], Batch [932/938], Loss: 0.7737501263618469\n",
      "Validation: Epoch [4], Batch [933/938], Loss: 1.0736525058746338\n",
      "Validation: Epoch [4], Batch [934/938], Loss: 1.0555500984191895\n",
      "Validation: Epoch [4], Batch [935/938], Loss: 0.7232342958450317\n",
      "Validation: Epoch [4], Batch [936/938], Loss: 0.6401704549789429\n",
      "Validation: Epoch [4], Batch [937/938], Loss: 0.6791598796844482\n",
      "Validation: Epoch [4], Batch [938/938], Loss: 0.9182443618774414\n",
      "Accuracy of test set: 0.7158666666666667\n",
      "Train: Epoch [5], Batch [1/938], Loss: 0.6185586452484131\n",
      "Train: Epoch [5], Batch [2/938], Loss: 1.0224721431732178\n",
      "Train: Epoch [5], Batch [3/938], Loss: 0.7667315006256104\n",
      "Train: Epoch [5], Batch [4/938], Loss: 0.711965024471283\n",
      "Train: Epoch [5], Batch [5/938], Loss: 0.6803390979766846\n",
      "Train: Epoch [5], Batch [6/938], Loss: 0.7417789697647095\n",
      "Train: Epoch [5], Batch [7/938], Loss: 0.9038640260696411\n",
      "Train: Epoch [5], Batch [8/938], Loss: 0.5667357444763184\n",
      "Train: Epoch [5], Batch [9/938], Loss: 0.8725898265838623\n",
      "Train: Epoch [5], Batch [10/938], Loss: 1.008455753326416\n",
      "Train: Epoch [5], Batch [11/938], Loss: 0.7030377388000488\n",
      "Train: Epoch [5], Batch [12/938], Loss: 0.9923784732818604\n",
      "Train: Epoch [5], Batch [13/938], Loss: 0.5112468600273132\n",
      "Train: Epoch [5], Batch [14/938], Loss: 0.7247769832611084\n",
      "Train: Epoch [5], Batch [15/938], Loss: 0.6447591781616211\n",
      "Train: Epoch [5], Batch [16/938], Loss: 0.6445010900497437\n",
      "Train: Epoch [5], Batch [17/938], Loss: 0.9075450897216797\n",
      "Train: Epoch [5], Batch [18/938], Loss: 0.6557130217552185\n",
      "Train: Epoch [5], Batch [19/938], Loss: 0.7062337398529053\n",
      "Train: Epoch [5], Batch [20/938], Loss: 0.6822994947433472\n",
      "Train: Epoch [5], Batch [21/938], Loss: 0.6900081634521484\n",
      "Train: Epoch [5], Batch [22/938], Loss: 0.7391867637634277\n",
      "Train: Epoch [5], Batch [23/938], Loss: 0.6760276556015015\n",
      "Train: Epoch [5], Batch [24/938], Loss: 0.8521450161933899\n",
      "Train: Epoch [5], Batch [25/938], Loss: 0.6473150253295898\n",
      "Train: Epoch [5], Batch [26/938], Loss: 0.6972647905349731\n",
      "Train: Epoch [5], Batch [27/938], Loss: 0.6102396249771118\n",
      "Train: Epoch [5], Batch [28/938], Loss: 0.7947622537612915\n",
      "Train: Epoch [5], Batch [29/938], Loss: 0.7953391075134277\n",
      "Train: Epoch [5], Batch [30/938], Loss: 0.6448602676391602\n",
      "Train: Epoch [5], Batch [31/938], Loss: 0.8077429533004761\n",
      "Train: Epoch [5], Batch [32/938], Loss: 0.5843477845191956\n",
      "Train: Epoch [5], Batch [33/938], Loss: 0.6430532932281494\n",
      "Train: Epoch [5], Batch [34/938], Loss: 0.8243616223335266\n",
      "Train: Epoch [5], Batch [35/938], Loss: 0.6127380132675171\n",
      "Train: Epoch [5], Batch [36/938], Loss: 0.7909610271453857\n",
      "Train: Epoch [5], Batch [37/938], Loss: 0.6712132692337036\n",
      "Train: Epoch [5], Batch [38/938], Loss: 0.6663869619369507\n",
      "Train: Epoch [5], Batch [39/938], Loss: 0.710945725440979\n",
      "Train: Epoch [5], Batch [40/938], Loss: 0.7634992599487305\n",
      "Train: Epoch [5], Batch [41/938], Loss: 0.8340875506401062\n",
      "Train: Epoch [5], Batch [42/938], Loss: 0.6122000217437744\n",
      "Train: Epoch [5], Batch [43/938], Loss: 0.7695848345756531\n",
      "Train: Epoch [5], Batch [44/938], Loss: 0.8814960718154907\n",
      "Train: Epoch [5], Batch [45/938], Loss: 0.7212854027748108\n",
      "Train: Epoch [5], Batch [46/938], Loss: 0.5835621356964111\n",
      "Train: Epoch [5], Batch [47/938], Loss: 0.6779390573501587\n",
      "Train: Epoch [5], Batch [48/938], Loss: 0.8225834369659424\n",
      "Train: Epoch [5], Batch [49/938], Loss: 0.7377730011940002\n",
      "Train: Epoch [5], Batch [50/938], Loss: 0.8409750461578369\n",
      "Train: Epoch [5], Batch [51/938], Loss: 0.761020839214325\n",
      "Train: Epoch [5], Batch [52/938], Loss: 0.6483055353164673\n",
      "Train: Epoch [5], Batch [53/938], Loss: 0.6696579456329346\n",
      "Train: Epoch [5], Batch [54/938], Loss: 0.5270920991897583\n",
      "Train: Epoch [5], Batch [55/938], Loss: 1.0423214435577393\n",
      "Train: Epoch [5], Batch [56/938], Loss: 0.7331791520118713\n",
      "Train: Epoch [5], Batch [57/938], Loss: 0.6851912140846252\n",
      "Train: Epoch [5], Batch [58/938], Loss: 0.585131049156189\n",
      "Train: Epoch [5], Batch [59/938], Loss: 0.8368238806724548\n",
      "Train: Epoch [5], Batch [60/938], Loss: 0.7030541896820068\n",
      "Train: Epoch [5], Batch [61/938], Loss: 0.49302032589912415\n",
      "Train: Epoch [5], Batch [62/938], Loss: 0.5992878675460815\n",
      "Train: Epoch [5], Batch [63/938], Loss: 1.099351406097412\n",
      "Train: Epoch [5], Batch [64/938], Loss: 0.9393731355667114\n",
      "Train: Epoch [5], Batch [65/938], Loss: 0.7864003777503967\n",
      "Train: Epoch [5], Batch [66/938], Loss: 0.7358636856079102\n",
      "Train: Epoch [5], Batch [67/938], Loss: 0.8606468439102173\n",
      "Train: Epoch [5], Batch [68/938], Loss: 0.6960878372192383\n",
      "Train: Epoch [5], Batch [69/938], Loss: 0.7081546783447266\n",
      "Train: Epoch [5], Batch [70/938], Loss: 0.8235191702842712\n",
      "Train: Epoch [5], Batch [71/938], Loss: 0.8314252495765686\n",
      "Train: Epoch [5], Batch [72/938], Loss: 0.9297140836715698\n",
      "Train: Epoch [5], Batch [73/938], Loss: 0.6854944229125977\n",
      "Train: Epoch [5], Batch [74/938], Loss: 0.7345484495162964\n",
      "Train: Epoch [5], Batch [75/938], Loss: 0.567195475101471\n",
      "Train: Epoch [5], Batch [76/938], Loss: 0.7017325758934021\n",
      "Train: Epoch [5], Batch [77/938], Loss: 0.7308813333511353\n",
      "Train: Epoch [5], Batch [78/938], Loss: 0.9295703172683716\n",
      "Train: Epoch [5], Batch [79/938], Loss: 0.7014108896255493\n",
      "Train: Epoch [5], Batch [80/938], Loss: 0.7386115789413452\n",
      "Train: Epoch [5], Batch [81/938], Loss: 0.6931679248809814\n",
      "Train: Epoch [5], Batch [82/938], Loss: 0.7448385953903198\n",
      "Train: Epoch [5], Batch [83/938], Loss: 0.9136314392089844\n",
      "Train: Epoch [5], Batch [84/938], Loss: 0.7060255408287048\n",
      "Train: Epoch [5], Batch [85/938], Loss: 0.9594153165817261\n",
      "Train: Epoch [5], Batch [86/938], Loss: 0.5897926092147827\n",
      "Train: Epoch [5], Batch [87/938], Loss: 0.9375521540641785\n",
      "Train: Epoch [5], Batch [88/938], Loss: 0.655821681022644\n",
      "Train: Epoch [5], Batch [89/938], Loss: 0.824157178401947\n",
      "Train: Epoch [5], Batch [90/938], Loss: 0.9505684971809387\n",
      "Train: Epoch [5], Batch [91/938], Loss: 0.7550138235092163\n",
      "Train: Epoch [5], Batch [92/938], Loss: 0.5887103080749512\n",
      "Train: Epoch [5], Batch [93/938], Loss: 0.8906042575836182\n",
      "Train: Epoch [5], Batch [94/938], Loss: 0.7115998268127441\n",
      "Train: Epoch [5], Batch [95/938], Loss: 0.7497130632400513\n",
      "Train: Epoch [5], Batch [96/938], Loss: 0.5850308537483215\n",
      "Train: Epoch [5], Batch [97/938], Loss: 0.7457312345504761\n",
      "Train: Epoch [5], Batch [98/938], Loss: 0.7101689577102661\n",
      "Train: Epoch [5], Batch [99/938], Loss: 0.5576918125152588\n",
      "Train: Epoch [5], Batch [100/938], Loss: 0.6472464203834534\n",
      "Train: Epoch [5], Batch [101/938], Loss: 0.9114173650741577\n",
      "Train: Epoch [5], Batch [102/938], Loss: 0.7051481604576111\n",
      "Train: Epoch [5], Batch [103/938], Loss: 0.7276232242584229\n",
      "Train: Epoch [5], Batch [104/938], Loss: 0.7335048913955688\n",
      "Train: Epoch [5], Batch [105/938], Loss: 0.7387654781341553\n",
      "Train: Epoch [5], Batch [106/938], Loss: 0.7698681354522705\n",
      "Train: Epoch [5], Batch [107/938], Loss: 0.8632116913795471\n",
      "Train: Epoch [5], Batch [108/938], Loss: 0.8573535084724426\n",
      "Train: Epoch [5], Batch [109/938], Loss: 0.9095356464385986\n",
      "Train: Epoch [5], Batch [110/938], Loss: 0.8309732675552368\n",
      "Train: Epoch [5], Batch [111/938], Loss: 0.639511227607727\n",
      "Train: Epoch [5], Batch [112/938], Loss: 0.8954592943191528\n",
      "Train: Epoch [5], Batch [113/938], Loss: 0.6323956251144409\n",
      "Train: Epoch [5], Batch [114/938], Loss: 0.6362543702125549\n",
      "Train: Epoch [5], Batch [115/938], Loss: 0.921974778175354\n",
      "Train: Epoch [5], Batch [116/938], Loss: 0.5966233015060425\n",
      "Train: Epoch [5], Batch [117/938], Loss: 0.8317442536354065\n",
      "Train: Epoch [5], Batch [118/938], Loss: 0.5771212577819824\n",
      "Train: Epoch [5], Batch [119/938], Loss: 0.673433780670166\n",
      "Train: Epoch [5], Batch [120/938], Loss: 0.640701413154602\n",
      "Train: Epoch [5], Batch [121/938], Loss: 0.620810866355896\n",
      "Train: Epoch [5], Batch [122/938], Loss: 0.5731552839279175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [5], Batch [123/938], Loss: 0.8740455508232117\n",
      "Train: Epoch [5], Batch [124/938], Loss: 0.7596230506896973\n",
      "Train: Epoch [5], Batch [125/938], Loss: 0.6164513230323792\n",
      "Train: Epoch [5], Batch [126/938], Loss: 0.7265770435333252\n",
      "Train: Epoch [5], Batch [127/938], Loss: 0.9537807703018188\n",
      "Train: Epoch [5], Batch [128/938], Loss: 0.8499376773834229\n",
      "Train: Epoch [5], Batch [129/938], Loss: 0.6151702404022217\n",
      "Train: Epoch [5], Batch [130/938], Loss: 0.8057094812393188\n",
      "Train: Epoch [5], Batch [131/938], Loss: 0.8670462369918823\n",
      "Train: Epoch [5], Batch [132/938], Loss: 0.7835708856582642\n",
      "Train: Epoch [5], Batch [133/938], Loss: 0.6961347460746765\n",
      "Train: Epoch [5], Batch [134/938], Loss: 0.7656960487365723\n",
      "Train: Epoch [5], Batch [135/938], Loss: 0.5557795166969299\n",
      "Train: Epoch [5], Batch [136/938], Loss: 0.8736320734024048\n",
      "Train: Epoch [5], Batch [137/938], Loss: 0.5556037425994873\n",
      "Train: Epoch [5], Batch [138/938], Loss: 0.8165926337242126\n",
      "Train: Epoch [5], Batch [139/938], Loss: 0.6509624719619751\n",
      "Train: Epoch [5], Batch [140/938], Loss: 0.9829819798469543\n",
      "Train: Epoch [5], Batch [141/938], Loss: 0.948632538318634\n",
      "Train: Epoch [5], Batch [142/938], Loss: 0.8313083052635193\n",
      "Train: Epoch [5], Batch [143/938], Loss: 0.8527015447616577\n",
      "Train: Epoch [5], Batch [144/938], Loss: 0.7780420184135437\n",
      "Train: Epoch [5], Batch [145/938], Loss: 0.5798782110214233\n",
      "Train: Epoch [5], Batch [146/938], Loss: 0.8116961717605591\n",
      "Train: Epoch [5], Batch [147/938], Loss: 0.7236475944519043\n",
      "Train: Epoch [5], Batch [148/938], Loss: 0.5611900687217712\n",
      "Train: Epoch [5], Batch [149/938], Loss: 0.6893984079360962\n",
      "Train: Epoch [5], Batch [150/938], Loss: 0.6304425001144409\n",
      "Train: Epoch [5], Batch [151/938], Loss: 0.6942996978759766\n",
      "Train: Epoch [5], Batch [152/938], Loss: 0.8371540307998657\n",
      "Train: Epoch [5], Batch [153/938], Loss: 0.6583458185195923\n",
      "Train: Epoch [5], Batch [154/938], Loss: 1.0023165941238403\n",
      "Train: Epoch [5], Batch [155/938], Loss: 0.7559613585472107\n",
      "Train: Epoch [5], Batch [156/938], Loss: 0.8220340013504028\n",
      "Train: Epoch [5], Batch [157/938], Loss: 0.8676073551177979\n",
      "Train: Epoch [5], Batch [158/938], Loss: 0.5760212540626526\n",
      "Train: Epoch [5], Batch [159/938], Loss: 0.8039395809173584\n",
      "Train: Epoch [5], Batch [160/938], Loss: 0.7293925881385803\n",
      "Train: Epoch [5], Batch [161/938], Loss: 0.6823943853378296\n",
      "Train: Epoch [5], Batch [162/938], Loss: 0.5670939683914185\n",
      "Train: Epoch [5], Batch [163/938], Loss: 0.7575406432151794\n",
      "Train: Epoch [5], Batch [164/938], Loss: 0.7881053686141968\n",
      "Train: Epoch [5], Batch [165/938], Loss: 0.8727371692657471\n",
      "Train: Epoch [5], Batch [166/938], Loss: 0.939008891582489\n",
      "Train: Epoch [5], Batch [167/938], Loss: 0.6962084174156189\n",
      "Train: Epoch [5], Batch [168/938], Loss: 0.5906145572662354\n",
      "Train: Epoch [5], Batch [169/938], Loss: 0.6240425109863281\n",
      "Train: Epoch [5], Batch [170/938], Loss: 0.7297995686531067\n",
      "Train: Epoch [5], Batch [171/938], Loss: 0.8754274249076843\n",
      "Train: Epoch [5], Batch [172/938], Loss: 0.5543181896209717\n",
      "Train: Epoch [5], Batch [173/938], Loss: 0.7520363926887512\n",
      "Train: Epoch [5], Batch [174/938], Loss: 0.6715794801712036\n",
      "Train: Epoch [5], Batch [175/938], Loss: 0.7337843179702759\n",
      "Train: Epoch [5], Batch [176/938], Loss: 1.1508182287216187\n",
      "Train: Epoch [5], Batch [177/938], Loss: 0.6505804657936096\n",
      "Train: Epoch [5], Batch [178/938], Loss: 0.6682320237159729\n",
      "Train: Epoch [5], Batch [179/938], Loss: 0.9729882478713989\n",
      "Train: Epoch [5], Batch [180/938], Loss: 0.5628646612167358\n",
      "Train: Epoch [5], Batch [181/938], Loss: 0.744156002998352\n",
      "Train: Epoch [5], Batch [182/938], Loss: 0.6171215176582336\n",
      "Train: Epoch [5], Batch [183/938], Loss: 0.8064894676208496\n",
      "Train: Epoch [5], Batch [184/938], Loss: 0.8005915880203247\n",
      "Train: Epoch [5], Batch [185/938], Loss: 0.7030996680259705\n",
      "Train: Epoch [5], Batch [186/938], Loss: 0.5796089172363281\n",
      "Train: Epoch [5], Batch [187/938], Loss: 0.7058252096176147\n",
      "Train: Epoch [5], Batch [188/938], Loss: 0.7461207509040833\n",
      "Train: Epoch [5], Batch [189/938], Loss: 0.9027989506721497\n",
      "Train: Epoch [5], Batch [190/938], Loss: 0.6134105324745178\n",
      "Train: Epoch [5], Batch [191/938], Loss: 0.5457713603973389\n",
      "Train: Epoch [5], Batch [192/938], Loss: 0.7071080207824707\n",
      "Train: Epoch [5], Batch [193/938], Loss: 0.7542394399642944\n",
      "Train: Epoch [5], Batch [194/938], Loss: 0.7341864109039307\n",
      "Train: Epoch [5], Batch [195/938], Loss: 0.8710857629776001\n",
      "Train: Epoch [5], Batch [196/938], Loss: 0.6503276228904724\n",
      "Train: Epoch [5], Batch [197/938], Loss: 0.8067827820777893\n",
      "Train: Epoch [5], Batch [198/938], Loss: 0.6054485440254211\n",
      "Train: Epoch [5], Batch [199/938], Loss: 0.7490440607070923\n",
      "Train: Epoch [5], Batch [200/938], Loss: 0.8685011267662048\n",
      "Train: Epoch [5], Batch [201/938], Loss: 0.821581244468689\n",
      "Train: Epoch [5], Batch [202/938], Loss: 0.46608003973960876\n",
      "Train: Epoch [5], Batch [203/938], Loss: 0.6687451004981995\n",
      "Train: Epoch [5], Batch [204/938], Loss: 0.9244102239608765\n",
      "Train: Epoch [5], Batch [205/938], Loss: 0.7471864223480225\n",
      "Train: Epoch [5], Batch [206/938], Loss: 0.667971670627594\n",
      "Train: Epoch [5], Batch [207/938], Loss: 0.650540828704834\n",
      "Train: Epoch [5], Batch [208/938], Loss: 0.8049635887145996\n",
      "Train: Epoch [5], Batch [209/938], Loss: 0.7297570705413818\n",
      "Train: Epoch [5], Batch [210/938], Loss: 0.8479092717170715\n",
      "Train: Epoch [5], Batch [211/938], Loss: 0.7318470478057861\n",
      "Train: Epoch [5], Batch [212/938], Loss: 0.7603151798248291\n",
      "Train: Epoch [5], Batch [213/938], Loss: 0.667953610420227\n",
      "Train: Epoch [5], Batch [214/938], Loss: 0.7655878067016602\n",
      "Train: Epoch [5], Batch [215/938], Loss: 0.5737894773483276\n",
      "Train: Epoch [5], Batch [216/938], Loss: 0.6641745567321777\n",
      "Train: Epoch [5], Batch [217/938], Loss: 0.7102495431900024\n",
      "Train: Epoch [5], Batch [218/938], Loss: 0.5815324783325195\n",
      "Train: Epoch [5], Batch [219/938], Loss: 0.7775940299034119\n",
      "Train: Epoch [5], Batch [220/938], Loss: 0.7649946212768555\n",
      "Train: Epoch [5], Batch [221/938], Loss: 0.6082348227500916\n",
      "Train: Epoch [5], Batch [222/938], Loss: 0.6752495765686035\n",
      "Train: Epoch [5], Batch [223/938], Loss: 0.7746850848197937\n",
      "Train: Epoch [5], Batch [224/938], Loss: 0.7468656897544861\n",
      "Train: Epoch [5], Batch [225/938], Loss: 0.8659836053848267\n",
      "Train: Epoch [5], Batch [226/938], Loss: 0.8772700428962708\n",
      "Train: Epoch [5], Batch [227/938], Loss: 0.6732140779495239\n",
      "Train: Epoch [5], Batch [228/938], Loss: 0.751600980758667\n",
      "Train: Epoch [5], Batch [229/938], Loss: 0.7336380481719971\n",
      "Train: Epoch [5], Batch [230/938], Loss: 0.7405291795730591\n",
      "Train: Epoch [5], Batch [231/938], Loss: 0.9364767670631409\n",
      "Train: Epoch [5], Batch [232/938], Loss: 0.5269026756286621\n",
      "Train: Epoch [5], Batch [233/938], Loss: 0.9631912708282471\n",
      "Train: Epoch [5], Batch [234/938], Loss: 0.7600886225700378\n",
      "Train: Epoch [5], Batch [235/938], Loss: 0.6768269538879395\n",
      "Train: Epoch [5], Batch [236/938], Loss: 0.5928567051887512\n",
      "Train: Epoch [5], Batch [237/938], Loss: 0.6758238077163696\n",
      "Train: Epoch [5], Batch [238/938], Loss: 0.8008778095245361\n",
      "Train: Epoch [5], Batch [239/938], Loss: 0.6973081827163696\n",
      "Train: Epoch [5], Batch [240/938], Loss: 0.7156002521514893\n",
      "Train: Epoch [5], Batch [241/938], Loss: 0.6066577434539795\n",
      "Train: Epoch [5], Batch [242/938], Loss: 0.6713224649429321\n",
      "Train: Epoch [5], Batch [243/938], Loss: 0.7953671216964722\n",
      "Train: Epoch [5], Batch [244/938], Loss: 0.6915251016616821\n",
      "Train: Epoch [5], Batch [245/938], Loss: 0.8147070407867432\n",
      "Train: Epoch [5], Batch [246/938], Loss: 0.7335167527198792\n",
      "Train: Epoch [5], Batch [247/938], Loss: 0.7902393341064453\n",
      "Train: Epoch [5], Batch [248/938], Loss: 0.5416593551635742\n",
      "Train: Epoch [5], Batch [249/938], Loss: 0.9926061630249023\n",
      "Train: Epoch [5], Batch [250/938], Loss: 0.7013158202171326\n",
      "Train: Epoch [5], Batch [251/938], Loss: 0.6560827493667603\n",
      "Train: Epoch [5], Batch [252/938], Loss: 0.5357998609542847\n",
      "Train: Epoch [5], Batch [253/938], Loss: 0.5801359415054321\n",
      "Train: Epoch [5], Batch [254/938], Loss: 0.8001002073287964\n",
      "Train: Epoch [5], Batch [255/938], Loss: 0.8050900101661682\n",
      "Train: Epoch [5], Batch [256/938], Loss: 0.6808702945709229\n",
      "Train: Epoch [5], Batch [257/938], Loss: 0.7266993522644043\n",
      "Train: Epoch [5], Batch [258/938], Loss: 0.7557746171951294\n",
      "Train: Epoch [5], Batch [259/938], Loss: 0.6087743043899536\n",
      "Train: Epoch [5], Batch [260/938], Loss: 0.7743692398071289\n",
      "Train: Epoch [5], Batch [261/938], Loss: 0.6236330270767212\n",
      "Train: Epoch [5], Batch [262/938], Loss: 0.7582741975784302\n",
      "Train: Epoch [5], Batch [263/938], Loss: 0.6433422565460205\n",
      "Train: Epoch [5], Batch [264/938], Loss: 0.5954523682594299\n",
      "Train: Epoch [5], Batch [265/938], Loss: 0.7082990407943726\n",
      "Train: Epoch [5], Batch [266/938], Loss: 0.5357288122177124\n",
      "Train: Epoch [5], Batch [267/938], Loss: 0.524653971195221\n",
      "Train: Epoch [5], Batch [268/938], Loss: 0.6450896263122559\n",
      "Train: Epoch [5], Batch [269/938], Loss: 0.6369178295135498\n",
      "Train: Epoch [5], Batch [270/938], Loss: 0.7154614925384521\n",
      "Train: Epoch [5], Batch [271/938], Loss: 0.5414876937866211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [5], Batch [272/938], Loss: 0.754239559173584\n",
      "Train: Epoch [5], Batch [273/938], Loss: 0.6352379322052002\n",
      "Train: Epoch [5], Batch [274/938], Loss: 0.8315318822860718\n",
      "Train: Epoch [5], Batch [275/938], Loss: 0.5415343046188354\n",
      "Train: Epoch [5], Batch [276/938], Loss: 0.8297806978225708\n",
      "Train: Epoch [5], Batch [277/938], Loss: 0.6288541555404663\n",
      "Train: Epoch [5], Batch [278/938], Loss: 0.8495810031890869\n",
      "Train: Epoch [5], Batch [279/938], Loss: 0.5901471376419067\n",
      "Train: Epoch [5], Batch [280/938], Loss: 0.643230676651001\n",
      "Train: Epoch [5], Batch [281/938], Loss: 0.5709861516952515\n",
      "Train: Epoch [5], Batch [282/938], Loss: 0.8035519123077393\n",
      "Train: Epoch [5], Batch [283/938], Loss: 0.5709218978881836\n",
      "Train: Epoch [5], Batch [284/938], Loss: 0.7320233583450317\n",
      "Train: Epoch [5], Batch [285/938], Loss: 0.7328678369522095\n",
      "Train: Epoch [5], Batch [286/938], Loss: 0.7011454105377197\n",
      "Train: Epoch [5], Batch [287/938], Loss: 0.5375656485557556\n",
      "Train: Epoch [5], Batch [288/938], Loss: 0.5808064341545105\n",
      "Train: Epoch [5], Batch [289/938], Loss: 0.7297817468643188\n",
      "Train: Epoch [5], Batch [290/938], Loss: 0.8060863614082336\n",
      "Train: Epoch [5], Batch [291/938], Loss: 0.6264322996139526\n",
      "Train: Epoch [5], Batch [292/938], Loss: 0.6715599894523621\n",
      "Train: Epoch [5], Batch [293/938], Loss: 0.7988845109939575\n",
      "Train: Epoch [5], Batch [294/938], Loss: 0.709149956703186\n",
      "Train: Epoch [5], Batch [295/938], Loss: 0.6395540833473206\n",
      "Train: Epoch [5], Batch [296/938], Loss: 0.5994885563850403\n",
      "Train: Epoch [5], Batch [297/938], Loss: 0.8090010285377502\n",
      "Train: Epoch [5], Batch [298/938], Loss: 0.5667001605033875\n",
      "Train: Epoch [5], Batch [299/938], Loss: 0.8031560778617859\n",
      "Train: Epoch [5], Batch [300/938], Loss: 0.6343547105789185\n",
      "Train: Epoch [5], Batch [301/938], Loss: 0.694783091545105\n",
      "Train: Epoch [5], Batch [302/938], Loss: 0.6621482372283936\n",
      "Train: Epoch [5], Batch [303/938], Loss: 0.9254558086395264\n",
      "Train: Epoch [5], Batch [304/938], Loss: 0.6597088575363159\n",
      "Train: Epoch [5], Batch [305/938], Loss: 0.8652420043945312\n",
      "Train: Epoch [5], Batch [306/938], Loss: 0.6964032649993896\n",
      "Train: Epoch [5], Batch [307/938], Loss: 0.7191675305366516\n",
      "Train: Epoch [5], Batch [308/938], Loss: 0.6101068258285522\n",
      "Train: Epoch [5], Batch [309/938], Loss: 0.6956782341003418\n",
      "Train: Epoch [5], Batch [310/938], Loss: 0.5995532870292664\n",
      "Train: Epoch [5], Batch [311/938], Loss: 0.7118139266967773\n",
      "Train: Epoch [5], Batch [312/938], Loss: 0.6004155874252319\n",
      "Train: Epoch [5], Batch [313/938], Loss: 0.6502887010574341\n",
      "Train: Epoch [5], Batch [314/938], Loss: 0.5217459797859192\n",
      "Train: Epoch [5], Batch [315/938], Loss: 0.6534649133682251\n",
      "Train: Epoch [5], Batch [316/938], Loss: 0.7286199331283569\n",
      "Train: Epoch [5], Batch [317/938], Loss: 0.6690642833709717\n",
      "Train: Epoch [5], Batch [318/938], Loss: 0.7177646160125732\n",
      "Train: Epoch [5], Batch [319/938], Loss: 0.7038125991821289\n",
      "Train: Epoch [5], Batch [320/938], Loss: 0.7666839957237244\n",
      "Train: Epoch [5], Batch [321/938], Loss: 0.9130295515060425\n",
      "Train: Epoch [5], Batch [322/938], Loss: 0.6517646312713623\n",
      "Train: Epoch [5], Batch [323/938], Loss: 0.5768080353736877\n",
      "Train: Epoch [5], Batch [324/938], Loss: 0.7136526107788086\n",
      "Train: Epoch [5], Batch [325/938], Loss: 0.5976154208183289\n",
      "Train: Epoch [5], Batch [326/938], Loss: 0.8486710786819458\n",
      "Train: Epoch [5], Batch [327/938], Loss: 0.8404228091239929\n",
      "Train: Epoch [5], Batch [328/938], Loss: 0.8292228579521179\n",
      "Train: Epoch [5], Batch [329/938], Loss: 0.627061128616333\n",
      "Train: Epoch [5], Batch [330/938], Loss: 0.7613396644592285\n",
      "Train: Epoch [5], Batch [331/938], Loss: 0.5906444787979126\n",
      "Train: Epoch [5], Batch [332/938], Loss: 0.6794942617416382\n",
      "Train: Epoch [5], Batch [333/938], Loss: 0.5843310356140137\n",
      "Train: Epoch [5], Batch [334/938], Loss: 0.7263990640640259\n",
      "Train: Epoch [5], Batch [335/938], Loss: 0.757995069026947\n",
      "Train: Epoch [5], Batch [336/938], Loss: 0.4443361163139343\n",
      "Train: Epoch [5], Batch [337/938], Loss: 0.8588495850563049\n",
      "Train: Epoch [5], Batch [338/938], Loss: 0.4886974096298218\n",
      "Train: Epoch [5], Batch [339/938], Loss: 0.678249716758728\n",
      "Train: Epoch [5], Batch [340/938], Loss: 0.4604722261428833\n",
      "Train: Epoch [5], Batch [341/938], Loss: 0.6775868535041809\n",
      "Train: Epoch [5], Batch [342/938], Loss: 0.612463116645813\n",
      "Train: Epoch [5], Batch [343/938], Loss: 0.6068195104598999\n",
      "Train: Epoch [5], Batch [344/938], Loss: 0.6811896562576294\n",
      "Train: Epoch [5], Batch [345/938], Loss: 0.7474048137664795\n",
      "Train: Epoch [5], Batch [346/938], Loss: 0.7387166023254395\n",
      "Train: Epoch [5], Batch [347/938], Loss: 0.690858006477356\n",
      "Train: Epoch [5], Batch [348/938], Loss: 0.6228732466697693\n",
      "Train: Epoch [5], Batch [349/938], Loss: 0.8013894557952881\n",
      "Train: Epoch [5], Batch [350/938], Loss: 0.7333943247795105\n",
      "Train: Epoch [5], Batch [351/938], Loss: 0.6248126029968262\n",
      "Train: Epoch [5], Batch [352/938], Loss: 0.6985604166984558\n",
      "Train: Epoch [5], Batch [353/938], Loss: 0.5951017141342163\n",
      "Train: Epoch [5], Batch [354/938], Loss: 0.6465030908584595\n",
      "Train: Epoch [5], Batch [355/938], Loss: 0.7163911461830139\n",
      "Train: Epoch [5], Batch [356/938], Loss: 0.6512181162834167\n",
      "Train: Epoch [5], Batch [357/938], Loss: 0.7511038184165955\n",
      "Train: Epoch [5], Batch [358/938], Loss: 0.7050490379333496\n",
      "Train: Epoch [5], Batch [359/938], Loss: 0.7396787405014038\n",
      "Train: Epoch [5], Batch [360/938], Loss: 0.7780873775482178\n",
      "Train: Epoch [5], Batch [361/938], Loss: 0.6666457056999207\n",
      "Train: Epoch [5], Batch [362/938], Loss: 0.6858546733856201\n",
      "Train: Epoch [5], Batch [363/938], Loss: 0.9015570282936096\n",
      "Train: Epoch [5], Batch [364/938], Loss: 0.6377294063568115\n",
      "Train: Epoch [5], Batch [365/938], Loss: 1.0256365537643433\n",
      "Train: Epoch [5], Batch [366/938], Loss: 1.0327340364456177\n",
      "Train: Epoch [5], Batch [367/938], Loss: 0.7978085279464722\n",
      "Train: Epoch [5], Batch [368/938], Loss: 0.6144757270812988\n",
      "Train: Epoch [5], Batch [369/938], Loss: 0.8148245811462402\n",
      "Train: Epoch [5], Batch [370/938], Loss: 0.5556741952896118\n",
      "Train: Epoch [5], Batch [371/938], Loss: 0.6194577813148499\n",
      "Train: Epoch [5], Batch [372/938], Loss: 0.6540117859840393\n",
      "Train: Epoch [5], Batch [373/938], Loss: 0.6552104949951172\n",
      "Train: Epoch [5], Batch [374/938], Loss: 0.7655625939369202\n",
      "Train: Epoch [5], Batch [375/938], Loss: 0.5932990908622742\n",
      "Train: Epoch [5], Batch [376/938], Loss: 0.6986933946609497\n",
      "Train: Epoch [5], Batch [377/938], Loss: 0.652501106262207\n",
      "Train: Epoch [5], Batch [378/938], Loss: 0.5978457927703857\n",
      "Train: Epoch [5], Batch [379/938], Loss: 0.7026943564414978\n",
      "Train: Epoch [5], Batch [380/938], Loss: 0.7957871556282043\n",
      "Train: Epoch [5], Batch [381/938], Loss: 0.5452672839164734\n",
      "Train: Epoch [5], Batch [382/938], Loss: 0.55741286277771\n",
      "Train: Epoch [5], Batch [383/938], Loss: 0.5988662838935852\n",
      "Train: Epoch [5], Batch [384/938], Loss: 0.6743537783622742\n",
      "Train: Epoch [5], Batch [385/938], Loss: 0.7739024758338928\n",
      "Train: Epoch [5], Batch [386/938], Loss: 0.6569381952285767\n",
      "Train: Epoch [5], Batch [387/938], Loss: 0.6665544509887695\n",
      "Train: Epoch [5], Batch [388/938], Loss: 0.7276131510734558\n",
      "Train: Epoch [5], Batch [389/938], Loss: 0.5777082443237305\n",
      "Train: Epoch [5], Batch [390/938], Loss: 0.6129649877548218\n",
      "Train: Epoch [5], Batch [391/938], Loss: 0.7398171424865723\n",
      "Train: Epoch [5], Batch [392/938], Loss: 0.6690841913223267\n",
      "Train: Epoch [5], Batch [393/938], Loss: 0.8950948119163513\n",
      "Train: Epoch [5], Batch [394/938], Loss: 0.719315767288208\n",
      "Train: Epoch [5], Batch [395/938], Loss: 0.7044543027877808\n",
      "Train: Epoch [5], Batch [396/938], Loss: 0.59794020652771\n",
      "Train: Epoch [5], Batch [397/938], Loss: 0.7257683873176575\n",
      "Train: Epoch [5], Batch [398/938], Loss: 0.76136314868927\n",
      "Train: Epoch [5], Batch [399/938], Loss: 0.7032400965690613\n",
      "Train: Epoch [5], Batch [400/938], Loss: 0.7218130826950073\n",
      "Train: Epoch [5], Batch [401/938], Loss: 0.6127765774726868\n",
      "Train: Epoch [5], Batch [402/938], Loss: 0.8053445816040039\n",
      "Train: Epoch [5], Batch [403/938], Loss: 0.8239744901657104\n",
      "Train: Epoch [5], Batch [404/938], Loss: 0.6960847973823547\n",
      "Train: Epoch [5], Batch [405/938], Loss: 0.7364770174026489\n",
      "Train: Epoch [5], Batch [406/938], Loss: 0.6917067766189575\n",
      "Train: Epoch [5], Batch [407/938], Loss: 0.5791634321212769\n",
      "Train: Epoch [5], Batch [408/938], Loss: 0.8500014543533325\n",
      "Train: Epoch [5], Batch [409/938], Loss: 0.7104575037956238\n",
      "Train: Epoch [5], Batch [410/938], Loss: 0.9797936677932739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [5], Batch [411/938], Loss: 0.8144112825393677\n",
      "Train: Epoch [5], Batch [412/938], Loss: 0.902800440788269\n",
      "Train: Epoch [5], Batch [413/938], Loss: 0.7319344282150269\n",
      "Train: Epoch [5], Batch [414/938], Loss: 0.7609848976135254\n",
      "Train: Epoch [5], Batch [415/938], Loss: 0.6294214725494385\n",
      "Train: Epoch [5], Batch [416/938], Loss: 0.7354910373687744\n",
      "Train: Epoch [5], Batch [417/938], Loss: 0.6571380496025085\n",
      "Train: Epoch [5], Batch [418/938], Loss: 0.803051233291626\n",
      "Train: Epoch [5], Batch [419/938], Loss: 0.6643774509429932\n",
      "Train: Epoch [5], Batch [420/938], Loss: 0.587011456489563\n",
      "Train: Epoch [5], Batch [421/938], Loss: 0.4998089671134949\n",
      "Train: Epoch [5], Batch [422/938], Loss: 0.9856871366500854\n",
      "Train: Epoch [5], Batch [423/938], Loss: 0.7236363887786865\n",
      "Train: Epoch [5], Batch [424/938], Loss: 0.7315771579742432\n",
      "Train: Epoch [5], Batch [425/938], Loss: 0.8431272506713867\n",
      "Train: Epoch [5], Batch [426/938], Loss: 0.5901116132736206\n",
      "Train: Epoch [5], Batch [427/938], Loss: 0.5541598796844482\n",
      "Train: Epoch [5], Batch [428/938], Loss: 0.6356978416442871\n",
      "Train: Epoch [5], Batch [429/938], Loss: 0.7310695648193359\n",
      "Train: Epoch [5], Batch [430/938], Loss: 0.7212672233581543\n",
      "Train: Epoch [5], Batch [431/938], Loss: 0.9170550107955933\n",
      "Train: Epoch [5], Batch [432/938], Loss: 0.5757443904876709\n",
      "Train: Epoch [5], Batch [433/938], Loss: 0.5792250633239746\n",
      "Train: Epoch [5], Batch [434/938], Loss: 0.5705684423446655\n",
      "Train: Epoch [5], Batch [435/938], Loss: 0.629061758518219\n",
      "Train: Epoch [5], Batch [436/938], Loss: 0.6953101754188538\n",
      "Train: Epoch [5], Batch [437/938], Loss: 0.7411981225013733\n",
      "Train: Epoch [5], Batch [438/938], Loss: 0.6660668849945068\n",
      "Train: Epoch [5], Batch [439/938], Loss: 0.6400115489959717\n",
      "Train: Epoch [5], Batch [440/938], Loss: 0.5480673313140869\n",
      "Train: Epoch [5], Batch [441/938], Loss: 0.7186500430107117\n",
      "Train: Epoch [5], Batch [442/938], Loss: 0.664260983467102\n",
      "Train: Epoch [5], Batch [443/938], Loss: 0.5554314851760864\n",
      "Train: Epoch [5], Batch [444/938], Loss: 0.5332250595092773\n",
      "Train: Epoch [5], Batch [445/938], Loss: 1.0305978059768677\n",
      "Train: Epoch [5], Batch [446/938], Loss: 0.6566590070724487\n",
      "Train: Epoch [5], Batch [447/938], Loss: 0.6348382234573364\n",
      "Train: Epoch [5], Batch [448/938], Loss: 0.6049530506134033\n",
      "Train: Epoch [5], Batch [449/938], Loss: 0.5372509956359863\n",
      "Train: Epoch [5], Batch [450/938], Loss: 0.6093194484710693\n",
      "Train: Epoch [5], Batch [451/938], Loss: 0.47666752338409424\n",
      "Train: Epoch [5], Batch [452/938], Loss: 0.7467663884162903\n",
      "Train: Epoch [5], Batch [453/938], Loss: 0.9080041646957397\n",
      "Train: Epoch [5], Batch [454/938], Loss: 0.6904283761978149\n",
      "Train: Epoch [5], Batch [455/938], Loss: 0.5541410446166992\n",
      "Train: Epoch [5], Batch [456/938], Loss: 0.6640759706497192\n",
      "Train: Epoch [5], Batch [457/938], Loss: 0.7278754115104675\n",
      "Train: Epoch [5], Batch [458/938], Loss: 0.5832431316375732\n",
      "Train: Epoch [5], Batch [459/938], Loss: 0.83612060546875\n",
      "Train: Epoch [5], Batch [460/938], Loss: 0.7331191301345825\n",
      "Train: Epoch [5], Batch [461/938], Loss: 0.6732999086380005\n",
      "Train: Epoch [5], Batch [462/938], Loss: 0.7296968698501587\n",
      "Train: Epoch [5], Batch [463/938], Loss: 0.7375422716140747\n",
      "Train: Epoch [5], Batch [464/938], Loss: 0.6213486790657043\n",
      "Train: Epoch [5], Batch [465/938], Loss: 0.8109879493713379\n",
      "Train: Epoch [5], Batch [466/938], Loss: 0.5627927780151367\n",
      "Train: Epoch [5], Batch [467/938], Loss: 0.8422352075576782\n",
      "Train: Epoch [5], Batch [468/938], Loss: 0.6950419545173645\n",
      "Train: Epoch [5], Batch [469/938], Loss: 0.7114608287811279\n",
      "Train: Epoch [5], Batch [470/938], Loss: 0.625198245048523\n",
      "Train: Epoch [5], Batch [471/938], Loss: 0.7400383353233337\n",
      "Train: Epoch [5], Batch [472/938], Loss: 0.892043948173523\n",
      "Train: Epoch [5], Batch [473/938], Loss: 0.7156637907028198\n",
      "Train: Epoch [5], Batch [474/938], Loss: 0.6484888792037964\n",
      "Train: Epoch [5], Batch [475/938], Loss: 0.6581165790557861\n",
      "Train: Epoch [5], Batch [476/938], Loss: 0.9119325876235962\n",
      "Train: Epoch [5], Batch [477/938], Loss: 0.9071984887123108\n",
      "Train: Epoch [5], Batch [478/938], Loss: 0.526948869228363\n",
      "Train: Epoch [5], Batch [479/938], Loss: 0.8132627606391907\n",
      "Train: Epoch [5], Batch [480/938], Loss: 0.6076398491859436\n",
      "Train: Epoch [5], Batch [481/938], Loss: 0.5090715289115906\n",
      "Train: Epoch [5], Batch [482/938], Loss: 0.7690378427505493\n",
      "Train: Epoch [5], Batch [483/938], Loss: 0.5553319454193115\n",
      "Train: Epoch [5], Batch [484/938], Loss: 0.7474192380905151\n",
      "Train: Epoch [5], Batch [485/938], Loss: 0.7274318933486938\n",
      "Train: Epoch [5], Batch [486/938], Loss: 0.756959855556488\n",
      "Train: Epoch [5], Batch [487/938], Loss: 0.3927518129348755\n",
      "Train: Epoch [5], Batch [488/938], Loss: 0.44551944732666016\n",
      "Train: Epoch [5], Batch [489/938], Loss: 0.7288106083869934\n",
      "Train: Epoch [5], Batch [490/938], Loss: 1.0130337476730347\n",
      "Train: Epoch [5], Batch [491/938], Loss: 0.5793589353561401\n",
      "Train: Epoch [5], Batch [492/938], Loss: 0.6131722927093506\n",
      "Train: Epoch [5], Batch [493/938], Loss: 0.5077865123748779\n",
      "Train: Epoch [5], Batch [494/938], Loss: 0.7208511233329773\n",
      "Train: Epoch [5], Batch [495/938], Loss: 0.6312553286552429\n",
      "Train: Epoch [5], Batch [496/938], Loss: 0.5685878992080688\n",
      "Train: Epoch [5], Batch [497/938], Loss: 0.7291679978370667\n",
      "Train: Epoch [5], Batch [498/938], Loss: 0.6289436221122742\n",
      "Train: Epoch [5], Batch [499/938], Loss: 0.7388406991958618\n",
      "Train: Epoch [5], Batch [500/938], Loss: 0.5544931888580322\n",
      "Train: Epoch [5], Batch [501/938], Loss: 0.7429895401000977\n",
      "Train: Epoch [5], Batch [502/938], Loss: 0.7466268539428711\n",
      "Train: Epoch [5], Batch [503/938], Loss: 0.655515730381012\n",
      "Train: Epoch [5], Batch [504/938], Loss: 0.8421449065208435\n",
      "Train: Epoch [5], Batch [505/938], Loss: 0.807075023651123\n",
      "Train: Epoch [5], Batch [506/938], Loss: 0.5827955603599548\n",
      "Train: Epoch [5], Batch [507/938], Loss: 0.7742115259170532\n",
      "Train: Epoch [5], Batch [508/938], Loss: 0.6798703670501709\n",
      "Train: Epoch [5], Batch [509/938], Loss: 0.6411818265914917\n",
      "Train: Epoch [5], Batch [510/938], Loss: 0.8079434633255005\n",
      "Train: Epoch [5], Batch [511/938], Loss: 0.5430167317390442\n",
      "Train: Epoch [5], Batch [512/938], Loss: 0.6846965551376343\n",
      "Train: Epoch [5], Batch [513/938], Loss: 0.8690677881240845\n",
      "Train: Epoch [5], Batch [514/938], Loss: 0.7147098779678345\n",
      "Train: Epoch [5], Batch [515/938], Loss: 0.7538598775863647\n",
      "Train: Epoch [5], Batch [516/938], Loss: 0.649436354637146\n",
      "Train: Epoch [5], Batch [517/938], Loss: 0.623005747795105\n",
      "Train: Epoch [5], Batch [518/938], Loss: 0.5733553767204285\n",
      "Train: Epoch [5], Batch [519/938], Loss: 0.6488386392593384\n",
      "Train: Epoch [5], Batch [520/938], Loss: 0.6052095890045166\n",
      "Train: Epoch [5], Batch [521/938], Loss: 0.7624362707138062\n",
      "Train: Epoch [5], Batch [522/938], Loss: 0.7836730480194092\n",
      "Train: Epoch [5], Batch [523/938], Loss: 0.785801351070404\n",
      "Train: Epoch [5], Batch [524/938], Loss: 0.4999934136867523\n",
      "Train: Epoch [5], Batch [525/938], Loss: 0.5597213506698608\n",
      "Train: Epoch [5], Batch [526/938], Loss: 0.8234796524047852\n",
      "Train: Epoch [5], Batch [527/938], Loss: 0.6814051866531372\n",
      "Train: Epoch [5], Batch [528/938], Loss: 0.6528230309486389\n",
      "Train: Epoch [5], Batch [529/938], Loss: 0.5383549928665161\n",
      "Train: Epoch [5], Batch [530/938], Loss: 0.5149582028388977\n",
      "Train: Epoch [5], Batch [531/938], Loss: 0.6985177993774414\n",
      "Train: Epoch [5], Batch [532/938], Loss: 0.5953512191772461\n",
      "Train: Epoch [5], Batch [533/938], Loss: 0.8208386898040771\n",
      "Train: Epoch [5], Batch [534/938], Loss: 0.6376427412033081\n",
      "Train: Epoch [5], Batch [535/938], Loss: 0.72698974609375\n",
      "Train: Epoch [5], Batch [536/938], Loss: 0.5627894997596741\n",
      "Train: Epoch [5], Batch [537/938], Loss: 0.8164850473403931\n",
      "Train: Epoch [5], Batch [538/938], Loss: 0.557534396648407\n",
      "Train: Epoch [5], Batch [539/938], Loss: 0.6453827619552612\n",
      "Train: Epoch [5], Batch [540/938], Loss: 0.660339891910553\n",
      "Train: Epoch [5], Batch [541/938], Loss: 0.5790551900863647\n",
      "Train: Epoch [5], Batch [542/938], Loss: 0.8805977702140808\n",
      "Train: Epoch [5], Batch [543/938], Loss: 0.7652332782745361\n",
      "Train: Epoch [5], Batch [544/938], Loss: 0.5945229530334473\n",
      "Train: Epoch [5], Batch [545/938], Loss: 0.646473228931427\n",
      "Train: Epoch [5], Batch [546/938], Loss: 0.8529647588729858\n",
      "Train: Epoch [5], Batch [547/938], Loss: 0.8833234906196594\n",
      "Train: Epoch [5], Batch [548/938], Loss: 0.9823726415634155\n",
      "Train: Epoch [5], Batch [549/938], Loss: 0.6803998351097107\n",
      "Train: Epoch [5], Batch [550/938], Loss: 0.64652419090271\n",
      "Train: Epoch [5], Batch [551/938], Loss: 0.49939167499542236\n",
      "Train: Epoch [5], Batch [552/938], Loss: 0.7711144685745239\n",
      "Train: Epoch [5], Batch [553/938], Loss: 0.616051971912384\n",
      "Train: Epoch [5], Batch [554/938], Loss: 0.7255493402481079\n",
      "Train: Epoch [5], Batch [555/938], Loss: 0.6979143023490906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [5], Batch [556/938], Loss: 0.5432916879653931\n",
      "Train: Epoch [5], Batch [557/938], Loss: 0.7423093318939209\n",
      "Train: Epoch [5], Batch [558/938], Loss: 0.7741035223007202\n",
      "Train: Epoch [5], Batch [559/938], Loss: 0.4867842197418213\n",
      "Train: Epoch [5], Batch [560/938], Loss: 0.7998160123825073\n",
      "Train: Epoch [5], Batch [561/938], Loss: 0.473135769367218\n",
      "Train: Epoch [5], Batch [562/938], Loss: 0.7547528743743896\n",
      "Train: Epoch [5], Batch [563/938], Loss: 0.8351749777793884\n",
      "Train: Epoch [5], Batch [564/938], Loss: 0.9075245261192322\n",
      "Train: Epoch [5], Batch [565/938], Loss: 0.7323294878005981\n",
      "Train: Epoch [5], Batch [566/938], Loss: 0.822364330291748\n",
      "Train: Epoch [5], Batch [567/938], Loss: 0.6083992719650269\n",
      "Train: Epoch [5], Batch [568/938], Loss: 0.7005777359008789\n",
      "Train: Epoch [5], Batch [569/938], Loss: 0.6978924870491028\n",
      "Train: Epoch [5], Batch [570/938], Loss: 0.6091690063476562\n",
      "Train: Epoch [5], Batch [571/938], Loss: 0.6460083723068237\n",
      "Train: Epoch [5], Batch [572/938], Loss: 0.700045108795166\n",
      "Train: Epoch [5], Batch [573/938], Loss: 0.6253625154495239\n",
      "Train: Epoch [5], Batch [574/938], Loss: 0.6906760931015015\n",
      "Train: Epoch [5], Batch [575/938], Loss: 0.6777138710021973\n",
      "Train: Epoch [5], Batch [576/938], Loss: 0.6555497646331787\n",
      "Train: Epoch [5], Batch [577/938], Loss: 0.7622606754302979\n",
      "Train: Epoch [5], Batch [578/938], Loss: 0.8105689287185669\n",
      "Train: Epoch [5], Batch [579/938], Loss: 0.704586386680603\n",
      "Train: Epoch [5], Batch [580/938], Loss: 0.7090237140655518\n",
      "Train: Epoch [5], Batch [581/938], Loss: 0.5132401585578918\n",
      "Train: Epoch [5], Batch [582/938], Loss: 0.5301081538200378\n",
      "Train: Epoch [5], Batch [583/938], Loss: 0.7514804601669312\n",
      "Train: Epoch [5], Batch [584/938], Loss: 0.7690039277076721\n",
      "Train: Epoch [5], Batch [585/938], Loss: 0.7245392799377441\n",
      "Train: Epoch [5], Batch [586/938], Loss: 0.5745872855186462\n",
      "Train: Epoch [5], Batch [587/938], Loss: 0.5666646361351013\n",
      "Train: Epoch [5], Batch [588/938], Loss: 0.6825937032699585\n",
      "Train: Epoch [5], Batch [589/938], Loss: 0.7065196633338928\n",
      "Train: Epoch [5], Batch [590/938], Loss: 0.6724607944488525\n",
      "Train: Epoch [5], Batch [591/938], Loss: 0.4807771146297455\n",
      "Train: Epoch [5], Batch [592/938], Loss: 1.0144126415252686\n",
      "Train: Epoch [5], Batch [593/938], Loss: 0.9452781081199646\n",
      "Train: Epoch [5], Batch [594/938], Loss: 0.8957242369651794\n",
      "Train: Epoch [5], Batch [595/938], Loss: 0.6558113098144531\n",
      "Train: Epoch [5], Batch [596/938], Loss: 0.5742855668067932\n",
      "Train: Epoch [5], Batch [597/938], Loss: 0.6531239151954651\n",
      "Train: Epoch [5], Batch [598/938], Loss: 0.6434878706932068\n",
      "Train: Epoch [5], Batch [599/938], Loss: 0.6814694404602051\n",
      "Train: Epoch [5], Batch [600/938], Loss: 0.7237653732299805\n",
      "Train: Epoch [5], Batch [601/938], Loss: 0.5487087368965149\n",
      "Train: Epoch [5], Batch [602/938], Loss: 0.9924814105033875\n",
      "Train: Epoch [5], Batch [603/938], Loss: 0.5931856036186218\n",
      "Train: Epoch [5], Batch [604/938], Loss: 0.6158495545387268\n",
      "Train: Epoch [5], Batch [605/938], Loss: 0.8649994134902954\n",
      "Train: Epoch [5], Batch [606/938], Loss: 0.620877742767334\n",
      "Train: Epoch [5], Batch [607/938], Loss: 0.6951178312301636\n",
      "Train: Epoch [5], Batch [608/938], Loss: 0.7526515126228333\n",
      "Train: Epoch [5], Batch [609/938], Loss: 0.5525002479553223\n",
      "Train: Epoch [5], Batch [610/938], Loss: 0.6429646015167236\n",
      "Train: Epoch [5], Batch [611/938], Loss: 0.7265256643295288\n",
      "Train: Epoch [5], Batch [612/938], Loss: 0.7419960498809814\n",
      "Train: Epoch [5], Batch [613/938], Loss: 0.5687284469604492\n",
      "Train: Epoch [5], Batch [614/938], Loss: 0.7794761657714844\n",
      "Train: Epoch [5], Batch [615/938], Loss: 0.7259426116943359\n",
      "Train: Epoch [5], Batch [616/938], Loss: 0.6408148407936096\n",
      "Train: Epoch [5], Batch [617/938], Loss: 0.7283850908279419\n",
      "Train: Epoch [5], Batch [618/938], Loss: 0.5398826599121094\n",
      "Train: Epoch [5], Batch [619/938], Loss: 0.7796396017074585\n",
      "Train: Epoch [5], Batch [620/938], Loss: 0.6129573583602905\n",
      "Train: Epoch [5], Batch [621/938], Loss: 0.7003427743911743\n",
      "Train: Epoch [5], Batch [622/938], Loss: 0.5147854089736938\n",
      "Train: Epoch [5], Batch [623/938], Loss: 0.6727286577224731\n",
      "Train: Epoch [5], Batch [624/938], Loss: 0.6725762486457825\n",
      "Train: Epoch [5], Batch [625/938], Loss: 0.6130543947219849\n",
      "Train: Epoch [5], Batch [626/938], Loss: 0.833512544631958\n",
      "Train: Epoch [5], Batch [627/938], Loss: 0.7022278904914856\n",
      "Train: Epoch [5], Batch [628/938], Loss: 0.5899027585983276\n",
      "Train: Epoch [5], Batch [629/938], Loss: 0.8049681186676025\n",
      "Train: Epoch [5], Batch [630/938], Loss: 0.6997108459472656\n",
      "Train: Epoch [5], Batch [631/938], Loss: 0.6329929828643799\n",
      "Train: Epoch [5], Batch [632/938], Loss: 0.7694789171218872\n",
      "Train: Epoch [5], Batch [633/938], Loss: 0.5516277551651001\n",
      "Train: Epoch [5], Batch [634/938], Loss: 0.7371729016304016\n",
      "Train: Epoch [5], Batch [635/938], Loss: 0.7376723289489746\n",
      "Train: Epoch [5], Batch [636/938], Loss: 0.6541904211044312\n",
      "Train: Epoch [5], Batch [637/938], Loss: 0.6570278406143188\n",
      "Train: Epoch [5], Batch [638/938], Loss: 0.8681055903434753\n",
      "Train: Epoch [5], Batch [639/938], Loss: 0.7257695198059082\n",
      "Train: Epoch [5], Batch [640/938], Loss: 0.6011905670166016\n",
      "Train: Epoch [5], Batch [641/938], Loss: 0.6678044199943542\n",
      "Train: Epoch [5], Batch [642/938], Loss: 0.7197260856628418\n",
      "Train: Epoch [5], Batch [643/938], Loss: 0.5589583516120911\n",
      "Train: Epoch [5], Batch [644/938], Loss: 0.6380276679992676\n",
      "Train: Epoch [5], Batch [645/938], Loss: 0.7483550310134888\n",
      "Train: Epoch [5], Batch [646/938], Loss: 0.7192052602767944\n",
      "Train: Epoch [5], Batch [647/938], Loss: 0.601091742515564\n",
      "Train: Epoch [5], Batch [648/938], Loss: 0.7460740804672241\n",
      "Train: Epoch [5], Batch [649/938], Loss: 0.7346193790435791\n",
      "Train: Epoch [5], Batch [650/938], Loss: 0.5760828256607056\n",
      "Train: Epoch [5], Batch [651/938], Loss: 0.6189031600952148\n",
      "Train: Epoch [5], Batch [652/938], Loss: 0.5976291298866272\n",
      "Train: Epoch [5], Batch [653/938], Loss: 0.4835289716720581\n",
      "Train: Epoch [5], Batch [654/938], Loss: 0.539217472076416\n",
      "Train: Epoch [5], Batch [655/938], Loss: 0.8052332401275635\n",
      "Train: Epoch [5], Batch [656/938], Loss: 0.5699926614761353\n",
      "Train: Epoch [5], Batch [657/938], Loss: 0.6789422035217285\n",
      "Train: Epoch [5], Batch [658/938], Loss: 0.6178539991378784\n",
      "Train: Epoch [5], Batch [659/938], Loss: 0.522503674030304\n",
      "Train: Epoch [5], Batch [660/938], Loss: 0.4178474545478821\n",
      "Train: Epoch [5], Batch [661/938], Loss: 0.5737022161483765\n",
      "Train: Epoch [5], Batch [662/938], Loss: 0.661278486251831\n",
      "Train: Epoch [5], Batch [663/938], Loss: 0.8858252763748169\n",
      "Train: Epoch [5], Batch [664/938], Loss: 0.7638974189758301\n",
      "Train: Epoch [5], Batch [665/938], Loss: 0.7397515773773193\n",
      "Train: Epoch [5], Batch [666/938], Loss: 0.6423543691635132\n",
      "Train: Epoch [5], Batch [667/938], Loss: 0.6173117160797119\n",
      "Train: Epoch [5], Batch [668/938], Loss: 0.6067901849746704\n",
      "Train: Epoch [5], Batch [669/938], Loss: 0.6174027919769287\n",
      "Train: Epoch [5], Batch [670/938], Loss: 0.5298616886138916\n",
      "Train: Epoch [5], Batch [671/938], Loss: 0.667099118232727\n",
      "Train: Epoch [5], Batch [672/938], Loss: 0.5598987340927124\n",
      "Train: Epoch [5], Batch [673/938], Loss: 0.5582058429718018\n",
      "Train: Epoch [5], Batch [674/938], Loss: 0.7468007206916809\n",
      "Train: Epoch [5], Batch [675/938], Loss: 0.7280116081237793\n",
      "Train: Epoch [5], Batch [676/938], Loss: 0.5571942329406738\n",
      "Train: Epoch [5], Batch [677/938], Loss: 0.7803237438201904\n",
      "Train: Epoch [5], Batch [678/938], Loss: 0.5255850553512573\n",
      "Train: Epoch [5], Batch [679/938], Loss: 0.8591196537017822\n",
      "Train: Epoch [5], Batch [680/938], Loss: 0.7790718674659729\n",
      "Train: Epoch [5], Batch [681/938], Loss: 0.6018621325492859\n",
      "Train: Epoch [5], Batch [682/938], Loss: 0.7172260284423828\n",
      "Train: Epoch [5], Batch [683/938], Loss: 0.5874754786491394\n",
      "Train: Epoch [5], Batch [684/938], Loss: 0.6906591653823853\n",
      "Train: Epoch [5], Batch [685/938], Loss: 0.6059821248054504\n",
      "Train: Epoch [5], Batch [686/938], Loss: 0.6114931702613831\n",
      "Train: Epoch [5], Batch [687/938], Loss: 0.8299343585968018\n",
      "Train: Epoch [5], Batch [688/938], Loss: 0.6395375728607178\n",
      "Train: Epoch [5], Batch [689/938], Loss: 0.8111132383346558\n",
      "Train: Epoch [5], Batch [690/938], Loss: 0.6258307695388794\n",
      "Train: Epoch [5], Batch [691/938], Loss: 0.7155720591545105\n",
      "Train: Epoch [5], Batch [692/938], Loss: 0.7723535299301147\n",
      "Train: Epoch [5], Batch [693/938], Loss: 0.5861485004425049\n",
      "Train: Epoch [5], Batch [694/938], Loss: 0.5896379947662354\n",
      "Train: Epoch [5], Batch [695/938], Loss: 0.6892188787460327\n",
      "Train: Epoch [5], Batch [696/938], Loss: 0.5801529288291931\n",
      "Train: Epoch [5], Batch [697/938], Loss: 0.7624398469924927\n",
      "Train: Epoch [5], Batch [698/938], Loss: 0.6856023073196411\n",
      "Train: Epoch [5], Batch [699/938], Loss: 0.7314691543579102\n",
      "Train: Epoch [5], Batch [700/938], Loss: 0.6794719696044922\n",
      "Train: Epoch [5], Batch [701/938], Loss: 0.6247642040252686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [5], Batch [702/938], Loss: 0.7263142466545105\n",
      "Train: Epoch [5], Batch [703/938], Loss: 0.5365448594093323\n",
      "Train: Epoch [5], Batch [704/938], Loss: 0.5663419961929321\n",
      "Train: Epoch [5], Batch [705/938], Loss: 0.6222030520439148\n",
      "Train: Epoch [5], Batch [706/938], Loss: 0.6452990770339966\n",
      "Train: Epoch [5], Batch [707/938], Loss: 0.6938058733940125\n",
      "Train: Epoch [5], Batch [708/938], Loss: 0.6868674755096436\n",
      "Train: Epoch [5], Batch [709/938], Loss: 0.5596616268157959\n",
      "Train: Epoch [5], Batch [710/938], Loss: 0.8442156314849854\n",
      "Train: Epoch [5], Batch [711/938], Loss: 0.7428660988807678\n",
      "Train: Epoch [5], Batch [712/938], Loss: 0.5896369218826294\n",
      "Train: Epoch [5], Batch [713/938], Loss: 0.6258660554885864\n",
      "Train: Epoch [5], Batch [714/938], Loss: 0.7063597440719604\n",
      "Train: Epoch [5], Batch [715/938], Loss: 0.7036173343658447\n",
      "Train: Epoch [5], Batch [716/938], Loss: 0.9249777793884277\n",
      "Train: Epoch [5], Batch [717/938], Loss: 0.5272115468978882\n",
      "Train: Epoch [5], Batch [718/938], Loss: 0.7546594142913818\n",
      "Train: Epoch [5], Batch [719/938], Loss: 0.8017446994781494\n",
      "Train: Epoch [5], Batch [720/938], Loss: 0.706577479839325\n",
      "Train: Epoch [5], Batch [721/938], Loss: 0.7285538911819458\n",
      "Train: Epoch [5], Batch [722/938], Loss: 0.8635647296905518\n",
      "Train: Epoch [5], Batch [723/938], Loss: 0.707229733467102\n",
      "Train: Epoch [5], Batch [724/938], Loss: 0.6445475816726685\n",
      "Train: Epoch [5], Batch [725/938], Loss: 0.8181444406509399\n",
      "Train: Epoch [5], Batch [726/938], Loss: 0.6058523654937744\n",
      "Train: Epoch [5], Batch [727/938], Loss: 0.6331237554550171\n",
      "Train: Epoch [5], Batch [728/938], Loss: 0.5230274200439453\n",
      "Train: Epoch [5], Batch [729/938], Loss: 0.6988481283187866\n",
      "Train: Epoch [5], Batch [730/938], Loss: 0.5846347808837891\n",
      "Train: Epoch [5], Batch [731/938], Loss: 0.6386281251907349\n",
      "Train: Epoch [5], Batch [732/938], Loss: 0.6225563287734985\n",
      "Train: Epoch [5], Batch [733/938], Loss: 0.7410049438476562\n",
      "Train: Epoch [5], Batch [734/938], Loss: 0.6885321140289307\n",
      "Train: Epoch [5], Batch [735/938], Loss: 0.6101529598236084\n",
      "Train: Epoch [5], Batch [736/938], Loss: 0.5516879558563232\n",
      "Train: Epoch [5], Batch [737/938], Loss: 0.5995439291000366\n",
      "Train: Epoch [5], Batch [738/938], Loss: 0.8291564583778381\n",
      "Train: Epoch [5], Batch [739/938], Loss: 0.6093006134033203\n",
      "Train: Epoch [5], Batch [740/938], Loss: 0.6561993360519409\n",
      "Train: Epoch [5], Batch [741/938], Loss: 0.5078604221343994\n",
      "Train: Epoch [5], Batch [742/938], Loss: 0.5554807782173157\n",
      "Train: Epoch [5], Batch [743/938], Loss: 0.6253468990325928\n",
      "Train: Epoch [5], Batch [744/938], Loss: 0.7247778177261353\n",
      "Train: Epoch [5], Batch [745/938], Loss: 0.6076592206954956\n",
      "Train: Epoch [5], Batch [746/938], Loss: 0.6620692014694214\n",
      "Train: Epoch [5], Batch [747/938], Loss: 0.6042461395263672\n",
      "Train: Epoch [5], Batch [748/938], Loss: 0.6592538356781006\n",
      "Train: Epoch [5], Batch [749/938], Loss: 0.522130012512207\n",
      "Train: Epoch [5], Batch [750/938], Loss: 0.6598751544952393\n",
      "Train: Epoch [5], Batch [751/938], Loss: 0.7167114615440369\n",
      "Train: Epoch [5], Batch [752/938], Loss: 0.7491806149482727\n",
      "Train: Epoch [5], Batch [753/938], Loss: 0.6959152221679688\n",
      "Train: Epoch [5], Batch [754/938], Loss: 0.41965043544769287\n",
      "Train: Epoch [5], Batch [755/938], Loss: 0.5235337018966675\n",
      "Train: Epoch [5], Batch [756/938], Loss: 0.6771811246871948\n",
      "Train: Epoch [5], Batch [757/938], Loss: 0.8997834920883179\n",
      "Train: Epoch [5], Batch [758/938], Loss: 0.5205937027931213\n",
      "Train: Epoch [5], Batch [759/938], Loss: 0.5329400300979614\n",
      "Train: Epoch [5], Batch [760/938], Loss: 0.5326826572418213\n",
      "Train: Epoch [5], Batch [761/938], Loss: 0.7843649983406067\n",
      "Train: Epoch [5], Batch [762/938], Loss: 0.6418240070343018\n",
      "Train: Epoch [5], Batch [763/938], Loss: 0.4942023456096649\n",
      "Train: Epoch [5], Batch [764/938], Loss: 0.5004615187644958\n",
      "Train: Epoch [5], Batch [765/938], Loss: 0.6185636520385742\n",
      "Train: Epoch [5], Batch [766/938], Loss: 1.1220998764038086\n",
      "Train: Epoch [5], Batch [767/938], Loss: 0.4616800844669342\n",
      "Train: Epoch [5], Batch [768/938], Loss: 0.6609202027320862\n",
      "Train: Epoch [5], Batch [769/938], Loss: 0.6925627589225769\n",
      "Train: Epoch [5], Batch [770/938], Loss: 0.5241217613220215\n",
      "Train: Epoch [5], Batch [771/938], Loss: 0.6327872276306152\n",
      "Train: Epoch [5], Batch [772/938], Loss: 0.7117440104484558\n",
      "Train: Epoch [5], Batch [773/938], Loss: 0.6751803159713745\n",
      "Train: Epoch [5], Batch [774/938], Loss: 0.7192801237106323\n",
      "Train: Epoch [5], Batch [775/938], Loss: 0.8058168888092041\n",
      "Train: Epoch [5], Batch [776/938], Loss: 0.5640282034873962\n",
      "Train: Epoch [5], Batch [777/938], Loss: 0.5946469306945801\n",
      "Train: Epoch [5], Batch [778/938], Loss: 0.7374926805496216\n",
      "Train: Epoch [5], Batch [779/938], Loss: 0.6075081825256348\n",
      "Train: Epoch [5], Batch [780/938], Loss: 0.6543806195259094\n",
      "Train: Epoch [5], Batch [781/938], Loss: 0.5867131948471069\n",
      "Train: Epoch [5], Batch [782/938], Loss: 0.9707601070404053\n",
      "Train: Epoch [5], Batch [783/938], Loss: 0.6721623539924622\n",
      "Train: Epoch [5], Batch [784/938], Loss: 0.6220917701721191\n",
      "Train: Epoch [5], Batch [785/938], Loss: 0.4456949532032013\n",
      "Train: Epoch [5], Batch [786/938], Loss: 0.5063633918762207\n",
      "Train: Epoch [5], Batch [787/938], Loss: 0.6669383645057678\n",
      "Train: Epoch [5], Batch [788/938], Loss: 0.6458839178085327\n",
      "Train: Epoch [5], Batch [789/938], Loss: 0.5776818990707397\n",
      "Train: Epoch [5], Batch [790/938], Loss: 0.6648925542831421\n",
      "Train: Epoch [5], Batch [791/938], Loss: 0.4986019730567932\n",
      "Train: Epoch [5], Batch [792/938], Loss: 0.7133681774139404\n",
      "Train: Epoch [5], Batch [793/938], Loss: 0.6038167476654053\n",
      "Train: Epoch [5], Batch [794/938], Loss: 0.6742705702781677\n",
      "Train: Epoch [5], Batch [795/938], Loss: 0.6823488473892212\n",
      "Train: Epoch [5], Batch [796/938], Loss: 0.7638777494430542\n",
      "Train: Epoch [5], Batch [797/938], Loss: 0.5235955119132996\n",
      "Train: Epoch [5], Batch [798/938], Loss: 0.5184059143066406\n",
      "Train: Epoch [5], Batch [799/938], Loss: 0.6412860751152039\n",
      "Train: Epoch [5], Batch [800/938], Loss: 0.8766493201255798\n",
      "Train: Epoch [5], Batch [801/938], Loss: 0.5775085687637329\n",
      "Train: Epoch [5], Batch [802/938], Loss: 0.6821781992912292\n",
      "Train: Epoch [5], Batch [803/938], Loss: 0.7980901598930359\n",
      "Train: Epoch [5], Batch [804/938], Loss: 0.8058158755302429\n",
      "Train: Epoch [5], Batch [805/938], Loss: 0.6703156232833862\n",
      "Train: Epoch [5], Batch [806/938], Loss: 0.5685692429542542\n",
      "Train: Epoch [5], Batch [807/938], Loss: 0.6639868021011353\n",
      "Train: Epoch [5], Batch [808/938], Loss: 0.9141488075256348\n",
      "Train: Epoch [5], Batch [809/938], Loss: 0.6509867310523987\n",
      "Train: Epoch [5], Batch [810/938], Loss: 0.612216591835022\n",
      "Train: Epoch [5], Batch [811/938], Loss: 0.8748486042022705\n",
      "Train: Epoch [5], Batch [812/938], Loss: 0.6725375652313232\n",
      "Train: Epoch [5], Batch [813/938], Loss: 0.634294867515564\n",
      "Train: Epoch [5], Batch [814/938], Loss: 0.68326735496521\n",
      "Train: Epoch [5], Batch [815/938], Loss: 0.5007760524749756\n",
      "Train: Epoch [5], Batch [816/938], Loss: 0.5751560926437378\n",
      "Train: Epoch [5], Batch [817/938], Loss: 0.5754684209823608\n",
      "Train: Epoch [5], Batch [818/938], Loss: 0.5470302104949951\n",
      "Train: Epoch [5], Batch [819/938], Loss: 0.6226871609687805\n",
      "Train: Epoch [5], Batch [820/938], Loss: 0.6122008562088013\n",
      "Train: Epoch [5], Batch [821/938], Loss: 0.5221576690673828\n",
      "Train: Epoch [5], Batch [822/938], Loss: 0.6342202425003052\n",
      "Train: Epoch [5], Batch [823/938], Loss: 0.6604664921760559\n",
      "Train: Epoch [5], Batch [824/938], Loss: 0.656014084815979\n",
      "Train: Epoch [5], Batch [825/938], Loss: 0.5937000513076782\n",
      "Train: Epoch [5], Batch [826/938], Loss: 0.523629903793335\n",
      "Train: Epoch [5], Batch [827/938], Loss: 0.6849555373191833\n",
      "Train: Epoch [5], Batch [828/938], Loss: 0.6793648600578308\n",
      "Train: Epoch [5], Batch [829/938], Loss: 0.5409297943115234\n",
      "Train: Epoch [5], Batch [830/938], Loss: 0.5746570229530334\n",
      "Train: Epoch [5], Batch [831/938], Loss: 0.7512465715408325\n",
      "Train: Epoch [5], Batch [832/938], Loss: 0.5534273982048035\n",
      "Train: Epoch [5], Batch [833/938], Loss: 0.5926142930984497\n",
      "Train: Epoch [5], Batch [834/938], Loss: 0.885894775390625\n",
      "Train: Epoch [5], Batch [835/938], Loss: 0.7978072762489319\n",
      "Train: Epoch [5], Batch [836/938], Loss: 0.5740491151809692\n",
      "Train: Epoch [5], Batch [837/938], Loss: 0.7768411040306091\n",
      "Train: Epoch [5], Batch [838/938], Loss: 0.548000693321228\n",
      "Train: Epoch [5], Batch [839/938], Loss: 0.7156592011451721\n",
      "Train: Epoch [5], Batch [840/938], Loss: 0.5953521728515625\n",
      "Train: Epoch [5], Batch [841/938], Loss: 0.8298873901367188\n",
      "Train: Epoch [5], Batch [842/938], Loss: 0.5947697162628174\n",
      "Train: Epoch [5], Batch [843/938], Loss: 0.6514009237289429\n",
      "Train: Epoch [5], Batch [844/938], Loss: 0.7905183434486389\n",
      "Train: Epoch [5], Batch [845/938], Loss: 0.7658571600914001\n",
      "Train: Epoch [5], Batch [846/938], Loss: 0.7918881177902222\n",
      "Train: Epoch [5], Batch [847/938], Loss: 0.5738778114318848\n",
      "Train: Epoch [5], Batch [848/938], Loss: 0.5570262670516968\n",
      "Train: Epoch [5], Batch [849/938], Loss: 0.6990797519683838\n",
      "Train: Epoch [5], Batch [850/938], Loss: 0.8034131526947021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [5], Batch [851/938], Loss: 1.280775547027588\n",
      "Train: Epoch [5], Batch [852/938], Loss: 0.7671661972999573\n",
      "Train: Epoch [5], Batch [853/938], Loss: 0.6582033634185791\n",
      "Train: Epoch [5], Batch [854/938], Loss: 0.6776337623596191\n",
      "Train: Epoch [5], Batch [855/938], Loss: 0.7097558975219727\n",
      "Train: Epoch [5], Batch [856/938], Loss: 0.7719789147377014\n",
      "Train: Epoch [5], Batch [857/938], Loss: 0.7221072912216187\n",
      "Train: Epoch [5], Batch [858/938], Loss: 0.63787841796875\n",
      "Train: Epoch [5], Batch [859/938], Loss: 0.5062755942344666\n",
      "Train: Epoch [5], Batch [860/938], Loss: 0.6964019536972046\n",
      "Train: Epoch [5], Batch [861/938], Loss: 0.5395516157150269\n",
      "Train: Epoch [5], Batch [862/938], Loss: 0.750019907951355\n",
      "Train: Epoch [5], Batch [863/938], Loss: 0.6501787900924683\n",
      "Train: Epoch [5], Batch [864/938], Loss: 0.5017648935317993\n",
      "Train: Epoch [5], Batch [865/938], Loss: 0.46373674273490906\n",
      "Train: Epoch [5], Batch [866/938], Loss: 0.4766806960105896\n",
      "Train: Epoch [5], Batch [867/938], Loss: 0.68802809715271\n",
      "Train: Epoch [5], Batch [868/938], Loss: 0.7522706985473633\n",
      "Train: Epoch [5], Batch [869/938], Loss: 0.5996330976486206\n",
      "Train: Epoch [5], Batch [870/938], Loss: 0.820559024810791\n",
      "Train: Epoch [5], Batch [871/938], Loss: 0.6773107051849365\n",
      "Train: Epoch [5], Batch [872/938], Loss: 0.6276918649673462\n",
      "Train: Epoch [5], Batch [873/938], Loss: 0.4613305926322937\n",
      "Train: Epoch [5], Batch [874/938], Loss: 0.7988017797470093\n",
      "Train: Epoch [5], Batch [875/938], Loss: 0.6746060848236084\n",
      "Train: Epoch [5], Batch [876/938], Loss: 0.6603280901908875\n",
      "Train: Epoch [5], Batch [877/938], Loss: 0.5447425246238708\n",
      "Train: Epoch [5], Batch [878/938], Loss: 0.6220341920852661\n",
      "Train: Epoch [5], Batch [879/938], Loss: 0.6124331951141357\n",
      "Train: Epoch [5], Batch [880/938], Loss: 0.6505018472671509\n",
      "Train: Epoch [5], Batch [881/938], Loss: 0.8517552018165588\n",
      "Train: Epoch [5], Batch [882/938], Loss: 0.8350647687911987\n",
      "Train: Epoch [5], Batch [883/938], Loss: 0.6164209842681885\n",
      "Train: Epoch [5], Batch [884/938], Loss: 0.8486906290054321\n",
      "Train: Epoch [5], Batch [885/938], Loss: 0.8140707015991211\n",
      "Train: Epoch [5], Batch [886/938], Loss: 0.5666065216064453\n",
      "Train: Epoch [5], Batch [887/938], Loss: 0.6794116497039795\n",
      "Train: Epoch [5], Batch [888/938], Loss: 0.7661278247833252\n",
      "Train: Epoch [5], Batch [889/938], Loss: 0.4847022593021393\n",
      "Train: Epoch [5], Batch [890/938], Loss: 0.705292820930481\n",
      "Train: Epoch [5], Batch [891/938], Loss: 0.7163058519363403\n",
      "Train: Epoch [5], Batch [892/938], Loss: 0.6169866919517517\n",
      "Train: Epoch [5], Batch [893/938], Loss: 0.6802599430084229\n",
      "Train: Epoch [5], Batch [894/938], Loss: 0.8327494263648987\n",
      "Train: Epoch [5], Batch [895/938], Loss: 0.6158254146575928\n",
      "Train: Epoch [5], Batch [896/938], Loss: 0.6372179985046387\n",
      "Train: Epoch [5], Batch [897/938], Loss: 0.7684653401374817\n",
      "Train: Epoch [5], Batch [898/938], Loss: 0.7084168195724487\n",
      "Train: Epoch [5], Batch [899/938], Loss: 0.6865729689598083\n",
      "Train: Epoch [5], Batch [900/938], Loss: 0.6529597043991089\n",
      "Train: Epoch [5], Batch [901/938], Loss: 0.6461947560310364\n",
      "Train: Epoch [5], Batch [902/938], Loss: 0.6185510158538818\n",
      "Train: Epoch [5], Batch [903/938], Loss: 0.8125281929969788\n",
      "Train: Epoch [5], Batch [904/938], Loss: 0.8348853588104248\n",
      "Train: Epoch [5], Batch [905/938], Loss: 0.5864019393920898\n",
      "Train: Epoch [5], Batch [906/938], Loss: 0.8088715076446533\n",
      "Train: Epoch [5], Batch [907/938], Loss: 0.6729101538658142\n",
      "Train: Epoch [5], Batch [908/938], Loss: 0.7002886533737183\n",
      "Train: Epoch [5], Batch [909/938], Loss: 0.7230217456817627\n",
      "Train: Epoch [5], Batch [910/938], Loss: 0.5378180742263794\n",
      "Train: Epoch [5], Batch [911/938], Loss: 0.5433529615402222\n",
      "Train: Epoch [5], Batch [912/938], Loss: 0.5087352991104126\n",
      "Train: Epoch [5], Batch [913/938], Loss: 0.6145068407058716\n",
      "Train: Epoch [5], Batch [914/938], Loss: 0.6261398196220398\n",
      "Train: Epoch [5], Batch [915/938], Loss: 0.5836266875267029\n",
      "Train: Epoch [5], Batch [916/938], Loss: 0.6174177527427673\n",
      "Train: Epoch [5], Batch [917/938], Loss: 0.713056206703186\n",
      "Train: Epoch [5], Batch [918/938], Loss: 0.5339701175689697\n",
      "Train: Epoch [5], Batch [919/938], Loss: 0.776208758354187\n",
      "Train: Epoch [5], Batch [920/938], Loss: 0.6487069129943848\n",
      "Train: Epoch [5], Batch [921/938], Loss: 0.774992823600769\n",
      "Train: Epoch [5], Batch [922/938], Loss: 0.5647621154785156\n",
      "Train: Epoch [5], Batch [923/938], Loss: 0.6500710248947144\n",
      "Train: Epoch [5], Batch [924/938], Loss: 0.608636736869812\n",
      "Train: Epoch [5], Batch [925/938], Loss: 0.5572775602340698\n",
      "Train: Epoch [5], Batch [926/938], Loss: 0.606654167175293\n",
      "Train: Epoch [5], Batch [927/938], Loss: 0.49116355180740356\n",
      "Train: Epoch [5], Batch [928/938], Loss: 0.5660696029663086\n",
      "Train: Epoch [5], Batch [929/938], Loss: 0.6828907132148743\n",
      "Train: Epoch [5], Batch [930/938], Loss: 0.5859662294387817\n",
      "Train: Epoch [5], Batch [931/938], Loss: 0.6549763083457947\n",
      "Train: Epoch [5], Batch [932/938], Loss: 0.6433623433113098\n",
      "Train: Epoch [5], Batch [933/938], Loss: 0.6997978687286377\n",
      "Train: Epoch [5], Batch [934/938], Loss: 0.7061938643455505\n",
      "Train: Epoch [5], Batch [935/938], Loss: 0.6187361478805542\n",
      "Train: Epoch [5], Batch [936/938], Loss: 0.7814165949821472\n",
      "Train: Epoch [5], Batch [937/938], Loss: 0.5826218128204346\n",
      "Train: Epoch [5], Batch [938/938], Loss: 0.5592403411865234\n",
      "Accuracy of train set: 0.7465166666666667\n",
      "Validation: Epoch [5], Batch [1/938], Loss: 0.5809439420700073\n",
      "Validation: Epoch [5], Batch [2/938], Loss: 0.7063429355621338\n",
      "Validation: Epoch [5], Batch [3/938], Loss: 0.5619484186172485\n",
      "Validation: Epoch [5], Batch [4/938], Loss: 0.6679263114929199\n",
      "Validation: Epoch [5], Batch [5/938], Loss: 0.6614091396331787\n",
      "Validation: Epoch [5], Batch [6/938], Loss: 0.5388194918632507\n",
      "Validation: Epoch [5], Batch [7/938], Loss: 0.8495298624038696\n",
      "Validation: Epoch [5], Batch [8/938], Loss: 0.8353556394577026\n",
      "Validation: Epoch [5], Batch [9/938], Loss: 0.7403684854507446\n",
      "Validation: Epoch [5], Batch [10/938], Loss: 0.6317692399024963\n",
      "Validation: Epoch [5], Batch [11/938], Loss: 0.5977420806884766\n",
      "Validation: Epoch [5], Batch [12/938], Loss: 0.5714335441589355\n",
      "Validation: Epoch [5], Batch [13/938], Loss: 0.7208001613616943\n",
      "Validation: Epoch [5], Batch [14/938], Loss: 0.5305585265159607\n",
      "Validation: Epoch [5], Batch [15/938], Loss: 0.7457113265991211\n",
      "Validation: Epoch [5], Batch [16/938], Loss: 0.5879653692245483\n",
      "Validation: Epoch [5], Batch [17/938], Loss: 0.7088274955749512\n",
      "Validation: Epoch [5], Batch [18/938], Loss: 0.694684624671936\n",
      "Validation: Epoch [5], Batch [19/938], Loss: 0.6405268907546997\n",
      "Validation: Epoch [5], Batch [20/938], Loss: 0.7481051087379456\n",
      "Validation: Epoch [5], Batch [21/938], Loss: 0.681261420249939\n",
      "Validation: Epoch [5], Batch [22/938], Loss: 0.7073420286178589\n",
      "Validation: Epoch [5], Batch [23/938], Loss: 0.6795163154602051\n",
      "Validation: Epoch [5], Batch [24/938], Loss: 0.8746176958084106\n",
      "Validation: Epoch [5], Batch [25/938], Loss: 0.5807346105575562\n",
      "Validation: Epoch [5], Batch [26/938], Loss: 0.7185227870941162\n",
      "Validation: Epoch [5], Batch [27/938], Loss: 0.6703435182571411\n",
      "Validation: Epoch [5], Batch [28/938], Loss: 0.6596198081970215\n",
      "Validation: Epoch [5], Batch [29/938], Loss: 0.6307989954948425\n",
      "Validation: Epoch [5], Batch [30/938], Loss: 0.6795612573623657\n",
      "Validation: Epoch [5], Batch [31/938], Loss: 0.5227532386779785\n",
      "Validation: Epoch [5], Batch [32/938], Loss: 0.6702486872673035\n",
      "Validation: Epoch [5], Batch [33/938], Loss: 0.5664471983909607\n",
      "Validation: Epoch [5], Batch [34/938], Loss: 0.5429188013076782\n",
      "Validation: Epoch [5], Batch [35/938], Loss: 0.7196016311645508\n",
      "Validation: Epoch [5], Batch [36/938], Loss: 0.6946870684623718\n",
      "Validation: Epoch [5], Batch [37/938], Loss: 0.5462457537651062\n",
      "Validation: Epoch [5], Batch [38/938], Loss: 0.8299194574356079\n",
      "Validation: Epoch [5], Batch [39/938], Loss: 0.6321352124214172\n",
      "Validation: Epoch [5], Batch [40/938], Loss: 0.5820541381835938\n",
      "Validation: Epoch [5], Batch [41/938], Loss: 0.7424250841140747\n",
      "Validation: Epoch [5], Batch [42/938], Loss: 0.7216317057609558\n",
      "Validation: Epoch [5], Batch [43/938], Loss: 0.7186144590377808\n",
      "Validation: Epoch [5], Batch [44/938], Loss: 0.8602731227874756\n",
      "Validation: Epoch [5], Batch [45/938], Loss: 0.6268843412399292\n",
      "Validation: Epoch [5], Batch [46/938], Loss: 0.6579024791717529\n",
      "Validation: Epoch [5], Batch [47/938], Loss: 0.4532947242259979\n",
      "Validation: Epoch [5], Batch [48/938], Loss: 0.7463792562484741\n",
      "Validation: Epoch [5], Batch [49/938], Loss: 0.573265552520752\n",
      "Validation: Epoch [5], Batch [50/938], Loss: 0.8476854562759399\n",
      "Validation: Epoch [5], Batch [51/938], Loss: 0.6058199405670166\n",
      "Validation: Epoch [5], Batch [52/938], Loss: 0.6130185127258301\n",
      "Validation: Epoch [5], Batch [53/938], Loss: 0.5041338801383972\n",
      "Validation: Epoch [5], Batch [54/938], Loss: 0.5856916308403015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [55/938], Loss: 0.616605281829834\n",
      "Validation: Epoch [5], Batch [56/938], Loss: 0.529657781124115\n",
      "Validation: Epoch [5], Batch [57/938], Loss: 0.6697505712509155\n",
      "Validation: Epoch [5], Batch [58/938], Loss: 0.728689432144165\n",
      "Validation: Epoch [5], Batch [59/938], Loss: 0.7280792593955994\n",
      "Validation: Epoch [5], Batch [60/938], Loss: 0.6669701933860779\n",
      "Validation: Epoch [5], Batch [61/938], Loss: 0.5933293104171753\n",
      "Validation: Epoch [5], Batch [62/938], Loss: 0.5080252885818481\n",
      "Validation: Epoch [5], Batch [63/938], Loss: 0.8429937362670898\n",
      "Validation: Epoch [5], Batch [64/938], Loss: 0.5139358043670654\n",
      "Validation: Epoch [5], Batch [65/938], Loss: 0.6769441366195679\n",
      "Validation: Epoch [5], Batch [66/938], Loss: 0.6313966512680054\n",
      "Validation: Epoch [5], Batch [67/938], Loss: 0.7305576801300049\n",
      "Validation: Epoch [5], Batch [68/938], Loss: 0.44973960518836975\n",
      "Validation: Epoch [5], Batch [69/938], Loss: 0.5462482571601868\n",
      "Validation: Epoch [5], Batch [70/938], Loss: 0.8678535223007202\n",
      "Validation: Epoch [5], Batch [71/938], Loss: 0.5560891628265381\n",
      "Validation: Epoch [5], Batch [72/938], Loss: 0.7575173377990723\n",
      "Validation: Epoch [5], Batch [73/938], Loss: 0.5753464102745056\n",
      "Validation: Epoch [5], Batch [74/938], Loss: 0.7424256801605225\n",
      "Validation: Epoch [5], Batch [75/938], Loss: 0.614090085029602\n",
      "Validation: Epoch [5], Batch [76/938], Loss: 0.7771700620651245\n",
      "Validation: Epoch [5], Batch [77/938], Loss: 0.6096904277801514\n",
      "Validation: Epoch [5], Batch [78/938], Loss: 0.5149582624435425\n",
      "Validation: Epoch [5], Batch [79/938], Loss: 0.6664905548095703\n",
      "Validation: Epoch [5], Batch [80/938], Loss: 0.8124086856842041\n",
      "Validation: Epoch [5], Batch [81/938], Loss: 0.8528225421905518\n",
      "Validation: Epoch [5], Batch [82/938], Loss: 0.6255544424057007\n",
      "Validation: Epoch [5], Batch [83/938], Loss: 0.9885334968566895\n",
      "Validation: Epoch [5], Batch [84/938], Loss: 0.6341270208358765\n",
      "Validation: Epoch [5], Batch [85/938], Loss: 0.5955570936203003\n",
      "Validation: Epoch [5], Batch [86/938], Loss: 0.5881320238113403\n",
      "Validation: Epoch [5], Batch [87/938], Loss: 0.8486061692237854\n",
      "Validation: Epoch [5], Batch [88/938], Loss: 0.544274091720581\n",
      "Validation: Epoch [5], Batch [89/938], Loss: 0.773530125617981\n",
      "Validation: Epoch [5], Batch [90/938], Loss: 0.5165413618087769\n",
      "Validation: Epoch [5], Batch [91/938], Loss: 0.7499288320541382\n",
      "Validation: Epoch [5], Batch [92/938], Loss: 0.5467677712440491\n",
      "Validation: Epoch [5], Batch [93/938], Loss: 0.8746081590652466\n",
      "Validation: Epoch [5], Batch [94/938], Loss: 0.7784292697906494\n",
      "Validation: Epoch [5], Batch [95/938], Loss: 0.6252240538597107\n",
      "Validation: Epoch [5], Batch [96/938], Loss: 0.7869290113449097\n",
      "Validation: Epoch [5], Batch [97/938], Loss: 0.8323971033096313\n",
      "Validation: Epoch [5], Batch [98/938], Loss: 0.8898000717163086\n",
      "Validation: Epoch [5], Batch [99/938], Loss: 0.8123732805252075\n",
      "Validation: Epoch [5], Batch [100/938], Loss: 0.46148681640625\n",
      "Validation: Epoch [5], Batch [101/938], Loss: 0.6841425895690918\n",
      "Validation: Epoch [5], Batch [102/938], Loss: 0.6619241237640381\n",
      "Validation: Epoch [5], Batch [103/938], Loss: 0.5374788045883179\n",
      "Validation: Epoch [5], Batch [104/938], Loss: 0.5942842960357666\n",
      "Validation: Epoch [5], Batch [105/938], Loss: 0.6220072507858276\n",
      "Validation: Epoch [5], Batch [106/938], Loss: 0.8277738094329834\n",
      "Validation: Epoch [5], Batch [107/938], Loss: 0.5585135221481323\n",
      "Validation: Epoch [5], Batch [108/938], Loss: 0.7575218677520752\n",
      "Validation: Epoch [5], Batch [109/938], Loss: 0.5583416223526001\n",
      "Validation: Epoch [5], Batch [110/938], Loss: 0.642047643661499\n",
      "Validation: Epoch [5], Batch [111/938], Loss: 0.6011358499526978\n",
      "Validation: Epoch [5], Batch [112/938], Loss: 0.6045892238616943\n",
      "Validation: Epoch [5], Batch [113/938], Loss: 0.9365188479423523\n",
      "Validation: Epoch [5], Batch [114/938], Loss: 0.6067156791687012\n",
      "Validation: Epoch [5], Batch [115/938], Loss: 0.8632242679595947\n",
      "Validation: Epoch [5], Batch [116/938], Loss: 0.641799807548523\n",
      "Validation: Epoch [5], Batch [117/938], Loss: 0.5800656676292419\n",
      "Validation: Epoch [5], Batch [118/938], Loss: 0.6870986223220825\n",
      "Validation: Epoch [5], Batch [119/938], Loss: 0.6755919456481934\n",
      "Validation: Epoch [5], Batch [120/938], Loss: 0.7566436529159546\n",
      "Validation: Epoch [5], Batch [121/938], Loss: 0.6337030529975891\n",
      "Validation: Epoch [5], Batch [122/938], Loss: 0.7681963443756104\n",
      "Validation: Epoch [5], Batch [123/938], Loss: 0.7505890727043152\n",
      "Validation: Epoch [5], Batch [124/938], Loss: 0.6187046766281128\n",
      "Validation: Epoch [5], Batch [125/938], Loss: 0.8926755785942078\n",
      "Validation: Epoch [5], Batch [126/938], Loss: 0.5831506252288818\n",
      "Validation: Epoch [5], Batch [127/938], Loss: 0.7706383466720581\n",
      "Validation: Epoch [5], Batch [128/938], Loss: 0.8049246072769165\n",
      "Validation: Epoch [5], Batch [129/938], Loss: 0.8286514282226562\n",
      "Validation: Epoch [5], Batch [130/938], Loss: 0.5926176905632019\n",
      "Validation: Epoch [5], Batch [131/938], Loss: 0.7289537191390991\n",
      "Validation: Epoch [5], Batch [132/938], Loss: 0.7529147863388062\n",
      "Validation: Epoch [5], Batch [133/938], Loss: 0.6332583427429199\n",
      "Validation: Epoch [5], Batch [134/938], Loss: 0.7680274248123169\n",
      "Validation: Epoch [5], Batch [135/938], Loss: 0.6228103041648865\n",
      "Validation: Epoch [5], Batch [136/938], Loss: 0.7087171673774719\n",
      "Validation: Epoch [5], Batch [137/938], Loss: 0.7509941458702087\n",
      "Validation: Epoch [5], Batch [138/938], Loss: 0.6032803654670715\n",
      "Validation: Epoch [5], Batch [139/938], Loss: 0.5979905128479004\n",
      "Validation: Epoch [5], Batch [140/938], Loss: 0.7372497916221619\n",
      "Validation: Epoch [5], Batch [141/938], Loss: 0.7470659017562866\n",
      "Validation: Epoch [5], Batch [142/938], Loss: 0.6286360025405884\n",
      "Validation: Epoch [5], Batch [143/938], Loss: 0.9700230360031128\n",
      "Validation: Epoch [5], Batch [144/938], Loss: 0.5337627530097961\n",
      "Validation: Epoch [5], Batch [145/938], Loss: 0.6118782162666321\n",
      "Validation: Epoch [5], Batch [146/938], Loss: 0.6723355054855347\n",
      "Validation: Epoch [5], Batch [147/938], Loss: 0.7112795114517212\n",
      "Validation: Epoch [5], Batch [148/938], Loss: 0.5483384132385254\n",
      "Validation: Epoch [5], Batch [149/938], Loss: 0.5365016460418701\n",
      "Validation: Epoch [5], Batch [150/938], Loss: 0.5581381320953369\n",
      "Validation: Epoch [5], Batch [151/938], Loss: 0.5789802074432373\n",
      "Validation: Epoch [5], Batch [152/938], Loss: 0.7530704140663147\n",
      "Validation: Epoch [5], Batch [153/938], Loss: 0.7742369174957275\n",
      "Validation: Epoch [5], Batch [154/938], Loss: 0.7545638084411621\n",
      "Validation: Epoch [5], Batch [155/938], Loss: 0.7204304933547974\n",
      "Validation: Epoch [5], Batch [156/938], Loss: 0.5893646478652954\n",
      "Validation: Epoch [5], Batch [157/938], Loss: 0.7044295072555542\n",
      "Validation: Epoch [5], Batch [158/938], Loss: 0.5956814289093018\n",
      "Validation: Epoch [5], Batch [159/938], Loss: 0.5473323464393616\n",
      "Validation: Epoch [5], Batch [160/938], Loss: 0.5262097120285034\n",
      "Validation: Epoch [5], Batch [161/938], Loss: 0.5157830715179443\n",
      "Validation: Epoch [5], Batch [162/938], Loss: 0.43316102027893066\n",
      "Validation: Epoch [5], Batch [163/938], Loss: 0.5829373598098755\n",
      "Validation: Epoch [5], Batch [164/938], Loss: 0.6619597673416138\n",
      "Validation: Epoch [5], Batch [165/938], Loss: 0.7005432844161987\n",
      "Validation: Epoch [5], Batch [166/938], Loss: 0.4663749933242798\n",
      "Validation: Epoch [5], Batch [167/938], Loss: 0.8189758062362671\n",
      "Validation: Epoch [5], Batch [168/938], Loss: 0.7009583711624146\n",
      "Validation: Epoch [5], Batch [169/938], Loss: 0.63956618309021\n",
      "Validation: Epoch [5], Batch [170/938], Loss: 0.7394669055938721\n",
      "Validation: Epoch [5], Batch [171/938], Loss: 0.5515655279159546\n",
      "Validation: Epoch [5], Batch [172/938], Loss: 0.487555593252182\n",
      "Validation: Epoch [5], Batch [173/938], Loss: 0.694246768951416\n",
      "Validation: Epoch [5], Batch [174/938], Loss: 0.551781177520752\n",
      "Validation: Epoch [5], Batch [175/938], Loss: 0.6002060174942017\n",
      "Validation: Epoch [5], Batch [176/938], Loss: 0.5664259791374207\n",
      "Validation: Epoch [5], Batch [177/938], Loss: 0.7065629959106445\n",
      "Validation: Epoch [5], Batch [178/938], Loss: 0.4652695953845978\n",
      "Validation: Epoch [5], Batch [179/938], Loss: 0.8859732747077942\n",
      "Validation: Epoch [5], Batch [180/938], Loss: 0.5490376353263855\n",
      "Validation: Epoch [5], Batch [181/938], Loss: 0.5320813655853271\n",
      "Validation: Epoch [5], Batch [182/938], Loss: 0.7410410642623901\n",
      "Validation: Epoch [5], Batch [183/938], Loss: 0.6225376129150391\n",
      "Validation: Epoch [5], Batch [184/938], Loss: 0.468700110912323\n",
      "Validation: Epoch [5], Batch [185/938], Loss: 0.6508805751800537\n",
      "Validation: Epoch [5], Batch [186/938], Loss: 0.5417779684066772\n",
      "Validation: Epoch [5], Batch [187/938], Loss: 0.537498950958252\n",
      "Validation: Epoch [5], Batch [188/938], Loss: 0.670582115650177\n",
      "Validation: Epoch [5], Batch [189/938], Loss: 0.7264671325683594\n",
      "Validation: Epoch [5], Batch [190/938], Loss: 0.7366520166397095\n",
      "Validation: Epoch [5], Batch [191/938], Loss: 0.6983176469802856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [192/938], Loss: 0.6620229482650757\n",
      "Validation: Epoch [5], Batch [193/938], Loss: 0.702124834060669\n",
      "Validation: Epoch [5], Batch [194/938], Loss: 0.5372190475463867\n",
      "Validation: Epoch [5], Batch [195/938], Loss: 0.6409870386123657\n",
      "Validation: Epoch [5], Batch [196/938], Loss: 0.4723164439201355\n",
      "Validation: Epoch [5], Batch [197/938], Loss: 0.647919237613678\n",
      "Validation: Epoch [5], Batch [198/938], Loss: 0.48109865188598633\n",
      "Validation: Epoch [5], Batch [199/938], Loss: 0.6539721488952637\n",
      "Validation: Epoch [5], Batch [200/938], Loss: 0.7070450782775879\n",
      "Validation: Epoch [5], Batch [201/938], Loss: 0.7059290409088135\n",
      "Validation: Epoch [5], Batch [202/938], Loss: 0.7737150192260742\n",
      "Validation: Epoch [5], Batch [203/938], Loss: 0.6709978580474854\n",
      "Validation: Epoch [5], Batch [204/938], Loss: 0.5483663082122803\n",
      "Validation: Epoch [5], Batch [205/938], Loss: 0.6901084780693054\n",
      "Validation: Epoch [5], Batch [206/938], Loss: 0.6278502345085144\n",
      "Validation: Epoch [5], Batch [207/938], Loss: 0.7653635144233704\n",
      "Validation: Epoch [5], Batch [208/938], Loss: 0.8003643751144409\n",
      "Validation: Epoch [5], Batch [209/938], Loss: 0.6017228364944458\n",
      "Validation: Epoch [5], Batch [210/938], Loss: 0.552760660648346\n",
      "Validation: Epoch [5], Batch [211/938], Loss: 0.790142834186554\n",
      "Validation: Epoch [5], Batch [212/938], Loss: 0.6703770756721497\n",
      "Validation: Epoch [5], Batch [213/938], Loss: 0.7543638944625854\n",
      "Validation: Epoch [5], Batch [214/938], Loss: 0.784414529800415\n",
      "Validation: Epoch [5], Batch [215/938], Loss: 0.69938063621521\n",
      "Validation: Epoch [5], Batch [216/938], Loss: 0.6787852048873901\n",
      "Validation: Epoch [5], Batch [217/938], Loss: 0.5358031988143921\n",
      "Validation: Epoch [5], Batch [218/938], Loss: 0.8763927221298218\n",
      "Validation: Epoch [5], Batch [219/938], Loss: 0.7245056629180908\n",
      "Validation: Epoch [5], Batch [220/938], Loss: 0.6063565015792847\n",
      "Validation: Epoch [5], Batch [221/938], Loss: 1.0487697124481201\n",
      "Validation: Epoch [5], Batch [222/938], Loss: 0.5746074318885803\n",
      "Validation: Epoch [5], Batch [223/938], Loss: 0.4900470972061157\n",
      "Validation: Epoch [5], Batch [224/938], Loss: 0.5093518495559692\n",
      "Validation: Epoch [5], Batch [225/938], Loss: 0.666713535785675\n",
      "Validation: Epoch [5], Batch [226/938], Loss: 0.657187283039093\n",
      "Validation: Epoch [5], Batch [227/938], Loss: 0.8123120069503784\n",
      "Validation: Epoch [5], Batch [228/938], Loss: 0.6588797569274902\n",
      "Validation: Epoch [5], Batch [229/938], Loss: 0.6063429117202759\n",
      "Validation: Epoch [5], Batch [230/938], Loss: 0.7174566984176636\n",
      "Validation: Epoch [5], Batch [231/938], Loss: 0.9462109804153442\n",
      "Validation: Epoch [5], Batch [232/938], Loss: 0.7137247323989868\n",
      "Validation: Epoch [5], Batch [233/938], Loss: 0.5848737955093384\n",
      "Validation: Epoch [5], Batch [234/938], Loss: 0.7064380645751953\n",
      "Validation: Epoch [5], Batch [235/938], Loss: 0.4994363486766815\n",
      "Validation: Epoch [5], Batch [236/938], Loss: 0.6103857159614563\n",
      "Validation: Epoch [5], Batch [237/938], Loss: 0.7151187658309937\n",
      "Validation: Epoch [5], Batch [238/938], Loss: 0.7995734214782715\n",
      "Validation: Epoch [5], Batch [239/938], Loss: 0.3826547861099243\n",
      "Validation: Epoch [5], Batch [240/938], Loss: 0.5338600873947144\n",
      "Validation: Epoch [5], Batch [241/938], Loss: 0.8913520574569702\n",
      "Validation: Epoch [5], Batch [242/938], Loss: 0.6802196502685547\n",
      "Validation: Epoch [5], Batch [243/938], Loss: 0.5738856792449951\n",
      "Validation: Epoch [5], Batch [244/938], Loss: 0.8408710360527039\n",
      "Validation: Epoch [5], Batch [245/938], Loss: 0.7656479477882385\n",
      "Validation: Epoch [5], Batch [246/938], Loss: 0.6649339199066162\n",
      "Validation: Epoch [5], Batch [247/938], Loss: 0.7352780103683472\n",
      "Validation: Epoch [5], Batch [248/938], Loss: 0.5539981126785278\n",
      "Validation: Epoch [5], Batch [249/938], Loss: 0.7728672027587891\n",
      "Validation: Epoch [5], Batch [250/938], Loss: 0.7423498630523682\n",
      "Validation: Epoch [5], Batch [251/938], Loss: 0.6101604700088501\n",
      "Validation: Epoch [5], Batch [252/938], Loss: 0.6718049049377441\n",
      "Validation: Epoch [5], Batch [253/938], Loss: 0.5857968330383301\n",
      "Validation: Epoch [5], Batch [254/938], Loss: 0.5088930130004883\n",
      "Validation: Epoch [5], Batch [255/938], Loss: 0.6348267197608948\n",
      "Validation: Epoch [5], Batch [256/938], Loss: 0.5589560270309448\n",
      "Validation: Epoch [5], Batch [257/938], Loss: 0.674629807472229\n",
      "Validation: Epoch [5], Batch [258/938], Loss: 0.6595457792282104\n",
      "Validation: Epoch [5], Batch [259/938], Loss: 0.6219089031219482\n",
      "Validation: Epoch [5], Batch [260/938], Loss: 0.6717197895050049\n",
      "Validation: Epoch [5], Batch [261/938], Loss: 0.7741677165031433\n",
      "Validation: Epoch [5], Batch [262/938], Loss: 0.7518501877784729\n",
      "Validation: Epoch [5], Batch [263/938], Loss: 0.8559200763702393\n",
      "Validation: Epoch [5], Batch [264/938], Loss: 0.48869162797927856\n",
      "Validation: Epoch [5], Batch [265/938], Loss: 0.7561143636703491\n",
      "Validation: Epoch [5], Batch [266/938], Loss: 0.6796411871910095\n",
      "Validation: Epoch [5], Batch [267/938], Loss: 0.5135312080383301\n",
      "Validation: Epoch [5], Batch [268/938], Loss: 0.6343141198158264\n",
      "Validation: Epoch [5], Batch [269/938], Loss: 0.7992113828659058\n",
      "Validation: Epoch [5], Batch [270/938], Loss: 0.686160683631897\n",
      "Validation: Epoch [5], Batch [271/938], Loss: 0.7004368305206299\n",
      "Validation: Epoch [5], Batch [272/938], Loss: 0.6192601323127747\n",
      "Validation: Epoch [5], Batch [273/938], Loss: 0.5162147879600525\n",
      "Validation: Epoch [5], Batch [274/938], Loss: 0.7829380035400391\n",
      "Validation: Epoch [5], Batch [275/938], Loss: 0.5774327516555786\n",
      "Validation: Epoch [5], Batch [276/938], Loss: 0.7409065365791321\n",
      "Validation: Epoch [5], Batch [277/938], Loss: 0.7997298836708069\n",
      "Validation: Epoch [5], Batch [278/938], Loss: 0.5073656439781189\n",
      "Validation: Epoch [5], Batch [279/938], Loss: 0.7816624045372009\n",
      "Validation: Epoch [5], Batch [280/938], Loss: 0.9574667811393738\n",
      "Validation: Epoch [5], Batch [281/938], Loss: 0.6966555118560791\n",
      "Validation: Epoch [5], Batch [282/938], Loss: 0.7940897941589355\n",
      "Validation: Epoch [5], Batch [283/938], Loss: 0.4725654721260071\n",
      "Validation: Epoch [5], Batch [284/938], Loss: 0.5859695672988892\n",
      "Validation: Epoch [5], Batch [285/938], Loss: 0.7182839512825012\n",
      "Validation: Epoch [5], Batch [286/938], Loss: 0.8403899669647217\n",
      "Validation: Epoch [5], Batch [287/938], Loss: 0.6991339921951294\n",
      "Validation: Epoch [5], Batch [288/938], Loss: 0.6092510223388672\n",
      "Validation: Epoch [5], Batch [289/938], Loss: 0.9464962482452393\n",
      "Validation: Epoch [5], Batch [290/938], Loss: 0.7336013317108154\n",
      "Validation: Epoch [5], Batch [291/938], Loss: 0.662643313407898\n",
      "Validation: Epoch [5], Batch [292/938], Loss: 0.702620267868042\n",
      "Validation: Epoch [5], Batch [293/938], Loss: 0.4492814540863037\n",
      "Validation: Epoch [5], Batch [294/938], Loss: 0.7712262868881226\n",
      "Validation: Epoch [5], Batch [295/938], Loss: 0.727807879447937\n",
      "Validation: Epoch [5], Batch [296/938], Loss: 0.6588596105575562\n",
      "Validation: Epoch [5], Batch [297/938], Loss: 0.6975926160812378\n",
      "Validation: Epoch [5], Batch [298/938], Loss: 0.6776899695396423\n",
      "Validation: Epoch [5], Batch [299/938], Loss: 0.978259265422821\n",
      "Validation: Epoch [5], Batch [300/938], Loss: 0.5778226852416992\n",
      "Validation: Epoch [5], Batch [301/938], Loss: 0.7109626531600952\n",
      "Validation: Epoch [5], Batch [302/938], Loss: 0.6910074949264526\n",
      "Validation: Epoch [5], Batch [303/938], Loss: 0.6237052083015442\n",
      "Validation: Epoch [5], Batch [304/938], Loss: 0.5878251791000366\n",
      "Validation: Epoch [5], Batch [305/938], Loss: 0.58136385679245\n",
      "Validation: Epoch [5], Batch [306/938], Loss: 0.6812476515769958\n",
      "Validation: Epoch [5], Batch [307/938], Loss: 0.6259089708328247\n",
      "Validation: Epoch [5], Batch [308/938], Loss: 0.6181191802024841\n",
      "Validation: Epoch [5], Batch [309/938], Loss: 0.8030494451522827\n",
      "Validation: Epoch [5], Batch [310/938], Loss: 0.6987718939781189\n",
      "Validation: Epoch [5], Batch [311/938], Loss: 0.805640697479248\n",
      "Validation: Epoch [5], Batch [312/938], Loss: 0.6131711006164551\n",
      "Validation: Epoch [5], Batch [313/938], Loss: 0.6626386642456055\n",
      "Validation: Epoch [5], Batch [314/938], Loss: 0.6269564032554626\n",
      "Validation: Epoch [5], Batch [315/938], Loss: 0.6285861730575562\n",
      "Validation: Epoch [5], Batch [316/938], Loss: 0.9512948989868164\n",
      "Validation: Epoch [5], Batch [317/938], Loss: 0.5876790881156921\n",
      "Validation: Epoch [5], Batch [318/938], Loss: 0.6973156929016113\n",
      "Validation: Epoch [5], Batch [319/938], Loss: 0.5590264201164246\n",
      "Validation: Epoch [5], Batch [320/938], Loss: 0.6002131700515747\n",
      "Validation: Epoch [5], Batch [321/938], Loss: 0.6069713830947876\n",
      "Validation: Epoch [5], Batch [322/938], Loss: 0.8206145763397217\n",
      "Validation: Epoch [5], Batch [323/938], Loss: 0.49342241883277893\n",
      "Validation: Epoch [5], Batch [324/938], Loss: 0.8199214935302734\n",
      "Validation: Epoch [5], Batch [325/938], Loss: 0.5685818791389465\n",
      "Validation: Epoch [5], Batch [326/938], Loss: 0.8966487646102905\n",
      "Validation: Epoch [5], Batch [327/938], Loss: 0.5798695087432861\n",
      "Validation: Epoch [5], Batch [328/938], Loss: 0.6429636478424072\n",
      "Validation: Epoch [5], Batch [329/938], Loss: 0.7541378140449524\n",
      "Validation: Epoch [5], Batch [330/938], Loss: 0.5722289085388184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [331/938], Loss: 0.6414852738380432\n",
      "Validation: Epoch [5], Batch [332/938], Loss: 0.69475257396698\n",
      "Validation: Epoch [5], Batch [333/938], Loss: 0.6670432090759277\n",
      "Validation: Epoch [5], Batch [334/938], Loss: 0.6431393623352051\n",
      "Validation: Epoch [5], Batch [335/938], Loss: 0.7946566343307495\n",
      "Validation: Epoch [5], Batch [336/938], Loss: 0.7884483933448792\n",
      "Validation: Epoch [5], Batch [337/938], Loss: 0.533064603805542\n",
      "Validation: Epoch [5], Batch [338/938], Loss: 0.6118849515914917\n",
      "Validation: Epoch [5], Batch [339/938], Loss: 0.7294182777404785\n",
      "Validation: Epoch [5], Batch [340/938], Loss: 0.711392343044281\n",
      "Validation: Epoch [5], Batch [341/938], Loss: 0.7777485251426697\n",
      "Validation: Epoch [5], Batch [342/938], Loss: 0.4765087366104126\n",
      "Validation: Epoch [5], Batch [343/938], Loss: 0.809341311454773\n",
      "Validation: Epoch [5], Batch [344/938], Loss: 0.7017266750335693\n",
      "Validation: Epoch [5], Batch [345/938], Loss: 0.6827560663223267\n",
      "Validation: Epoch [5], Batch [346/938], Loss: 0.8483399152755737\n",
      "Validation: Epoch [5], Batch [347/938], Loss: 0.5692481994628906\n",
      "Validation: Epoch [5], Batch [348/938], Loss: 0.7854751348495483\n",
      "Validation: Epoch [5], Batch [349/938], Loss: 0.771109938621521\n",
      "Validation: Epoch [5], Batch [350/938], Loss: 0.6219626665115356\n",
      "Validation: Epoch [5], Batch [351/938], Loss: 0.7225186824798584\n",
      "Validation: Epoch [5], Batch [352/938], Loss: 0.7857282161712646\n",
      "Validation: Epoch [5], Batch [353/938], Loss: 0.7170230150222778\n",
      "Validation: Epoch [5], Batch [354/938], Loss: 0.7450199127197266\n",
      "Validation: Epoch [5], Batch [355/938], Loss: 0.6419528722763062\n",
      "Validation: Epoch [5], Batch [356/938], Loss: 0.6981900930404663\n",
      "Validation: Epoch [5], Batch [357/938], Loss: 0.8125802278518677\n",
      "Validation: Epoch [5], Batch [358/938], Loss: 0.7509802579879761\n",
      "Validation: Epoch [5], Batch [359/938], Loss: 0.7572896480560303\n",
      "Validation: Epoch [5], Batch [360/938], Loss: 0.6389009952545166\n",
      "Validation: Epoch [5], Batch [361/938], Loss: 0.5317788124084473\n",
      "Validation: Epoch [5], Batch [362/938], Loss: 0.8022510409355164\n",
      "Validation: Epoch [5], Batch [363/938], Loss: 0.5236252546310425\n",
      "Validation: Epoch [5], Batch [364/938], Loss: 0.6354985237121582\n",
      "Validation: Epoch [5], Batch [365/938], Loss: 0.7285226583480835\n",
      "Validation: Epoch [5], Batch [366/938], Loss: 0.7522758841514587\n",
      "Validation: Epoch [5], Batch [367/938], Loss: 0.635112464427948\n",
      "Validation: Epoch [5], Batch [368/938], Loss: 0.618087649345398\n",
      "Validation: Epoch [5], Batch [369/938], Loss: 0.6205564737319946\n",
      "Validation: Epoch [5], Batch [370/938], Loss: 0.7299660444259644\n",
      "Validation: Epoch [5], Batch [371/938], Loss: 0.7348931431770325\n",
      "Validation: Epoch [5], Batch [372/938], Loss: 0.6269107460975647\n",
      "Validation: Epoch [5], Batch [373/938], Loss: 0.7453184127807617\n",
      "Validation: Epoch [5], Batch [374/938], Loss: 0.632737398147583\n",
      "Validation: Epoch [5], Batch [375/938], Loss: 1.0072427988052368\n",
      "Validation: Epoch [5], Batch [376/938], Loss: 0.7258484363555908\n",
      "Validation: Epoch [5], Batch [377/938], Loss: 0.4789280593395233\n",
      "Validation: Epoch [5], Batch [378/938], Loss: 0.8214471340179443\n",
      "Validation: Epoch [5], Batch [379/938], Loss: 0.7147867679595947\n",
      "Validation: Epoch [5], Batch [380/938], Loss: 0.7024035453796387\n",
      "Validation: Epoch [5], Batch [381/938], Loss: 0.761420488357544\n",
      "Validation: Epoch [5], Batch [382/938], Loss: 0.554662823677063\n",
      "Validation: Epoch [5], Batch [383/938], Loss: 0.6682148575782776\n",
      "Validation: Epoch [5], Batch [384/938], Loss: 0.5652680397033691\n",
      "Validation: Epoch [5], Batch [385/938], Loss: 0.6370490789413452\n",
      "Validation: Epoch [5], Batch [386/938], Loss: 0.4999981224536896\n",
      "Validation: Epoch [5], Batch [387/938], Loss: 0.6188881993293762\n",
      "Validation: Epoch [5], Batch [388/938], Loss: 0.7183743119239807\n",
      "Validation: Epoch [5], Batch [389/938], Loss: 0.7636977434158325\n",
      "Validation: Epoch [5], Batch [390/938], Loss: 0.6602617502212524\n",
      "Validation: Epoch [5], Batch [391/938], Loss: 0.8219591379165649\n",
      "Validation: Epoch [5], Batch [392/938], Loss: 0.7624525427818298\n",
      "Validation: Epoch [5], Batch [393/938], Loss: 0.6335550546646118\n",
      "Validation: Epoch [5], Batch [394/938], Loss: 0.625178337097168\n",
      "Validation: Epoch [5], Batch [395/938], Loss: 0.6223719120025635\n",
      "Validation: Epoch [5], Batch [396/938], Loss: 0.6146209836006165\n",
      "Validation: Epoch [5], Batch [397/938], Loss: 0.6629472970962524\n",
      "Validation: Epoch [5], Batch [398/938], Loss: 0.639305591583252\n",
      "Validation: Epoch [5], Batch [399/938], Loss: 0.734281063079834\n",
      "Validation: Epoch [5], Batch [400/938], Loss: 0.8987942934036255\n",
      "Validation: Epoch [5], Batch [401/938], Loss: 0.7267619371414185\n",
      "Validation: Epoch [5], Batch [402/938], Loss: 0.5031765103340149\n",
      "Validation: Epoch [5], Batch [403/938], Loss: 0.5769982933998108\n",
      "Validation: Epoch [5], Batch [404/938], Loss: 0.6109771728515625\n",
      "Validation: Epoch [5], Batch [405/938], Loss: 0.6788955330848694\n",
      "Validation: Epoch [5], Batch [406/938], Loss: 0.48572754859924316\n",
      "Validation: Epoch [5], Batch [407/938], Loss: 0.8835792541503906\n",
      "Validation: Epoch [5], Batch [408/938], Loss: 0.7147727012634277\n",
      "Validation: Epoch [5], Batch [409/938], Loss: 0.7475156784057617\n",
      "Validation: Epoch [5], Batch [410/938], Loss: 0.5038213133811951\n",
      "Validation: Epoch [5], Batch [411/938], Loss: 0.6024682521820068\n",
      "Validation: Epoch [5], Batch [412/938], Loss: 0.8930774927139282\n",
      "Validation: Epoch [5], Batch [413/938], Loss: 0.5825436115264893\n",
      "Validation: Epoch [5], Batch [414/938], Loss: 0.8555948734283447\n",
      "Validation: Epoch [5], Batch [415/938], Loss: 0.5946145057678223\n",
      "Validation: Epoch [5], Batch [416/938], Loss: 0.7351943254470825\n",
      "Validation: Epoch [5], Batch [417/938], Loss: 0.7719990015029907\n",
      "Validation: Epoch [5], Batch [418/938], Loss: 0.7155856490135193\n",
      "Validation: Epoch [5], Batch [419/938], Loss: 0.835789144039154\n",
      "Validation: Epoch [5], Batch [420/938], Loss: 0.5623177289962769\n",
      "Validation: Epoch [5], Batch [421/938], Loss: 0.8476735949516296\n",
      "Validation: Epoch [5], Batch [422/938], Loss: 0.7746219038963318\n",
      "Validation: Epoch [5], Batch [423/938], Loss: 0.6837632656097412\n",
      "Validation: Epoch [5], Batch [424/938], Loss: 0.6664546728134155\n",
      "Validation: Epoch [5], Batch [425/938], Loss: 0.6756126880645752\n",
      "Validation: Epoch [5], Batch [426/938], Loss: 0.7291616797447205\n",
      "Validation: Epoch [5], Batch [427/938], Loss: 0.6716868877410889\n",
      "Validation: Epoch [5], Batch [428/938], Loss: 1.042190432548523\n",
      "Validation: Epoch [5], Batch [429/938], Loss: 0.5024553537368774\n",
      "Validation: Epoch [5], Batch [430/938], Loss: 0.6216644644737244\n",
      "Validation: Epoch [5], Batch [431/938], Loss: 0.6342223286628723\n",
      "Validation: Epoch [5], Batch [432/938], Loss: 0.6747868657112122\n",
      "Validation: Epoch [5], Batch [433/938], Loss: 0.8804689049720764\n",
      "Validation: Epoch [5], Batch [434/938], Loss: 0.6808037161827087\n",
      "Validation: Epoch [5], Batch [435/938], Loss: 0.5777813196182251\n",
      "Validation: Epoch [5], Batch [436/938], Loss: 0.773230791091919\n",
      "Validation: Epoch [5], Batch [437/938], Loss: 0.7527233362197876\n",
      "Validation: Epoch [5], Batch [438/938], Loss: 0.6444941759109497\n",
      "Validation: Epoch [5], Batch [439/938], Loss: 0.7405292391777039\n",
      "Validation: Epoch [5], Batch [440/938], Loss: 0.6675294637680054\n",
      "Validation: Epoch [5], Batch [441/938], Loss: 0.48874837160110474\n",
      "Validation: Epoch [5], Batch [442/938], Loss: 0.7129186391830444\n",
      "Validation: Epoch [5], Batch [443/938], Loss: 0.6332647800445557\n",
      "Validation: Epoch [5], Batch [444/938], Loss: 0.7058179378509521\n",
      "Validation: Epoch [5], Batch [445/938], Loss: 0.9238117933273315\n",
      "Validation: Epoch [5], Batch [446/938], Loss: 0.7172014713287354\n",
      "Validation: Epoch [5], Batch [447/938], Loss: 0.8553404211997986\n",
      "Validation: Epoch [5], Batch [448/938], Loss: 0.6291948556900024\n",
      "Validation: Epoch [5], Batch [449/938], Loss: 0.7168521881103516\n",
      "Validation: Epoch [5], Batch [450/938], Loss: 0.6504468321800232\n",
      "Validation: Epoch [5], Batch [451/938], Loss: 0.5319730639457703\n",
      "Validation: Epoch [5], Batch [452/938], Loss: 0.5952911376953125\n",
      "Validation: Epoch [5], Batch [453/938], Loss: 0.4850198030471802\n",
      "Validation: Epoch [5], Batch [454/938], Loss: 0.6246510744094849\n",
      "Validation: Epoch [5], Batch [455/938], Loss: 0.6854344606399536\n",
      "Validation: Epoch [5], Batch [456/938], Loss: 0.5458710789680481\n",
      "Validation: Epoch [5], Batch [457/938], Loss: 0.5954727530479431\n",
      "Validation: Epoch [5], Batch [458/938], Loss: 0.627885103225708\n",
      "Validation: Epoch [5], Batch [459/938], Loss: 0.6265620589256287\n",
      "Validation: Epoch [5], Batch [460/938], Loss: 0.6465761661529541\n",
      "Validation: Epoch [5], Batch [461/938], Loss: 0.9075915217399597\n",
      "Validation: Epoch [5], Batch [462/938], Loss: 0.5751069784164429\n",
      "Validation: Epoch [5], Batch [463/938], Loss: 0.5929818153381348\n",
      "Validation: Epoch [5], Batch [464/938], Loss: 0.5945595502853394\n",
      "Validation: Epoch [5], Batch [465/938], Loss: 0.6720497608184814\n",
      "Validation: Epoch [5], Batch [466/938], Loss: 0.7190552949905396\n",
      "Validation: Epoch [5], Batch [467/938], Loss: 0.5999512672424316\n",
      "Validation: Epoch [5], Batch [468/938], Loss: 0.815746009349823\n",
      "Validation: Epoch [5], Batch [469/938], Loss: 0.6185483932495117\n",
      "Validation: Epoch [5], Batch [470/938], Loss: 0.42767608165740967\n",
      "Validation: Epoch [5], Batch [471/938], Loss: 0.4832553267478943\n",
      "Validation: Epoch [5], Batch [472/938], Loss: 0.5201833248138428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [473/938], Loss: 0.6383583545684814\n",
      "Validation: Epoch [5], Batch [474/938], Loss: 0.6923949718475342\n",
      "Validation: Epoch [5], Batch [475/938], Loss: 0.6142810583114624\n",
      "Validation: Epoch [5], Batch [476/938], Loss: 0.832929253578186\n",
      "Validation: Epoch [5], Batch [477/938], Loss: 0.6110162138938904\n",
      "Validation: Epoch [5], Batch [478/938], Loss: 0.6564724445343018\n",
      "Validation: Epoch [5], Batch [479/938], Loss: 0.6224381923675537\n",
      "Validation: Epoch [5], Batch [480/938], Loss: 0.5361449122428894\n",
      "Validation: Epoch [5], Batch [481/938], Loss: 0.830077052116394\n",
      "Validation: Epoch [5], Batch [482/938], Loss: 0.7102771997451782\n",
      "Validation: Epoch [5], Batch [483/938], Loss: 0.5725593566894531\n",
      "Validation: Epoch [5], Batch [484/938], Loss: 0.7189093828201294\n",
      "Validation: Epoch [5], Batch [485/938], Loss: 0.6259862780570984\n",
      "Validation: Epoch [5], Batch [486/938], Loss: 0.6471731662750244\n",
      "Validation: Epoch [5], Batch [487/938], Loss: 0.6122028231620789\n",
      "Validation: Epoch [5], Batch [488/938], Loss: 0.6680296659469604\n",
      "Validation: Epoch [5], Batch [489/938], Loss: 0.7446753978729248\n",
      "Validation: Epoch [5], Batch [490/938], Loss: 0.6818621754646301\n",
      "Validation: Epoch [5], Batch [491/938], Loss: 0.7138729095458984\n",
      "Validation: Epoch [5], Batch [492/938], Loss: 0.6395203471183777\n",
      "Validation: Epoch [5], Batch [493/938], Loss: 0.7377442121505737\n",
      "Validation: Epoch [5], Batch [494/938], Loss: 0.7068668603897095\n",
      "Validation: Epoch [5], Batch [495/938], Loss: 0.6325564980506897\n",
      "Validation: Epoch [5], Batch [496/938], Loss: 0.5474790930747986\n",
      "Validation: Epoch [5], Batch [497/938], Loss: 0.7091338634490967\n",
      "Validation: Epoch [5], Batch [498/938], Loss: 0.7711622714996338\n",
      "Validation: Epoch [5], Batch [499/938], Loss: 0.631222128868103\n",
      "Validation: Epoch [5], Batch [500/938], Loss: 0.6677516102790833\n",
      "Validation: Epoch [5], Batch [501/938], Loss: 0.7298121452331543\n",
      "Validation: Epoch [5], Batch [502/938], Loss: 0.5176602005958557\n",
      "Validation: Epoch [5], Batch [503/938], Loss: 0.6506503224372864\n",
      "Validation: Epoch [5], Batch [504/938], Loss: 0.9745863676071167\n",
      "Validation: Epoch [5], Batch [505/938], Loss: 0.8045650124549866\n",
      "Validation: Epoch [5], Batch [506/938], Loss: 0.5175693035125732\n",
      "Validation: Epoch [5], Batch [507/938], Loss: 0.6974189281463623\n",
      "Validation: Epoch [5], Batch [508/938], Loss: 0.5913706421852112\n",
      "Validation: Epoch [5], Batch [509/938], Loss: 0.7726250886917114\n",
      "Validation: Epoch [5], Batch [510/938], Loss: 0.5668454766273499\n",
      "Validation: Epoch [5], Batch [511/938], Loss: 0.6996539831161499\n",
      "Validation: Epoch [5], Batch [512/938], Loss: 0.729089617729187\n",
      "Validation: Epoch [5], Batch [513/938], Loss: 0.6562421321868896\n",
      "Validation: Epoch [5], Batch [514/938], Loss: 0.674016535282135\n",
      "Validation: Epoch [5], Batch [515/938], Loss: 0.6427251100540161\n",
      "Validation: Epoch [5], Batch [516/938], Loss: 0.704227089881897\n",
      "Validation: Epoch [5], Batch [517/938], Loss: 0.9590710997581482\n",
      "Validation: Epoch [5], Batch [518/938], Loss: 0.7234815359115601\n",
      "Validation: Epoch [5], Batch [519/938], Loss: 0.9761334657669067\n",
      "Validation: Epoch [5], Batch [520/938], Loss: 0.6092149019241333\n",
      "Validation: Epoch [5], Batch [521/938], Loss: 0.6912727952003479\n",
      "Validation: Epoch [5], Batch [522/938], Loss: 0.6910367012023926\n",
      "Validation: Epoch [5], Batch [523/938], Loss: 0.743798553943634\n",
      "Validation: Epoch [5], Batch [524/938], Loss: 0.8266441226005554\n",
      "Validation: Epoch [5], Batch [525/938], Loss: 0.6851140260696411\n",
      "Validation: Epoch [5], Batch [526/938], Loss: 0.6449134349822998\n",
      "Validation: Epoch [5], Batch [527/938], Loss: 0.468129962682724\n",
      "Validation: Epoch [5], Batch [528/938], Loss: 0.6292938590049744\n",
      "Validation: Epoch [5], Batch [529/938], Loss: 0.7227796316146851\n",
      "Validation: Epoch [5], Batch [530/938], Loss: 0.6482630968093872\n",
      "Validation: Epoch [5], Batch [531/938], Loss: 0.9527233839035034\n",
      "Validation: Epoch [5], Batch [532/938], Loss: 0.6657509803771973\n",
      "Validation: Epoch [5], Batch [533/938], Loss: 0.6131779551506042\n",
      "Validation: Epoch [5], Batch [534/938], Loss: 0.5870407819747925\n",
      "Validation: Epoch [5], Batch [535/938], Loss: 0.7111331224441528\n",
      "Validation: Epoch [5], Batch [536/938], Loss: 0.5501207113265991\n",
      "Validation: Epoch [5], Batch [537/938], Loss: 0.6911408305168152\n",
      "Validation: Epoch [5], Batch [538/938], Loss: 0.6119095087051392\n",
      "Validation: Epoch [5], Batch [539/938], Loss: 0.5489278435707092\n",
      "Validation: Epoch [5], Batch [540/938], Loss: 0.5404523611068726\n",
      "Validation: Epoch [5], Batch [541/938], Loss: 0.5296720862388611\n",
      "Validation: Epoch [5], Batch [542/938], Loss: 0.6147159337997437\n",
      "Validation: Epoch [5], Batch [543/938], Loss: 0.575239896774292\n",
      "Validation: Epoch [5], Batch [544/938], Loss: 0.7719706296920776\n",
      "Validation: Epoch [5], Batch [545/938], Loss: 0.6973664164543152\n",
      "Validation: Epoch [5], Batch [546/938], Loss: 0.720548152923584\n",
      "Validation: Epoch [5], Batch [547/938], Loss: 0.46151578426361084\n",
      "Validation: Epoch [5], Batch [548/938], Loss: 0.6853816509246826\n",
      "Validation: Epoch [5], Batch [549/938], Loss: 0.624670147895813\n",
      "Validation: Epoch [5], Batch [550/938], Loss: 0.5583178400993347\n",
      "Validation: Epoch [5], Batch [551/938], Loss: 0.5070508718490601\n",
      "Validation: Epoch [5], Batch [552/938], Loss: 0.761198878288269\n",
      "Validation: Epoch [5], Batch [553/938], Loss: 0.8413888812065125\n",
      "Validation: Epoch [5], Batch [554/938], Loss: 0.525737464427948\n",
      "Validation: Epoch [5], Batch [555/938], Loss: 0.7166112065315247\n",
      "Validation: Epoch [5], Batch [556/938], Loss: 0.5356863737106323\n",
      "Validation: Epoch [5], Batch [557/938], Loss: 0.8662038445472717\n",
      "Validation: Epoch [5], Batch [558/938], Loss: 0.7120909690856934\n",
      "Validation: Epoch [5], Batch [559/938], Loss: 0.8048596382141113\n",
      "Validation: Epoch [5], Batch [560/938], Loss: 0.8116922974586487\n",
      "Validation: Epoch [5], Batch [561/938], Loss: 0.6019166707992554\n",
      "Validation: Epoch [5], Batch [562/938], Loss: 0.7138338088989258\n",
      "Validation: Epoch [5], Batch [563/938], Loss: 0.7412475943565369\n",
      "Validation: Epoch [5], Batch [564/938], Loss: 0.5979675054550171\n",
      "Validation: Epoch [5], Batch [565/938], Loss: 0.9605264067649841\n",
      "Validation: Epoch [5], Batch [566/938], Loss: 0.6830446124076843\n",
      "Validation: Epoch [5], Batch [567/938], Loss: 0.6437841653823853\n",
      "Validation: Epoch [5], Batch [568/938], Loss: 0.6184006333351135\n",
      "Validation: Epoch [5], Batch [569/938], Loss: 0.6475677490234375\n",
      "Validation: Epoch [5], Batch [570/938], Loss: 0.5568552017211914\n",
      "Validation: Epoch [5], Batch [571/938], Loss: 0.860173761844635\n",
      "Validation: Epoch [5], Batch [572/938], Loss: 0.6472973823547363\n",
      "Validation: Epoch [5], Batch [573/938], Loss: 0.5278913974761963\n",
      "Validation: Epoch [5], Batch [574/938], Loss: 0.580618143081665\n",
      "Validation: Epoch [5], Batch [575/938], Loss: 0.7228571176528931\n",
      "Validation: Epoch [5], Batch [576/938], Loss: 0.5440816879272461\n",
      "Validation: Epoch [5], Batch [577/938], Loss: 0.5581609606742859\n",
      "Validation: Epoch [5], Batch [578/938], Loss: 0.5151707530021667\n",
      "Validation: Epoch [5], Batch [579/938], Loss: 0.6491250991821289\n",
      "Validation: Epoch [5], Batch [580/938], Loss: 0.6138433218002319\n",
      "Validation: Epoch [5], Batch [581/938], Loss: 0.5812668800354004\n",
      "Validation: Epoch [5], Batch [582/938], Loss: 0.5781642198562622\n",
      "Validation: Epoch [5], Batch [583/938], Loss: 0.789859414100647\n",
      "Validation: Epoch [5], Batch [584/938], Loss: 0.6883469820022583\n",
      "Validation: Epoch [5], Batch [585/938], Loss: 0.8550820350646973\n",
      "Validation: Epoch [5], Batch [586/938], Loss: 0.6944693922996521\n",
      "Validation: Epoch [5], Batch [587/938], Loss: 0.7238807678222656\n",
      "Validation: Epoch [5], Batch [588/938], Loss: 0.5040978193283081\n",
      "Validation: Epoch [5], Batch [589/938], Loss: 0.5649591684341431\n",
      "Validation: Epoch [5], Batch [590/938], Loss: 0.618575394153595\n",
      "Validation: Epoch [5], Batch [591/938], Loss: 0.5616928339004517\n",
      "Validation: Epoch [5], Batch [592/938], Loss: 0.8836845755577087\n",
      "Validation: Epoch [5], Batch [593/938], Loss: 0.7081645131111145\n",
      "Validation: Epoch [5], Batch [594/938], Loss: 0.8266098499298096\n",
      "Validation: Epoch [5], Batch [595/938], Loss: 0.7511565685272217\n",
      "Validation: Epoch [5], Batch [596/938], Loss: 0.5682151913642883\n",
      "Validation: Epoch [5], Batch [597/938], Loss: 0.6574831008911133\n",
      "Validation: Epoch [5], Batch [598/938], Loss: 0.479294091463089\n",
      "Validation: Epoch [5], Batch [599/938], Loss: 0.7028032541275024\n",
      "Validation: Epoch [5], Batch [600/938], Loss: 0.5725032091140747\n",
      "Validation: Epoch [5], Batch [601/938], Loss: 0.7122942209243774\n",
      "Validation: Epoch [5], Batch [602/938], Loss: 0.5704076290130615\n",
      "Validation: Epoch [5], Batch [603/938], Loss: 0.7020730972290039\n",
      "Validation: Epoch [5], Batch [604/938], Loss: 0.6247942447662354\n",
      "Validation: Epoch [5], Batch [605/938], Loss: 0.8737984299659729\n",
      "Validation: Epoch [5], Batch [606/938], Loss: 0.7274772524833679\n",
      "Validation: Epoch [5], Batch [607/938], Loss: 0.8013176321983337\n",
      "Validation: Epoch [5], Batch [608/938], Loss: 0.5304563045501709\n",
      "Validation: Epoch [5], Batch [609/938], Loss: 0.877456784248352\n",
      "Validation: Epoch [5], Batch [610/938], Loss: 0.7077927589416504\n",
      "Validation: Epoch [5], Batch [611/938], Loss: 0.593964695930481\n",
      "Validation: Epoch [5], Batch [612/938], Loss: 0.719894289970398\n",
      "Validation: Epoch [5], Batch [613/938], Loss: 0.7804052233695984\n",
      "Validation: Epoch [5], Batch [614/938], Loss: 0.5616486072540283\n",
      "Validation: Epoch [5], Batch [615/938], Loss: 0.6475109457969666\n",
      "Validation: Epoch [5], Batch [616/938], Loss: 0.49078866839408875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [617/938], Loss: 0.5209646821022034\n",
      "Validation: Epoch [5], Batch [618/938], Loss: 0.7646282315254211\n",
      "Validation: Epoch [5], Batch [619/938], Loss: 0.5944621562957764\n",
      "Validation: Epoch [5], Batch [620/938], Loss: 0.6709278225898743\n",
      "Validation: Epoch [5], Batch [621/938], Loss: 0.800828754901886\n",
      "Validation: Epoch [5], Batch [622/938], Loss: 0.8019299507141113\n",
      "Validation: Epoch [5], Batch [623/938], Loss: 0.7356685400009155\n",
      "Validation: Epoch [5], Batch [624/938], Loss: 0.7281352281570435\n",
      "Validation: Epoch [5], Batch [625/938], Loss: 0.8105103373527527\n",
      "Validation: Epoch [5], Batch [626/938], Loss: 0.6835100054740906\n",
      "Validation: Epoch [5], Batch [627/938], Loss: 0.5306796431541443\n",
      "Validation: Epoch [5], Batch [628/938], Loss: 0.6425204277038574\n",
      "Validation: Epoch [5], Batch [629/938], Loss: 0.7415069937705994\n",
      "Validation: Epoch [5], Batch [630/938], Loss: 0.6913042664527893\n",
      "Validation: Epoch [5], Batch [631/938], Loss: 0.5017205476760864\n",
      "Validation: Epoch [5], Batch [632/938], Loss: 0.5687903761863708\n",
      "Validation: Epoch [5], Batch [633/938], Loss: 0.705639123916626\n",
      "Validation: Epoch [5], Batch [634/938], Loss: 0.8573703765869141\n",
      "Validation: Epoch [5], Batch [635/938], Loss: 0.8597884178161621\n",
      "Validation: Epoch [5], Batch [636/938], Loss: 0.7174800634384155\n",
      "Validation: Epoch [5], Batch [637/938], Loss: 0.6716650724411011\n",
      "Validation: Epoch [5], Batch [638/938], Loss: 0.579850435256958\n",
      "Validation: Epoch [5], Batch [639/938], Loss: 0.5381901860237122\n",
      "Validation: Epoch [5], Batch [640/938], Loss: 0.7746156454086304\n",
      "Validation: Epoch [5], Batch [641/938], Loss: 0.9328703880310059\n",
      "Validation: Epoch [5], Batch [642/938], Loss: 0.47005540132522583\n",
      "Validation: Epoch [5], Batch [643/938], Loss: 0.6485311985015869\n",
      "Validation: Epoch [5], Batch [644/938], Loss: 0.6393812298774719\n",
      "Validation: Epoch [5], Batch [645/938], Loss: 0.7238523960113525\n",
      "Validation: Epoch [5], Batch [646/938], Loss: 0.6967860460281372\n",
      "Validation: Epoch [5], Batch [647/938], Loss: 0.7877005338668823\n",
      "Validation: Epoch [5], Batch [648/938], Loss: 0.6374999284744263\n",
      "Validation: Epoch [5], Batch [649/938], Loss: 0.693793773651123\n",
      "Validation: Epoch [5], Batch [650/938], Loss: 0.7636516094207764\n",
      "Validation: Epoch [5], Batch [651/938], Loss: 0.5025250315666199\n",
      "Validation: Epoch [5], Batch [652/938], Loss: 0.5895476341247559\n",
      "Validation: Epoch [5], Batch [653/938], Loss: 0.6524191498756409\n",
      "Validation: Epoch [5], Batch [654/938], Loss: 0.846297025680542\n",
      "Validation: Epoch [5], Batch [655/938], Loss: 0.7263160943984985\n",
      "Validation: Epoch [5], Batch [656/938], Loss: 0.7044478058815002\n",
      "Validation: Epoch [5], Batch [657/938], Loss: 0.7430671453475952\n",
      "Validation: Epoch [5], Batch [658/938], Loss: 0.6148098707199097\n",
      "Validation: Epoch [5], Batch [659/938], Loss: 0.5417361259460449\n",
      "Validation: Epoch [5], Batch [660/938], Loss: 0.6411772966384888\n",
      "Validation: Epoch [5], Batch [661/938], Loss: 0.716143012046814\n",
      "Validation: Epoch [5], Batch [662/938], Loss: 0.7794989347457886\n",
      "Validation: Epoch [5], Batch [663/938], Loss: 0.7017766237258911\n",
      "Validation: Epoch [5], Batch [664/938], Loss: 0.6441837549209595\n",
      "Validation: Epoch [5], Batch [665/938], Loss: 0.7865921258926392\n",
      "Validation: Epoch [5], Batch [666/938], Loss: 0.8277162313461304\n",
      "Validation: Epoch [5], Batch [667/938], Loss: 0.8110560774803162\n",
      "Validation: Epoch [5], Batch [668/938], Loss: 0.8892884850502014\n",
      "Validation: Epoch [5], Batch [669/938], Loss: 0.7194985747337341\n",
      "Validation: Epoch [5], Batch [670/938], Loss: 0.8823155760765076\n",
      "Validation: Epoch [5], Batch [671/938], Loss: 0.5598364472389221\n",
      "Validation: Epoch [5], Batch [672/938], Loss: 0.9109159111976624\n",
      "Validation: Epoch [5], Batch [673/938], Loss: 0.712108850479126\n",
      "Validation: Epoch [5], Batch [674/938], Loss: 0.7363262176513672\n",
      "Validation: Epoch [5], Batch [675/938], Loss: 0.48736315965652466\n",
      "Validation: Epoch [5], Batch [676/938], Loss: 0.6411843299865723\n",
      "Validation: Epoch [5], Batch [677/938], Loss: 0.6303480863571167\n",
      "Validation: Epoch [5], Batch [678/938], Loss: 0.6925365924835205\n",
      "Validation: Epoch [5], Batch [679/938], Loss: 0.5756414532661438\n",
      "Validation: Epoch [5], Batch [680/938], Loss: 0.6652043461799622\n",
      "Validation: Epoch [5], Batch [681/938], Loss: 0.7205132246017456\n",
      "Validation: Epoch [5], Batch [682/938], Loss: 0.5949300527572632\n",
      "Validation: Epoch [5], Batch [683/938], Loss: 0.5830752849578857\n",
      "Validation: Epoch [5], Batch [684/938], Loss: 0.7075619697570801\n",
      "Validation: Epoch [5], Batch [685/938], Loss: 0.7220720648765564\n",
      "Validation: Epoch [5], Batch [686/938], Loss: 0.5685145854949951\n",
      "Validation: Epoch [5], Batch [687/938], Loss: 0.7249643802642822\n",
      "Validation: Epoch [5], Batch [688/938], Loss: 0.7375829815864563\n",
      "Validation: Epoch [5], Batch [689/938], Loss: 0.6639225482940674\n",
      "Validation: Epoch [5], Batch [690/938], Loss: 0.6046590805053711\n",
      "Validation: Epoch [5], Batch [691/938], Loss: 0.8780131340026855\n",
      "Validation: Epoch [5], Batch [692/938], Loss: 0.9147088527679443\n",
      "Validation: Epoch [5], Batch [693/938], Loss: 0.7385913133621216\n",
      "Validation: Epoch [5], Batch [694/938], Loss: 0.7117823958396912\n",
      "Validation: Epoch [5], Batch [695/938], Loss: 0.5648184418678284\n",
      "Validation: Epoch [5], Batch [696/938], Loss: 0.7601353526115417\n",
      "Validation: Epoch [5], Batch [697/938], Loss: 0.7456493377685547\n",
      "Validation: Epoch [5], Batch [698/938], Loss: 0.6372213959693909\n",
      "Validation: Epoch [5], Batch [699/938], Loss: 0.7670712471008301\n",
      "Validation: Epoch [5], Batch [700/938], Loss: 0.6558445692062378\n",
      "Validation: Epoch [5], Batch [701/938], Loss: 0.7026847004890442\n",
      "Validation: Epoch [5], Batch [702/938], Loss: 0.7132090330123901\n",
      "Validation: Epoch [5], Batch [703/938], Loss: 0.5183478593826294\n",
      "Validation: Epoch [5], Batch [704/938], Loss: 0.7104699015617371\n",
      "Validation: Epoch [5], Batch [705/938], Loss: 0.9075279235839844\n",
      "Validation: Epoch [5], Batch [706/938], Loss: 0.4136098027229309\n",
      "Validation: Epoch [5], Batch [707/938], Loss: 0.8469219207763672\n",
      "Validation: Epoch [5], Batch [708/938], Loss: 0.6940186023712158\n",
      "Validation: Epoch [5], Batch [709/938], Loss: 0.5730070471763611\n",
      "Validation: Epoch [5], Batch [710/938], Loss: 0.7064577341079712\n",
      "Validation: Epoch [5], Batch [711/938], Loss: 0.6245537996292114\n",
      "Validation: Epoch [5], Batch [712/938], Loss: 0.7358465194702148\n",
      "Validation: Epoch [5], Batch [713/938], Loss: 0.7481430172920227\n",
      "Validation: Epoch [5], Batch [714/938], Loss: 0.5939913988113403\n",
      "Validation: Epoch [5], Batch [715/938], Loss: 0.726030707359314\n",
      "Validation: Epoch [5], Batch [716/938], Loss: 0.6147284507751465\n",
      "Validation: Epoch [5], Batch [717/938], Loss: 0.8185862898826599\n",
      "Validation: Epoch [5], Batch [718/938], Loss: 0.7304679155349731\n",
      "Validation: Epoch [5], Batch [719/938], Loss: 0.7058328986167908\n",
      "Validation: Epoch [5], Batch [720/938], Loss: 0.7422252893447876\n",
      "Validation: Epoch [5], Batch [721/938], Loss: 0.69264817237854\n",
      "Validation: Epoch [5], Batch [722/938], Loss: 0.626130998134613\n",
      "Validation: Epoch [5], Batch [723/938], Loss: 0.6374804973602295\n",
      "Validation: Epoch [5], Batch [724/938], Loss: 0.6841430068016052\n",
      "Validation: Epoch [5], Batch [725/938], Loss: 0.5355298519134521\n",
      "Validation: Epoch [5], Batch [726/938], Loss: 0.6310114860534668\n",
      "Validation: Epoch [5], Batch [727/938], Loss: 0.5129895210266113\n",
      "Validation: Epoch [5], Batch [728/938], Loss: 0.6420464515686035\n",
      "Validation: Epoch [5], Batch [729/938], Loss: 0.5868712067604065\n",
      "Validation: Epoch [5], Batch [730/938], Loss: 0.5527690649032593\n",
      "Validation: Epoch [5], Batch [731/938], Loss: 0.6884782314300537\n",
      "Validation: Epoch [5], Batch [732/938], Loss: 0.7535425424575806\n",
      "Validation: Epoch [5], Batch [733/938], Loss: 0.6468002796173096\n",
      "Validation: Epoch [5], Batch [734/938], Loss: 0.8513801097869873\n",
      "Validation: Epoch [5], Batch [735/938], Loss: 0.5725259780883789\n",
      "Validation: Epoch [5], Batch [736/938], Loss: 0.5832575559616089\n",
      "Validation: Epoch [5], Batch [737/938], Loss: 0.6122519969940186\n",
      "Validation: Epoch [5], Batch [738/938], Loss: 0.6483232975006104\n",
      "Validation: Epoch [5], Batch [739/938], Loss: 0.5717134475708008\n",
      "Validation: Epoch [5], Batch [740/938], Loss: 0.7178033590316772\n",
      "Validation: Epoch [5], Batch [741/938], Loss: 0.8103663921356201\n",
      "Validation: Epoch [5], Batch [742/938], Loss: 0.7701365947723389\n",
      "Validation: Epoch [5], Batch [743/938], Loss: 0.7117443084716797\n",
      "Validation: Epoch [5], Batch [744/938], Loss: 0.654941201210022\n",
      "Validation: Epoch [5], Batch [745/938], Loss: 0.5116374492645264\n",
      "Validation: Epoch [5], Batch [746/938], Loss: 0.6748316287994385\n",
      "Validation: Epoch [5], Batch [747/938], Loss: 0.5211042165756226\n",
      "Validation: Epoch [5], Batch [748/938], Loss: 0.691085934638977\n",
      "Validation: Epoch [5], Batch [749/938], Loss: 0.7826567888259888\n",
      "Validation: Epoch [5], Batch [750/938], Loss: 0.6742508411407471\n",
      "Validation: Epoch [5], Batch [751/938], Loss: 0.9139478206634521\n",
      "Validation: Epoch [5], Batch [752/938], Loss: 0.7110940217971802\n",
      "Validation: Epoch [5], Batch [753/938], Loss: 0.8089775443077087\n",
      "Validation: Epoch [5], Batch [754/938], Loss: 0.4910668730735779\n",
      "Validation: Epoch [5], Batch [755/938], Loss: 0.7594448328018188\n",
      "Validation: Epoch [5], Batch [756/938], Loss: 0.6124796867370605\n",
      "Validation: Epoch [5], Batch [757/938], Loss: 0.8427496552467346\n",
      "Validation: Epoch [5], Batch [758/938], Loss: 0.46461591124534607\n",
      "Validation: Epoch [5], Batch [759/938], Loss: 0.5241384506225586\n",
      "Validation: Epoch [5], Batch [760/938], Loss: 0.7151064276695251\n",
      "Validation: Epoch [5], Batch [761/938], Loss: 0.6810528635978699\n",
      "Validation: Epoch [5], Batch [762/938], Loss: 0.7065763473510742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [763/938], Loss: 0.7951984405517578\n",
      "Validation: Epoch [5], Batch [764/938], Loss: 0.5024631023406982\n",
      "Validation: Epoch [5], Batch [765/938], Loss: 0.6537038087844849\n",
      "Validation: Epoch [5], Batch [766/938], Loss: 0.6054686903953552\n",
      "Validation: Epoch [5], Batch [767/938], Loss: 0.5994435548782349\n",
      "Validation: Epoch [5], Batch [768/938], Loss: 0.757758378982544\n",
      "Validation: Epoch [5], Batch [769/938], Loss: 0.7914127111434937\n",
      "Validation: Epoch [5], Batch [770/938], Loss: 0.6336052417755127\n",
      "Validation: Epoch [5], Batch [771/938], Loss: 0.7811269164085388\n",
      "Validation: Epoch [5], Batch [772/938], Loss: 0.6521943807601929\n",
      "Validation: Epoch [5], Batch [773/938], Loss: 0.688072919845581\n",
      "Validation: Epoch [5], Batch [774/938], Loss: 0.8815418481826782\n",
      "Validation: Epoch [5], Batch [775/938], Loss: 0.4212139844894409\n",
      "Validation: Epoch [5], Batch [776/938], Loss: 0.7193334102630615\n",
      "Validation: Epoch [5], Batch [777/938], Loss: 0.6816834807395935\n",
      "Validation: Epoch [5], Batch [778/938], Loss: 0.777573823928833\n",
      "Validation: Epoch [5], Batch [779/938], Loss: 0.6988154649734497\n",
      "Validation: Epoch [5], Batch [780/938], Loss: 0.6566449403762817\n",
      "Validation: Epoch [5], Batch [781/938], Loss: 0.5202093720436096\n",
      "Validation: Epoch [5], Batch [782/938], Loss: 0.6797208786010742\n",
      "Validation: Epoch [5], Batch [783/938], Loss: 0.8793888688087463\n",
      "Validation: Epoch [5], Batch [784/938], Loss: 0.5370955467224121\n",
      "Validation: Epoch [5], Batch [785/938], Loss: 0.5368506908416748\n",
      "Validation: Epoch [5], Batch [786/938], Loss: 0.7828171253204346\n",
      "Validation: Epoch [5], Batch [787/938], Loss: 0.810302734375\n",
      "Validation: Epoch [5], Batch [788/938], Loss: 0.6979771852493286\n",
      "Validation: Epoch [5], Batch [789/938], Loss: 0.7841173410415649\n",
      "Validation: Epoch [5], Batch [790/938], Loss: 0.8520607352256775\n",
      "Validation: Epoch [5], Batch [791/938], Loss: 0.6723325252532959\n",
      "Validation: Epoch [5], Batch [792/938], Loss: 0.6523216962814331\n",
      "Validation: Epoch [5], Batch [793/938], Loss: 0.5157865285873413\n",
      "Validation: Epoch [5], Batch [794/938], Loss: 0.6004878878593445\n",
      "Validation: Epoch [5], Batch [795/938], Loss: 0.7508642673492432\n",
      "Validation: Epoch [5], Batch [796/938], Loss: 0.5533099174499512\n",
      "Validation: Epoch [5], Batch [797/938], Loss: 0.773348331451416\n",
      "Validation: Epoch [5], Batch [798/938], Loss: 0.6059768199920654\n",
      "Validation: Epoch [5], Batch [799/938], Loss: 0.7252724170684814\n",
      "Validation: Epoch [5], Batch [800/938], Loss: 0.5042429566383362\n",
      "Validation: Epoch [5], Batch [801/938], Loss: 0.5574483871459961\n",
      "Validation: Epoch [5], Batch [802/938], Loss: 0.6491763591766357\n",
      "Validation: Epoch [5], Batch [803/938], Loss: 0.576622724533081\n",
      "Validation: Epoch [5], Batch [804/938], Loss: 0.7508609890937805\n",
      "Validation: Epoch [5], Batch [805/938], Loss: 0.8278325796127319\n",
      "Validation: Epoch [5], Batch [806/938], Loss: 0.4803394675254822\n",
      "Validation: Epoch [5], Batch [807/938], Loss: 0.526777446269989\n",
      "Validation: Epoch [5], Batch [808/938], Loss: 0.5762394666671753\n",
      "Validation: Epoch [5], Batch [809/938], Loss: 0.7882649898529053\n",
      "Validation: Epoch [5], Batch [810/938], Loss: 0.5674050450325012\n",
      "Validation: Epoch [5], Batch [811/938], Loss: 0.9345222115516663\n",
      "Validation: Epoch [5], Batch [812/938], Loss: 0.6301673650741577\n",
      "Validation: Epoch [5], Batch [813/938], Loss: 0.6411910057067871\n",
      "Validation: Epoch [5], Batch [814/938], Loss: 0.7556218504905701\n",
      "Validation: Epoch [5], Batch [815/938], Loss: 0.5807276964187622\n",
      "Validation: Epoch [5], Batch [816/938], Loss: 0.5907394886016846\n",
      "Validation: Epoch [5], Batch [817/938], Loss: 0.5469241142272949\n",
      "Validation: Epoch [5], Batch [818/938], Loss: 0.6383311748504639\n",
      "Validation: Epoch [5], Batch [819/938], Loss: 0.4732992649078369\n",
      "Validation: Epoch [5], Batch [820/938], Loss: 0.6034329533576965\n",
      "Validation: Epoch [5], Batch [821/938], Loss: 0.6154046058654785\n",
      "Validation: Epoch [5], Batch [822/938], Loss: 0.7536525726318359\n",
      "Validation: Epoch [5], Batch [823/938], Loss: 0.6924440860748291\n",
      "Validation: Epoch [5], Batch [824/938], Loss: 0.6451399326324463\n",
      "Validation: Epoch [5], Batch [825/938], Loss: 0.5319200158119202\n",
      "Validation: Epoch [5], Batch [826/938], Loss: 0.8467994332313538\n",
      "Validation: Epoch [5], Batch [827/938], Loss: 0.7076938152313232\n",
      "Validation: Epoch [5], Batch [828/938], Loss: 0.6144542694091797\n",
      "Validation: Epoch [5], Batch [829/938], Loss: 0.6111721992492676\n",
      "Validation: Epoch [5], Batch [830/938], Loss: 0.6884312629699707\n",
      "Validation: Epoch [5], Batch [831/938], Loss: 0.7437680959701538\n",
      "Validation: Epoch [5], Batch [832/938], Loss: 0.7959297299385071\n",
      "Validation: Epoch [5], Batch [833/938], Loss: 0.9000447988510132\n",
      "Validation: Epoch [5], Batch [834/938], Loss: 0.821460485458374\n",
      "Validation: Epoch [5], Batch [835/938], Loss: 0.6200470924377441\n",
      "Validation: Epoch [5], Batch [836/938], Loss: 0.6906328797340393\n",
      "Validation: Epoch [5], Batch [837/938], Loss: 0.7930662631988525\n",
      "Validation: Epoch [5], Batch [838/938], Loss: 0.6621240377426147\n",
      "Validation: Epoch [5], Batch [839/938], Loss: 0.7171484231948853\n",
      "Validation: Epoch [5], Batch [840/938], Loss: 0.6316035985946655\n",
      "Validation: Epoch [5], Batch [841/938], Loss: 0.700657844543457\n",
      "Validation: Epoch [5], Batch [842/938], Loss: 1.111257553100586\n",
      "Validation: Epoch [5], Batch [843/938], Loss: 0.5556946992874146\n",
      "Validation: Epoch [5], Batch [844/938], Loss: 0.500798225402832\n",
      "Validation: Epoch [5], Batch [845/938], Loss: 0.5891664624214172\n",
      "Validation: Epoch [5], Batch [846/938], Loss: 0.557309091091156\n",
      "Validation: Epoch [5], Batch [847/938], Loss: 0.6928455829620361\n",
      "Validation: Epoch [5], Batch [848/938], Loss: 0.6089972853660583\n",
      "Validation: Epoch [5], Batch [849/938], Loss: 0.7645626068115234\n",
      "Validation: Epoch [5], Batch [850/938], Loss: 0.5126765966415405\n",
      "Validation: Epoch [5], Batch [851/938], Loss: 0.6792144179344177\n",
      "Validation: Epoch [5], Batch [852/938], Loss: 0.6997517347335815\n",
      "Validation: Epoch [5], Batch [853/938], Loss: 0.6108761429786682\n",
      "Validation: Epoch [5], Batch [854/938], Loss: 0.5929898619651794\n",
      "Validation: Epoch [5], Batch [855/938], Loss: 0.7400204539299011\n",
      "Validation: Epoch [5], Batch [856/938], Loss: 0.841912031173706\n",
      "Validation: Epoch [5], Batch [857/938], Loss: 0.582686185836792\n",
      "Validation: Epoch [5], Batch [858/938], Loss: 0.7320733070373535\n",
      "Validation: Epoch [5], Batch [859/938], Loss: 0.8310379981994629\n",
      "Validation: Epoch [5], Batch [860/938], Loss: 0.44194307923316956\n",
      "Validation: Epoch [5], Batch [861/938], Loss: 0.7123550176620483\n",
      "Validation: Epoch [5], Batch [862/938], Loss: 0.595360279083252\n",
      "Validation: Epoch [5], Batch [863/938], Loss: 0.8462266325950623\n",
      "Validation: Epoch [5], Batch [864/938], Loss: 0.6157859563827515\n",
      "Validation: Epoch [5], Batch [865/938], Loss: 0.6477585434913635\n",
      "Validation: Epoch [5], Batch [866/938], Loss: 0.6581135988235474\n",
      "Validation: Epoch [5], Batch [867/938], Loss: 0.4629102647304535\n",
      "Validation: Epoch [5], Batch [868/938], Loss: 0.589058518409729\n",
      "Validation: Epoch [5], Batch [869/938], Loss: 0.6353771686553955\n",
      "Validation: Epoch [5], Batch [870/938], Loss: 0.4885144829750061\n",
      "Validation: Epoch [5], Batch [871/938], Loss: 0.822696328163147\n",
      "Validation: Epoch [5], Batch [872/938], Loss: 0.645244300365448\n",
      "Validation: Epoch [5], Batch [873/938], Loss: 0.6302979588508606\n",
      "Validation: Epoch [5], Batch [874/938], Loss: 0.7480425834655762\n",
      "Validation: Epoch [5], Batch [875/938], Loss: 0.8854921460151672\n",
      "Validation: Epoch [5], Batch [876/938], Loss: 0.8053345680236816\n",
      "Validation: Epoch [5], Batch [877/938], Loss: 0.5716062784194946\n",
      "Validation: Epoch [5], Batch [878/938], Loss: 0.5691007375717163\n",
      "Validation: Epoch [5], Batch [879/938], Loss: 0.6912789940834045\n",
      "Validation: Epoch [5], Batch [880/938], Loss: 0.8424822688102722\n",
      "Validation: Epoch [5], Batch [881/938], Loss: 0.7457732558250427\n",
      "Validation: Epoch [5], Batch [882/938], Loss: 0.626514732837677\n",
      "Validation: Epoch [5], Batch [883/938], Loss: 0.5374712347984314\n",
      "Validation: Epoch [5], Batch [884/938], Loss: 0.5858672857284546\n",
      "Validation: Epoch [5], Batch [885/938], Loss: 0.7837098836898804\n",
      "Validation: Epoch [5], Batch [886/938], Loss: 0.6883992552757263\n",
      "Validation: Epoch [5], Batch [887/938], Loss: 0.8268475532531738\n",
      "Validation: Epoch [5], Batch [888/938], Loss: 0.5949668288230896\n",
      "Validation: Epoch [5], Batch [889/938], Loss: 0.7293695211410522\n",
      "Validation: Epoch [5], Batch [890/938], Loss: 0.5254611968994141\n",
      "Validation: Epoch [5], Batch [891/938], Loss: 0.6389690637588501\n",
      "Validation: Epoch [5], Batch [892/938], Loss: 0.6451799869537354\n",
      "Validation: Epoch [5], Batch [893/938], Loss: 0.5721466541290283\n",
      "Validation: Epoch [5], Batch [894/938], Loss: 0.6958367824554443\n",
      "Validation: Epoch [5], Batch [895/938], Loss: 0.7942477464675903\n",
      "Validation: Epoch [5], Batch [896/938], Loss: 0.48264506459236145\n",
      "Validation: Epoch [5], Batch [897/938], Loss: 0.7532743811607361\n",
      "Validation: Epoch [5], Batch [898/938], Loss: 0.5809334516525269\n",
      "Validation: Epoch [5], Batch [899/938], Loss: 0.6317651271820068\n",
      "Validation: Epoch [5], Batch [900/938], Loss: 0.759551465511322\n",
      "Validation: Epoch [5], Batch [901/938], Loss: 0.5726784467697144\n",
      "Validation: Epoch [5], Batch [902/938], Loss: 0.657282829284668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [5], Batch [903/938], Loss: 0.625543475151062\n",
      "Validation: Epoch [5], Batch [904/938], Loss: 0.646601676940918\n",
      "Validation: Epoch [5], Batch [905/938], Loss: 0.6935775876045227\n",
      "Validation: Epoch [5], Batch [906/938], Loss: 0.8327669501304626\n",
      "Validation: Epoch [5], Batch [907/938], Loss: 0.6951712965965271\n",
      "Validation: Epoch [5], Batch [908/938], Loss: 0.76026451587677\n",
      "Validation: Epoch [5], Batch [909/938], Loss: 0.5286659002304077\n",
      "Validation: Epoch [5], Batch [910/938], Loss: 0.529381513595581\n",
      "Validation: Epoch [5], Batch [911/938], Loss: 0.8360520005226135\n",
      "Validation: Epoch [5], Batch [912/938], Loss: 0.6815697550773621\n",
      "Validation: Epoch [5], Batch [913/938], Loss: 0.5127271413803101\n",
      "Validation: Epoch [5], Batch [914/938], Loss: 0.6309149861335754\n",
      "Validation: Epoch [5], Batch [915/938], Loss: 0.8910112380981445\n",
      "Validation: Epoch [5], Batch [916/938], Loss: 0.7445433139801025\n",
      "Validation: Epoch [5], Batch [917/938], Loss: 0.5772374868392944\n",
      "Validation: Epoch [5], Batch [918/938], Loss: 0.5550786256790161\n",
      "Validation: Epoch [5], Batch [919/938], Loss: 0.5682189464569092\n",
      "Validation: Epoch [5], Batch [920/938], Loss: 0.6231905817985535\n",
      "Validation: Epoch [5], Batch [921/938], Loss: 0.6337111592292786\n",
      "Validation: Epoch [5], Batch [922/938], Loss: 0.5353002548217773\n",
      "Validation: Epoch [5], Batch [923/938], Loss: 0.5261116027832031\n",
      "Validation: Epoch [5], Batch [924/938], Loss: 0.7635284066200256\n",
      "Validation: Epoch [5], Batch [925/938], Loss: 0.5618318319320679\n",
      "Validation: Epoch [5], Batch [926/938], Loss: 0.5863929986953735\n",
      "Validation: Epoch [5], Batch [927/938], Loss: 0.4305914342403412\n",
      "Validation: Epoch [5], Batch [928/938], Loss: 0.726121187210083\n",
      "Validation: Epoch [5], Batch [929/938], Loss: 0.5360792875289917\n",
      "Validation: Epoch [5], Batch [930/938], Loss: 0.6218973994255066\n",
      "Validation: Epoch [5], Batch [931/938], Loss: 0.7110871076583862\n",
      "Validation: Epoch [5], Batch [932/938], Loss: 0.6230395436286926\n",
      "Validation: Epoch [5], Batch [933/938], Loss: 1.1116819381713867\n",
      "Validation: Epoch [5], Batch [934/938], Loss: 0.6747846603393555\n",
      "Validation: Epoch [5], Batch [935/938], Loss: 0.6023582816123962\n",
      "Validation: Epoch [5], Batch [936/938], Loss: 0.4722873270511627\n",
      "Validation: Epoch [5], Batch [937/938], Loss: 0.5009703040122986\n",
      "Validation: Epoch [5], Batch [938/938], Loss: 0.7009531259536743\n",
      "Accuracy of test set: 0.7415833333333334\n",
      "Train: Epoch [6], Batch [1/938], Loss: 0.674026370048523\n",
      "Train: Epoch [6], Batch [2/938], Loss: 0.5601675510406494\n",
      "Train: Epoch [6], Batch [3/938], Loss: 0.7747741341590881\n",
      "Train: Epoch [6], Batch [4/938], Loss: 0.5894293785095215\n",
      "Train: Epoch [6], Batch [5/938], Loss: 0.5961339473724365\n",
      "Train: Epoch [6], Batch [6/938], Loss: 0.6678417921066284\n",
      "Train: Epoch [6], Batch [7/938], Loss: 0.7658340334892273\n",
      "Train: Epoch [6], Batch [8/938], Loss: 0.830376386642456\n",
      "Train: Epoch [6], Batch [9/938], Loss: 0.596051037311554\n",
      "Train: Epoch [6], Batch [10/938], Loss: 0.5706167221069336\n",
      "Train: Epoch [6], Batch [11/938], Loss: 0.7515621185302734\n",
      "Train: Epoch [6], Batch [12/938], Loss: 0.6899171471595764\n",
      "Train: Epoch [6], Batch [13/938], Loss: 0.6669341921806335\n",
      "Train: Epoch [6], Batch [14/938], Loss: 0.5389336347579956\n",
      "Train: Epoch [6], Batch [15/938], Loss: 0.7523947954177856\n",
      "Train: Epoch [6], Batch [16/938], Loss: 0.6300696134567261\n",
      "Train: Epoch [6], Batch [17/938], Loss: 0.7014894485473633\n",
      "Train: Epoch [6], Batch [18/938], Loss: 0.6596457958221436\n",
      "Train: Epoch [6], Batch [19/938], Loss: 0.7040655612945557\n",
      "Train: Epoch [6], Batch [20/938], Loss: 0.6846760511398315\n",
      "Train: Epoch [6], Batch [21/938], Loss: 0.6212805509567261\n",
      "Train: Epoch [6], Batch [22/938], Loss: 0.5256839990615845\n",
      "Train: Epoch [6], Batch [23/938], Loss: 0.660362958908081\n",
      "Train: Epoch [6], Batch [24/938], Loss: 0.6608929634094238\n",
      "Train: Epoch [6], Batch [25/938], Loss: 0.5358890295028687\n",
      "Train: Epoch [6], Batch [26/938], Loss: 0.6705940365791321\n",
      "Train: Epoch [6], Batch [27/938], Loss: 0.6936599612236023\n",
      "Train: Epoch [6], Batch [28/938], Loss: 0.7280234098434448\n",
      "Train: Epoch [6], Batch [29/938], Loss: 0.8476132154464722\n",
      "Train: Epoch [6], Batch [30/938], Loss: 0.6987495422363281\n",
      "Train: Epoch [6], Batch [31/938], Loss: 0.6878070831298828\n",
      "Train: Epoch [6], Batch [32/938], Loss: 0.555891752243042\n",
      "Train: Epoch [6], Batch [33/938], Loss: 0.5070468187332153\n",
      "Train: Epoch [6], Batch [34/938], Loss: 0.7395256161689758\n",
      "Train: Epoch [6], Batch [35/938], Loss: 0.6153316497802734\n",
      "Train: Epoch [6], Batch [36/938], Loss: 0.7189795970916748\n",
      "Train: Epoch [6], Batch [37/938], Loss: 0.6689286231994629\n",
      "Train: Epoch [6], Batch [38/938], Loss: 0.5126698017120361\n",
      "Train: Epoch [6], Batch [39/938], Loss: 0.5161882638931274\n",
      "Train: Epoch [6], Batch [40/938], Loss: 0.5886355638504028\n",
      "Train: Epoch [6], Batch [41/938], Loss: 0.5842432379722595\n",
      "Train: Epoch [6], Batch [42/938], Loss: 0.9103837013244629\n",
      "Train: Epoch [6], Batch [43/938], Loss: 0.6587978005409241\n",
      "Train: Epoch [6], Batch [44/938], Loss: 0.9143021702766418\n",
      "Train: Epoch [6], Batch [45/938], Loss: 0.8480890989303589\n",
      "Train: Epoch [6], Batch [46/938], Loss: 0.5561642646789551\n",
      "Train: Epoch [6], Batch [47/938], Loss: 0.6266951560974121\n",
      "Train: Epoch [6], Batch [48/938], Loss: 0.6268852949142456\n",
      "Train: Epoch [6], Batch [49/938], Loss: 0.7950091361999512\n",
      "Train: Epoch [6], Batch [50/938], Loss: 0.663588285446167\n",
      "Train: Epoch [6], Batch [51/938], Loss: 0.6444263458251953\n",
      "Train: Epoch [6], Batch [52/938], Loss: 0.5755521059036255\n",
      "Train: Epoch [6], Batch [53/938], Loss: 0.7000441551208496\n",
      "Train: Epoch [6], Batch [54/938], Loss: 0.6432645916938782\n",
      "Train: Epoch [6], Batch [55/938], Loss: 0.6957611441612244\n",
      "Train: Epoch [6], Batch [56/938], Loss: 0.46521130204200745\n",
      "Train: Epoch [6], Batch [57/938], Loss: 0.8881237506866455\n",
      "Train: Epoch [6], Batch [58/938], Loss: 0.4641838073730469\n",
      "Train: Epoch [6], Batch [59/938], Loss: 0.6533937454223633\n",
      "Train: Epoch [6], Batch [60/938], Loss: 0.6405887603759766\n",
      "Train: Epoch [6], Batch [61/938], Loss: 0.6167148947715759\n",
      "Train: Epoch [6], Batch [62/938], Loss: 0.6255366802215576\n",
      "Train: Epoch [6], Batch [63/938], Loss: 0.6056687831878662\n",
      "Train: Epoch [6], Batch [64/938], Loss: 0.6975328922271729\n",
      "Train: Epoch [6], Batch [65/938], Loss: 0.5710066556930542\n",
      "Train: Epoch [6], Batch [66/938], Loss: 0.5874587893486023\n",
      "Train: Epoch [6], Batch [67/938], Loss: 0.638332188129425\n",
      "Train: Epoch [6], Batch [68/938], Loss: 0.8073155879974365\n",
      "Train: Epoch [6], Batch [69/938], Loss: 0.5336255431175232\n",
      "Train: Epoch [6], Batch [70/938], Loss: 0.8182455897331238\n",
      "Train: Epoch [6], Batch [71/938], Loss: 0.846859335899353\n",
      "Train: Epoch [6], Batch [72/938], Loss: 0.5598336458206177\n",
      "Train: Epoch [6], Batch [73/938], Loss: 0.8536316752433777\n",
      "Train: Epoch [6], Batch [74/938], Loss: 0.5437450408935547\n",
      "Train: Epoch [6], Batch [75/938], Loss: 0.6862995028495789\n",
      "Train: Epoch [6], Batch [76/938], Loss: 0.9146278500556946\n",
      "Train: Epoch [6], Batch [77/938], Loss: 0.5364220142364502\n",
      "Train: Epoch [6], Batch [78/938], Loss: 0.9303107261657715\n",
      "Train: Epoch [6], Batch [79/938], Loss: 0.7671792507171631\n",
      "Train: Epoch [6], Batch [80/938], Loss: 0.6369895339012146\n",
      "Train: Epoch [6], Batch [81/938], Loss: 0.6571781039237976\n",
      "Train: Epoch [6], Batch [82/938], Loss: 0.7853790521621704\n",
      "Train: Epoch [6], Batch [83/938], Loss: 0.6469587087631226\n",
      "Train: Epoch [6], Batch [84/938], Loss: 0.4955199658870697\n",
      "Train: Epoch [6], Batch [85/938], Loss: 0.6192666888237\n",
      "Train: Epoch [6], Batch [86/938], Loss: 0.6821380853652954\n",
      "Train: Epoch [6], Batch [87/938], Loss: 0.7113777995109558\n",
      "Train: Epoch [6], Batch [88/938], Loss: 0.7940728664398193\n",
      "Train: Epoch [6], Batch [89/938], Loss: 0.6443173885345459\n",
      "Train: Epoch [6], Batch [90/938], Loss: 0.7622252702713013\n",
      "Train: Epoch [6], Batch [91/938], Loss: 0.7696181535720825\n",
      "Train: Epoch [6], Batch [92/938], Loss: 0.7927546501159668\n",
      "Train: Epoch [6], Batch [93/938], Loss: 0.5291544198989868\n",
      "Train: Epoch [6], Batch [94/938], Loss: 0.4705545902252197\n",
      "Train: Epoch [6], Batch [95/938], Loss: 0.6827768087387085\n",
      "Train: Epoch [6], Batch [96/938], Loss: 0.9385530948638916\n",
      "Train: Epoch [6], Batch [97/938], Loss: 0.62420254945755\n",
      "Train: Epoch [6], Batch [98/938], Loss: 0.7168424725532532\n",
      "Train: Epoch [6], Batch [99/938], Loss: 0.7635488510131836\n",
      "Train: Epoch [6], Batch [100/938], Loss: 0.4824569523334503\n",
      "Train: Epoch [6], Batch [101/938], Loss: 0.7639977335929871\n",
      "Train: Epoch [6], Batch [102/938], Loss: 0.6181941628456116\n",
      "Train: Epoch [6], Batch [103/938], Loss: 0.7504843473434448\n",
      "Train: Epoch [6], Batch [104/938], Loss: 0.6111832857131958\n",
      "Train: Epoch [6], Batch [105/938], Loss: 0.5642461776733398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [6], Batch [106/938], Loss: 0.5819178819656372\n",
      "Train: Epoch [6], Batch [107/938], Loss: 0.6043179035186768\n",
      "Train: Epoch [6], Batch [108/938], Loss: 0.8079788684844971\n",
      "Train: Epoch [6], Batch [109/938], Loss: 0.6121023893356323\n",
      "Train: Epoch [6], Batch [110/938], Loss: 0.6192654371261597\n",
      "Train: Epoch [6], Batch [111/938], Loss: 0.703907310962677\n",
      "Train: Epoch [6], Batch [112/938], Loss: 0.5873750448226929\n",
      "Train: Epoch [6], Batch [113/938], Loss: 0.5610899925231934\n",
      "Train: Epoch [6], Batch [114/938], Loss: 0.7900257110595703\n",
      "Train: Epoch [6], Batch [115/938], Loss: 0.5639965534210205\n",
      "Train: Epoch [6], Batch [116/938], Loss: 0.8724827766418457\n",
      "Train: Epoch [6], Batch [117/938], Loss: 0.5692456960678101\n",
      "Train: Epoch [6], Batch [118/938], Loss: 0.6660610437393188\n",
      "Train: Epoch [6], Batch [119/938], Loss: 0.5312283039093018\n",
      "Train: Epoch [6], Batch [120/938], Loss: 0.8327752351760864\n",
      "Train: Epoch [6], Batch [121/938], Loss: 0.6570305228233337\n",
      "Train: Epoch [6], Batch [122/938], Loss: 0.7352423071861267\n",
      "Train: Epoch [6], Batch [123/938], Loss: 0.7016094326972961\n",
      "Train: Epoch [6], Batch [124/938], Loss: 0.9108402729034424\n",
      "Train: Epoch [6], Batch [125/938], Loss: 0.7125461101531982\n",
      "Train: Epoch [6], Batch [126/938], Loss: 0.5651397705078125\n",
      "Train: Epoch [6], Batch [127/938], Loss: 0.3800359070301056\n",
      "Train: Epoch [6], Batch [128/938], Loss: 0.5576491355895996\n",
      "Train: Epoch [6], Batch [129/938], Loss: 0.6029049158096313\n",
      "Train: Epoch [6], Batch [130/938], Loss: 0.5242921710014343\n",
      "Train: Epoch [6], Batch [131/938], Loss: 0.657561182975769\n",
      "Train: Epoch [6], Batch [132/938], Loss: 0.6828751564025879\n",
      "Train: Epoch [6], Batch [133/938], Loss: 0.5635913014411926\n",
      "Train: Epoch [6], Batch [134/938], Loss: 0.7285214066505432\n",
      "Train: Epoch [6], Batch [135/938], Loss: 0.8067820072174072\n",
      "Train: Epoch [6], Batch [136/938], Loss: 0.673922061920166\n",
      "Train: Epoch [6], Batch [137/938], Loss: 0.654879093170166\n",
      "Train: Epoch [6], Batch [138/938], Loss: 0.5182948112487793\n",
      "Train: Epoch [6], Batch [139/938], Loss: 0.5602262020111084\n",
      "Train: Epoch [6], Batch [140/938], Loss: 0.6558841466903687\n",
      "Train: Epoch [6], Batch [141/938], Loss: 0.6391792893409729\n",
      "Train: Epoch [6], Batch [142/938], Loss: 0.6966139078140259\n",
      "Train: Epoch [6], Batch [143/938], Loss: 0.600435733795166\n",
      "Train: Epoch [6], Batch [144/938], Loss: 0.5865334272384644\n",
      "Train: Epoch [6], Batch [145/938], Loss: 0.5968080163002014\n",
      "Train: Epoch [6], Batch [146/938], Loss: 0.6676175594329834\n",
      "Train: Epoch [6], Batch [147/938], Loss: 0.6494600176811218\n",
      "Train: Epoch [6], Batch [148/938], Loss: 0.653541624546051\n",
      "Train: Epoch [6], Batch [149/938], Loss: 0.5016160011291504\n",
      "Train: Epoch [6], Batch [150/938], Loss: 0.6744576096534729\n",
      "Train: Epoch [6], Batch [151/938], Loss: 0.8748617172241211\n",
      "Train: Epoch [6], Batch [152/938], Loss: 0.7667490243911743\n",
      "Train: Epoch [6], Batch [153/938], Loss: 0.5245912075042725\n",
      "Train: Epoch [6], Batch [154/938], Loss: 0.5813547968864441\n",
      "Train: Epoch [6], Batch [155/938], Loss: 0.6826419830322266\n",
      "Train: Epoch [6], Batch [156/938], Loss: 0.5756490230560303\n",
      "Train: Epoch [6], Batch [157/938], Loss: 0.5604819655418396\n",
      "Train: Epoch [6], Batch [158/938], Loss: 0.5376176834106445\n",
      "Train: Epoch [6], Batch [159/938], Loss: 0.6853389739990234\n",
      "Train: Epoch [6], Batch [160/938], Loss: 0.6545530557632446\n",
      "Train: Epoch [6], Batch [161/938], Loss: 0.6779002547264099\n",
      "Train: Epoch [6], Batch [162/938], Loss: 0.6104179620742798\n",
      "Train: Epoch [6], Batch [163/938], Loss: 0.5365322828292847\n",
      "Train: Epoch [6], Batch [164/938], Loss: 0.955985426902771\n",
      "Train: Epoch [6], Batch [165/938], Loss: 0.5805155038833618\n",
      "Train: Epoch [6], Batch [166/938], Loss: 0.5572717189788818\n",
      "Train: Epoch [6], Batch [167/938], Loss: 0.5883657932281494\n",
      "Train: Epoch [6], Batch [168/938], Loss: 0.6607570052146912\n",
      "Train: Epoch [6], Batch [169/938], Loss: 0.5428178310394287\n",
      "Train: Epoch [6], Batch [170/938], Loss: 0.6406978368759155\n",
      "Train: Epoch [6], Batch [171/938], Loss: 0.5804499387741089\n",
      "Train: Epoch [6], Batch [172/938], Loss: 0.7934998869895935\n",
      "Train: Epoch [6], Batch [173/938], Loss: 0.6668832898139954\n",
      "Train: Epoch [6], Batch [174/938], Loss: 0.6431069374084473\n",
      "Train: Epoch [6], Batch [175/938], Loss: 0.7316850423812866\n",
      "Train: Epoch [6], Batch [176/938], Loss: 0.5747395753860474\n",
      "Train: Epoch [6], Batch [177/938], Loss: 0.7934335470199585\n",
      "Train: Epoch [6], Batch [178/938], Loss: 0.6270917654037476\n",
      "Train: Epoch [6], Batch [179/938], Loss: 0.7048280239105225\n",
      "Train: Epoch [6], Batch [180/938], Loss: 0.5848591327667236\n",
      "Train: Epoch [6], Batch [181/938], Loss: 0.5827733278274536\n",
      "Train: Epoch [6], Batch [182/938], Loss: 0.4709378480911255\n",
      "Train: Epoch [6], Batch [183/938], Loss: 0.7305159568786621\n",
      "Train: Epoch [6], Batch [184/938], Loss: 0.5715320706367493\n",
      "Train: Epoch [6], Batch [185/938], Loss: 0.5380355715751648\n",
      "Train: Epoch [6], Batch [186/938], Loss: 0.639824628829956\n",
      "Train: Epoch [6], Batch [187/938], Loss: 0.9177785515785217\n",
      "Train: Epoch [6], Batch [188/938], Loss: 0.5792339444160461\n",
      "Train: Epoch [6], Batch [189/938], Loss: 0.7794082164764404\n",
      "Train: Epoch [6], Batch [190/938], Loss: 0.44064176082611084\n",
      "Train: Epoch [6], Batch [191/938], Loss: 0.6363927125930786\n",
      "Train: Epoch [6], Batch [192/938], Loss: 0.7087535858154297\n",
      "Train: Epoch [6], Batch [193/938], Loss: 0.6379337310791016\n",
      "Train: Epoch [6], Batch [194/938], Loss: 0.8308173418045044\n",
      "Train: Epoch [6], Batch [195/938], Loss: 0.7029248476028442\n",
      "Train: Epoch [6], Batch [196/938], Loss: 0.6974520683288574\n",
      "Train: Epoch [6], Batch [197/938], Loss: 0.5686557292938232\n",
      "Train: Epoch [6], Batch [198/938], Loss: 0.5011576414108276\n",
      "Train: Epoch [6], Batch [199/938], Loss: 0.6920626163482666\n",
      "Train: Epoch [6], Batch [200/938], Loss: 0.6510471105575562\n",
      "Train: Epoch [6], Batch [201/938], Loss: 0.6888013482093811\n",
      "Train: Epoch [6], Batch [202/938], Loss: 0.6932060718536377\n",
      "Train: Epoch [6], Batch [203/938], Loss: 0.49247193336486816\n",
      "Train: Epoch [6], Batch [204/938], Loss: 0.8361602425575256\n",
      "Train: Epoch [6], Batch [205/938], Loss: 0.5645349025726318\n",
      "Train: Epoch [6], Batch [206/938], Loss: 0.5917600989341736\n",
      "Train: Epoch [6], Batch [207/938], Loss: 0.6354796290397644\n",
      "Train: Epoch [6], Batch [208/938], Loss: 0.6116498708724976\n",
      "Train: Epoch [6], Batch [209/938], Loss: 0.6171700358390808\n",
      "Train: Epoch [6], Batch [210/938], Loss: 0.5687845945358276\n",
      "Train: Epoch [6], Batch [211/938], Loss: 0.7107608318328857\n",
      "Train: Epoch [6], Batch [212/938], Loss: 0.7520579099655151\n",
      "Train: Epoch [6], Batch [213/938], Loss: 0.6751999855041504\n",
      "Train: Epoch [6], Batch [214/938], Loss: 0.4996519088745117\n",
      "Train: Epoch [6], Batch [215/938], Loss: 0.4786064028739929\n",
      "Train: Epoch [6], Batch [216/938], Loss: 0.6984591484069824\n",
      "Train: Epoch [6], Batch [217/938], Loss: 0.6866210699081421\n",
      "Train: Epoch [6], Batch [218/938], Loss: 0.7027735710144043\n",
      "Train: Epoch [6], Batch [219/938], Loss: 0.6256430149078369\n",
      "Train: Epoch [6], Batch [220/938], Loss: 0.7168236970901489\n",
      "Train: Epoch [6], Batch [221/938], Loss: 0.638312578201294\n",
      "Train: Epoch [6], Batch [222/938], Loss: 0.697688102722168\n",
      "Train: Epoch [6], Batch [223/938], Loss: 0.832766592502594\n",
      "Train: Epoch [6], Batch [224/938], Loss: 0.5509291887283325\n",
      "Train: Epoch [6], Batch [225/938], Loss: 0.7010285258293152\n",
      "Train: Epoch [6], Batch [226/938], Loss: 0.6286812424659729\n",
      "Train: Epoch [6], Batch [227/938], Loss: 0.6503574848175049\n",
      "Train: Epoch [6], Batch [228/938], Loss: 0.8022298216819763\n",
      "Train: Epoch [6], Batch [229/938], Loss: 0.5957993268966675\n",
      "Train: Epoch [6], Batch [230/938], Loss: 0.4876721203327179\n",
      "Train: Epoch [6], Batch [231/938], Loss: 0.6838276982307434\n",
      "Train: Epoch [6], Batch [232/938], Loss: 0.6876010298728943\n",
      "Train: Epoch [6], Batch [233/938], Loss: 0.5331668257713318\n",
      "Train: Epoch [6], Batch [234/938], Loss: 0.7717735171318054\n",
      "Train: Epoch [6], Batch [235/938], Loss: 0.7513278126716614\n",
      "Train: Epoch [6], Batch [236/938], Loss: 0.8077106475830078\n",
      "Train: Epoch [6], Batch [237/938], Loss: 0.6998445391654968\n",
      "Train: Epoch [6], Batch [238/938], Loss: 0.7170655727386475\n",
      "Train: Epoch [6], Batch [239/938], Loss: 0.5527445077896118\n",
      "Train: Epoch [6], Batch [240/938], Loss: 0.629565954208374\n",
      "Train: Epoch [6], Batch [241/938], Loss: 0.6633489727973938\n",
      "Train: Epoch [6], Batch [242/938], Loss: 0.712757408618927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [6], Batch [243/938], Loss: 0.5751044154167175\n",
      "Train: Epoch [6], Batch [244/938], Loss: 0.7265255451202393\n",
      "Train: Epoch [6], Batch [245/938], Loss: 0.7936888933181763\n",
      "Train: Epoch [6], Batch [246/938], Loss: 0.5426929593086243\n",
      "Train: Epoch [6], Batch [247/938], Loss: 0.5355485081672668\n",
      "Train: Epoch [6], Batch [248/938], Loss: 0.6549636125564575\n",
      "Train: Epoch [6], Batch [249/938], Loss: 0.5830516815185547\n",
      "Train: Epoch [6], Batch [250/938], Loss: 0.5861737132072449\n",
      "Train: Epoch [6], Batch [251/938], Loss: 0.8634722828865051\n",
      "Train: Epoch [6], Batch [252/938], Loss: 0.7769052386283875\n",
      "Train: Epoch [6], Batch [253/938], Loss: 0.6967236995697021\n",
      "Train: Epoch [6], Batch [254/938], Loss: 0.7170241475105286\n",
      "Train: Epoch [6], Batch [255/938], Loss: 0.5521087646484375\n",
      "Train: Epoch [6], Batch [256/938], Loss: 0.4839933216571808\n",
      "Train: Epoch [6], Batch [257/938], Loss: 0.708132266998291\n",
      "Train: Epoch [6], Batch [258/938], Loss: 0.6350526809692383\n",
      "Train: Epoch [6], Batch [259/938], Loss: 0.6993623971939087\n",
      "Train: Epoch [6], Batch [260/938], Loss: 0.6503416299819946\n",
      "Train: Epoch [6], Batch [261/938], Loss: 0.5285544395446777\n",
      "Train: Epoch [6], Batch [262/938], Loss: 0.5349176526069641\n",
      "Train: Epoch [6], Batch [263/938], Loss: 0.7398711442947388\n",
      "Train: Epoch [6], Batch [264/938], Loss: 0.7292159795761108\n",
      "Train: Epoch [6], Batch [265/938], Loss: 0.6979702711105347\n",
      "Train: Epoch [6], Batch [266/938], Loss: 0.7061598896980286\n",
      "Train: Epoch [6], Batch [267/938], Loss: 0.5053640604019165\n",
      "Train: Epoch [6], Batch [268/938], Loss: 0.5712059736251831\n",
      "Train: Epoch [6], Batch [269/938], Loss: 0.48410511016845703\n",
      "Train: Epoch [6], Batch [270/938], Loss: 0.7704811692237854\n",
      "Train: Epoch [6], Batch [271/938], Loss: 0.6925027370452881\n",
      "Train: Epoch [6], Batch [272/938], Loss: 0.7275214791297913\n",
      "Train: Epoch [6], Batch [273/938], Loss: 0.9437241554260254\n",
      "Train: Epoch [6], Batch [274/938], Loss: 0.5427124500274658\n",
      "Train: Epoch [6], Batch [275/938], Loss: 0.6010029911994934\n",
      "Train: Epoch [6], Batch [276/938], Loss: 0.49528318643569946\n",
      "Train: Epoch [6], Batch [277/938], Loss: 0.6428096294403076\n",
      "Train: Epoch [6], Batch [278/938], Loss: 0.5213710069656372\n",
      "Train: Epoch [6], Batch [279/938], Loss: 0.6213033199310303\n",
      "Train: Epoch [6], Batch [280/938], Loss: 0.7083605527877808\n",
      "Train: Epoch [6], Batch [281/938], Loss: 0.5471958518028259\n",
      "Train: Epoch [6], Batch [282/938], Loss: 0.6368049383163452\n",
      "Train: Epoch [6], Batch [283/938], Loss: 0.7108094692230225\n",
      "Train: Epoch [6], Batch [284/938], Loss: 0.7100280523300171\n",
      "Train: Epoch [6], Batch [285/938], Loss: 0.5438550114631653\n",
      "Train: Epoch [6], Batch [286/938], Loss: 0.6851998567581177\n",
      "Train: Epoch [6], Batch [287/938], Loss: 0.5100110769271851\n",
      "Train: Epoch [6], Batch [288/938], Loss: 0.6152508854866028\n",
      "Train: Epoch [6], Batch [289/938], Loss: 0.48597022891044617\n",
      "Train: Epoch [6], Batch [290/938], Loss: 0.6460345387458801\n",
      "Train: Epoch [6], Batch [291/938], Loss: 0.5724413990974426\n",
      "Train: Epoch [6], Batch [292/938], Loss: 0.5254061222076416\n",
      "Train: Epoch [6], Batch [293/938], Loss: 0.6332439184188843\n",
      "Train: Epoch [6], Batch [294/938], Loss: 0.47071975469589233\n",
      "Train: Epoch [6], Batch [295/938], Loss: 0.8297801613807678\n",
      "Train: Epoch [6], Batch [296/938], Loss: 0.6316565275192261\n",
      "Train: Epoch [6], Batch [297/938], Loss: 0.5152338743209839\n",
      "Train: Epoch [6], Batch [298/938], Loss: 0.775455892086029\n",
      "Train: Epoch [6], Batch [299/938], Loss: 0.7484760284423828\n",
      "Train: Epoch [6], Batch [300/938], Loss: 0.6696690917015076\n",
      "Train: Epoch [6], Batch [301/938], Loss: 0.4859617054462433\n",
      "Train: Epoch [6], Batch [302/938], Loss: 0.6350055932998657\n",
      "Train: Epoch [6], Batch [303/938], Loss: 0.7020232081413269\n",
      "Train: Epoch [6], Batch [304/938], Loss: 0.7542264461517334\n",
      "Train: Epoch [6], Batch [305/938], Loss: 0.5499755144119263\n",
      "Train: Epoch [6], Batch [306/938], Loss: 0.8612266778945923\n",
      "Train: Epoch [6], Batch [307/938], Loss: 0.6903554201126099\n",
      "Train: Epoch [6], Batch [308/938], Loss: 0.8290420770645142\n",
      "Train: Epoch [6], Batch [309/938], Loss: 0.7641295194625854\n",
      "Train: Epoch [6], Batch [310/938], Loss: 0.7454540729522705\n",
      "Train: Epoch [6], Batch [311/938], Loss: 0.7645162343978882\n",
      "Train: Epoch [6], Batch [312/938], Loss: 0.7481412887573242\n",
      "Train: Epoch [6], Batch [313/938], Loss: 0.5205158591270447\n",
      "Train: Epoch [6], Batch [314/938], Loss: 0.5684785842895508\n",
      "Train: Epoch [6], Batch [315/938], Loss: 0.7415577173233032\n",
      "Train: Epoch [6], Batch [316/938], Loss: 0.657539963722229\n",
      "Train: Epoch [6], Batch [317/938], Loss: 0.6061825752258301\n",
      "Train: Epoch [6], Batch [318/938], Loss: 0.7802261710166931\n",
      "Train: Epoch [6], Batch [319/938], Loss: 0.523155689239502\n",
      "Train: Epoch [6], Batch [320/938], Loss: 0.6836191415786743\n",
      "Train: Epoch [6], Batch [321/938], Loss: 0.7325981855392456\n",
      "Train: Epoch [6], Batch [322/938], Loss: 0.5030269622802734\n",
      "Train: Epoch [6], Batch [323/938], Loss: 0.8175280690193176\n",
      "Train: Epoch [6], Batch [324/938], Loss: 0.8799110651016235\n",
      "Train: Epoch [6], Batch [325/938], Loss: 0.511987566947937\n",
      "Train: Epoch [6], Batch [326/938], Loss: 0.6569646596908569\n",
      "Train: Epoch [6], Batch [327/938], Loss: 0.6688966751098633\n",
      "Train: Epoch [6], Batch [328/938], Loss: 0.6277021169662476\n",
      "Train: Epoch [6], Batch [329/938], Loss: 0.5797048807144165\n",
      "Train: Epoch [6], Batch [330/938], Loss: 0.7245991826057434\n",
      "Train: Epoch [6], Batch [331/938], Loss: 0.7587974071502686\n",
      "Train: Epoch [6], Batch [332/938], Loss: 0.9570263624191284\n",
      "Train: Epoch [6], Batch [333/938], Loss: 0.5653728246688843\n",
      "Train: Epoch [6], Batch [334/938], Loss: 0.6982613801956177\n",
      "Train: Epoch [6], Batch [335/938], Loss: 0.6245631575584412\n",
      "Train: Epoch [6], Batch [336/938], Loss: 0.552842378616333\n",
      "Train: Epoch [6], Batch [337/938], Loss: 0.6014816761016846\n",
      "Train: Epoch [6], Batch [338/938], Loss: 0.5771914720535278\n",
      "Train: Epoch [6], Batch [339/938], Loss: 0.7174365520477295\n",
      "Train: Epoch [6], Batch [340/938], Loss: 0.4983411133289337\n",
      "Train: Epoch [6], Batch [341/938], Loss: 0.6240023970603943\n",
      "Train: Epoch [6], Batch [342/938], Loss: 0.6804429292678833\n",
      "Train: Epoch [6], Batch [343/938], Loss: 0.6846064925193787\n",
      "Train: Epoch [6], Batch [344/938], Loss: 0.62044757604599\n",
      "Train: Epoch [6], Batch [345/938], Loss: 0.5999336242675781\n",
      "Train: Epoch [6], Batch [346/938], Loss: 0.5972366333007812\n",
      "Train: Epoch [6], Batch [347/938], Loss: 0.7940078973770142\n",
      "Train: Epoch [6], Batch [348/938], Loss: 0.5423992872238159\n",
      "Train: Epoch [6], Batch [349/938], Loss: 0.5931491255760193\n",
      "Train: Epoch [6], Batch [350/938], Loss: 0.8211631178855896\n",
      "Train: Epoch [6], Batch [351/938], Loss: 0.9431715607643127\n",
      "Train: Epoch [6], Batch [352/938], Loss: 0.4192584753036499\n",
      "Train: Epoch [6], Batch [353/938], Loss: 0.7875128984451294\n",
      "Train: Epoch [6], Batch [354/938], Loss: 0.6158046126365662\n",
      "Train: Epoch [6], Batch [355/938], Loss: 0.46322083473205566\n",
      "Train: Epoch [6], Batch [356/938], Loss: 0.7335214018821716\n",
      "Train: Epoch [6], Batch [357/938], Loss: 0.7903132438659668\n",
      "Train: Epoch [6], Batch [358/938], Loss: 0.6874428391456604\n",
      "Train: Epoch [6], Batch [359/938], Loss: 0.7229595184326172\n",
      "Train: Epoch [6], Batch [360/938], Loss: 0.549903154373169\n",
      "Train: Epoch [6], Batch [361/938], Loss: 0.6080002784729004\n",
      "Train: Epoch [6], Batch [362/938], Loss: 0.529267430305481\n",
      "Train: Epoch [6], Batch [363/938], Loss: 0.6752805113792419\n",
      "Train: Epoch [6], Batch [364/938], Loss: 0.7570487260818481\n",
      "Train: Epoch [6], Batch [365/938], Loss: 0.7374506592750549\n",
      "Train: Epoch [6], Batch [366/938], Loss: 0.7008030414581299\n",
      "Train: Epoch [6], Batch [367/938], Loss: 0.6136715412139893\n",
      "Train: Epoch [6], Batch [368/938], Loss: 0.5790512561798096\n",
      "Train: Epoch [6], Batch [369/938], Loss: 0.5137168169021606\n",
      "Train: Epoch [6], Batch [370/938], Loss: 0.674068033695221\n",
      "Train: Epoch [6], Batch [371/938], Loss: 0.5015778541564941\n",
      "Train: Epoch [6], Batch [372/938], Loss: 0.6535400748252869\n",
      "Train: Epoch [6], Batch [373/938], Loss: 0.4803573489189148\n",
      "Train: Epoch [6], Batch [374/938], Loss: 0.754453182220459\n",
      "Train: Epoch [6], Batch [375/938], Loss: 0.596865713596344\n",
      "Train: Epoch [6], Batch [376/938], Loss: 0.637691855430603\n",
      "Train: Epoch [6], Batch [377/938], Loss: 0.5867070555686951\n",
      "Train: Epoch [6], Batch [378/938], Loss: 0.6995381116867065\n",
      "Train: Epoch [6], Batch [379/938], Loss: 0.5165876150131226\n",
      "Train: Epoch [6], Batch [380/938], Loss: 0.5162962675094604\n",
      "Train: Epoch [6], Batch [381/938], Loss: 0.6427777409553528\n",
      "Train: Epoch [6], Batch [382/938], Loss: 0.6181000471115112\n",
      "Train: Epoch [6], Batch [383/938], Loss: 0.7115534543991089\n",
      "Train: Epoch [6], Batch [384/938], Loss: 0.6335926055908203\n",
      "Train: Epoch [6], Batch [385/938], Loss: 0.6238545179367065\n",
      "Train: Epoch [6], Batch [386/938], Loss: 0.6351838111877441\n",
      "Train: Epoch [6], Batch [387/938], Loss: 0.5574615001678467\n",
      "Train: Epoch [6], Batch [388/938], Loss: 0.5796282291412354\n",
      "Train: Epoch [6], Batch [389/938], Loss: 0.7687080502510071\n",
      "Train: Epoch [6], Batch [390/938], Loss: 0.6266233921051025\n",
      "Train: Epoch [6], Batch [391/938], Loss: 0.868520975112915\n",
      "Train: Epoch [6], Batch [392/938], Loss: 0.47281986474990845\n",
      "Train: Epoch [6], Batch [393/938], Loss: 0.609548807144165\n",
      "Train: Epoch [6], Batch [394/938], Loss: 0.503309965133667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [6], Batch [395/938], Loss: 0.687332034111023\n",
      "Train: Epoch [6], Batch [396/938], Loss: 0.553068995475769\n",
      "Train: Epoch [6], Batch [397/938], Loss: 0.6955289840698242\n",
      "Train: Epoch [6], Batch [398/938], Loss: 0.8571343421936035\n",
      "Train: Epoch [6], Batch [399/938], Loss: 0.6368081569671631\n",
      "Train: Epoch [6], Batch [400/938], Loss: 0.8756740093231201\n",
      "Train: Epoch [6], Batch [401/938], Loss: 0.6258127093315125\n",
      "Train: Epoch [6], Batch [402/938], Loss: 0.6725538969039917\n",
      "Train: Epoch [6], Batch [403/938], Loss: 0.7034103870391846\n",
      "Train: Epoch [6], Batch [404/938], Loss: 0.5327224135398865\n",
      "Train: Epoch [6], Batch [405/938], Loss: 0.6442305445671082\n",
      "Train: Epoch [6], Batch [406/938], Loss: 0.7844163179397583\n",
      "Train: Epoch [6], Batch [407/938], Loss: 0.6213842630386353\n",
      "Train: Epoch [6], Batch [408/938], Loss: 0.49038028717041016\n",
      "Train: Epoch [6], Batch [409/938], Loss: 0.483991801738739\n",
      "Train: Epoch [6], Batch [410/938], Loss: 0.6074963808059692\n",
      "Train: Epoch [6], Batch [411/938], Loss: 0.6071884036064148\n",
      "Train: Epoch [6], Batch [412/938], Loss: 0.6118936538696289\n",
      "Train: Epoch [6], Batch [413/938], Loss: 0.5301729440689087\n",
      "Train: Epoch [6], Batch [414/938], Loss: 0.6788555383682251\n",
      "Train: Epoch [6], Batch [415/938], Loss: 0.6253049373626709\n",
      "Train: Epoch [6], Batch [416/938], Loss: 0.5643827319145203\n",
      "Train: Epoch [6], Batch [417/938], Loss: 0.6393829584121704\n",
      "Train: Epoch [6], Batch [418/938], Loss: 0.5664732456207275\n",
      "Train: Epoch [6], Batch [419/938], Loss: 0.6636387705802917\n",
      "Train: Epoch [6], Batch [420/938], Loss: 0.5910055041313171\n",
      "Train: Epoch [6], Batch [421/938], Loss: 0.6723302006721497\n",
      "Train: Epoch [6], Batch [422/938], Loss: 0.6439053416252136\n",
      "Train: Epoch [6], Batch [423/938], Loss: 0.6071388721466064\n",
      "Train: Epoch [6], Batch [424/938], Loss: 0.5465141534805298\n",
      "Train: Epoch [6], Batch [425/938], Loss: 0.8170449733734131\n",
      "Train: Epoch [6], Batch [426/938], Loss: 0.6932456493377686\n",
      "Train: Epoch [6], Batch [427/938], Loss: 0.6606476306915283\n",
      "Train: Epoch [6], Batch [428/938], Loss: 0.45238637924194336\n",
      "Train: Epoch [6], Batch [429/938], Loss: 0.5836334228515625\n",
      "Train: Epoch [6], Batch [430/938], Loss: 0.6050332188606262\n",
      "Train: Epoch [6], Batch [431/938], Loss: 0.8969341516494751\n",
      "Train: Epoch [6], Batch [432/938], Loss: 0.615742564201355\n",
      "Train: Epoch [6], Batch [433/938], Loss: 0.5012028217315674\n",
      "Train: Epoch [6], Batch [434/938], Loss: 0.74742192029953\n",
      "Train: Epoch [6], Batch [435/938], Loss: 0.6969277262687683\n",
      "Train: Epoch [6], Batch [436/938], Loss: 0.5862325429916382\n",
      "Train: Epoch [6], Batch [437/938], Loss: 0.6622114777565002\n",
      "Train: Epoch [6], Batch [438/938], Loss: 0.5750427842140198\n",
      "Train: Epoch [6], Batch [439/938], Loss: 0.7520556449890137\n",
      "Train: Epoch [6], Batch [440/938], Loss: 0.6607929468154907\n",
      "Train: Epoch [6], Batch [441/938], Loss: 0.5187104940414429\n",
      "Train: Epoch [6], Batch [442/938], Loss: 0.603856086730957\n",
      "Train: Epoch [6], Batch [443/938], Loss: 0.47932854294776917\n",
      "Train: Epoch [6], Batch [444/938], Loss: 0.47267046570777893\n",
      "Train: Epoch [6], Batch [445/938], Loss: 0.750311017036438\n",
      "Train: Epoch [6], Batch [446/938], Loss: 0.8250688314437866\n",
      "Train: Epoch [6], Batch [447/938], Loss: 0.648744523525238\n",
      "Train: Epoch [6], Batch [448/938], Loss: 0.6639349460601807\n",
      "Train: Epoch [6], Batch [449/938], Loss: 0.5903618335723877\n",
      "Train: Epoch [6], Batch [450/938], Loss: 0.5567864775657654\n",
      "Train: Epoch [6], Batch [451/938], Loss: 0.6962667107582092\n",
      "Train: Epoch [6], Batch [452/938], Loss: 0.5396600961685181\n",
      "Train: Epoch [6], Batch [453/938], Loss: 0.7338308691978455\n",
      "Train: Epoch [6], Batch [454/938], Loss: 0.6430035829544067\n",
      "Train: Epoch [6], Batch [455/938], Loss: 0.6651281118392944\n",
      "Train: Epoch [6], Batch [456/938], Loss: 0.4987872838973999\n",
      "Train: Epoch [6], Batch [457/938], Loss: 0.7529678344726562\n",
      "Train: Epoch [6], Batch [458/938], Loss: 0.6587899327278137\n",
      "Train: Epoch [6], Batch [459/938], Loss: 0.7529204487800598\n",
      "Train: Epoch [6], Batch [460/938], Loss: 0.5526700615882874\n",
      "Train: Epoch [6], Batch [461/938], Loss: 0.5082889795303345\n",
      "Train: Epoch [6], Batch [462/938], Loss: 0.6330757141113281\n",
      "Train: Epoch [6], Batch [463/938], Loss: 0.594079852104187\n",
      "Train: Epoch [6], Batch [464/938], Loss: 0.9006122946739197\n",
      "Train: Epoch [6], Batch [465/938], Loss: 0.5694589614868164\n",
      "Train: Epoch [6], Batch [466/938], Loss: 0.6292296648025513\n",
      "Train: Epoch [6], Batch [467/938], Loss: 0.5450676679611206\n",
      "Train: Epoch [6], Batch [468/938], Loss: 0.5786551237106323\n",
      "Train: Epoch [6], Batch [469/938], Loss: 0.5931997299194336\n",
      "Train: Epoch [6], Batch [470/938], Loss: 0.6429455280303955\n",
      "Train: Epoch [6], Batch [471/938], Loss: 0.7290701270103455\n",
      "Train: Epoch [6], Batch [472/938], Loss: 0.9182631969451904\n",
      "Train: Epoch [6], Batch [473/938], Loss: 0.693845808506012\n",
      "Train: Epoch [6], Batch [474/938], Loss: 0.5129691958427429\n",
      "Train: Epoch [6], Batch [475/938], Loss: 0.7452820539474487\n",
      "Train: Epoch [6], Batch [476/938], Loss: 0.530551552772522\n",
      "Train: Epoch [6], Batch [477/938], Loss: 0.6823428869247437\n",
      "Train: Epoch [6], Batch [478/938], Loss: 0.5299599170684814\n",
      "Train: Epoch [6], Batch [479/938], Loss: 0.5302760601043701\n",
      "Train: Epoch [6], Batch [480/938], Loss: 0.921298623085022\n",
      "Train: Epoch [6], Batch [481/938], Loss: 0.6903089880943298\n",
      "Train: Epoch [6], Batch [482/938], Loss: 0.5087733268737793\n",
      "Train: Epoch [6], Batch [483/938], Loss: 0.6890212297439575\n",
      "Train: Epoch [6], Batch [484/938], Loss: 0.42100515961647034\n",
      "Train: Epoch [6], Batch [485/938], Loss: 0.5700756311416626\n",
      "Train: Epoch [6], Batch [486/938], Loss: 0.6169605255126953\n",
      "Train: Epoch [6], Batch [487/938], Loss: 0.5764726400375366\n",
      "Train: Epoch [6], Batch [488/938], Loss: 0.8684394359588623\n",
      "Train: Epoch [6], Batch [489/938], Loss: 0.792129635810852\n",
      "Train: Epoch [6], Batch [490/938], Loss: 0.5251047611236572\n",
      "Train: Epoch [6], Batch [491/938], Loss: 0.4377681016921997\n",
      "Train: Epoch [6], Batch [492/938], Loss: 0.7493197917938232\n",
      "Train: Epoch [6], Batch [493/938], Loss: 0.5942168235778809\n",
      "Train: Epoch [6], Batch [494/938], Loss: 0.5237311124801636\n",
      "Train: Epoch [6], Batch [495/938], Loss: 0.6286827325820923\n",
      "Train: Epoch [6], Batch [496/938], Loss: 0.8454534411430359\n",
      "Train: Epoch [6], Batch [497/938], Loss: 0.6141624450683594\n",
      "Train: Epoch [6], Batch [498/938], Loss: 0.5583491325378418\n",
      "Train: Epoch [6], Batch [499/938], Loss: 0.7293144464492798\n",
      "Train: Epoch [6], Batch [500/938], Loss: 0.5810410380363464\n",
      "Train: Epoch [6], Batch [501/938], Loss: 0.741182804107666\n",
      "Train: Epoch [6], Batch [502/938], Loss: 0.49275633692741394\n",
      "Train: Epoch [6], Batch [503/938], Loss: 0.6057199239730835\n",
      "Train: Epoch [6], Batch [504/938], Loss: 0.6311087608337402\n",
      "Train: Epoch [6], Batch [505/938], Loss: 0.6581374406814575\n",
      "Train: Epoch [6], Batch [506/938], Loss: 0.7970141172409058\n",
      "Train: Epoch [6], Batch [507/938], Loss: 0.6299251317977905\n",
      "Train: Epoch [6], Batch [508/938], Loss: 0.6265564560890198\n",
      "Train: Epoch [6], Batch [509/938], Loss: 0.9152130484580994\n",
      "Train: Epoch [6], Batch [510/938], Loss: 0.5978803634643555\n",
      "Train: Epoch [6], Batch [511/938], Loss: 1.049457311630249\n",
      "Train: Epoch [6], Batch [512/938], Loss: 0.7619402408599854\n",
      "Train: Epoch [6], Batch [513/938], Loss: 0.6797765493392944\n",
      "Train: Epoch [6], Batch [514/938], Loss: 0.5821501016616821\n",
      "Train: Epoch [6], Batch [515/938], Loss: 0.6956813335418701\n",
      "Train: Epoch [6], Batch [516/938], Loss: 0.7417896389961243\n",
      "Train: Epoch [6], Batch [517/938], Loss: 0.5882899761199951\n",
      "Train: Epoch [6], Batch [518/938], Loss: 0.6904179453849792\n",
      "Train: Epoch [6], Batch [519/938], Loss: 0.5334223508834839\n",
      "Train: Epoch [6], Batch [520/938], Loss: 0.6413049697875977\n",
      "Train: Epoch [6], Batch [521/938], Loss: 0.7011220455169678\n",
      "Train: Epoch [6], Batch [522/938], Loss: 0.5984517931938171\n",
      "Train: Epoch [6], Batch [523/938], Loss: 0.5847128033638\n",
      "Train: Epoch [6], Batch [524/938], Loss: 0.6852842569351196\n",
      "Train: Epoch [6], Batch [525/938], Loss: 0.5955488085746765\n",
      "Train: Epoch [6], Batch [526/938], Loss: 0.5852214694023132\n",
      "Train: Epoch [6], Batch [527/938], Loss: 0.6400784254074097\n",
      "Train: Epoch [6], Batch [528/938], Loss: 0.8408865928649902\n",
      "Train: Epoch [6], Batch [529/938], Loss: 0.9200684428215027\n",
      "Train: Epoch [6], Batch [530/938], Loss: 0.7309861183166504\n",
      "Train: Epoch [6], Batch [531/938], Loss: 0.8082582354545593\n",
      "Train: Epoch [6], Batch [532/938], Loss: 0.6937271356582642\n",
      "Train: Epoch [6], Batch [533/938], Loss: 0.6383246183395386\n",
      "Train: Epoch [6], Batch [534/938], Loss: 0.5686386227607727\n",
      "Train: Epoch [6], Batch [535/938], Loss: 0.6253888607025146\n",
      "Train: Epoch [6], Batch [536/938], Loss: 0.6008304953575134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [6], Batch [537/938], Loss: 0.7929483652114868\n",
      "Train: Epoch [6], Batch [538/938], Loss: 0.5133107304573059\n",
      "Train: Epoch [6], Batch [539/938], Loss: 0.5821302533149719\n",
      "Train: Epoch [6], Batch [540/938], Loss: 0.6022239923477173\n",
      "Train: Epoch [6], Batch [541/938], Loss: 0.35974353551864624\n",
      "Train: Epoch [6], Batch [542/938], Loss: 0.5285391807556152\n",
      "Train: Epoch [6], Batch [543/938], Loss: 0.6732584238052368\n",
      "Train: Epoch [6], Batch [544/938], Loss: 0.6977806687355042\n",
      "Train: Epoch [6], Batch [545/938], Loss: 0.6426700353622437\n",
      "Train: Epoch [6], Batch [546/938], Loss: 0.4410586357116699\n",
      "Train: Epoch [6], Batch [547/938], Loss: 0.6896404027938843\n",
      "Train: Epoch [6], Batch [548/938], Loss: 0.6119303703308105\n",
      "Train: Epoch [6], Batch [549/938], Loss: 0.668968677520752\n",
      "Train: Epoch [6], Batch [550/938], Loss: 0.4952854812145233\n",
      "Train: Epoch [6], Batch [551/938], Loss: 0.6061880588531494\n",
      "Train: Epoch [6], Batch [552/938], Loss: 0.5265403389930725\n",
      "Train: Epoch [6], Batch [553/938], Loss: 0.5941281318664551\n",
      "Train: Epoch [6], Batch [554/938], Loss: 0.7749344110488892\n",
      "Train: Epoch [6], Batch [555/938], Loss: 0.6781314015388489\n",
      "Train: Epoch [6], Batch [556/938], Loss: 0.7774447798728943\n",
      "Train: Epoch [6], Batch [557/938], Loss: 0.5554771423339844\n",
      "Train: Epoch [6], Batch [558/938], Loss: 0.6879614591598511\n",
      "Train: Epoch [6], Batch [559/938], Loss: 0.412936270236969\n",
      "Train: Epoch [6], Batch [560/938], Loss: 0.6879711747169495\n",
      "Train: Epoch [6], Batch [561/938], Loss: 0.8497650623321533\n",
      "Train: Epoch [6], Batch [562/938], Loss: 0.5195221304893494\n",
      "Train: Epoch [6], Batch [563/938], Loss: 0.7960284352302551\n",
      "Train: Epoch [6], Batch [564/938], Loss: 0.5308524370193481\n",
      "Train: Epoch [6], Batch [565/938], Loss: 0.5814659595489502\n",
      "Train: Epoch [6], Batch [566/938], Loss: 0.5442144274711609\n",
      "Train: Epoch [6], Batch [567/938], Loss: 0.7100659608840942\n",
      "Train: Epoch [6], Batch [568/938], Loss: 0.5611090660095215\n",
      "Train: Epoch [6], Batch [569/938], Loss: 0.5684035420417786\n",
      "Train: Epoch [6], Batch [570/938], Loss: 0.5236144065856934\n",
      "Train: Epoch [6], Batch [571/938], Loss: 0.7684494256973267\n",
      "Train: Epoch [6], Batch [572/938], Loss: 0.7322929501533508\n",
      "Train: Epoch [6], Batch [573/938], Loss: 0.9191054105758667\n",
      "Train: Epoch [6], Batch [574/938], Loss: 0.6861861348152161\n",
      "Train: Epoch [6], Batch [575/938], Loss: 0.49046048521995544\n",
      "Train: Epoch [6], Batch [576/938], Loss: 0.600501298904419\n",
      "Train: Epoch [6], Batch [577/938], Loss: 0.596610426902771\n",
      "Train: Epoch [6], Batch [578/938], Loss: 0.7420623302459717\n",
      "Train: Epoch [6], Batch [579/938], Loss: 0.532573401927948\n",
      "Train: Epoch [6], Batch [580/938], Loss: 0.5882828235626221\n",
      "Train: Epoch [6], Batch [581/938], Loss: 0.6132565140724182\n",
      "Train: Epoch [6], Batch [582/938], Loss: 0.47072482109069824\n",
      "Train: Epoch [6], Batch [583/938], Loss: 0.6964368224143982\n",
      "Train: Epoch [6], Batch [584/938], Loss: 0.6232438087463379\n",
      "Train: Epoch [6], Batch [585/938], Loss: 0.695999801158905\n",
      "Train: Epoch [6], Batch [586/938], Loss: 0.5309308767318726\n",
      "Train: Epoch [6], Batch [587/938], Loss: 0.5759061574935913\n",
      "Train: Epoch [6], Batch [588/938], Loss: 0.5190383195877075\n",
      "Train: Epoch [6], Batch [589/938], Loss: 0.6454464197158813\n",
      "Train: Epoch [6], Batch [590/938], Loss: 0.7916953563690186\n",
      "Train: Epoch [6], Batch [591/938], Loss: 0.6039494872093201\n",
      "Train: Epoch [6], Batch [592/938], Loss: 0.7010387778282166\n",
      "Train: Epoch [6], Batch [593/938], Loss: 0.5199207663536072\n",
      "Train: Epoch [6], Batch [594/938], Loss: 0.4832580089569092\n",
      "Train: Epoch [6], Batch [595/938], Loss: 0.7704518437385559\n",
      "Train: Epoch [6], Batch [596/938], Loss: 0.5326896905899048\n",
      "Train: Epoch [6], Batch [597/938], Loss: 0.6676539778709412\n",
      "Train: Epoch [6], Batch [598/938], Loss: 0.7087345123291016\n",
      "Train: Epoch [6], Batch [599/938], Loss: 0.7142360806465149\n",
      "Train: Epoch [6], Batch [600/938], Loss: 0.5305746793746948\n",
      "Train: Epoch [6], Batch [601/938], Loss: 0.4313061833381653\n",
      "Train: Epoch [6], Batch [602/938], Loss: 0.8824766874313354\n",
      "Train: Epoch [6], Batch [603/938], Loss: 0.45319873094558716\n",
      "Train: Epoch [6], Batch [604/938], Loss: 0.7665631771087646\n",
      "Train: Epoch [6], Batch [605/938], Loss: 0.549207866191864\n",
      "Train: Epoch [6], Batch [606/938], Loss: 0.6656171083450317\n",
      "Train: Epoch [6], Batch [607/938], Loss: 0.6899499893188477\n",
      "Train: Epoch [6], Batch [608/938], Loss: 0.5707080960273743\n",
      "Train: Epoch [6], Batch [609/938], Loss: 0.6718159914016724\n",
      "Train: Epoch [6], Batch [610/938], Loss: 0.6012675166130066\n",
      "Train: Epoch [6], Batch [611/938], Loss: 0.7462067604064941\n",
      "Train: Epoch [6], Batch [612/938], Loss: 0.4442678689956665\n",
      "Train: Epoch [6], Batch [613/938], Loss: 0.648187518119812\n",
      "Train: Epoch [6], Batch [614/938], Loss: 0.6243747472763062\n",
      "Train: Epoch [6], Batch [615/938], Loss: 0.708402156829834\n",
      "Train: Epoch [6], Batch [616/938], Loss: 0.7003824710845947\n",
      "Train: Epoch [6], Batch [617/938], Loss: 0.42532652616500854\n",
      "Train: Epoch [6], Batch [618/938], Loss: 0.794006884098053\n",
      "Train: Epoch [6], Batch [619/938], Loss: 1.0026652812957764\n",
      "Train: Epoch [6], Batch [620/938], Loss: 0.5885403156280518\n",
      "Train: Epoch [6], Batch [621/938], Loss: 0.8904815316200256\n",
      "Train: Epoch [6], Batch [622/938], Loss: 0.8460879325866699\n",
      "Train: Epoch [6], Batch [623/938], Loss: 0.6395175457000732\n",
      "Train: Epoch [6], Batch [624/938], Loss: 0.859869658946991\n",
      "Train: Epoch [6], Batch [625/938], Loss: 0.6081390380859375\n",
      "Train: Epoch [6], Batch [626/938], Loss: 0.5659027099609375\n",
      "Train: Epoch [6], Batch [627/938], Loss: 0.5467115640640259\n",
      "Train: Epoch [6], Batch [628/938], Loss: 0.9817589521408081\n",
      "Train: Epoch [6], Batch [629/938], Loss: 0.5273597836494446\n",
      "Train: Epoch [6], Batch [630/938], Loss: 0.8133153319358826\n",
      "Train: Epoch [6], Batch [631/938], Loss: 0.7084619998931885\n",
      "Train: Epoch [6], Batch [632/938], Loss: 0.6571327447891235\n",
      "Train: Epoch [6], Batch [633/938], Loss: 0.8726048469543457\n",
      "Train: Epoch [6], Batch [634/938], Loss: 0.5808897018432617\n",
      "Train: Epoch [6], Batch [635/938], Loss: 0.7316415905952454\n",
      "Train: Epoch [6], Batch [636/938], Loss: 0.4465220868587494\n",
      "Train: Epoch [6], Batch [637/938], Loss: 0.6859230995178223\n",
      "Train: Epoch [6], Batch [638/938], Loss: 0.6430450677871704\n",
      "Train: Epoch [6], Batch [639/938], Loss: 0.5218896269798279\n",
      "Train: Epoch [6], Batch [640/938], Loss: 0.5957021117210388\n",
      "Train: Epoch [6], Batch [641/938], Loss: 0.7889605760574341\n",
      "Train: Epoch [6], Batch [642/938], Loss: 0.5441575050354004\n",
      "Train: Epoch [6], Batch [643/938], Loss: 0.5180160403251648\n",
      "Train: Epoch [6], Batch [644/938], Loss: 0.6520602703094482\n",
      "Train: Epoch [6], Batch [645/938], Loss: 0.4706979990005493\n",
      "Train: Epoch [6], Batch [646/938], Loss: 0.7362639904022217\n",
      "Train: Epoch [6], Batch [647/938], Loss: 0.5161097049713135\n",
      "Train: Epoch [6], Batch [648/938], Loss: 0.43834584951400757\n",
      "Train: Epoch [6], Batch [649/938], Loss: 0.563878059387207\n",
      "Train: Epoch [6], Batch [650/938], Loss: 0.4698721468448639\n",
      "Train: Epoch [6], Batch [651/938], Loss: 0.645958423614502\n",
      "Train: Epoch [6], Batch [652/938], Loss: 0.5827381014823914\n",
      "Train: Epoch [6], Batch [653/938], Loss: 0.5412479639053345\n",
      "Train: Epoch [6], Batch [654/938], Loss: 0.6405991315841675\n",
      "Train: Epoch [6], Batch [655/938], Loss: 0.598792314529419\n",
      "Train: Epoch [6], Batch [656/938], Loss: 0.6304346323013306\n",
      "Train: Epoch [6], Batch [657/938], Loss: 0.5915684103965759\n",
      "Train: Epoch [6], Batch [658/938], Loss: 0.5952024459838867\n",
      "Train: Epoch [6], Batch [659/938], Loss: 0.5958614349365234\n",
      "Train: Epoch [6], Batch [660/938], Loss: 0.5074223279953003\n",
      "Train: Epoch [6], Batch [661/938], Loss: 0.644606351852417\n",
      "Train: Epoch [6], Batch [662/938], Loss: 0.588141679763794\n",
      "Train: Epoch [6], Batch [663/938], Loss: 0.5330943465232849\n",
      "Train: Epoch [6], Batch [664/938], Loss: 0.6908347010612488\n",
      "Train: Epoch [6], Batch [665/938], Loss: 0.5857030153274536\n",
      "Train: Epoch [6], Batch [666/938], Loss: 0.5722904205322266\n",
      "Train: Epoch [6], Batch [667/938], Loss: 0.6712319850921631\n",
      "Train: Epoch [6], Batch [668/938], Loss: 0.7208606600761414\n",
      "Train: Epoch [6], Batch [669/938], Loss: 0.5755110383033752\n",
      "Train: Epoch [6], Batch [670/938], Loss: 0.5316943526268005\n",
      "Train: Epoch [6], Batch [671/938], Loss: 0.5831432938575745\n",
      "Train: Epoch [6], Batch [672/938], Loss: 0.9233387112617493\n",
      "Train: Epoch [6], Batch [673/938], Loss: 0.7072803974151611\n",
      "Train: Epoch [6], Batch [674/938], Loss: 0.6083014607429504\n",
      "Train: Epoch [6], Batch [675/938], Loss: 0.6191955208778381\n",
      "Train: Epoch [6], Batch [676/938], Loss: 0.6199227571487427\n",
      "Train: Epoch [6], Batch [677/938], Loss: 0.42657461762428284\n",
      "Train: Epoch [6], Batch [678/938], Loss: 0.6919738054275513\n",
      "Train: Epoch [6], Batch [679/938], Loss: 0.5656986236572266\n",
      "Train: Epoch [6], Batch [680/938], Loss: 0.4984653890132904\n",
      "Train: Epoch [6], Batch [681/938], Loss: 0.7589056491851807\n",
      "Train: Epoch [6], Batch [682/938], Loss: 0.49993544816970825\n",
      "Train: Epoch [6], Batch [683/938], Loss: 0.6189864277839661\n",
      "Train: Epoch [6], Batch [684/938], Loss: 0.6614288091659546\n",
      "Train: Epoch [6], Batch [685/938], Loss: 0.6864708065986633\n",
      "Train: Epoch [6], Batch [686/938], Loss: 0.5664079189300537\n",
      "Train: Epoch [6], Batch [687/938], Loss: 0.6921117305755615\n",
      "Train: Epoch [6], Batch [688/938], Loss: 0.7976126670837402\n",
      "Train: Epoch [6], Batch [689/938], Loss: 0.6718382835388184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [6], Batch [690/938], Loss: 0.7661218643188477\n",
      "Train: Epoch [6], Batch [691/938], Loss: 0.6803691387176514\n",
      "Train: Epoch [6], Batch [692/938], Loss: 0.7337876558303833\n",
      "Train: Epoch [6], Batch [693/938], Loss: 0.716768205165863\n",
      "Train: Epoch [6], Batch [694/938], Loss: 0.658257007598877\n",
      "Train: Epoch [6], Batch [695/938], Loss: 0.5885157585144043\n",
      "Train: Epoch [6], Batch [696/938], Loss: 0.6901754140853882\n",
      "Train: Epoch [6], Batch [697/938], Loss: 0.5464679002761841\n",
      "Train: Epoch [6], Batch [698/938], Loss: 0.5533326268196106\n",
      "Train: Epoch [6], Batch [699/938], Loss: 0.7226017713546753\n",
      "Train: Epoch [6], Batch [700/938], Loss: 0.6354302763938904\n",
      "Train: Epoch [6], Batch [701/938], Loss: 0.6092046499252319\n",
      "Train: Epoch [6], Batch [702/938], Loss: 0.5771440267562866\n",
      "Train: Epoch [6], Batch [703/938], Loss: 0.6384810209274292\n",
      "Train: Epoch [6], Batch [704/938], Loss: 0.5656372308731079\n",
      "Train: Epoch [6], Batch [705/938], Loss: 0.6366009712219238\n",
      "Train: Epoch [6], Batch [706/938], Loss: 0.6049730777740479\n",
      "Train: Epoch [6], Batch [707/938], Loss: 0.6999602317810059\n",
      "Train: Epoch [6], Batch [708/938], Loss: 0.6925321817398071\n",
      "Train: Epoch [6], Batch [709/938], Loss: 0.6212921142578125\n",
      "Train: Epoch [6], Batch [710/938], Loss: 0.7049971222877502\n",
      "Train: Epoch [6], Batch [711/938], Loss: 0.6599705219268799\n",
      "Train: Epoch [6], Batch [712/938], Loss: 0.5886735916137695\n",
      "Train: Epoch [6], Batch [713/938], Loss: 0.4976256787776947\n",
      "Train: Epoch [6], Batch [714/938], Loss: 0.6189959645271301\n",
      "Train: Epoch [6], Batch [715/938], Loss: 0.7460671067237854\n",
      "Train: Epoch [6], Batch [716/938], Loss: 0.5139240026473999\n",
      "Train: Epoch [6], Batch [717/938], Loss: 0.5643618106842041\n",
      "Train: Epoch [6], Batch [718/938], Loss: 0.6404156684875488\n",
      "Train: Epoch [6], Batch [719/938], Loss: 0.5661444664001465\n",
      "Train: Epoch [6], Batch [720/938], Loss: 0.47736310958862305\n",
      "Train: Epoch [6], Batch [721/938], Loss: 0.6194665431976318\n",
      "Train: Epoch [6], Batch [722/938], Loss: 0.6251910924911499\n",
      "Train: Epoch [6], Batch [723/938], Loss: 0.5873118042945862\n",
      "Train: Epoch [6], Batch [724/938], Loss: 0.5949084162712097\n",
      "Train: Epoch [6], Batch [725/938], Loss: 0.6646686792373657\n",
      "Train: Epoch [6], Batch [726/938], Loss: 0.6084296703338623\n",
      "Train: Epoch [6], Batch [727/938], Loss: 0.593238353729248\n",
      "Train: Epoch [6], Batch [728/938], Loss: 0.9572710990905762\n",
      "Train: Epoch [6], Batch [729/938], Loss: 0.6930207014083862\n",
      "Train: Epoch [6], Batch [730/938], Loss: 0.6096360087394714\n",
      "Train: Epoch [6], Batch [731/938], Loss: 0.48541077971458435\n",
      "Train: Epoch [6], Batch [732/938], Loss: 0.5464231967926025\n",
      "Train: Epoch [6], Batch [733/938], Loss: 0.47273319959640503\n",
      "Train: Epoch [6], Batch [734/938], Loss: 0.46932631731033325\n",
      "Train: Epoch [6], Batch [735/938], Loss: 0.534460186958313\n",
      "Train: Epoch [6], Batch [736/938], Loss: 0.7148257493972778\n",
      "Train: Epoch [6], Batch [737/938], Loss: 0.5925611257553101\n",
      "Train: Epoch [6], Batch [738/938], Loss: 0.7723307609558105\n",
      "Train: Epoch [6], Batch [739/938], Loss: 0.5775812864303589\n",
      "Train: Epoch [6], Batch [740/938], Loss: 0.4769679307937622\n",
      "Train: Epoch [6], Batch [741/938], Loss: 0.45643195509910583\n",
      "Train: Epoch [6], Batch [742/938], Loss: 0.6149834394454956\n",
      "Train: Epoch [6], Batch [743/938], Loss: 0.7284687161445618\n",
      "Train: Epoch [6], Batch [744/938], Loss: 0.5751233100891113\n",
      "Train: Epoch [6], Batch [745/938], Loss: 0.4080621004104614\n",
      "Train: Epoch [6], Batch [746/938], Loss: 0.6635437607765198\n",
      "Train: Epoch [6], Batch [747/938], Loss: 0.6780272722244263\n",
      "Train: Epoch [6], Batch [748/938], Loss: 0.5377686023712158\n",
      "Train: Epoch [6], Batch [749/938], Loss: 0.5288835763931274\n",
      "Train: Epoch [6], Batch [750/938], Loss: 0.4972943067550659\n",
      "Train: Epoch [6], Batch [751/938], Loss: 0.5443884134292603\n",
      "Train: Epoch [6], Batch [752/938], Loss: 0.6179590225219727\n",
      "Train: Epoch [6], Batch [753/938], Loss: 0.5063738822937012\n",
      "Train: Epoch [6], Batch [754/938], Loss: 0.5894202589988708\n",
      "Train: Epoch [6], Batch [755/938], Loss: 0.5575573444366455\n",
      "Train: Epoch [6], Batch [756/938], Loss: 0.8012903928756714\n",
      "Train: Epoch [6], Batch [757/938], Loss: 0.46299564838409424\n",
      "Train: Epoch [6], Batch [758/938], Loss: 0.6178908348083496\n",
      "Train: Epoch [6], Batch [759/938], Loss: 0.5649839043617249\n",
      "Train: Epoch [6], Batch [760/938], Loss: 0.6396584510803223\n",
      "Train: Epoch [6], Batch [761/938], Loss: 0.33494165539741516\n",
      "Train: Epoch [6], Batch [762/938], Loss: 0.8535315990447998\n",
      "Train: Epoch [6], Batch [763/938], Loss: 0.4932541847229004\n",
      "Train: Epoch [6], Batch [764/938], Loss: 0.8217067718505859\n",
      "Train: Epoch [6], Batch [765/938], Loss: 0.9219841361045837\n",
      "Train: Epoch [6], Batch [766/938], Loss: 0.7630029916763306\n",
      "Train: Epoch [6], Batch [767/938], Loss: 0.6629794836044312\n",
      "Train: Epoch [6], Batch [768/938], Loss: 0.6498957872390747\n",
      "Train: Epoch [6], Batch [769/938], Loss: 0.6314689517021179\n",
      "Train: Epoch [6], Batch [770/938], Loss: 0.6994384527206421\n",
      "Train: Epoch [6], Batch [771/938], Loss: 0.7084236741065979\n",
      "Train: Epoch [6], Batch [772/938], Loss: 0.6897963881492615\n",
      "Train: Epoch [6], Batch [773/938], Loss: 0.5628032684326172\n",
      "Train: Epoch [6], Batch [774/938], Loss: 0.7517347931861877\n",
      "Train: Epoch [6], Batch [775/938], Loss: 0.5227571725845337\n",
      "Train: Epoch [6], Batch [776/938], Loss: 0.5785396099090576\n",
      "Train: Epoch [6], Batch [777/938], Loss: 0.5456355810165405\n",
      "Train: Epoch [6], Batch [778/938], Loss: 0.590394139289856\n",
      "Train: Epoch [6], Batch [779/938], Loss: 0.5004678964614868\n",
      "Train: Epoch [6], Batch [780/938], Loss: 0.518218994140625\n",
      "Train: Epoch [6], Batch [781/938], Loss: 0.6202587485313416\n",
      "Train: Epoch [6], Batch [782/938], Loss: 0.6281520128250122\n",
      "Train: Epoch [6], Batch [783/938], Loss: 0.5366584062576294\n",
      "Train: Epoch [6], Batch [784/938], Loss: 0.6406570672988892\n",
      "Train: Epoch [6], Batch [785/938], Loss: 0.7967795133590698\n",
      "Train: Epoch [6], Batch [786/938], Loss: 0.5246446132659912\n",
      "Train: Epoch [6], Batch [787/938], Loss: 0.8494507670402527\n",
      "Train: Epoch [6], Batch [788/938], Loss: 0.6487746238708496\n",
      "Train: Epoch [6], Batch [789/938], Loss: 0.5387935638427734\n",
      "Train: Epoch [6], Batch [790/938], Loss: 0.4643087685108185\n",
      "Train: Epoch [6], Batch [791/938], Loss: 0.5733848810195923\n",
      "Train: Epoch [6], Batch [792/938], Loss: 0.531379222869873\n",
      "Train: Epoch [6], Batch [793/938], Loss: 0.7093855738639832\n",
      "Train: Epoch [6], Batch [794/938], Loss: 0.862919807434082\n",
      "Train: Epoch [6], Batch [795/938], Loss: 0.517734706401825\n",
      "Train: Epoch [6], Batch [796/938], Loss: 0.519862949848175\n",
      "Train: Epoch [6], Batch [797/938], Loss: 0.6326735019683838\n",
      "Train: Epoch [6], Batch [798/938], Loss: 0.6204507350921631\n",
      "Train: Epoch [6], Batch [799/938], Loss: 0.6307719349861145\n",
      "Train: Epoch [6], Batch [800/938], Loss: 0.6014765501022339\n",
      "Train: Epoch [6], Batch [801/938], Loss: 0.7377488613128662\n",
      "Train: Epoch [6], Batch [802/938], Loss: 0.6847645044326782\n",
      "Train: Epoch [6], Batch [803/938], Loss: 0.7276867628097534\n",
      "Train: Epoch [6], Batch [804/938], Loss: 0.674159586429596\n",
      "Train: Epoch [6], Batch [805/938], Loss: 0.6978818774223328\n",
      "Train: Epoch [6], Batch [806/938], Loss: 0.4144055247306824\n",
      "Train: Epoch [6], Batch [807/938], Loss: 0.6524767875671387\n",
      "Train: Epoch [6], Batch [808/938], Loss: 0.6102892756462097\n",
      "Train: Epoch [6], Batch [809/938], Loss: 0.7343778014183044\n",
      "Train: Epoch [6], Batch [810/938], Loss: 0.7322491407394409\n",
      "Train: Epoch [6], Batch [811/938], Loss: 0.5278688669204712\n",
      "Train: Epoch [6], Batch [812/938], Loss: 0.6444977521896362\n",
      "Train: Epoch [6], Batch [813/938], Loss: 0.37764590978622437\n",
      "Train: Epoch [6], Batch [814/938], Loss: 0.6408063173294067\n",
      "Train: Epoch [6], Batch [815/938], Loss: 0.6499792337417603\n",
      "Train: Epoch [6], Batch [816/938], Loss: 0.6245921850204468\n",
      "Train: Epoch [6], Batch [817/938], Loss: 0.6160202026367188\n",
      "Train: Epoch [6], Batch [818/938], Loss: 0.6424878835678101\n",
      "Train: Epoch [6], Batch [819/938], Loss: 0.7082839608192444\n",
      "Train: Epoch [6], Batch [820/938], Loss: 0.5632209777832031\n",
      "Train: Epoch [6], Batch [821/938], Loss: 0.774383544921875\n",
      "Train: Epoch [6], Batch [822/938], Loss: 0.6620470285415649\n",
      "Train: Epoch [6], Batch [823/938], Loss: 0.7677830457687378\n",
      "Train: Epoch [6], Batch [824/938], Loss: 0.48800569772720337\n",
      "Train: Epoch [6], Batch [825/938], Loss: 0.6862156987190247\n",
      "Train: Epoch [6], Batch [826/938], Loss: 0.4777434766292572\n",
      "Train: Epoch [6], Batch [827/938], Loss: 0.5435075163841248\n",
      "Train: Epoch [6], Batch [828/938], Loss: 0.5711003541946411\n",
      "Train: Epoch [6], Batch [829/938], Loss: 0.7161740064620972\n",
      "Train: Epoch [6], Batch [830/938], Loss: 0.530179500579834\n",
      "Train: Epoch [6], Batch [831/938], Loss: 0.6788190603256226\n",
      "Train: Epoch [6], Batch [832/938], Loss: 0.6044057607650757\n",
      "Train: Epoch [6], Batch [833/938], Loss: 0.5505610108375549\n",
      "Train: Epoch [6], Batch [834/938], Loss: 0.38066422939300537\n",
      "Train: Epoch [6], Batch [835/938], Loss: 0.725888729095459\n",
      "Train: Epoch [6], Batch [836/938], Loss: 0.498269259929657\n",
      "Train: Epoch [6], Batch [837/938], Loss: 0.6677900552749634\n",
      "Train: Epoch [6], Batch [838/938], Loss: 0.6328718662261963\n",
      "Train: Epoch [6], Batch [839/938], Loss: 0.7105071544647217\n",
      "Train: Epoch [6], Batch [840/938], Loss: 0.6468154191970825\n",
      "Train: Epoch [6], Batch [841/938], Loss: 0.5257514715194702\n",
      "Train: Epoch [6], Batch [842/938], Loss: 0.7123777270317078\n",
      "Train: Epoch [6], Batch [843/938], Loss: 0.8783471584320068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [6], Batch [844/938], Loss: 0.5535130500793457\n",
      "Train: Epoch [6], Batch [845/938], Loss: 0.8939815759658813\n",
      "Train: Epoch [6], Batch [846/938], Loss: 0.7037315368652344\n",
      "Train: Epoch [6], Batch [847/938], Loss: 0.6976923942565918\n",
      "Train: Epoch [6], Batch [848/938], Loss: 0.501911461353302\n",
      "Train: Epoch [6], Batch [849/938], Loss: 0.9481407403945923\n",
      "Train: Epoch [6], Batch [850/938], Loss: 0.7542623281478882\n",
      "Train: Epoch [6], Batch [851/938], Loss: 0.5938265323638916\n",
      "Train: Epoch [6], Batch [852/938], Loss: 0.6752957105636597\n",
      "Train: Epoch [6], Batch [853/938], Loss: 0.5741173028945923\n",
      "Train: Epoch [6], Batch [854/938], Loss: 0.7013180255889893\n",
      "Train: Epoch [6], Batch [855/938], Loss: 0.6821395754814148\n",
      "Train: Epoch [6], Batch [856/938], Loss: 0.5069890022277832\n",
      "Train: Epoch [6], Batch [857/938], Loss: 0.5494308471679688\n",
      "Train: Epoch [6], Batch [858/938], Loss: 0.6584515571594238\n",
      "Train: Epoch [6], Batch [859/938], Loss: 0.45907509326934814\n",
      "Train: Epoch [6], Batch [860/938], Loss: 0.5315414667129517\n",
      "Train: Epoch [6], Batch [861/938], Loss: 0.7586238384246826\n",
      "Train: Epoch [6], Batch [862/938], Loss: 0.5189254283905029\n",
      "Train: Epoch [6], Batch [863/938], Loss: 0.6796408891677856\n",
      "Train: Epoch [6], Batch [864/938], Loss: 0.7148258686065674\n",
      "Train: Epoch [6], Batch [865/938], Loss: 0.6108015775680542\n",
      "Train: Epoch [6], Batch [866/938], Loss: 0.6437009572982788\n",
      "Train: Epoch [6], Batch [867/938], Loss: 0.9213765263557434\n",
      "Train: Epoch [6], Batch [868/938], Loss: 0.4479690194129944\n",
      "Train: Epoch [6], Batch [869/938], Loss: 0.6739778518676758\n",
      "Train: Epoch [6], Batch [870/938], Loss: 0.6846731901168823\n",
      "Train: Epoch [6], Batch [871/938], Loss: 0.47036212682724\n",
      "Train: Epoch [6], Batch [872/938], Loss: 0.8068872094154358\n",
      "Train: Epoch [6], Batch [873/938], Loss: 0.6068979501724243\n",
      "Train: Epoch [6], Batch [874/938], Loss: 0.5576869249343872\n",
      "Train: Epoch [6], Batch [875/938], Loss: 0.7565106749534607\n",
      "Train: Epoch [6], Batch [876/938], Loss: 0.4902116358280182\n",
      "Train: Epoch [6], Batch [877/938], Loss: 0.771216869354248\n",
      "Train: Epoch [6], Batch [878/938], Loss: 0.8508584499359131\n",
      "Train: Epoch [6], Batch [879/938], Loss: 0.5279438495635986\n",
      "Train: Epoch [6], Batch [880/938], Loss: 0.5043590068817139\n",
      "Train: Epoch [6], Batch [881/938], Loss: 0.4657937288284302\n",
      "Train: Epoch [6], Batch [882/938], Loss: 0.5095948576927185\n",
      "Train: Epoch [6], Batch [883/938], Loss: 0.5626481771469116\n",
      "Train: Epoch [6], Batch [884/938], Loss: 0.6953765153884888\n",
      "Train: Epoch [6], Batch [885/938], Loss: 0.8489300012588501\n",
      "Train: Epoch [6], Batch [886/938], Loss: 0.6999930143356323\n",
      "Train: Epoch [6], Batch [887/938], Loss: 0.6887502074241638\n",
      "Train: Epoch [6], Batch [888/938], Loss: 0.6346884965896606\n",
      "Train: Epoch [6], Batch [889/938], Loss: 0.7019622325897217\n",
      "Train: Epoch [6], Batch [890/938], Loss: 0.5266234278678894\n",
      "Train: Epoch [6], Batch [891/938], Loss: 0.796074390411377\n",
      "Train: Epoch [6], Batch [892/938], Loss: 0.45861145853996277\n",
      "Train: Epoch [6], Batch [893/938], Loss: 0.662800669670105\n",
      "Train: Epoch [6], Batch [894/938], Loss: 0.5358622670173645\n",
      "Train: Epoch [6], Batch [895/938], Loss: 0.44403624534606934\n",
      "Train: Epoch [6], Batch [896/938], Loss: 0.8263744711875916\n",
      "Train: Epoch [6], Batch [897/938], Loss: 0.6362806558609009\n",
      "Train: Epoch [6], Batch [898/938], Loss: 0.517736554145813\n",
      "Train: Epoch [6], Batch [899/938], Loss: 0.5972740650177002\n",
      "Train: Epoch [6], Batch [900/938], Loss: 0.5832273364067078\n",
      "Train: Epoch [6], Batch [901/938], Loss: 0.7203577756881714\n",
      "Train: Epoch [6], Batch [902/938], Loss: 0.6446954607963562\n",
      "Train: Epoch [6], Batch [903/938], Loss: 0.5159614086151123\n",
      "Train: Epoch [6], Batch [904/938], Loss: 0.45971328020095825\n",
      "Train: Epoch [6], Batch [905/938], Loss: 0.5358425378799438\n",
      "Train: Epoch [6], Batch [906/938], Loss: 0.5654096603393555\n",
      "Train: Epoch [6], Batch [907/938], Loss: 0.5718485713005066\n",
      "Train: Epoch [6], Batch [908/938], Loss: 0.35520732402801514\n",
      "Train: Epoch [6], Batch [909/938], Loss: 0.5560387372970581\n",
      "Train: Epoch [6], Batch [910/938], Loss: 0.442609041929245\n",
      "Train: Epoch [6], Batch [911/938], Loss: 0.5196282863616943\n",
      "Train: Epoch [6], Batch [912/938], Loss: 0.6445419788360596\n",
      "Train: Epoch [6], Batch [913/938], Loss: 0.45086461305618286\n",
      "Train: Epoch [6], Batch [914/938], Loss: 0.8503031730651855\n",
      "Train: Epoch [6], Batch [915/938], Loss: 0.5723265409469604\n",
      "Train: Epoch [6], Batch [916/938], Loss: 0.5351117849349976\n",
      "Train: Epoch [6], Batch [917/938], Loss: 0.6219423413276672\n",
      "Train: Epoch [6], Batch [918/938], Loss: 0.5973572731018066\n",
      "Train: Epoch [6], Batch [919/938], Loss: 0.6456941366195679\n",
      "Train: Epoch [6], Batch [920/938], Loss: 0.7959900498390198\n",
      "Train: Epoch [6], Batch [921/938], Loss: 0.9831540584564209\n",
      "Train: Epoch [6], Batch [922/938], Loss: 0.5922746658325195\n",
      "Train: Epoch [6], Batch [923/938], Loss: 0.6999732255935669\n",
      "Train: Epoch [6], Batch [924/938], Loss: 0.5865064859390259\n",
      "Train: Epoch [6], Batch [925/938], Loss: 0.4963676333427429\n",
      "Train: Epoch [6], Batch [926/938], Loss: 0.6179181337356567\n",
      "Train: Epoch [6], Batch [927/938], Loss: 0.5025741457939148\n",
      "Train: Epoch [6], Batch [928/938], Loss: 0.5973533391952515\n",
      "Train: Epoch [6], Batch [929/938], Loss: 0.4949362874031067\n",
      "Train: Epoch [6], Batch [930/938], Loss: 0.6081733703613281\n",
      "Train: Epoch [6], Batch [931/938], Loss: 0.541235625743866\n",
      "Train: Epoch [6], Batch [932/938], Loss: 0.471224844455719\n",
      "Train: Epoch [6], Batch [933/938], Loss: 0.5568816065788269\n",
      "Train: Epoch [6], Batch [934/938], Loss: 0.6078615784645081\n",
      "Train: Epoch [6], Batch [935/938], Loss: 0.5387493371963501\n",
      "Train: Epoch [6], Batch [936/938], Loss: 0.561124324798584\n",
      "Train: Epoch [6], Batch [937/938], Loss: 0.6623896956443787\n",
      "Train: Epoch [6], Batch [938/938], Loss: 0.41245490312576294\n",
      "Accuracy of train set: 0.7689\n",
      "Validation: Epoch [6], Batch [1/938], Loss: 0.668822169303894\n",
      "Validation: Epoch [6], Batch [2/938], Loss: 0.7841318845748901\n",
      "Validation: Epoch [6], Batch [3/938], Loss: 0.6395684480667114\n",
      "Validation: Epoch [6], Batch [4/938], Loss: 0.6175548434257507\n",
      "Validation: Epoch [6], Batch [5/938], Loss: 0.6143471002578735\n",
      "Validation: Epoch [6], Batch [6/938], Loss: 0.6813943386077881\n",
      "Validation: Epoch [6], Batch [7/938], Loss: 0.6782007217407227\n",
      "Validation: Epoch [6], Batch [8/938], Loss: 0.5111432671546936\n",
      "Validation: Epoch [6], Batch [9/938], Loss: 0.6412562131881714\n",
      "Validation: Epoch [6], Batch [10/938], Loss: 0.6908923387527466\n",
      "Validation: Epoch [6], Batch [11/938], Loss: 0.5939681529998779\n",
      "Validation: Epoch [6], Batch [12/938], Loss: 0.5262606739997864\n",
      "Validation: Epoch [6], Batch [13/938], Loss: 0.5880290865898132\n",
      "Validation: Epoch [6], Batch [14/938], Loss: 0.6417664289474487\n",
      "Validation: Epoch [6], Batch [15/938], Loss: 0.7005897760391235\n",
      "Validation: Epoch [6], Batch [16/938], Loss: 0.6863671541213989\n",
      "Validation: Epoch [6], Batch [17/938], Loss: 0.655120313167572\n",
      "Validation: Epoch [6], Batch [18/938], Loss: 0.7042327523231506\n",
      "Validation: Epoch [6], Batch [19/938], Loss: 0.4802808463573456\n",
      "Validation: Epoch [6], Batch [20/938], Loss: 0.5933922529220581\n",
      "Validation: Epoch [6], Batch [21/938], Loss: 0.5251295566558838\n",
      "Validation: Epoch [6], Batch [22/938], Loss: 0.6879592537879944\n",
      "Validation: Epoch [6], Batch [23/938], Loss: 0.6239526271820068\n",
      "Validation: Epoch [6], Batch [24/938], Loss: 0.6274875402450562\n",
      "Validation: Epoch [6], Batch [25/938], Loss: 0.6504711508750916\n",
      "Validation: Epoch [6], Batch [26/938], Loss: 0.46750545501708984\n",
      "Validation: Epoch [6], Batch [27/938], Loss: 0.5544346570968628\n",
      "Validation: Epoch [6], Batch [28/938], Loss: 0.6159408092498779\n",
      "Validation: Epoch [6], Batch [29/938], Loss: 0.507522702217102\n",
      "Validation: Epoch [6], Batch [30/938], Loss: 0.4292834401130676\n",
      "Validation: Epoch [6], Batch [31/938], Loss: 0.7133881449699402\n",
      "Validation: Epoch [6], Batch [32/938], Loss: 0.5249050855636597\n",
      "Validation: Epoch [6], Batch [33/938], Loss: 0.8128077983856201\n",
      "Validation: Epoch [6], Batch [34/938], Loss: 0.5611010193824768\n",
      "Validation: Epoch [6], Batch [35/938], Loss: 0.5101906061172485\n",
      "Validation: Epoch [6], Batch [36/938], Loss: 0.511243462562561\n",
      "Validation: Epoch [6], Batch [37/938], Loss: 0.6681722402572632\n",
      "Validation: Epoch [6], Batch [38/938], Loss: 0.5589336156845093\n",
      "Validation: Epoch [6], Batch [39/938], Loss: 0.5554109811782837\n",
      "Validation: Epoch [6], Batch [40/938], Loss: 0.4682137966156006\n",
      "Validation: Epoch [6], Batch [41/938], Loss: 0.5705201625823975\n",
      "Validation: Epoch [6], Batch [42/938], Loss: 0.5126100778579712\n",
      "Validation: Epoch [6], Batch [43/938], Loss: 0.5139232873916626\n",
      "Validation: Epoch [6], Batch [44/938], Loss: 0.52214515209198\n",
      "Validation: Epoch [6], Batch [45/938], Loss: 0.730984628200531\n",
      "Validation: Epoch [6], Batch [46/938], Loss: 0.7165119647979736\n",
      "Validation: Epoch [6], Batch [47/938], Loss: 0.6914053559303284\n",
      "Validation: Epoch [6], Batch [48/938], Loss: 0.6737505197525024\n",
      "Validation: Epoch [6], Batch [49/938], Loss: 0.635327935218811\n",
      "Validation: Epoch [6], Batch [50/938], Loss: 1.0121506452560425\n",
      "Validation: Epoch [6], Batch [51/938], Loss: 0.7654713988304138\n",
      "Validation: Epoch [6], Batch [52/938], Loss: 0.857431173324585\n",
      "Validation: Epoch [6], Batch [53/938], Loss: 0.7735046148300171\n",
      "Validation: Epoch [6], Batch [54/938], Loss: 0.5862237811088562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [55/938], Loss: 0.6893239617347717\n",
      "Validation: Epoch [6], Batch [56/938], Loss: 0.42628908157348633\n",
      "Validation: Epoch [6], Batch [57/938], Loss: 0.4849013686180115\n",
      "Validation: Epoch [6], Batch [58/938], Loss: 0.6435592174530029\n",
      "Validation: Epoch [6], Batch [59/938], Loss: 0.8271201848983765\n",
      "Validation: Epoch [6], Batch [60/938], Loss: 0.6533241271972656\n",
      "Validation: Epoch [6], Batch [61/938], Loss: 0.5924907922744751\n",
      "Validation: Epoch [6], Batch [62/938], Loss: 0.5638731122016907\n",
      "Validation: Epoch [6], Batch [63/938], Loss: 0.5482754707336426\n",
      "Validation: Epoch [6], Batch [64/938], Loss: 0.7762008905410767\n",
      "Validation: Epoch [6], Batch [65/938], Loss: 0.8495649099349976\n",
      "Validation: Epoch [6], Batch [66/938], Loss: 0.6695355176925659\n",
      "Validation: Epoch [6], Batch [67/938], Loss: 0.8170363306999207\n",
      "Validation: Epoch [6], Batch [68/938], Loss: 0.6539663076400757\n",
      "Validation: Epoch [6], Batch [69/938], Loss: 0.7893515825271606\n",
      "Validation: Epoch [6], Batch [70/938], Loss: 0.6759493350982666\n",
      "Validation: Epoch [6], Batch [71/938], Loss: 0.7511909604072571\n",
      "Validation: Epoch [6], Batch [72/938], Loss: 0.44502636790275574\n",
      "Validation: Epoch [6], Batch [73/938], Loss: 0.9977559447288513\n",
      "Validation: Epoch [6], Batch [74/938], Loss: 0.696679949760437\n",
      "Validation: Epoch [6], Batch [75/938], Loss: 0.548328697681427\n",
      "Validation: Epoch [6], Batch [76/938], Loss: 0.5828918218612671\n",
      "Validation: Epoch [6], Batch [77/938], Loss: 0.5532373189926147\n",
      "Validation: Epoch [6], Batch [78/938], Loss: 0.5313569903373718\n",
      "Validation: Epoch [6], Batch [79/938], Loss: 0.5351460576057434\n",
      "Validation: Epoch [6], Batch [80/938], Loss: 0.5528205633163452\n",
      "Validation: Epoch [6], Batch [81/938], Loss: 0.5782199501991272\n",
      "Validation: Epoch [6], Batch [82/938], Loss: 0.6536643505096436\n",
      "Validation: Epoch [6], Batch [83/938], Loss: 0.5547434687614441\n",
      "Validation: Epoch [6], Batch [84/938], Loss: 0.6026641726493835\n",
      "Validation: Epoch [6], Batch [85/938], Loss: 0.538030743598938\n",
      "Validation: Epoch [6], Batch [86/938], Loss: 0.5070509314537048\n",
      "Validation: Epoch [6], Batch [87/938], Loss: 0.5664909482002258\n",
      "Validation: Epoch [6], Batch [88/938], Loss: 0.5081768035888672\n",
      "Validation: Epoch [6], Batch [89/938], Loss: 0.6600239276885986\n",
      "Validation: Epoch [6], Batch [90/938], Loss: 0.5375722646713257\n",
      "Validation: Epoch [6], Batch [91/938], Loss: 0.7260333299636841\n",
      "Validation: Epoch [6], Batch [92/938], Loss: 0.5885171294212341\n",
      "Validation: Epoch [6], Batch [93/938], Loss: 0.5200269222259521\n",
      "Validation: Epoch [6], Batch [94/938], Loss: 0.4016522169113159\n",
      "Validation: Epoch [6], Batch [95/938], Loss: 0.7577046751976013\n",
      "Validation: Epoch [6], Batch [96/938], Loss: 0.5778993368148804\n",
      "Validation: Epoch [6], Batch [97/938], Loss: 0.7815876007080078\n",
      "Validation: Epoch [6], Batch [98/938], Loss: 0.6094975471496582\n",
      "Validation: Epoch [6], Batch [99/938], Loss: 0.7375653982162476\n",
      "Validation: Epoch [6], Batch [100/938], Loss: 0.3962547779083252\n",
      "Validation: Epoch [6], Batch [101/938], Loss: 0.5506515502929688\n",
      "Validation: Epoch [6], Batch [102/938], Loss: 0.7650642395019531\n",
      "Validation: Epoch [6], Batch [103/938], Loss: 0.6529781222343445\n",
      "Validation: Epoch [6], Batch [104/938], Loss: 0.6812517642974854\n",
      "Validation: Epoch [6], Batch [105/938], Loss: 0.6511266231536865\n",
      "Validation: Epoch [6], Batch [106/938], Loss: 0.5849934816360474\n",
      "Validation: Epoch [6], Batch [107/938], Loss: 0.6553105115890503\n",
      "Validation: Epoch [6], Batch [108/938], Loss: 0.6987627744674683\n",
      "Validation: Epoch [6], Batch [109/938], Loss: 0.5792924165725708\n",
      "Validation: Epoch [6], Batch [110/938], Loss: 0.5480119585990906\n",
      "Validation: Epoch [6], Batch [111/938], Loss: 0.46347254514694214\n",
      "Validation: Epoch [6], Batch [112/938], Loss: 0.7106142044067383\n",
      "Validation: Epoch [6], Batch [113/938], Loss: 0.595746636390686\n",
      "Validation: Epoch [6], Batch [114/938], Loss: 0.5340743064880371\n",
      "Validation: Epoch [6], Batch [115/938], Loss: 0.5152408480644226\n",
      "Validation: Epoch [6], Batch [116/938], Loss: 0.692730724811554\n",
      "Validation: Epoch [6], Batch [117/938], Loss: 0.7883744239807129\n",
      "Validation: Epoch [6], Batch [118/938], Loss: 0.6034290790557861\n",
      "Validation: Epoch [6], Batch [119/938], Loss: 0.8271864056587219\n",
      "Validation: Epoch [6], Batch [120/938], Loss: 0.5248637795448303\n",
      "Validation: Epoch [6], Batch [121/938], Loss: 0.5800665616989136\n",
      "Validation: Epoch [6], Batch [122/938], Loss: 0.49634456634521484\n",
      "Validation: Epoch [6], Batch [123/938], Loss: 0.6753988862037659\n",
      "Validation: Epoch [6], Batch [124/938], Loss: 0.8180708885192871\n",
      "Validation: Epoch [6], Batch [125/938], Loss: 0.6249723434448242\n",
      "Validation: Epoch [6], Batch [126/938], Loss: 0.7557219862937927\n",
      "Validation: Epoch [6], Batch [127/938], Loss: 0.5884031653404236\n",
      "Validation: Epoch [6], Batch [128/938], Loss: 0.7619507312774658\n",
      "Validation: Epoch [6], Batch [129/938], Loss: 1.0970864295959473\n",
      "Validation: Epoch [6], Batch [130/938], Loss: 0.6032111644744873\n",
      "Validation: Epoch [6], Batch [131/938], Loss: 0.662115752696991\n",
      "Validation: Epoch [6], Batch [132/938], Loss: 0.5463600158691406\n",
      "Validation: Epoch [6], Batch [133/938], Loss: 0.7013664245605469\n",
      "Validation: Epoch [6], Batch [134/938], Loss: 0.809836745262146\n",
      "Validation: Epoch [6], Batch [135/938], Loss: 0.8525329828262329\n",
      "Validation: Epoch [6], Batch [136/938], Loss: 0.4552178978919983\n",
      "Validation: Epoch [6], Batch [137/938], Loss: 0.5739985704421997\n",
      "Validation: Epoch [6], Batch [138/938], Loss: 0.6829684972763062\n",
      "Validation: Epoch [6], Batch [139/938], Loss: 0.7725961208343506\n",
      "Validation: Epoch [6], Batch [140/938], Loss: 0.6092871427536011\n",
      "Validation: Epoch [6], Batch [141/938], Loss: 0.5213837623596191\n",
      "Validation: Epoch [6], Batch [142/938], Loss: 0.5340931415557861\n",
      "Validation: Epoch [6], Batch [143/938], Loss: 0.5661773681640625\n",
      "Validation: Epoch [6], Batch [144/938], Loss: 0.5733001232147217\n",
      "Validation: Epoch [6], Batch [145/938], Loss: 0.4257734417915344\n",
      "Validation: Epoch [6], Batch [146/938], Loss: 0.5328853726387024\n",
      "Validation: Epoch [6], Batch [147/938], Loss: 0.5994791984558105\n",
      "Validation: Epoch [6], Batch [148/938], Loss: 0.6455371975898743\n",
      "Validation: Epoch [6], Batch [149/938], Loss: 0.547124981880188\n",
      "Validation: Epoch [6], Batch [150/938], Loss: 0.7564362287521362\n",
      "Validation: Epoch [6], Batch [151/938], Loss: 0.592440128326416\n",
      "Validation: Epoch [6], Batch [152/938], Loss: 0.5305439233779907\n",
      "Validation: Epoch [6], Batch [153/938], Loss: 0.36638665199279785\n",
      "Validation: Epoch [6], Batch [154/938], Loss: 0.5043301582336426\n",
      "Validation: Epoch [6], Batch [155/938], Loss: 0.5694454908370972\n",
      "Validation: Epoch [6], Batch [156/938], Loss: 0.6681571006774902\n",
      "Validation: Epoch [6], Batch [157/938], Loss: 0.8848966360092163\n",
      "Validation: Epoch [6], Batch [158/938], Loss: 0.5979018211364746\n",
      "Validation: Epoch [6], Batch [159/938], Loss: 0.6780032515525818\n",
      "Validation: Epoch [6], Batch [160/938], Loss: 0.7772432565689087\n",
      "Validation: Epoch [6], Batch [161/938], Loss: 0.6350831985473633\n",
      "Validation: Epoch [6], Batch [162/938], Loss: 0.6501089334487915\n",
      "Validation: Epoch [6], Batch [163/938], Loss: 0.6310740113258362\n",
      "Validation: Epoch [6], Batch [164/938], Loss: 0.5698573589324951\n",
      "Validation: Epoch [6], Batch [165/938], Loss: 0.6350541114807129\n",
      "Validation: Epoch [6], Batch [166/938], Loss: 0.699752688407898\n",
      "Validation: Epoch [6], Batch [167/938], Loss: 1.0228044986724854\n",
      "Validation: Epoch [6], Batch [168/938], Loss: 0.7774580121040344\n",
      "Validation: Epoch [6], Batch [169/938], Loss: 0.379938542842865\n",
      "Validation: Epoch [6], Batch [170/938], Loss: 0.6158962845802307\n",
      "Validation: Epoch [6], Batch [171/938], Loss: 0.5723885297775269\n",
      "Validation: Epoch [6], Batch [172/938], Loss: 0.6614653468132019\n",
      "Validation: Epoch [6], Batch [173/938], Loss: 0.6884645819664001\n",
      "Validation: Epoch [6], Batch [174/938], Loss: 0.41353753209114075\n",
      "Validation: Epoch [6], Batch [175/938], Loss: 0.47026553750038147\n",
      "Validation: Epoch [6], Batch [176/938], Loss: 0.7092936038970947\n",
      "Validation: Epoch [6], Batch [177/938], Loss: 0.4737244248390198\n",
      "Validation: Epoch [6], Batch [178/938], Loss: 0.49457231163978577\n",
      "Validation: Epoch [6], Batch [179/938], Loss: 0.611437201499939\n",
      "Validation: Epoch [6], Batch [180/938], Loss: 0.6753446459770203\n",
      "Validation: Epoch [6], Batch [181/938], Loss: 0.6190920472145081\n",
      "Validation: Epoch [6], Batch [182/938], Loss: 0.8706634044647217\n",
      "Validation: Epoch [6], Batch [183/938], Loss: 0.8614696264266968\n",
      "Validation: Epoch [6], Batch [184/938], Loss: 0.4380943775177002\n",
      "Validation: Epoch [6], Batch [185/938], Loss: 0.6770877838134766\n",
      "Validation: Epoch [6], Batch [186/938], Loss: 0.7755523920059204\n",
      "Validation: Epoch [6], Batch [187/938], Loss: 0.7274037003517151\n",
      "Validation: Epoch [6], Batch [188/938], Loss: 0.7707052230834961\n",
      "Validation: Epoch [6], Batch [189/938], Loss: 0.6748205423355103\n",
      "Validation: Epoch [6], Batch [190/938], Loss: 0.5920847058296204\n",
      "Validation: Epoch [6], Batch [191/938], Loss: 0.5914782881736755\n",
      "Validation: Epoch [6], Batch [192/938], Loss: 0.7090615034103394\n",
      "Validation: Epoch [6], Batch [193/938], Loss: 0.49296754598617554\n",
      "Validation: Epoch [6], Batch [194/938], Loss: 0.552750289440155\n",
      "Validation: Epoch [6], Batch [195/938], Loss: 0.6348013877868652\n",
      "Validation: Epoch [6], Batch [196/938], Loss: 0.6206765174865723\n",
      "Validation: Epoch [6], Batch [197/938], Loss: 0.6646280884742737\n",
      "Validation: Epoch [6], Batch [198/938], Loss: 0.6873310208320618\n",
      "Validation: Epoch [6], Batch [199/938], Loss: 0.546614408493042\n",
      "Validation: Epoch [6], Batch [200/938], Loss: 0.718848705291748\n",
      "Validation: Epoch [6], Batch [201/938], Loss: 0.5761280059814453\n",
      "Validation: Epoch [6], Batch [202/938], Loss: 0.5002567768096924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [203/938], Loss: 0.6372507810592651\n",
      "Validation: Epoch [6], Batch [204/938], Loss: 0.46270039677619934\n",
      "Validation: Epoch [6], Batch [205/938], Loss: 1.030765175819397\n",
      "Validation: Epoch [6], Batch [206/938], Loss: 0.5553444623947144\n",
      "Validation: Epoch [6], Batch [207/938], Loss: 0.48320019245147705\n",
      "Validation: Epoch [6], Batch [208/938], Loss: 0.4851875603199005\n",
      "Validation: Epoch [6], Batch [209/938], Loss: 0.9997977018356323\n",
      "Validation: Epoch [6], Batch [210/938], Loss: 0.5404490828514099\n",
      "Validation: Epoch [6], Batch [211/938], Loss: 0.44355684518814087\n",
      "Validation: Epoch [6], Batch [212/938], Loss: 0.6099514961242676\n",
      "Validation: Epoch [6], Batch [213/938], Loss: 0.7023261785507202\n",
      "Validation: Epoch [6], Batch [214/938], Loss: 0.5753276348114014\n",
      "Validation: Epoch [6], Batch [215/938], Loss: 0.7130604982376099\n",
      "Validation: Epoch [6], Batch [216/938], Loss: 0.737163782119751\n",
      "Validation: Epoch [6], Batch [217/938], Loss: 0.4568299651145935\n",
      "Validation: Epoch [6], Batch [218/938], Loss: 0.5057637691497803\n",
      "Validation: Epoch [6], Batch [219/938], Loss: 0.6462607979774475\n",
      "Validation: Epoch [6], Batch [220/938], Loss: 0.7122079730033875\n",
      "Validation: Epoch [6], Batch [221/938], Loss: 0.5920636653900146\n",
      "Validation: Epoch [6], Batch [222/938], Loss: 0.5354921817779541\n",
      "Validation: Epoch [6], Batch [223/938], Loss: 0.647754430770874\n",
      "Validation: Epoch [6], Batch [224/938], Loss: 0.9018425941467285\n",
      "Validation: Epoch [6], Batch [225/938], Loss: 0.7184618711471558\n",
      "Validation: Epoch [6], Batch [226/938], Loss: 0.6560567617416382\n",
      "Validation: Epoch [6], Batch [227/938], Loss: 0.602552056312561\n",
      "Validation: Epoch [6], Batch [228/938], Loss: 0.6489397883415222\n",
      "Validation: Epoch [6], Batch [229/938], Loss: 0.5546543002128601\n",
      "Validation: Epoch [6], Batch [230/938], Loss: 0.5971148014068604\n",
      "Validation: Epoch [6], Batch [231/938], Loss: 0.540922999382019\n",
      "Validation: Epoch [6], Batch [232/938], Loss: 0.7090268135070801\n",
      "Validation: Epoch [6], Batch [233/938], Loss: 0.8131048679351807\n",
      "Validation: Epoch [6], Batch [234/938], Loss: 0.6900233030319214\n",
      "Validation: Epoch [6], Batch [235/938], Loss: 0.8884361386299133\n",
      "Validation: Epoch [6], Batch [236/938], Loss: 0.5012285113334656\n",
      "Validation: Epoch [6], Batch [237/938], Loss: 0.5557081699371338\n",
      "Validation: Epoch [6], Batch [238/938], Loss: 0.43769240379333496\n",
      "Validation: Epoch [6], Batch [239/938], Loss: 0.6232277154922485\n",
      "Validation: Epoch [6], Batch [240/938], Loss: 0.58414626121521\n",
      "Validation: Epoch [6], Batch [241/938], Loss: 0.7898288369178772\n",
      "Validation: Epoch [6], Batch [242/938], Loss: 0.4913387894630432\n",
      "Validation: Epoch [6], Batch [243/938], Loss: 0.44859373569488525\n",
      "Validation: Epoch [6], Batch [244/938], Loss: 0.5943953394889832\n",
      "Validation: Epoch [6], Batch [245/938], Loss: 0.7042029500007629\n",
      "Validation: Epoch [6], Batch [246/938], Loss: 0.58055579662323\n",
      "Validation: Epoch [6], Batch [247/938], Loss: 0.6430956125259399\n",
      "Validation: Epoch [6], Batch [248/938], Loss: 0.5985344648361206\n",
      "Validation: Epoch [6], Batch [249/938], Loss: 0.6593292951583862\n",
      "Validation: Epoch [6], Batch [250/938], Loss: 0.5706029534339905\n",
      "Validation: Epoch [6], Batch [251/938], Loss: 0.5430160760879517\n",
      "Validation: Epoch [6], Batch [252/938], Loss: 0.7192132472991943\n",
      "Validation: Epoch [6], Batch [253/938], Loss: 0.658022403717041\n",
      "Validation: Epoch [6], Batch [254/938], Loss: 0.8074856996536255\n",
      "Validation: Epoch [6], Batch [255/938], Loss: 0.6050300598144531\n",
      "Validation: Epoch [6], Batch [256/938], Loss: 0.6905376315116882\n",
      "Validation: Epoch [6], Batch [257/938], Loss: 0.6424646377563477\n",
      "Validation: Epoch [6], Batch [258/938], Loss: 0.7310078144073486\n",
      "Validation: Epoch [6], Batch [259/938], Loss: 0.47217071056365967\n",
      "Validation: Epoch [6], Batch [260/938], Loss: 0.47955822944641113\n",
      "Validation: Epoch [6], Batch [261/938], Loss: 0.5147034525871277\n",
      "Validation: Epoch [6], Batch [262/938], Loss: 0.5408509969711304\n",
      "Validation: Epoch [6], Batch [263/938], Loss: 0.6698163151741028\n",
      "Validation: Epoch [6], Batch [264/938], Loss: 0.6568294763565063\n",
      "Validation: Epoch [6], Batch [265/938], Loss: 0.7788221836090088\n",
      "Validation: Epoch [6], Batch [266/938], Loss: 0.5889401435852051\n",
      "Validation: Epoch [6], Batch [267/938], Loss: 0.5829538106918335\n",
      "Validation: Epoch [6], Batch [268/938], Loss: 0.6792083382606506\n",
      "Validation: Epoch [6], Batch [269/938], Loss: 0.5151679515838623\n",
      "Validation: Epoch [6], Batch [270/938], Loss: 0.7173480987548828\n",
      "Validation: Epoch [6], Batch [271/938], Loss: 0.5463916063308716\n",
      "Validation: Epoch [6], Batch [272/938], Loss: 0.8175864219665527\n",
      "Validation: Epoch [6], Batch [273/938], Loss: 0.677664577960968\n",
      "Validation: Epoch [6], Batch [274/938], Loss: 0.5049130916595459\n",
      "Validation: Epoch [6], Batch [275/938], Loss: 0.6363272666931152\n",
      "Validation: Epoch [6], Batch [276/938], Loss: 0.525094747543335\n",
      "Validation: Epoch [6], Batch [277/938], Loss: 0.5247284770011902\n",
      "Validation: Epoch [6], Batch [278/938], Loss: 0.6880473494529724\n",
      "Validation: Epoch [6], Batch [279/938], Loss: 0.8287786245346069\n",
      "Validation: Epoch [6], Batch [280/938], Loss: 0.5538345575332642\n",
      "Validation: Epoch [6], Batch [281/938], Loss: 0.44902241230010986\n",
      "Validation: Epoch [6], Batch [282/938], Loss: 0.7024140357971191\n",
      "Validation: Epoch [6], Batch [283/938], Loss: 0.5685021281242371\n",
      "Validation: Epoch [6], Batch [284/938], Loss: 0.718542754650116\n",
      "Validation: Epoch [6], Batch [285/938], Loss: 0.6355858445167542\n",
      "Validation: Epoch [6], Batch [286/938], Loss: 0.660732626914978\n",
      "Validation: Epoch [6], Batch [287/938], Loss: 0.6709173917770386\n",
      "Validation: Epoch [6], Batch [288/938], Loss: 0.5234405994415283\n",
      "Validation: Epoch [6], Batch [289/938], Loss: 0.577608585357666\n",
      "Validation: Epoch [6], Batch [290/938], Loss: 0.8542317152023315\n",
      "Validation: Epoch [6], Batch [291/938], Loss: 0.6790882349014282\n",
      "Validation: Epoch [6], Batch [292/938], Loss: 0.5181978940963745\n",
      "Validation: Epoch [6], Batch [293/938], Loss: 0.49969950318336487\n",
      "Validation: Epoch [6], Batch [294/938], Loss: 0.7042931914329529\n",
      "Validation: Epoch [6], Batch [295/938], Loss: 0.7023693323135376\n",
      "Validation: Epoch [6], Batch [296/938], Loss: 0.4231860041618347\n",
      "Validation: Epoch [6], Batch [297/938], Loss: 0.6431344747543335\n",
      "Validation: Epoch [6], Batch [298/938], Loss: 0.5841810703277588\n",
      "Validation: Epoch [6], Batch [299/938], Loss: 0.6364139318466187\n",
      "Validation: Epoch [6], Batch [300/938], Loss: 0.6154091358184814\n",
      "Validation: Epoch [6], Batch [301/938], Loss: 0.6927760243415833\n",
      "Validation: Epoch [6], Batch [302/938], Loss: 0.7225438356399536\n",
      "Validation: Epoch [6], Batch [303/938], Loss: 0.6237174272537231\n",
      "Validation: Epoch [6], Batch [304/938], Loss: 0.546197772026062\n",
      "Validation: Epoch [6], Batch [305/938], Loss: 0.4656343162059784\n",
      "Validation: Epoch [6], Batch [306/938], Loss: 0.6677225828170776\n",
      "Validation: Epoch [6], Batch [307/938], Loss: 0.6421775221824646\n",
      "Validation: Epoch [6], Batch [308/938], Loss: 0.4713314175605774\n",
      "Validation: Epoch [6], Batch [309/938], Loss: 0.39425718784332275\n",
      "Validation: Epoch [6], Batch [310/938], Loss: 0.556169331073761\n",
      "Validation: Epoch [6], Batch [311/938], Loss: 0.6803871393203735\n",
      "Validation: Epoch [6], Batch [312/938], Loss: 0.45892730355262756\n",
      "Validation: Epoch [6], Batch [313/938], Loss: 0.5811159610748291\n",
      "Validation: Epoch [6], Batch [314/938], Loss: 0.6816056966781616\n",
      "Validation: Epoch [6], Batch [315/938], Loss: 0.45701736211776733\n",
      "Validation: Epoch [6], Batch [316/938], Loss: 0.6804656982421875\n",
      "Validation: Epoch [6], Batch [317/938], Loss: 0.6083735227584839\n",
      "Validation: Epoch [6], Batch [318/938], Loss: 0.5035973787307739\n",
      "Validation: Epoch [6], Batch [319/938], Loss: 0.6009881496429443\n",
      "Validation: Epoch [6], Batch [320/938], Loss: 0.4830418527126312\n",
      "Validation: Epoch [6], Batch [321/938], Loss: 0.7353982329368591\n",
      "Validation: Epoch [6], Batch [322/938], Loss: 0.859919548034668\n",
      "Validation: Epoch [6], Batch [323/938], Loss: 0.4518880844116211\n",
      "Validation: Epoch [6], Batch [324/938], Loss: 0.817072868347168\n",
      "Validation: Epoch [6], Batch [325/938], Loss: 0.6377988457679749\n",
      "Validation: Epoch [6], Batch [326/938], Loss: 0.6386296153068542\n",
      "Validation: Epoch [6], Batch [327/938], Loss: 0.5911380052566528\n",
      "Validation: Epoch [6], Batch [328/938], Loss: 0.704001784324646\n",
      "Validation: Epoch [6], Batch [329/938], Loss: 0.847629189491272\n",
      "Validation: Epoch [6], Batch [330/938], Loss: 0.8231162428855896\n",
      "Validation: Epoch [6], Batch [331/938], Loss: 0.7382667660713196\n",
      "Validation: Epoch [6], Batch [332/938], Loss: 0.638221025466919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [333/938], Loss: 0.6878662109375\n",
      "Validation: Epoch [6], Batch [334/938], Loss: 0.6238588094711304\n",
      "Validation: Epoch [6], Batch [335/938], Loss: 0.611652135848999\n",
      "Validation: Epoch [6], Batch [336/938], Loss: 0.46005916595458984\n",
      "Validation: Epoch [6], Batch [337/938], Loss: 0.7776970863342285\n",
      "Validation: Epoch [6], Batch [338/938], Loss: 0.5277823209762573\n",
      "Validation: Epoch [6], Batch [339/938], Loss: 0.5102474093437195\n",
      "Validation: Epoch [6], Batch [340/938], Loss: 0.7900435924530029\n",
      "Validation: Epoch [6], Batch [341/938], Loss: 0.8256585597991943\n",
      "Validation: Epoch [6], Batch [342/938], Loss: 0.6336466670036316\n",
      "Validation: Epoch [6], Batch [343/938], Loss: 0.630018949508667\n",
      "Validation: Epoch [6], Batch [344/938], Loss: 0.760106086730957\n",
      "Validation: Epoch [6], Batch [345/938], Loss: 0.5339514017105103\n",
      "Validation: Epoch [6], Batch [346/938], Loss: 0.7420390248298645\n",
      "Validation: Epoch [6], Batch [347/938], Loss: 0.5375545024871826\n",
      "Validation: Epoch [6], Batch [348/938], Loss: 0.6638990044593811\n",
      "Validation: Epoch [6], Batch [349/938], Loss: 0.43515053391456604\n",
      "Validation: Epoch [6], Batch [350/938], Loss: 0.4287755489349365\n",
      "Validation: Epoch [6], Batch [351/938], Loss: 0.6394511461257935\n",
      "Validation: Epoch [6], Batch [352/938], Loss: 0.6755232810974121\n",
      "Validation: Epoch [6], Batch [353/938], Loss: 0.48401397466659546\n",
      "Validation: Epoch [6], Batch [354/938], Loss: 0.5250024199485779\n",
      "Validation: Epoch [6], Batch [355/938], Loss: 0.6852096319198608\n",
      "Validation: Epoch [6], Batch [356/938], Loss: 0.53810715675354\n",
      "Validation: Epoch [6], Batch [357/938], Loss: 0.5921026468276978\n",
      "Validation: Epoch [6], Batch [358/938], Loss: 0.5850788354873657\n",
      "Validation: Epoch [6], Batch [359/938], Loss: 0.5867066383361816\n",
      "Validation: Epoch [6], Batch [360/938], Loss: 0.5833086967468262\n",
      "Validation: Epoch [6], Batch [361/938], Loss: 0.7730374336242676\n",
      "Validation: Epoch [6], Batch [362/938], Loss: 0.9154520630836487\n",
      "Validation: Epoch [6], Batch [363/938], Loss: 0.4853564202785492\n",
      "Validation: Epoch [6], Batch [364/938], Loss: 0.9790253043174744\n",
      "Validation: Epoch [6], Batch [365/938], Loss: 0.32368600368499756\n",
      "Validation: Epoch [6], Batch [366/938], Loss: 0.56412273645401\n",
      "Validation: Epoch [6], Batch [367/938], Loss: 0.6953753232955933\n",
      "Validation: Epoch [6], Batch [368/938], Loss: 0.5374127626419067\n",
      "Validation: Epoch [6], Batch [369/938], Loss: 0.6167227625846863\n",
      "Validation: Epoch [6], Batch [370/938], Loss: 0.6113409399986267\n",
      "Validation: Epoch [6], Batch [371/938], Loss: 0.8093684315681458\n",
      "Validation: Epoch [6], Batch [372/938], Loss: 0.6090729236602783\n",
      "Validation: Epoch [6], Batch [373/938], Loss: 0.6590243577957153\n",
      "Validation: Epoch [6], Batch [374/938], Loss: 0.6062735319137573\n",
      "Validation: Epoch [6], Batch [375/938], Loss: 0.5344662070274353\n",
      "Validation: Epoch [6], Batch [376/938], Loss: 0.5490149259567261\n",
      "Validation: Epoch [6], Batch [377/938], Loss: 0.4299674332141876\n",
      "Validation: Epoch [6], Batch [378/938], Loss: 0.5068286657333374\n",
      "Validation: Epoch [6], Batch [379/938], Loss: 0.551702618598938\n",
      "Validation: Epoch [6], Batch [380/938], Loss: 0.7977221012115479\n",
      "Validation: Epoch [6], Batch [381/938], Loss: 0.6174430251121521\n",
      "Validation: Epoch [6], Batch [382/938], Loss: 0.47497326135635376\n",
      "Validation: Epoch [6], Batch [383/938], Loss: 0.3615635335445404\n",
      "Validation: Epoch [6], Batch [384/938], Loss: 0.6621067523956299\n",
      "Validation: Epoch [6], Batch [385/938], Loss: 0.5896430015563965\n",
      "Validation: Epoch [6], Batch [386/938], Loss: 0.555816650390625\n",
      "Validation: Epoch [6], Batch [387/938], Loss: 0.8985317945480347\n",
      "Validation: Epoch [6], Batch [388/938], Loss: 0.6699420213699341\n",
      "Validation: Epoch [6], Batch [389/938], Loss: 0.6281501054763794\n",
      "Validation: Epoch [6], Batch [390/938], Loss: 0.7887612581253052\n",
      "Validation: Epoch [6], Batch [391/938], Loss: 0.5254796147346497\n",
      "Validation: Epoch [6], Batch [392/938], Loss: 0.858060359954834\n",
      "Validation: Epoch [6], Batch [393/938], Loss: 0.7030361890792847\n",
      "Validation: Epoch [6], Batch [394/938], Loss: 0.4888501763343811\n",
      "Validation: Epoch [6], Batch [395/938], Loss: 0.6303712129592896\n",
      "Validation: Epoch [6], Batch [396/938], Loss: 0.6925118565559387\n",
      "Validation: Epoch [6], Batch [397/938], Loss: 0.7451768517494202\n",
      "Validation: Epoch [6], Batch [398/938], Loss: 0.9079554080963135\n",
      "Validation: Epoch [6], Batch [399/938], Loss: 0.4745578169822693\n",
      "Validation: Epoch [6], Batch [400/938], Loss: 0.5978670120239258\n",
      "Validation: Epoch [6], Batch [401/938], Loss: 0.43401598930358887\n",
      "Validation: Epoch [6], Batch [402/938], Loss: 0.7458940148353577\n",
      "Validation: Epoch [6], Batch [403/938], Loss: 0.5776619911193848\n",
      "Validation: Epoch [6], Batch [404/938], Loss: 0.5570108890533447\n",
      "Validation: Epoch [6], Batch [405/938], Loss: 0.8141465187072754\n",
      "Validation: Epoch [6], Batch [406/938], Loss: 0.6077185869216919\n",
      "Validation: Epoch [6], Batch [407/938], Loss: 0.6320421099662781\n",
      "Validation: Epoch [6], Batch [408/938], Loss: 0.584359347820282\n",
      "Validation: Epoch [6], Batch [409/938], Loss: 0.8404849171638489\n",
      "Validation: Epoch [6], Batch [410/938], Loss: 0.49116960167884827\n",
      "Validation: Epoch [6], Batch [411/938], Loss: 0.4804319143295288\n",
      "Validation: Epoch [6], Batch [412/938], Loss: 0.627949595451355\n",
      "Validation: Epoch [6], Batch [413/938], Loss: 0.5641561150550842\n",
      "Validation: Epoch [6], Batch [414/938], Loss: 0.6936274766921997\n",
      "Validation: Epoch [6], Batch [415/938], Loss: 0.724869966506958\n",
      "Validation: Epoch [6], Batch [416/938], Loss: 0.5461433529853821\n",
      "Validation: Epoch [6], Batch [417/938], Loss: 0.5631899833679199\n",
      "Validation: Epoch [6], Batch [418/938], Loss: 0.7021064758300781\n",
      "Validation: Epoch [6], Batch [419/938], Loss: 0.6307275295257568\n",
      "Validation: Epoch [6], Batch [420/938], Loss: 0.7624902725219727\n",
      "Validation: Epoch [6], Batch [421/938], Loss: 0.5789477825164795\n",
      "Validation: Epoch [6], Batch [422/938], Loss: 0.518352210521698\n",
      "Validation: Epoch [6], Batch [423/938], Loss: 0.5553960800170898\n",
      "Validation: Epoch [6], Batch [424/938], Loss: 0.569653332233429\n",
      "Validation: Epoch [6], Batch [425/938], Loss: 0.5631658434867859\n",
      "Validation: Epoch [6], Batch [426/938], Loss: 0.786734938621521\n",
      "Validation: Epoch [6], Batch [427/938], Loss: 0.6279851198196411\n",
      "Validation: Epoch [6], Batch [428/938], Loss: 0.5311307311058044\n",
      "Validation: Epoch [6], Batch [429/938], Loss: 0.6453271508216858\n",
      "Validation: Epoch [6], Batch [430/938], Loss: 0.5753921866416931\n",
      "Validation: Epoch [6], Batch [431/938], Loss: 0.5410027503967285\n",
      "Validation: Epoch [6], Batch [432/938], Loss: 0.4988860785961151\n",
      "Validation: Epoch [6], Batch [433/938], Loss: 0.6035542488098145\n",
      "Validation: Epoch [6], Batch [434/938], Loss: 0.6668059825897217\n",
      "Validation: Epoch [6], Batch [435/938], Loss: 0.4648612141609192\n",
      "Validation: Epoch [6], Batch [436/938], Loss: 0.770579993724823\n",
      "Validation: Epoch [6], Batch [437/938], Loss: 0.5457621216773987\n",
      "Validation: Epoch [6], Batch [438/938], Loss: 0.5365651249885559\n",
      "Validation: Epoch [6], Batch [439/938], Loss: 0.5853503942489624\n",
      "Validation: Epoch [6], Batch [440/938], Loss: 0.6085455417633057\n",
      "Validation: Epoch [6], Batch [441/938], Loss: 0.40613052248954773\n",
      "Validation: Epoch [6], Batch [442/938], Loss: 0.44087865948677063\n",
      "Validation: Epoch [6], Batch [443/938], Loss: 0.6119320392608643\n",
      "Validation: Epoch [6], Batch [444/938], Loss: 0.4809553325176239\n",
      "Validation: Epoch [6], Batch [445/938], Loss: 0.5153338313102722\n",
      "Validation: Epoch [6], Batch [446/938], Loss: 0.5154780149459839\n",
      "Validation: Epoch [6], Batch [447/938], Loss: 0.895505428314209\n",
      "Validation: Epoch [6], Batch [448/938], Loss: 0.7704960107803345\n",
      "Validation: Epoch [6], Batch [449/938], Loss: 0.6079439520835876\n",
      "Validation: Epoch [6], Batch [450/938], Loss: 0.6472454071044922\n",
      "Validation: Epoch [6], Batch [451/938], Loss: 0.5697047710418701\n",
      "Validation: Epoch [6], Batch [452/938], Loss: 0.7198526859283447\n",
      "Validation: Epoch [6], Batch [453/938], Loss: 0.8349626660346985\n",
      "Validation: Epoch [6], Batch [454/938], Loss: 0.7412084341049194\n",
      "Validation: Epoch [6], Batch [455/938], Loss: 0.5264870524406433\n",
      "Validation: Epoch [6], Batch [456/938], Loss: 0.3901432752609253\n",
      "Validation: Epoch [6], Batch [457/938], Loss: 0.7012194395065308\n",
      "Validation: Epoch [6], Batch [458/938], Loss: 0.7073854207992554\n",
      "Validation: Epoch [6], Batch [459/938], Loss: 0.6442522406578064\n",
      "Validation: Epoch [6], Batch [460/938], Loss: 0.657579779624939\n",
      "Validation: Epoch [6], Batch [461/938], Loss: 0.7170587778091431\n",
      "Validation: Epoch [6], Batch [462/938], Loss: 0.42471784353256226\n",
      "Validation: Epoch [6], Batch [463/938], Loss: 0.49106478691101074\n",
      "Validation: Epoch [6], Batch [464/938], Loss: 0.5946352481842041\n",
      "Validation: Epoch [6], Batch [465/938], Loss: 0.5656653642654419\n",
      "Validation: Epoch [6], Batch [466/938], Loss: 0.7724477648735046\n",
      "Validation: Epoch [6], Batch [467/938], Loss: 0.6217901706695557\n",
      "Validation: Epoch [6], Batch [468/938], Loss: 0.7537382245063782\n",
      "Validation: Epoch [6], Batch [469/938], Loss: 0.7014113664627075\n",
      "Validation: Epoch [6], Batch [470/938], Loss: 0.6352444291114807\n",
      "Validation: Epoch [6], Batch [471/938], Loss: 0.5551978945732117\n",
      "Validation: Epoch [6], Batch [472/938], Loss: 0.460256963968277\n",
      "Validation: Epoch [6], Batch [473/938], Loss: 0.6226626038551331\n",
      "Validation: Epoch [6], Batch [474/938], Loss: 0.7721774578094482\n",
      "Validation: Epoch [6], Batch [475/938], Loss: 0.6652203798294067\n",
      "Validation: Epoch [6], Batch [476/938], Loss: 0.5147014856338501\n",
      "Validation: Epoch [6], Batch [477/938], Loss: 0.6298599243164062\n",
      "Validation: Epoch [6], Batch [478/938], Loss: 0.64240962266922\n",
      "Validation: Epoch [6], Batch [479/938], Loss: 0.6381614804267883\n",
      "Validation: Epoch [6], Batch [480/938], Loss: 0.7118644714355469\n",
      "Validation: Epoch [6], Batch [481/938], Loss: 0.4851638078689575\n",
      "Validation: Epoch [6], Batch [482/938], Loss: 0.5318045616149902\n",
      "Validation: Epoch [6], Batch [483/938], Loss: 0.5300006866455078\n",
      "Validation: Epoch [6], Batch [484/938], Loss: 0.7645971775054932\n",
      "Validation: Epoch [6], Batch [485/938], Loss: 0.5701193809509277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [486/938], Loss: 0.5564438104629517\n",
      "Validation: Epoch [6], Batch [487/938], Loss: 0.5655414462089539\n",
      "Validation: Epoch [6], Batch [488/938], Loss: 0.45617467164993286\n",
      "Validation: Epoch [6], Batch [489/938], Loss: 0.629002571105957\n",
      "Validation: Epoch [6], Batch [490/938], Loss: 0.7141813039779663\n",
      "Validation: Epoch [6], Batch [491/938], Loss: 0.6836563348770142\n",
      "Validation: Epoch [6], Batch [492/938], Loss: 0.7287728786468506\n",
      "Validation: Epoch [6], Batch [493/938], Loss: 0.555829644203186\n",
      "Validation: Epoch [6], Batch [494/938], Loss: 0.6337926387786865\n",
      "Validation: Epoch [6], Batch [495/938], Loss: 0.5228801965713501\n",
      "Validation: Epoch [6], Batch [496/938], Loss: 0.7175859212875366\n",
      "Validation: Epoch [6], Batch [497/938], Loss: 0.5853638648986816\n",
      "Validation: Epoch [6], Batch [498/938], Loss: 0.7060345411300659\n",
      "Validation: Epoch [6], Batch [499/938], Loss: 0.8230396509170532\n",
      "Validation: Epoch [6], Batch [500/938], Loss: 0.661300539970398\n",
      "Validation: Epoch [6], Batch [501/938], Loss: 0.5329983234405518\n",
      "Validation: Epoch [6], Batch [502/938], Loss: 0.4684271812438965\n",
      "Validation: Epoch [6], Batch [503/938], Loss: 0.6313953399658203\n",
      "Validation: Epoch [6], Batch [504/938], Loss: 0.7843512296676636\n",
      "Validation: Epoch [6], Batch [505/938], Loss: 0.6660274267196655\n",
      "Validation: Epoch [6], Batch [506/938], Loss: 0.5987810492515564\n",
      "Validation: Epoch [6], Batch [507/938], Loss: 0.6528587341308594\n",
      "Validation: Epoch [6], Batch [508/938], Loss: 0.4958968758583069\n",
      "Validation: Epoch [6], Batch [509/938], Loss: 0.6946605443954468\n",
      "Validation: Epoch [6], Batch [510/938], Loss: 0.498906672000885\n",
      "Validation: Epoch [6], Batch [511/938], Loss: 0.7176567316055298\n",
      "Validation: Epoch [6], Batch [512/938], Loss: 0.771240770816803\n",
      "Validation: Epoch [6], Batch [513/938], Loss: 0.6889468431472778\n",
      "Validation: Epoch [6], Batch [514/938], Loss: 0.6650774478912354\n",
      "Validation: Epoch [6], Batch [515/938], Loss: 0.6078108549118042\n",
      "Validation: Epoch [6], Batch [516/938], Loss: 0.6662435531616211\n",
      "Validation: Epoch [6], Batch [517/938], Loss: 0.5785330533981323\n",
      "Validation: Epoch [6], Batch [518/938], Loss: 0.576383113861084\n",
      "Validation: Epoch [6], Batch [519/938], Loss: 0.8017067313194275\n",
      "Validation: Epoch [6], Batch [520/938], Loss: 0.6062232255935669\n",
      "Validation: Epoch [6], Batch [521/938], Loss: 0.8629159927368164\n",
      "Validation: Epoch [6], Batch [522/938], Loss: 0.5376275181770325\n",
      "Validation: Epoch [6], Batch [523/938], Loss: 0.4866381883621216\n",
      "Validation: Epoch [6], Batch [524/938], Loss: 0.669695258140564\n",
      "Validation: Epoch [6], Batch [525/938], Loss: 0.8351297378540039\n",
      "Validation: Epoch [6], Batch [526/938], Loss: 0.8700266480445862\n",
      "Validation: Epoch [6], Batch [527/938], Loss: 0.6173914670944214\n",
      "Validation: Epoch [6], Batch [528/938], Loss: 0.5488328337669373\n",
      "Validation: Epoch [6], Batch [529/938], Loss: 0.6650905609130859\n",
      "Validation: Epoch [6], Batch [530/938], Loss: 0.4916505217552185\n",
      "Validation: Epoch [6], Batch [531/938], Loss: 0.37597471475601196\n",
      "Validation: Epoch [6], Batch [532/938], Loss: 0.581958532333374\n",
      "Validation: Epoch [6], Batch [533/938], Loss: 0.42655882239341736\n",
      "Validation: Epoch [6], Batch [534/938], Loss: 0.700310230255127\n",
      "Validation: Epoch [6], Batch [535/938], Loss: 0.4588749408721924\n",
      "Validation: Epoch [6], Batch [536/938], Loss: 0.45706743001937866\n",
      "Validation: Epoch [6], Batch [537/938], Loss: 0.5505810976028442\n",
      "Validation: Epoch [6], Batch [538/938], Loss: 0.5123907327651978\n",
      "Validation: Epoch [6], Batch [539/938], Loss: 0.6990365982055664\n",
      "Validation: Epoch [6], Batch [540/938], Loss: 0.5459468364715576\n",
      "Validation: Epoch [6], Batch [541/938], Loss: 0.5125495195388794\n",
      "Validation: Epoch [6], Batch [542/938], Loss: 0.7063857316970825\n",
      "Validation: Epoch [6], Batch [543/938], Loss: 0.488336443901062\n",
      "Validation: Epoch [6], Batch [544/938], Loss: 0.6467458605766296\n",
      "Validation: Epoch [6], Batch [545/938], Loss: 0.6449775099754333\n",
      "Validation: Epoch [6], Batch [546/938], Loss: 0.7168889045715332\n",
      "Validation: Epoch [6], Batch [547/938], Loss: 0.5835156440734863\n",
      "Validation: Epoch [6], Batch [548/938], Loss: 0.5664328932762146\n",
      "Validation: Epoch [6], Batch [549/938], Loss: 0.6112942695617676\n",
      "Validation: Epoch [6], Batch [550/938], Loss: 0.6088547706604004\n",
      "Validation: Epoch [6], Batch [551/938], Loss: 0.6924124360084534\n",
      "Validation: Epoch [6], Batch [552/938], Loss: 0.5225580930709839\n",
      "Validation: Epoch [6], Batch [553/938], Loss: 0.6668457984924316\n",
      "Validation: Epoch [6], Batch [554/938], Loss: 0.7109229564666748\n",
      "Validation: Epoch [6], Batch [555/938], Loss: 0.6635334491729736\n",
      "Validation: Epoch [6], Batch [556/938], Loss: 0.6778858304023743\n",
      "Validation: Epoch [6], Batch [557/938], Loss: 0.5482680797576904\n",
      "Validation: Epoch [6], Batch [558/938], Loss: 0.7290184497833252\n",
      "Validation: Epoch [6], Batch [559/938], Loss: 0.46855348348617554\n",
      "Validation: Epoch [6], Batch [560/938], Loss: 0.596848726272583\n",
      "Validation: Epoch [6], Batch [561/938], Loss: 0.7007240056991577\n",
      "Validation: Epoch [6], Batch [562/938], Loss: 0.5373598337173462\n",
      "Validation: Epoch [6], Batch [563/938], Loss: 0.6038511395454407\n",
      "Validation: Epoch [6], Batch [564/938], Loss: 0.5222993493080139\n",
      "Validation: Epoch [6], Batch [565/938], Loss: 0.581071674823761\n",
      "Validation: Epoch [6], Batch [566/938], Loss: 0.6471844911575317\n",
      "Validation: Epoch [6], Batch [567/938], Loss: 0.6584885120391846\n",
      "Validation: Epoch [6], Batch [568/938], Loss: 0.43617504835128784\n",
      "Validation: Epoch [6], Batch [569/938], Loss: 0.5096020102500916\n",
      "Validation: Epoch [6], Batch [570/938], Loss: 0.8975242376327515\n",
      "Validation: Epoch [6], Batch [571/938], Loss: 0.56309574842453\n",
      "Validation: Epoch [6], Batch [572/938], Loss: 0.47255557775497437\n",
      "Validation: Epoch [6], Batch [573/938], Loss: 0.41658270359039307\n",
      "Validation: Epoch [6], Batch [574/938], Loss: 0.5046653747558594\n",
      "Validation: Epoch [6], Batch [575/938], Loss: 0.619280993938446\n",
      "Validation: Epoch [6], Batch [576/938], Loss: 0.8751124143600464\n",
      "Validation: Epoch [6], Batch [577/938], Loss: 0.5905144214630127\n",
      "Validation: Epoch [6], Batch [578/938], Loss: 0.6133392453193665\n",
      "Validation: Epoch [6], Batch [579/938], Loss: 0.7323397397994995\n",
      "Validation: Epoch [6], Batch [580/938], Loss: 0.6807674765586853\n",
      "Validation: Epoch [6], Batch [581/938], Loss: 0.6800745129585266\n",
      "Validation: Epoch [6], Batch [582/938], Loss: 0.6081780791282654\n",
      "Validation: Epoch [6], Batch [583/938], Loss: 0.44670891761779785\n",
      "Validation: Epoch [6], Batch [584/938], Loss: 0.532484233379364\n",
      "Validation: Epoch [6], Batch [585/938], Loss: 0.5317671895027161\n",
      "Validation: Epoch [6], Batch [586/938], Loss: 0.6738541126251221\n",
      "Validation: Epoch [6], Batch [587/938], Loss: 0.5476067066192627\n",
      "Validation: Epoch [6], Batch [588/938], Loss: 0.7050420045852661\n",
      "Validation: Epoch [6], Batch [589/938], Loss: 0.5809133648872375\n",
      "Validation: Epoch [6], Batch [590/938], Loss: 0.7013736963272095\n",
      "Validation: Epoch [6], Batch [591/938], Loss: 1.0114209651947021\n",
      "Validation: Epoch [6], Batch [592/938], Loss: 0.7946428060531616\n",
      "Validation: Epoch [6], Batch [593/938], Loss: 0.48413893580436707\n",
      "Validation: Epoch [6], Batch [594/938], Loss: 0.6427321434020996\n",
      "Validation: Epoch [6], Batch [595/938], Loss: 0.4967125356197357\n",
      "Validation: Epoch [6], Batch [596/938], Loss: 0.6769706010818481\n",
      "Validation: Epoch [6], Batch [597/938], Loss: 0.5791577100753784\n",
      "Validation: Epoch [6], Batch [598/938], Loss: 0.6099720001220703\n",
      "Validation: Epoch [6], Batch [599/938], Loss: 0.5266475677490234\n",
      "Validation: Epoch [6], Batch [600/938], Loss: 0.7453125715255737\n",
      "Validation: Epoch [6], Batch [601/938], Loss: 0.657663106918335\n",
      "Validation: Epoch [6], Batch [602/938], Loss: 0.510984480381012\n",
      "Validation: Epoch [6], Batch [603/938], Loss: 0.789326012134552\n",
      "Validation: Epoch [6], Batch [604/938], Loss: 0.4401248097419739\n",
      "Validation: Epoch [6], Batch [605/938], Loss: 0.6741560697555542\n",
      "Validation: Epoch [6], Batch [606/938], Loss: 0.6482231020927429\n",
      "Validation: Epoch [6], Batch [607/938], Loss: 0.6186079978942871\n",
      "Validation: Epoch [6], Batch [608/938], Loss: 0.5418817400932312\n",
      "Validation: Epoch [6], Batch [609/938], Loss: 0.3845165967941284\n",
      "Validation: Epoch [6], Batch [610/938], Loss: 0.6209352612495422\n",
      "Validation: Epoch [6], Batch [611/938], Loss: 0.47764840722084045\n",
      "Validation: Epoch [6], Batch [612/938], Loss: 0.6157089471817017\n",
      "Validation: Epoch [6], Batch [613/938], Loss: 0.7791082859039307\n",
      "Validation: Epoch [6], Batch [614/938], Loss: 0.49831897020339966\n",
      "Validation: Epoch [6], Batch [615/938], Loss: 0.5584279298782349\n",
      "Validation: Epoch [6], Batch [616/938], Loss: 0.5034987926483154\n",
      "Validation: Epoch [6], Batch [617/938], Loss: 0.5049444437026978\n",
      "Validation: Epoch [6], Batch [618/938], Loss: 0.4715973734855652\n",
      "Validation: Epoch [6], Batch [619/938], Loss: 0.6274337768554688\n",
      "Validation: Epoch [6], Batch [620/938], Loss: 0.7383012771606445\n",
      "Validation: Epoch [6], Batch [621/938], Loss: 0.5397247076034546\n",
      "Validation: Epoch [6], Batch [622/938], Loss: 0.6107709407806396\n",
      "Validation: Epoch [6], Batch [623/938], Loss: 0.6701951026916504\n",
      "Validation: Epoch [6], Batch [624/938], Loss: 0.6075596809387207\n",
      "Validation: Epoch [6], Batch [625/938], Loss: 0.6349455118179321\n",
      "Validation: Epoch [6], Batch [626/938], Loss: 0.6327697038650513\n",
      "Validation: Epoch [6], Batch [627/938], Loss: 0.5784099698066711\n",
      "Validation: Epoch [6], Batch [628/938], Loss: 0.43918290734291077\n",
      "Validation: Epoch [6], Batch [629/938], Loss: 0.5468909740447998\n",
      "Validation: Epoch [6], Batch [630/938], Loss: 0.6937510967254639\n",
      "Validation: Epoch [6], Batch [631/938], Loss: 0.43741917610168457\n",
      "Validation: Epoch [6], Batch [632/938], Loss: 0.5662276148796082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [633/938], Loss: 0.6023398637771606\n",
      "Validation: Epoch [6], Batch [634/938], Loss: 0.5392642021179199\n",
      "Validation: Epoch [6], Batch [635/938], Loss: 0.6303459405899048\n",
      "Validation: Epoch [6], Batch [636/938], Loss: 0.7688347101211548\n",
      "Validation: Epoch [6], Batch [637/938], Loss: 0.5239549875259399\n",
      "Validation: Epoch [6], Batch [638/938], Loss: 0.6760201454162598\n",
      "Validation: Epoch [6], Batch [639/938], Loss: 0.6308953762054443\n",
      "Validation: Epoch [6], Batch [640/938], Loss: 0.8291594982147217\n",
      "Validation: Epoch [6], Batch [641/938], Loss: 0.4060285687446594\n",
      "Validation: Epoch [6], Batch [642/938], Loss: 0.6041627526283264\n",
      "Validation: Epoch [6], Batch [643/938], Loss: 0.8479967713356018\n",
      "Validation: Epoch [6], Batch [644/938], Loss: 0.5920202732086182\n",
      "Validation: Epoch [6], Batch [645/938], Loss: 0.6015141010284424\n",
      "Validation: Epoch [6], Batch [646/938], Loss: 0.5663886666297913\n",
      "Validation: Epoch [6], Batch [647/938], Loss: 0.5006383061408997\n",
      "Validation: Epoch [6], Batch [648/938], Loss: 0.776607871055603\n",
      "Validation: Epoch [6], Batch [649/938], Loss: 0.9020705223083496\n",
      "Validation: Epoch [6], Batch [650/938], Loss: 0.5647091269493103\n",
      "Validation: Epoch [6], Batch [651/938], Loss: 0.7795439958572388\n",
      "Validation: Epoch [6], Batch [652/938], Loss: 0.5279958248138428\n",
      "Validation: Epoch [6], Batch [653/938], Loss: 0.8627633452415466\n",
      "Validation: Epoch [6], Batch [654/938], Loss: 0.667040228843689\n",
      "Validation: Epoch [6], Batch [655/938], Loss: 0.6798638105392456\n",
      "Validation: Epoch [6], Batch [656/938], Loss: 0.5546222925186157\n",
      "Validation: Epoch [6], Batch [657/938], Loss: 0.7566775679588318\n",
      "Validation: Epoch [6], Batch [658/938], Loss: 0.3589634299278259\n",
      "Validation: Epoch [6], Batch [659/938], Loss: 0.8065212965011597\n",
      "Validation: Epoch [6], Batch [660/938], Loss: 0.6575464010238647\n",
      "Validation: Epoch [6], Batch [661/938], Loss: 0.5905440449714661\n",
      "Validation: Epoch [6], Batch [662/938], Loss: 0.7525634765625\n",
      "Validation: Epoch [6], Batch [663/938], Loss: 0.512946367263794\n",
      "Validation: Epoch [6], Batch [664/938], Loss: 0.5264379978179932\n",
      "Validation: Epoch [6], Batch [665/938], Loss: 0.5903494358062744\n",
      "Validation: Epoch [6], Batch [666/938], Loss: 0.66752028465271\n",
      "Validation: Epoch [6], Batch [667/938], Loss: 0.5154536962509155\n",
      "Validation: Epoch [6], Batch [668/938], Loss: 0.5265673398971558\n",
      "Validation: Epoch [6], Batch [669/938], Loss: 0.7660196423530579\n",
      "Validation: Epoch [6], Batch [670/938], Loss: 0.6011992692947388\n",
      "Validation: Epoch [6], Batch [671/938], Loss: 0.5743447542190552\n",
      "Validation: Epoch [6], Batch [672/938], Loss: 0.6935440301895142\n",
      "Validation: Epoch [6], Batch [673/938], Loss: 0.6141396760940552\n",
      "Validation: Epoch [6], Batch [674/938], Loss: 0.5760488510131836\n",
      "Validation: Epoch [6], Batch [675/938], Loss: 0.6557053327560425\n",
      "Validation: Epoch [6], Batch [676/938], Loss: 0.8819685578346252\n",
      "Validation: Epoch [6], Batch [677/938], Loss: 0.723796010017395\n",
      "Validation: Epoch [6], Batch [678/938], Loss: 0.6667433977127075\n",
      "Validation: Epoch [6], Batch [679/938], Loss: 0.6141108274459839\n",
      "Validation: Epoch [6], Batch [680/938], Loss: 0.5118509531021118\n",
      "Validation: Epoch [6], Batch [681/938], Loss: 0.6650344133377075\n",
      "Validation: Epoch [6], Batch [682/938], Loss: 0.5227862596511841\n",
      "Validation: Epoch [6], Batch [683/938], Loss: 0.6744144558906555\n",
      "Validation: Epoch [6], Batch [684/938], Loss: 0.6869930028915405\n",
      "Validation: Epoch [6], Batch [685/938], Loss: 0.5823965072631836\n",
      "Validation: Epoch [6], Batch [686/938], Loss: 0.4882698059082031\n",
      "Validation: Epoch [6], Batch [687/938], Loss: 0.5234556794166565\n",
      "Validation: Epoch [6], Batch [688/938], Loss: 0.6470698118209839\n",
      "Validation: Epoch [6], Batch [689/938], Loss: 0.5856636762619019\n",
      "Validation: Epoch [6], Batch [690/938], Loss: 0.6277318596839905\n",
      "Validation: Epoch [6], Batch [691/938], Loss: 0.6344344019889832\n",
      "Validation: Epoch [6], Batch [692/938], Loss: 0.6277289390563965\n",
      "Validation: Epoch [6], Batch [693/938], Loss: 0.6200514435768127\n",
      "Validation: Epoch [6], Batch [694/938], Loss: 0.5064712762832642\n",
      "Validation: Epoch [6], Batch [695/938], Loss: 0.9013532400131226\n",
      "Validation: Epoch [6], Batch [696/938], Loss: 0.6175550222396851\n",
      "Validation: Epoch [6], Batch [697/938], Loss: 0.6782507300376892\n",
      "Validation: Epoch [6], Batch [698/938], Loss: 0.48273634910583496\n",
      "Validation: Epoch [6], Batch [699/938], Loss: 0.7494548559188843\n",
      "Validation: Epoch [6], Batch [700/938], Loss: 0.8756700754165649\n",
      "Validation: Epoch [6], Batch [701/938], Loss: 0.6929482221603394\n",
      "Validation: Epoch [6], Batch [702/938], Loss: 0.5320032835006714\n",
      "Validation: Epoch [6], Batch [703/938], Loss: 0.5105096101760864\n",
      "Validation: Epoch [6], Batch [704/938], Loss: 0.5758745670318604\n",
      "Validation: Epoch [6], Batch [705/938], Loss: 0.5759907960891724\n",
      "Validation: Epoch [6], Batch [706/938], Loss: 0.7260817289352417\n",
      "Validation: Epoch [6], Batch [707/938], Loss: 0.6862521171569824\n",
      "Validation: Epoch [6], Batch [708/938], Loss: 0.6562394499778748\n",
      "Validation: Epoch [6], Batch [709/938], Loss: 0.5397905707359314\n",
      "Validation: Epoch [6], Batch [710/938], Loss: 0.7559070587158203\n",
      "Validation: Epoch [6], Batch [711/938], Loss: 0.5948033332824707\n",
      "Validation: Epoch [6], Batch [712/938], Loss: 0.46831652522087097\n",
      "Validation: Epoch [6], Batch [713/938], Loss: 0.5754995346069336\n",
      "Validation: Epoch [6], Batch [714/938], Loss: 0.789742112159729\n",
      "Validation: Epoch [6], Batch [715/938], Loss: 0.505255937576294\n",
      "Validation: Epoch [6], Batch [716/938], Loss: 0.6789407134056091\n",
      "Validation: Epoch [6], Batch [717/938], Loss: 0.5799429416656494\n",
      "Validation: Epoch [6], Batch [718/938], Loss: 0.5043579339981079\n",
      "Validation: Epoch [6], Batch [719/938], Loss: 0.6679704189300537\n",
      "Validation: Epoch [6], Batch [720/938], Loss: 0.7051882147789001\n",
      "Validation: Epoch [6], Batch [721/938], Loss: 0.5083814859390259\n",
      "Validation: Epoch [6], Batch [722/938], Loss: 0.7659673690795898\n",
      "Validation: Epoch [6], Batch [723/938], Loss: 0.6040716171264648\n",
      "Validation: Epoch [6], Batch [724/938], Loss: 0.6616194248199463\n",
      "Validation: Epoch [6], Batch [725/938], Loss: 0.5396191477775574\n",
      "Validation: Epoch [6], Batch [726/938], Loss: 0.606704592704773\n",
      "Validation: Epoch [6], Batch [727/938], Loss: 0.6871873140335083\n",
      "Validation: Epoch [6], Batch [728/938], Loss: 0.5938770771026611\n",
      "Validation: Epoch [6], Batch [729/938], Loss: 0.3172778785228729\n",
      "Validation: Epoch [6], Batch [730/938], Loss: 0.6134177446365356\n",
      "Validation: Epoch [6], Batch [731/938], Loss: 0.42850881814956665\n",
      "Validation: Epoch [6], Batch [732/938], Loss: 0.538628339767456\n",
      "Validation: Epoch [6], Batch [733/938], Loss: 0.759936511516571\n",
      "Validation: Epoch [6], Batch [734/938], Loss: 0.3907950818538666\n",
      "Validation: Epoch [6], Batch [735/938], Loss: 0.510594367980957\n",
      "Validation: Epoch [6], Batch [736/938], Loss: 0.5430644154548645\n",
      "Validation: Epoch [6], Batch [737/938], Loss: 0.44748327136039734\n",
      "Validation: Epoch [6], Batch [738/938], Loss: 0.712475061416626\n",
      "Validation: Epoch [6], Batch [739/938], Loss: 0.5142045617103577\n",
      "Validation: Epoch [6], Batch [740/938], Loss: 0.5860983729362488\n",
      "Validation: Epoch [6], Batch [741/938], Loss: 0.5823336243629456\n",
      "Validation: Epoch [6], Batch [742/938], Loss: 0.7419511079788208\n",
      "Validation: Epoch [6], Batch [743/938], Loss: 0.5954896211624146\n",
      "Validation: Epoch [6], Batch [744/938], Loss: 0.680504560470581\n",
      "Validation: Epoch [6], Batch [745/938], Loss: 0.58961021900177\n",
      "Validation: Epoch [6], Batch [746/938], Loss: 0.7244991660118103\n",
      "Validation: Epoch [6], Batch [747/938], Loss: 0.5973637104034424\n",
      "Validation: Epoch [6], Batch [748/938], Loss: 0.7617409229278564\n",
      "Validation: Epoch [6], Batch [749/938], Loss: 0.49191832542419434\n",
      "Validation: Epoch [6], Batch [750/938], Loss: 0.7141374349594116\n",
      "Validation: Epoch [6], Batch [751/938], Loss: 0.9730530977249146\n",
      "Validation: Epoch [6], Batch [752/938], Loss: 0.6300454139709473\n",
      "Validation: Epoch [6], Batch [753/938], Loss: 0.5438782572746277\n",
      "Validation: Epoch [6], Batch [754/938], Loss: 0.5753693580627441\n",
      "Validation: Epoch [6], Batch [755/938], Loss: 0.6488957405090332\n",
      "Validation: Epoch [6], Batch [756/938], Loss: 0.6344110369682312\n",
      "Validation: Epoch [6], Batch [757/938], Loss: 0.7082495093345642\n",
      "Validation: Epoch [6], Batch [758/938], Loss: 0.5451955795288086\n",
      "Validation: Epoch [6], Batch [759/938], Loss: 0.5460077524185181\n",
      "Validation: Epoch [6], Batch [760/938], Loss: 0.4981495141983032\n",
      "Validation: Epoch [6], Batch [761/938], Loss: 0.6861653923988342\n",
      "Validation: Epoch [6], Batch [762/938], Loss: 0.6694855690002441\n",
      "Validation: Epoch [6], Batch [763/938], Loss: 0.5542980432510376\n",
      "Validation: Epoch [6], Batch [764/938], Loss: 0.6752206683158875\n",
      "Validation: Epoch [6], Batch [765/938], Loss: 0.6236358880996704\n",
      "Validation: Epoch [6], Batch [766/938], Loss: 0.7438464164733887\n",
      "Validation: Epoch [6], Batch [767/938], Loss: 0.5698744058609009\n",
      "Validation: Epoch [6], Batch [768/938], Loss: 0.5884476900100708\n",
      "Validation: Epoch [6], Batch [769/938], Loss: 0.5764044523239136\n",
      "Validation: Epoch [6], Batch [770/938], Loss: 0.7838361263275146\n",
      "Validation: Epoch [6], Batch [771/938], Loss: 0.822443962097168\n",
      "Validation: Epoch [6], Batch [772/938], Loss: 0.5335283279418945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [773/938], Loss: 0.6531366109848022\n",
      "Validation: Epoch [6], Batch [774/938], Loss: 0.721333384513855\n",
      "Validation: Epoch [6], Batch [775/938], Loss: 0.5306156873703003\n",
      "Validation: Epoch [6], Batch [776/938], Loss: 0.4973354935646057\n",
      "Validation: Epoch [6], Batch [777/938], Loss: 0.4806259572505951\n",
      "Validation: Epoch [6], Batch [778/938], Loss: 0.6404588222503662\n",
      "Validation: Epoch [6], Batch [779/938], Loss: 0.5917567610740662\n",
      "Validation: Epoch [6], Batch [780/938], Loss: 0.6195416450500488\n",
      "Validation: Epoch [6], Batch [781/938], Loss: 0.4256356656551361\n",
      "Validation: Epoch [6], Batch [782/938], Loss: 0.520218551158905\n",
      "Validation: Epoch [6], Batch [783/938], Loss: 0.6800538301467896\n",
      "Validation: Epoch [6], Batch [784/938], Loss: 0.5724265575408936\n",
      "Validation: Epoch [6], Batch [785/938], Loss: 0.7919474840164185\n",
      "Validation: Epoch [6], Batch [786/938], Loss: 0.5993091464042664\n",
      "Validation: Epoch [6], Batch [787/938], Loss: 0.5473998188972473\n",
      "Validation: Epoch [6], Batch [788/938], Loss: 0.474773108959198\n",
      "Validation: Epoch [6], Batch [789/938], Loss: 0.5685051679611206\n",
      "Validation: Epoch [6], Batch [790/938], Loss: 0.6622339487075806\n",
      "Validation: Epoch [6], Batch [791/938], Loss: 0.6696081161499023\n",
      "Validation: Epoch [6], Batch [792/938], Loss: 0.61822509765625\n",
      "Validation: Epoch [6], Batch [793/938], Loss: 0.5136134624481201\n",
      "Validation: Epoch [6], Batch [794/938], Loss: 0.550274133682251\n",
      "Validation: Epoch [6], Batch [795/938], Loss: 0.5214802026748657\n",
      "Validation: Epoch [6], Batch [796/938], Loss: 0.6459842920303345\n",
      "Validation: Epoch [6], Batch [797/938], Loss: 0.6693931818008423\n",
      "Validation: Epoch [6], Batch [798/938], Loss: 0.7593933343887329\n",
      "Validation: Epoch [6], Batch [799/938], Loss: 0.8226526975631714\n",
      "Validation: Epoch [6], Batch [800/938], Loss: 0.8711467385292053\n",
      "Validation: Epoch [6], Batch [801/938], Loss: 0.8093280792236328\n",
      "Validation: Epoch [6], Batch [802/938], Loss: 0.668312668800354\n",
      "Validation: Epoch [6], Batch [803/938], Loss: 0.42889484763145447\n",
      "Validation: Epoch [6], Batch [804/938], Loss: 0.7337951064109802\n",
      "Validation: Epoch [6], Batch [805/938], Loss: 0.5460869073867798\n",
      "Validation: Epoch [6], Batch [806/938], Loss: 0.7794049978256226\n",
      "Validation: Epoch [6], Batch [807/938], Loss: 0.7530624270439148\n",
      "Validation: Epoch [6], Batch [808/938], Loss: 0.5550194978713989\n",
      "Validation: Epoch [6], Batch [809/938], Loss: 0.7982587814331055\n",
      "Validation: Epoch [6], Batch [810/938], Loss: 0.538085401058197\n",
      "Validation: Epoch [6], Batch [811/938], Loss: 0.5731856822967529\n",
      "Validation: Epoch [6], Batch [812/938], Loss: 0.42160600423812866\n",
      "Validation: Epoch [6], Batch [813/938], Loss: 0.7416788339614868\n",
      "Validation: Epoch [6], Batch [814/938], Loss: 0.475707471370697\n",
      "Validation: Epoch [6], Batch [815/938], Loss: 0.622434139251709\n",
      "Validation: Epoch [6], Batch [816/938], Loss: 0.5720647573471069\n",
      "Validation: Epoch [6], Batch [817/938], Loss: 0.7305202484130859\n",
      "Validation: Epoch [6], Batch [818/938], Loss: 0.44554680585861206\n",
      "Validation: Epoch [6], Batch [819/938], Loss: 0.6386395692825317\n",
      "Validation: Epoch [6], Batch [820/938], Loss: 0.7552188634872437\n",
      "Validation: Epoch [6], Batch [821/938], Loss: 0.8447562456130981\n",
      "Validation: Epoch [6], Batch [822/938], Loss: 0.5567638874053955\n",
      "Validation: Epoch [6], Batch [823/938], Loss: 0.5385589003562927\n",
      "Validation: Epoch [6], Batch [824/938], Loss: 0.7707812786102295\n",
      "Validation: Epoch [6], Batch [825/938], Loss: 0.5940839052200317\n",
      "Validation: Epoch [6], Batch [826/938], Loss: 0.5432572364807129\n",
      "Validation: Epoch [6], Batch [827/938], Loss: 0.5490888953208923\n",
      "Validation: Epoch [6], Batch [828/938], Loss: 0.5285012125968933\n",
      "Validation: Epoch [6], Batch [829/938], Loss: 0.6500027775764465\n",
      "Validation: Epoch [6], Batch [830/938], Loss: 0.6443756818771362\n",
      "Validation: Epoch [6], Batch [831/938], Loss: 0.5086780190467834\n",
      "Validation: Epoch [6], Batch [832/938], Loss: 0.6775702238082886\n",
      "Validation: Epoch [6], Batch [833/938], Loss: 0.4714253842830658\n",
      "Validation: Epoch [6], Batch [834/938], Loss: 0.7208521366119385\n",
      "Validation: Epoch [6], Batch [835/938], Loss: 0.4424162209033966\n",
      "Validation: Epoch [6], Batch [836/938], Loss: 0.5360637903213501\n",
      "Validation: Epoch [6], Batch [837/938], Loss: 0.5871163606643677\n",
      "Validation: Epoch [6], Batch [838/938], Loss: 0.7443779706954956\n",
      "Validation: Epoch [6], Batch [839/938], Loss: 0.6581310033798218\n",
      "Validation: Epoch [6], Batch [840/938], Loss: 0.6832849979400635\n",
      "Validation: Epoch [6], Batch [841/938], Loss: 0.6098675727844238\n",
      "Validation: Epoch [6], Batch [842/938], Loss: 0.3654323220252991\n",
      "Validation: Epoch [6], Batch [843/938], Loss: 0.6002436280250549\n",
      "Validation: Epoch [6], Batch [844/938], Loss: 0.5456434488296509\n",
      "Validation: Epoch [6], Batch [845/938], Loss: 0.5575776100158691\n",
      "Validation: Epoch [6], Batch [846/938], Loss: 0.6512648463249207\n",
      "Validation: Epoch [6], Batch [847/938], Loss: 0.6029107570648193\n",
      "Validation: Epoch [6], Batch [848/938], Loss: 0.5630204677581787\n",
      "Validation: Epoch [6], Batch [849/938], Loss: 0.5970109701156616\n",
      "Validation: Epoch [6], Batch [850/938], Loss: 0.8136054277420044\n",
      "Validation: Epoch [6], Batch [851/938], Loss: 0.5805279016494751\n",
      "Validation: Epoch [6], Batch [852/938], Loss: 0.7490310668945312\n",
      "Validation: Epoch [6], Batch [853/938], Loss: 0.5696553587913513\n",
      "Validation: Epoch [6], Batch [854/938], Loss: 0.5435804128646851\n",
      "Validation: Epoch [6], Batch [855/938], Loss: 0.4909899830818176\n",
      "Validation: Epoch [6], Batch [856/938], Loss: 0.656831681728363\n",
      "Validation: Epoch [6], Batch [857/938], Loss: 0.7541694641113281\n",
      "Validation: Epoch [6], Batch [858/938], Loss: 0.7221234440803528\n",
      "Validation: Epoch [6], Batch [859/938], Loss: 0.5043959021568298\n",
      "Validation: Epoch [6], Batch [860/938], Loss: 0.5286274552345276\n",
      "Validation: Epoch [6], Batch [861/938], Loss: 0.49342620372772217\n",
      "Validation: Epoch [6], Batch [862/938], Loss: 0.5629895925521851\n",
      "Validation: Epoch [6], Batch [863/938], Loss: 0.39103904366493225\n",
      "Validation: Epoch [6], Batch [864/938], Loss: 0.5588799118995667\n",
      "Validation: Epoch [6], Batch [865/938], Loss: 0.6084383726119995\n",
      "Validation: Epoch [6], Batch [866/938], Loss: 0.631806492805481\n",
      "Validation: Epoch [6], Batch [867/938], Loss: 0.670946478843689\n",
      "Validation: Epoch [6], Batch [868/938], Loss: 0.6070871949195862\n",
      "Validation: Epoch [6], Batch [869/938], Loss: 0.7240285873413086\n",
      "Validation: Epoch [6], Batch [870/938], Loss: 0.5571997165679932\n",
      "Validation: Epoch [6], Batch [871/938], Loss: 0.474048912525177\n",
      "Validation: Epoch [6], Batch [872/938], Loss: 0.5920839309692383\n",
      "Validation: Epoch [6], Batch [873/938], Loss: 0.575104832649231\n",
      "Validation: Epoch [6], Batch [874/938], Loss: 0.7372404932975769\n",
      "Validation: Epoch [6], Batch [875/938], Loss: 0.6243054866790771\n",
      "Validation: Epoch [6], Batch [876/938], Loss: 0.5191788673400879\n",
      "Validation: Epoch [6], Batch [877/938], Loss: 0.5831683874130249\n",
      "Validation: Epoch [6], Batch [878/938], Loss: 0.6626936197280884\n",
      "Validation: Epoch [6], Batch [879/938], Loss: 0.5133126974105835\n",
      "Validation: Epoch [6], Batch [880/938], Loss: 0.501546323299408\n",
      "Validation: Epoch [6], Batch [881/938], Loss: 0.5759770274162292\n",
      "Validation: Epoch [6], Batch [882/938], Loss: 0.4933660626411438\n",
      "Validation: Epoch [6], Batch [883/938], Loss: 0.6978610157966614\n",
      "Validation: Epoch [6], Batch [884/938], Loss: 0.6692798137664795\n",
      "Validation: Epoch [6], Batch [885/938], Loss: 0.9159270524978638\n",
      "Validation: Epoch [6], Batch [886/938], Loss: 0.6345047950744629\n",
      "Validation: Epoch [6], Batch [887/938], Loss: 0.7736059427261353\n",
      "Validation: Epoch [6], Batch [888/938], Loss: 0.5433233976364136\n",
      "Validation: Epoch [6], Batch [889/938], Loss: 0.7590878009796143\n",
      "Validation: Epoch [6], Batch [890/938], Loss: 0.6226450800895691\n",
      "Validation: Epoch [6], Batch [891/938], Loss: 0.7017530798912048\n",
      "Validation: Epoch [6], Batch [892/938], Loss: 0.7075360417366028\n",
      "Validation: Epoch [6], Batch [893/938], Loss: 0.5471310019493103\n",
      "Validation: Epoch [6], Batch [894/938], Loss: 0.6892950534820557\n",
      "Validation: Epoch [6], Batch [895/938], Loss: 0.48582780361175537\n",
      "Validation: Epoch [6], Batch [896/938], Loss: 0.9024096727371216\n",
      "Validation: Epoch [6], Batch [897/938], Loss: 0.4559733271598816\n",
      "Validation: Epoch [6], Batch [898/938], Loss: 0.763267993927002\n",
      "Validation: Epoch [6], Batch [899/938], Loss: 0.5471619963645935\n",
      "Validation: Epoch [6], Batch [900/938], Loss: 0.737139105796814\n",
      "Validation: Epoch [6], Batch [901/938], Loss: 0.5286315679550171\n",
      "Validation: Epoch [6], Batch [902/938], Loss: 0.6803130507469177\n",
      "Validation: Epoch [6], Batch [903/938], Loss: 0.6589250564575195\n",
      "Validation: Epoch [6], Batch [904/938], Loss: 0.5625522136688232\n",
      "Validation: Epoch [6], Batch [905/938], Loss: 0.6172624826431274\n",
      "Validation: Epoch [6], Batch [906/938], Loss: 0.5714888572692871\n",
      "Validation: Epoch [6], Batch [907/938], Loss: 0.608639121055603\n",
      "Validation: Epoch [6], Batch [908/938], Loss: 0.6905782222747803\n",
      "Validation: Epoch [6], Batch [909/938], Loss: 0.5071461796760559\n",
      "Validation: Epoch [6], Batch [910/938], Loss: 0.5311561226844788\n",
      "Validation: Epoch [6], Batch [911/938], Loss: 0.7416344881057739\n",
      "Validation: Epoch [6], Batch [912/938], Loss: 0.6013072729110718\n",
      "Validation: Epoch [6], Batch [913/938], Loss: 0.615890622138977\n",
      "Validation: Epoch [6], Batch [914/938], Loss: 0.633942723274231\n",
      "Validation: Epoch [6], Batch [915/938], Loss: 0.5583382844924927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [6], Batch [916/938], Loss: 0.4902072846889496\n",
      "Validation: Epoch [6], Batch [917/938], Loss: 0.6891379356384277\n",
      "Validation: Epoch [6], Batch [918/938], Loss: 0.6334156394004822\n",
      "Validation: Epoch [6], Batch [919/938], Loss: 0.46651408076286316\n",
      "Validation: Epoch [6], Batch [920/938], Loss: 0.8466862440109253\n",
      "Validation: Epoch [6], Batch [921/938], Loss: 0.7117233276367188\n",
      "Validation: Epoch [6], Batch [922/938], Loss: 0.8019219636917114\n",
      "Validation: Epoch [6], Batch [923/938], Loss: 0.852005124092102\n",
      "Validation: Epoch [6], Batch [924/938], Loss: 0.550830602645874\n",
      "Validation: Epoch [6], Batch [925/938], Loss: 0.6462258100509644\n",
      "Validation: Epoch [6], Batch [926/938], Loss: 0.705089271068573\n",
      "Validation: Epoch [6], Batch [927/938], Loss: 0.7858643531799316\n",
      "Validation: Epoch [6], Batch [928/938], Loss: 0.7777500748634338\n",
      "Validation: Epoch [6], Batch [929/938], Loss: 0.7910314798355103\n",
      "Validation: Epoch [6], Batch [930/938], Loss: 0.6229569911956787\n",
      "Validation: Epoch [6], Batch [931/938], Loss: 0.6003642082214355\n",
      "Validation: Epoch [6], Batch [932/938], Loss: 0.5320502519607544\n",
      "Validation: Epoch [6], Batch [933/938], Loss: 0.63460773229599\n",
      "Validation: Epoch [6], Batch [934/938], Loss: 0.5886493921279907\n",
      "Validation: Epoch [6], Batch [935/938], Loss: 0.8372172117233276\n",
      "Validation: Epoch [6], Batch [936/938], Loss: 0.868101954460144\n",
      "Validation: Epoch [6], Batch [937/938], Loss: 0.6188327670097351\n",
      "Validation: Epoch [6], Batch [938/938], Loss: 0.7084754705429077\n",
      "Accuracy of test set: 0.7792833333333333\n",
      "Train: Epoch [7], Batch [1/938], Loss: 0.5731069445610046\n",
      "Train: Epoch [7], Batch [2/938], Loss: 0.4846181273460388\n",
      "Train: Epoch [7], Batch [3/938], Loss: 0.6958663463592529\n",
      "Train: Epoch [7], Batch [4/938], Loss: 0.6055248975753784\n",
      "Train: Epoch [7], Batch [5/938], Loss: 0.5355687141418457\n",
      "Train: Epoch [7], Batch [6/938], Loss: 0.5318165421485901\n",
      "Train: Epoch [7], Batch [7/938], Loss: 0.7217534780502319\n",
      "Train: Epoch [7], Batch [8/938], Loss: 0.46131712198257446\n",
      "Train: Epoch [7], Batch [9/938], Loss: 0.5121736526489258\n",
      "Train: Epoch [7], Batch [10/938], Loss: 0.6469945907592773\n",
      "Train: Epoch [7], Batch [11/938], Loss: 0.7296874523162842\n",
      "Train: Epoch [7], Batch [12/938], Loss: 0.7320556640625\n",
      "Train: Epoch [7], Batch [13/938], Loss: 0.5682746171951294\n",
      "Train: Epoch [7], Batch [14/938], Loss: 0.6225851774215698\n",
      "Train: Epoch [7], Batch [15/938], Loss: 0.6219615936279297\n",
      "Train: Epoch [7], Batch [16/938], Loss: 0.50556480884552\n",
      "Train: Epoch [7], Batch [17/938], Loss: 0.5083473324775696\n",
      "Train: Epoch [7], Batch [18/938], Loss: 0.6877030730247498\n",
      "Train: Epoch [7], Batch [19/938], Loss: 0.41359829902648926\n",
      "Train: Epoch [7], Batch [20/938], Loss: 0.6174609065055847\n",
      "Train: Epoch [7], Batch [21/938], Loss: 0.547035813331604\n",
      "Train: Epoch [7], Batch [22/938], Loss: 0.5354921817779541\n",
      "Train: Epoch [7], Batch [23/938], Loss: 0.5721918344497681\n",
      "Train: Epoch [7], Batch [24/938], Loss: 0.6492754220962524\n",
      "Train: Epoch [7], Batch [25/938], Loss: 0.8086557388305664\n",
      "Train: Epoch [7], Batch [26/938], Loss: 0.5594059228897095\n",
      "Train: Epoch [7], Batch [27/938], Loss: 0.7553991079330444\n",
      "Train: Epoch [7], Batch [28/938], Loss: 0.5569895505905151\n",
      "Train: Epoch [7], Batch [29/938], Loss: 0.8402669429779053\n",
      "Train: Epoch [7], Batch [30/938], Loss: 0.4998094439506531\n",
      "Train: Epoch [7], Batch [31/938], Loss: 0.6715097427368164\n",
      "Train: Epoch [7], Batch [32/938], Loss: 0.6863821744918823\n",
      "Train: Epoch [7], Batch [33/938], Loss: 0.6132690906524658\n",
      "Train: Epoch [7], Batch [34/938], Loss: 0.6571601033210754\n",
      "Train: Epoch [7], Batch [35/938], Loss: 0.6548480987548828\n",
      "Train: Epoch [7], Batch [36/938], Loss: 0.685772716999054\n",
      "Train: Epoch [7], Batch [37/938], Loss: 0.6098848581314087\n",
      "Train: Epoch [7], Batch [38/938], Loss: 0.4830104112625122\n",
      "Train: Epoch [7], Batch [39/938], Loss: 0.40790995955467224\n",
      "Train: Epoch [7], Batch [40/938], Loss: 0.5867519378662109\n",
      "Train: Epoch [7], Batch [41/938], Loss: 0.6809529662132263\n",
      "Train: Epoch [7], Batch [42/938], Loss: 0.5777144432067871\n",
      "Train: Epoch [7], Batch [43/938], Loss: 0.39382994174957275\n",
      "Train: Epoch [7], Batch [44/938], Loss: 0.7369937300682068\n",
      "Train: Epoch [7], Batch [45/938], Loss: 0.4916883707046509\n",
      "Train: Epoch [7], Batch [46/938], Loss: 0.7257683873176575\n",
      "Train: Epoch [7], Batch [47/938], Loss: 0.733009934425354\n",
      "Train: Epoch [7], Batch [48/938], Loss: 0.6244944334030151\n",
      "Train: Epoch [7], Batch [49/938], Loss: 0.5250383615493774\n",
      "Train: Epoch [7], Batch [50/938], Loss: 0.4547421932220459\n",
      "Train: Epoch [7], Batch [51/938], Loss: 0.7504874467849731\n",
      "Train: Epoch [7], Batch [52/938], Loss: 0.5364803075790405\n",
      "Train: Epoch [7], Batch [53/938], Loss: 0.4426189363002777\n",
      "Train: Epoch [7], Batch [54/938], Loss: 0.7763326168060303\n",
      "Train: Epoch [7], Batch [55/938], Loss: 0.7620714902877808\n",
      "Train: Epoch [7], Batch [56/938], Loss: 0.8663402795791626\n",
      "Train: Epoch [7], Batch [57/938], Loss: 0.6462262868881226\n",
      "Train: Epoch [7], Batch [58/938], Loss: 0.5471717119216919\n",
      "Train: Epoch [7], Batch [59/938], Loss: 0.3567661643028259\n",
      "Train: Epoch [7], Batch [60/938], Loss: 0.6450783014297485\n",
      "Train: Epoch [7], Batch [61/938], Loss: 0.6595464944839478\n",
      "Train: Epoch [7], Batch [62/938], Loss: 0.5654897689819336\n",
      "Train: Epoch [7], Batch [63/938], Loss: 0.4599761664867401\n",
      "Train: Epoch [7], Batch [64/938], Loss: 0.5907936692237854\n",
      "Train: Epoch [7], Batch [65/938], Loss: 0.6919535994529724\n",
      "Train: Epoch [7], Batch [66/938], Loss: 0.6216965913772583\n",
      "Train: Epoch [7], Batch [67/938], Loss: 0.5033565759658813\n",
      "Train: Epoch [7], Batch [68/938], Loss: 0.8520588874816895\n",
      "Train: Epoch [7], Batch [69/938], Loss: 0.811426043510437\n",
      "Train: Epoch [7], Batch [70/938], Loss: 0.5151822566986084\n",
      "Train: Epoch [7], Batch [71/938], Loss: 0.7517732381820679\n",
      "Train: Epoch [7], Batch [72/938], Loss: 0.687224268913269\n",
      "Train: Epoch [7], Batch [73/938], Loss: 0.5496535301208496\n",
      "Train: Epoch [7], Batch [74/938], Loss: 0.691182553768158\n",
      "Train: Epoch [7], Batch [75/938], Loss: 0.6197211146354675\n",
      "Train: Epoch [7], Batch [76/938], Loss: 0.7064089775085449\n",
      "Train: Epoch [7], Batch [77/938], Loss: 0.5579804182052612\n",
      "Train: Epoch [7], Batch [78/938], Loss: 0.6001356244087219\n",
      "Train: Epoch [7], Batch [79/938], Loss: 0.6923294067382812\n",
      "Train: Epoch [7], Batch [80/938], Loss: 0.8110377192497253\n",
      "Train: Epoch [7], Batch [81/938], Loss: 0.5723448991775513\n",
      "Train: Epoch [7], Batch [82/938], Loss: 0.5554825067520142\n",
      "Train: Epoch [7], Batch [83/938], Loss: 0.4753577411174774\n",
      "Train: Epoch [7], Batch [84/938], Loss: 0.6570335030555725\n",
      "Train: Epoch [7], Batch [85/938], Loss: 0.5324558615684509\n",
      "Train: Epoch [7], Batch [86/938], Loss: 0.6647681593894958\n",
      "Train: Epoch [7], Batch [87/938], Loss: 0.5404167771339417\n",
      "Train: Epoch [7], Batch [88/938], Loss: 0.49802398681640625\n",
      "Train: Epoch [7], Batch [89/938], Loss: 0.5831735134124756\n",
      "Train: Epoch [7], Batch [90/938], Loss: 0.6395164728164673\n",
      "Train: Epoch [7], Batch [91/938], Loss: 0.5677808523178101\n",
      "Train: Epoch [7], Batch [92/938], Loss: 0.5494199991226196\n",
      "Train: Epoch [7], Batch [93/938], Loss: 0.48805665969848633\n",
      "Train: Epoch [7], Batch [94/938], Loss: 0.45371949672698975\n",
      "Train: Epoch [7], Batch [95/938], Loss: 0.5182614326477051\n",
      "Train: Epoch [7], Batch [96/938], Loss: 0.560208261013031\n",
      "Train: Epoch [7], Batch [97/938], Loss: 0.7917744517326355\n",
      "Train: Epoch [7], Batch [98/938], Loss: 0.44088470935821533\n",
      "Train: Epoch [7], Batch [99/938], Loss: 0.5876835584640503\n",
      "Train: Epoch [7], Batch [100/938], Loss: 0.6243712902069092\n",
      "Train: Epoch [7], Batch [101/938], Loss: 0.8755733966827393\n",
      "Train: Epoch [7], Batch [102/938], Loss: 0.5290212631225586\n",
      "Train: Epoch [7], Batch [103/938], Loss: 0.6954837441444397\n",
      "Train: Epoch [7], Batch [104/938], Loss: 0.433404803276062\n",
      "Train: Epoch [7], Batch [105/938], Loss: 0.5382164716720581\n",
      "Train: Epoch [7], Batch [106/938], Loss: 0.5331465005874634\n",
      "Train: Epoch [7], Batch [107/938], Loss: 0.6372666358947754\n",
      "Train: Epoch [7], Batch [108/938], Loss: 0.6475335359573364\n",
      "Train: Epoch [7], Batch [109/938], Loss: 0.5052475333213806\n",
      "Train: Epoch [7], Batch [110/938], Loss: 0.47925683856010437\n",
      "Train: Epoch [7], Batch [111/938], Loss: 0.9084917306900024\n",
      "Train: Epoch [7], Batch [112/938], Loss: 0.6697689294815063\n",
      "Train: Epoch [7], Batch [113/938], Loss: 0.7745844125747681\n",
      "Train: Epoch [7], Batch [114/938], Loss: 0.5911843776702881\n",
      "Train: Epoch [7], Batch [115/938], Loss: 0.6761224269866943\n",
      "Train: Epoch [7], Batch [116/938], Loss: 0.6297451257705688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [7], Batch [117/938], Loss: 0.6861156821250916\n",
      "Train: Epoch [7], Batch [118/938], Loss: 0.656565248966217\n",
      "Train: Epoch [7], Batch [119/938], Loss: 0.6585583686828613\n",
      "Train: Epoch [7], Batch [120/938], Loss: 0.5784045457839966\n",
      "Train: Epoch [7], Batch [121/938], Loss: 0.6224263310432434\n",
      "Train: Epoch [7], Batch [122/938], Loss: 0.9795562028884888\n",
      "Train: Epoch [7], Batch [123/938], Loss: 0.5795380473136902\n",
      "Train: Epoch [7], Batch [124/938], Loss: 0.6190218329429626\n",
      "Train: Epoch [7], Batch [125/938], Loss: 0.6028615236282349\n",
      "Train: Epoch [7], Batch [126/938], Loss: 0.7834475636482239\n",
      "Train: Epoch [7], Batch [127/938], Loss: 0.5526934266090393\n",
      "Train: Epoch [7], Batch [128/938], Loss: 0.5487838983535767\n",
      "Train: Epoch [7], Batch [129/938], Loss: 0.5694423913955688\n",
      "Train: Epoch [7], Batch [130/938], Loss: 0.4140079915523529\n",
      "Train: Epoch [7], Batch [131/938], Loss: 0.43548712134361267\n",
      "Train: Epoch [7], Batch [132/938], Loss: 0.718879222869873\n",
      "Train: Epoch [7], Batch [133/938], Loss: 0.5531402826309204\n",
      "Train: Epoch [7], Batch [134/938], Loss: 0.5588886737823486\n",
      "Train: Epoch [7], Batch [135/938], Loss: 0.6066443920135498\n",
      "Train: Epoch [7], Batch [136/938], Loss: 0.7379238605499268\n",
      "Train: Epoch [7], Batch [137/938], Loss: 0.619091272354126\n",
      "Train: Epoch [7], Batch [138/938], Loss: 0.7320783138275146\n",
      "Train: Epoch [7], Batch [139/938], Loss: 0.5470284819602966\n",
      "Train: Epoch [7], Batch [140/938], Loss: 0.3862326145172119\n",
      "Train: Epoch [7], Batch [141/938], Loss: 0.7521377205848694\n",
      "Train: Epoch [7], Batch [142/938], Loss: 0.7552740573883057\n",
      "Train: Epoch [7], Batch [143/938], Loss: 0.9584009051322937\n",
      "Train: Epoch [7], Batch [144/938], Loss: 0.5890934467315674\n",
      "Train: Epoch [7], Batch [145/938], Loss: 0.6840568780899048\n",
      "Train: Epoch [7], Batch [146/938], Loss: 0.6781800389289856\n",
      "Train: Epoch [7], Batch [147/938], Loss: 0.8943174481391907\n",
      "Train: Epoch [7], Batch [148/938], Loss: 0.5950100421905518\n",
      "Train: Epoch [7], Batch [149/938], Loss: 0.6982346773147583\n",
      "Train: Epoch [7], Batch [150/938], Loss: 0.6523631811141968\n",
      "Train: Epoch [7], Batch [151/938], Loss: 0.6014736890792847\n",
      "Train: Epoch [7], Batch [152/938], Loss: 0.5854064226150513\n",
      "Train: Epoch [7], Batch [153/938], Loss: 0.688660204410553\n",
      "Train: Epoch [7], Batch [154/938], Loss: 0.6468957662582397\n",
      "Train: Epoch [7], Batch [155/938], Loss: 0.5650104284286499\n",
      "Train: Epoch [7], Batch [156/938], Loss: 0.9527118802070618\n",
      "Train: Epoch [7], Batch [157/938], Loss: 0.4198288023471832\n",
      "Train: Epoch [7], Batch [158/938], Loss: 0.5657164454460144\n",
      "Train: Epoch [7], Batch [159/938], Loss: 0.5853307247161865\n",
      "Train: Epoch [7], Batch [160/938], Loss: 0.5460331439971924\n",
      "Train: Epoch [7], Batch [161/938], Loss: 0.7897696495056152\n",
      "Train: Epoch [7], Batch [162/938], Loss: 0.6908133029937744\n",
      "Train: Epoch [7], Batch [163/938], Loss: 0.5024288892745972\n",
      "Train: Epoch [7], Batch [164/938], Loss: 0.6875134706497192\n",
      "Train: Epoch [7], Batch [165/938], Loss: 0.6834773421287537\n",
      "Train: Epoch [7], Batch [166/938], Loss: 0.7081758975982666\n",
      "Train: Epoch [7], Batch [167/938], Loss: 0.35680609941482544\n",
      "Train: Epoch [7], Batch [168/938], Loss: 0.45562487840652466\n",
      "Train: Epoch [7], Batch [169/938], Loss: 0.4949333965778351\n",
      "Train: Epoch [7], Batch [170/938], Loss: 0.588910698890686\n",
      "Train: Epoch [7], Batch [171/938], Loss: 0.603766918182373\n",
      "Train: Epoch [7], Batch [172/938], Loss: 0.5943480134010315\n",
      "Train: Epoch [7], Batch [173/938], Loss: 0.49919480085372925\n",
      "Train: Epoch [7], Batch [174/938], Loss: 0.6053694486618042\n",
      "Train: Epoch [7], Batch [175/938], Loss: 0.547012984752655\n",
      "Train: Epoch [7], Batch [176/938], Loss: 0.5176135301589966\n",
      "Train: Epoch [7], Batch [177/938], Loss: 0.6931180953979492\n",
      "Train: Epoch [7], Batch [178/938], Loss: 0.5015997886657715\n",
      "Train: Epoch [7], Batch [179/938], Loss: 0.5364081263542175\n",
      "Train: Epoch [7], Batch [180/938], Loss: 0.712525486946106\n",
      "Train: Epoch [7], Batch [181/938], Loss: 0.6061721444129944\n",
      "Train: Epoch [7], Batch [182/938], Loss: 0.6138911247253418\n",
      "Train: Epoch [7], Batch [183/938], Loss: 0.5112936496734619\n",
      "Train: Epoch [7], Batch [184/938], Loss: 0.6562595367431641\n",
      "Train: Epoch [7], Batch [185/938], Loss: 0.6074837446212769\n",
      "Train: Epoch [7], Batch [186/938], Loss: 0.6472824811935425\n",
      "Train: Epoch [7], Batch [187/938], Loss: 0.5195817947387695\n",
      "Train: Epoch [7], Batch [188/938], Loss: 0.48570728302001953\n",
      "Train: Epoch [7], Batch [189/938], Loss: 0.49377262592315674\n",
      "Train: Epoch [7], Batch [190/938], Loss: 0.6341930627822876\n",
      "Train: Epoch [7], Batch [191/938], Loss: 0.6840533018112183\n",
      "Train: Epoch [7], Batch [192/938], Loss: 0.6361483335494995\n",
      "Train: Epoch [7], Batch [193/938], Loss: 0.5772218108177185\n",
      "Train: Epoch [7], Batch [194/938], Loss: 0.6244183778762817\n",
      "Train: Epoch [7], Batch [195/938], Loss: 0.4739842116832733\n",
      "Train: Epoch [7], Batch [196/938], Loss: 0.550667405128479\n",
      "Train: Epoch [7], Batch [197/938], Loss: 0.5456960201263428\n",
      "Train: Epoch [7], Batch [198/938], Loss: 0.6460734009742737\n",
      "Train: Epoch [7], Batch [199/938], Loss: 0.5873816013336182\n",
      "Train: Epoch [7], Batch [200/938], Loss: 0.673581600189209\n",
      "Train: Epoch [7], Batch [201/938], Loss: 0.6945289373397827\n",
      "Train: Epoch [7], Batch [202/938], Loss: 0.5164542198181152\n",
      "Train: Epoch [7], Batch [203/938], Loss: 0.7014932632446289\n",
      "Train: Epoch [7], Batch [204/938], Loss: 0.7650551199913025\n",
      "Train: Epoch [7], Batch [205/938], Loss: 0.39223337173461914\n",
      "Train: Epoch [7], Batch [206/938], Loss: 0.7759374380111694\n",
      "Train: Epoch [7], Batch [207/938], Loss: 0.49267327785491943\n",
      "Train: Epoch [7], Batch [208/938], Loss: 0.6102747321128845\n",
      "Train: Epoch [7], Batch [209/938], Loss: 0.5904889106750488\n",
      "Train: Epoch [7], Batch [210/938], Loss: 0.7774738073348999\n",
      "Train: Epoch [7], Batch [211/938], Loss: 1.1491705179214478\n",
      "Train: Epoch [7], Batch [212/938], Loss: 0.5596503615379333\n",
      "Train: Epoch [7], Batch [213/938], Loss: 0.659561038017273\n",
      "Train: Epoch [7], Batch [214/938], Loss: 0.5983066558837891\n",
      "Train: Epoch [7], Batch [215/938], Loss: 0.6662534475326538\n",
      "Train: Epoch [7], Batch [216/938], Loss: 0.6646684408187866\n",
      "Train: Epoch [7], Batch [217/938], Loss: 0.6408019065856934\n",
      "Train: Epoch [7], Batch [218/938], Loss: 0.7622711658477783\n",
      "Train: Epoch [7], Batch [219/938], Loss: 0.7447636127471924\n",
      "Train: Epoch [7], Batch [220/938], Loss: 0.38459834456443787\n",
      "Train: Epoch [7], Batch [221/938], Loss: 0.5453845858573914\n",
      "Train: Epoch [7], Batch [222/938], Loss: 0.5720885992050171\n",
      "Train: Epoch [7], Batch [223/938], Loss: 0.45685625076293945\n",
      "Train: Epoch [7], Batch [224/938], Loss: 0.6166468858718872\n",
      "Train: Epoch [7], Batch [225/938], Loss: 0.68152916431427\n",
      "Train: Epoch [7], Batch [226/938], Loss: 0.5882213711738586\n",
      "Train: Epoch [7], Batch [227/938], Loss: 0.5687991976737976\n",
      "Train: Epoch [7], Batch [228/938], Loss: 0.598776638507843\n",
      "Train: Epoch [7], Batch [229/938], Loss: 0.6311699748039246\n",
      "Train: Epoch [7], Batch [230/938], Loss: 0.5017343759536743\n",
      "Train: Epoch [7], Batch [231/938], Loss: 0.767808198928833\n",
      "Train: Epoch [7], Batch [232/938], Loss: 0.4616481065750122\n",
      "Train: Epoch [7], Batch [233/938], Loss: 0.7133166790008545\n",
      "Train: Epoch [7], Batch [234/938], Loss: 0.5682774782180786\n",
      "Train: Epoch [7], Batch [235/938], Loss: 0.5352991223335266\n",
      "Train: Epoch [7], Batch [236/938], Loss: 0.6147327423095703\n",
      "Train: Epoch [7], Batch [237/938], Loss: 0.5330650210380554\n",
      "Train: Epoch [7], Batch [238/938], Loss: 0.834701657295227\n",
      "Train: Epoch [7], Batch [239/938], Loss: 0.7506227493286133\n",
      "Train: Epoch [7], Batch [240/938], Loss: 0.540915846824646\n",
      "Train: Epoch [7], Batch [241/938], Loss: 0.7280511260032654\n",
      "Train: Epoch [7], Batch [242/938], Loss: 0.7370111346244812\n",
      "Train: Epoch [7], Batch [243/938], Loss: 0.5949034690856934\n",
      "Train: Epoch [7], Batch [244/938], Loss: 0.5140102505683899\n",
      "Train: Epoch [7], Batch [245/938], Loss: 0.7257727980613708\n",
      "Train: Epoch [7], Batch [246/938], Loss: 0.6155363321304321\n",
      "Train: Epoch [7], Batch [247/938], Loss: 0.6081543564796448\n",
      "Train: Epoch [7], Batch [248/938], Loss: 0.5477161407470703\n",
      "Train: Epoch [7], Batch [249/938], Loss: 0.490038126707077\n",
      "Train: Epoch [7], Batch [250/938], Loss: 0.6065893173217773\n",
      "Train: Epoch [7], Batch [251/938], Loss: 0.5036075711250305\n",
      "Train: Epoch [7], Batch [252/938], Loss: 0.5254001617431641\n",
      "Train: Epoch [7], Batch [253/938], Loss: 0.6527210474014282\n",
      "Train: Epoch [7], Batch [254/938], Loss: 0.6275972127914429\n",
      "Train: Epoch [7], Batch [255/938], Loss: 0.6525724530220032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [7], Batch [256/938], Loss: 0.510712742805481\n",
      "Train: Epoch [7], Batch [257/938], Loss: 0.5132385492324829\n",
      "Train: Epoch [7], Batch [258/938], Loss: 0.5603847503662109\n",
      "Train: Epoch [7], Batch [259/938], Loss: 0.5383166074752808\n",
      "Train: Epoch [7], Batch [260/938], Loss: 0.7546029090881348\n",
      "Train: Epoch [7], Batch [261/938], Loss: 0.5539408922195435\n",
      "Train: Epoch [7], Batch [262/938], Loss: 0.6378011107444763\n",
      "Train: Epoch [7], Batch [263/938], Loss: 0.4662001132965088\n",
      "Train: Epoch [7], Batch [264/938], Loss: 0.5594311952590942\n",
      "Train: Epoch [7], Batch [265/938], Loss: 0.6783479452133179\n",
      "Train: Epoch [7], Batch [266/938], Loss: 0.9996948838233948\n",
      "Train: Epoch [7], Batch [267/938], Loss: 0.6512541770935059\n",
      "Train: Epoch [7], Batch [268/938], Loss: 0.6258751749992371\n",
      "Train: Epoch [7], Batch [269/938], Loss: 0.6489173173904419\n",
      "Train: Epoch [7], Batch [270/938], Loss: 0.7041558027267456\n",
      "Train: Epoch [7], Batch [271/938], Loss: 0.5241252779960632\n",
      "Train: Epoch [7], Batch [272/938], Loss: 0.5393799543380737\n",
      "Train: Epoch [7], Batch [273/938], Loss: 0.5948917865753174\n",
      "Train: Epoch [7], Batch [274/938], Loss: 0.5101903080940247\n",
      "Train: Epoch [7], Batch [275/938], Loss: 0.5653038620948792\n",
      "Train: Epoch [7], Batch [276/938], Loss: 0.550500214099884\n",
      "Train: Epoch [7], Batch [277/938], Loss: 0.6659578680992126\n",
      "Train: Epoch [7], Batch [278/938], Loss: 0.6049375534057617\n",
      "Train: Epoch [7], Batch [279/938], Loss: 0.4103426933288574\n",
      "Train: Epoch [7], Batch [280/938], Loss: 0.5044086575508118\n",
      "Train: Epoch [7], Batch [281/938], Loss: 0.5322533845901489\n",
      "Train: Epoch [7], Batch [282/938], Loss: 0.6801586747169495\n",
      "Train: Epoch [7], Batch [283/938], Loss: 0.47634851932525635\n",
      "Train: Epoch [7], Batch [284/938], Loss: 0.8066630959510803\n",
      "Train: Epoch [7], Batch [285/938], Loss: 0.6046596765518188\n",
      "Train: Epoch [7], Batch [286/938], Loss: 0.5726251602172852\n",
      "Train: Epoch [7], Batch [287/938], Loss: 0.705625593662262\n",
      "Train: Epoch [7], Batch [288/938], Loss: 0.6582233905792236\n",
      "Train: Epoch [7], Batch [289/938], Loss: 0.59136962890625\n",
      "Train: Epoch [7], Batch [290/938], Loss: 0.5378143191337585\n",
      "Train: Epoch [7], Batch [291/938], Loss: 0.48803049325942993\n",
      "Train: Epoch [7], Batch [292/938], Loss: 0.6146348714828491\n",
      "Train: Epoch [7], Batch [293/938], Loss: 0.6347469687461853\n",
      "Train: Epoch [7], Batch [294/938], Loss: 0.49600929021835327\n",
      "Train: Epoch [7], Batch [295/938], Loss: 0.5336756110191345\n",
      "Train: Epoch [7], Batch [296/938], Loss: 0.5955214500427246\n",
      "Train: Epoch [7], Batch [297/938], Loss: 0.48377883434295654\n",
      "Train: Epoch [7], Batch [298/938], Loss: 0.46178996562957764\n",
      "Train: Epoch [7], Batch [299/938], Loss: 0.6030852198600769\n",
      "Train: Epoch [7], Batch [300/938], Loss: 0.7360835075378418\n",
      "Train: Epoch [7], Batch [301/938], Loss: 0.5163858532905579\n",
      "Train: Epoch [7], Batch [302/938], Loss: 0.5211209654808044\n",
      "Train: Epoch [7], Batch [303/938], Loss: 0.6395716667175293\n",
      "Train: Epoch [7], Batch [304/938], Loss: 0.5859605669975281\n",
      "Train: Epoch [7], Batch [305/938], Loss: 0.681909441947937\n",
      "Train: Epoch [7], Batch [306/938], Loss: 0.5737971067428589\n",
      "Train: Epoch [7], Batch [307/938], Loss: 0.5349022746086121\n",
      "Train: Epoch [7], Batch [308/938], Loss: 0.6242181062698364\n",
      "Train: Epoch [7], Batch [309/938], Loss: 0.47993147373199463\n",
      "Train: Epoch [7], Batch [310/938], Loss: 0.5611991286277771\n",
      "Train: Epoch [7], Batch [311/938], Loss: 0.6337858438491821\n",
      "Train: Epoch [7], Batch [312/938], Loss: 0.664178729057312\n",
      "Train: Epoch [7], Batch [313/938], Loss: 0.4151247441768646\n",
      "Train: Epoch [7], Batch [314/938], Loss: 0.557691216468811\n",
      "Train: Epoch [7], Batch [315/938], Loss: 0.5409097671508789\n",
      "Train: Epoch [7], Batch [316/938], Loss: 0.6014165282249451\n",
      "Train: Epoch [7], Batch [317/938], Loss: 0.6708049774169922\n",
      "Train: Epoch [7], Batch [318/938], Loss: 0.6649166345596313\n",
      "Train: Epoch [7], Batch [319/938], Loss: 0.727439820766449\n",
      "Train: Epoch [7], Batch [320/938], Loss: 0.7499713897705078\n",
      "Train: Epoch [7], Batch [321/938], Loss: 0.7268962860107422\n",
      "Train: Epoch [7], Batch [322/938], Loss: 0.7518667578697205\n",
      "Train: Epoch [7], Batch [323/938], Loss: 0.4739747941493988\n",
      "Train: Epoch [7], Batch [324/938], Loss: 0.4536810517311096\n",
      "Train: Epoch [7], Batch [325/938], Loss: 0.6731247305870056\n",
      "Train: Epoch [7], Batch [326/938], Loss: 0.436475545167923\n",
      "Train: Epoch [7], Batch [327/938], Loss: 0.6091281771659851\n",
      "Train: Epoch [7], Batch [328/938], Loss: 0.5589333772659302\n",
      "Train: Epoch [7], Batch [329/938], Loss: 0.5236710906028748\n",
      "Train: Epoch [7], Batch [330/938], Loss: 0.35195907950401306\n",
      "Train: Epoch [7], Batch [331/938], Loss: 0.511847972869873\n",
      "Train: Epoch [7], Batch [332/938], Loss: 0.4872897267341614\n",
      "Train: Epoch [7], Batch [333/938], Loss: 0.5903578996658325\n",
      "Train: Epoch [7], Batch [334/938], Loss: 0.6269620656967163\n",
      "Train: Epoch [7], Batch [335/938], Loss: 0.5519448518753052\n",
      "Train: Epoch [7], Batch [336/938], Loss: 0.6727473735809326\n",
      "Train: Epoch [7], Batch [337/938], Loss: 0.5919435024261475\n",
      "Train: Epoch [7], Batch [338/938], Loss: 0.5447378158569336\n",
      "Train: Epoch [7], Batch [339/938], Loss: 0.47865355014801025\n",
      "Train: Epoch [7], Batch [340/938], Loss: 0.6886869072914124\n",
      "Train: Epoch [7], Batch [341/938], Loss: 0.6522714495658875\n",
      "Train: Epoch [7], Batch [342/938], Loss: 0.8145246505737305\n",
      "Train: Epoch [7], Batch [343/938], Loss: 0.5106445550918579\n",
      "Train: Epoch [7], Batch [344/938], Loss: 0.6972374320030212\n",
      "Train: Epoch [7], Batch [345/938], Loss: 0.6479336619377136\n",
      "Train: Epoch [7], Batch [346/938], Loss: 0.5513107776641846\n",
      "Train: Epoch [7], Batch [347/938], Loss: 0.6197272539138794\n",
      "Train: Epoch [7], Batch [348/938], Loss: 0.58868008852005\n",
      "Train: Epoch [7], Batch [349/938], Loss: 0.6846650838851929\n",
      "Train: Epoch [7], Batch [350/938], Loss: 0.9869362115859985\n",
      "Train: Epoch [7], Batch [351/938], Loss: 0.605278730392456\n",
      "Train: Epoch [7], Batch [352/938], Loss: 0.41561779379844666\n",
      "Train: Epoch [7], Batch [353/938], Loss: 0.68617844581604\n",
      "Train: Epoch [7], Batch [354/938], Loss: 0.4472758173942566\n",
      "Train: Epoch [7], Batch [355/938], Loss: 0.5115917921066284\n",
      "Train: Epoch [7], Batch [356/938], Loss: 0.7585989236831665\n",
      "Train: Epoch [7], Batch [357/938], Loss: 0.6862958669662476\n",
      "Train: Epoch [7], Batch [358/938], Loss: 0.5555627942085266\n",
      "Train: Epoch [7], Batch [359/938], Loss: 0.6068271398544312\n",
      "Train: Epoch [7], Batch [360/938], Loss: 0.7719604969024658\n",
      "Train: Epoch [7], Batch [361/938], Loss: 0.5057247281074524\n",
      "Train: Epoch [7], Batch [362/938], Loss: 0.48766952753067017\n",
      "Train: Epoch [7], Batch [363/938], Loss: 0.5611047148704529\n",
      "Train: Epoch [7], Batch [364/938], Loss: 0.829633355140686\n",
      "Train: Epoch [7], Batch [365/938], Loss: 0.6411961913108826\n",
      "Train: Epoch [7], Batch [366/938], Loss: 0.5538132786750793\n",
      "Train: Epoch [7], Batch [367/938], Loss: 0.5452971458435059\n",
      "Train: Epoch [7], Batch [368/938], Loss: 0.49996405839920044\n",
      "Train: Epoch [7], Batch [369/938], Loss: 0.7126447558403015\n",
      "Train: Epoch [7], Batch [370/938], Loss: 0.45739299058914185\n",
      "Train: Epoch [7], Batch [371/938], Loss: 0.6676238775253296\n",
      "Train: Epoch [7], Batch [372/938], Loss: 0.6454741954803467\n",
      "Train: Epoch [7], Batch [373/938], Loss: 0.5195100903511047\n",
      "Train: Epoch [7], Batch [374/938], Loss: 0.565525472164154\n",
      "Train: Epoch [7], Batch [375/938], Loss: 0.5873878598213196\n",
      "Train: Epoch [7], Batch [376/938], Loss: 0.4951389729976654\n",
      "Train: Epoch [7], Batch [377/938], Loss: 0.5970766544342041\n",
      "Train: Epoch [7], Batch [378/938], Loss: 0.5889015793800354\n",
      "Train: Epoch [7], Batch [379/938], Loss: 0.5817608833312988\n",
      "Train: Epoch [7], Batch [380/938], Loss: 0.8184232711791992\n",
      "Train: Epoch [7], Batch [381/938], Loss: 0.8480302691459656\n",
      "Train: Epoch [7], Batch [382/938], Loss: 0.5752683877944946\n",
      "Train: Epoch [7], Batch [383/938], Loss: 0.6649551391601562\n",
      "Train: Epoch [7], Batch [384/938], Loss: 0.7441301345825195\n",
      "Train: Epoch [7], Batch [385/938], Loss: 0.6584007740020752\n",
      "Train: Epoch [7], Batch [386/938], Loss: 0.48829346895217896\n",
      "Train: Epoch [7], Batch [387/938], Loss: 0.5568293333053589\n",
      "Train: Epoch [7], Batch [388/938], Loss: 0.6112947463989258\n",
      "Train: Epoch [7], Batch [389/938], Loss: 0.6823807954788208\n",
      "Train: Epoch [7], Batch [390/938], Loss: 0.4499778747558594\n",
      "Train: Epoch [7], Batch [391/938], Loss: 0.42915552854537964\n",
      "Train: Epoch [7], Batch [392/938], Loss: 0.9645693898200989\n",
      "Train: Epoch [7], Batch [393/938], Loss: 0.4744572937488556\n",
      "Train: Epoch [7], Batch [394/938], Loss: 0.4173493981361389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [7], Batch [395/938], Loss: 0.5203616619110107\n",
      "Train: Epoch [7], Batch [396/938], Loss: 0.6735615134239197\n",
      "Train: Epoch [7], Batch [397/938], Loss: 0.6939151287078857\n",
      "Train: Epoch [7], Batch [398/938], Loss: 0.5462665557861328\n",
      "Train: Epoch [7], Batch [399/938], Loss: 0.8613358736038208\n",
      "Train: Epoch [7], Batch [400/938], Loss: 0.7733501195907593\n",
      "Train: Epoch [7], Batch [401/938], Loss: 0.46537625789642334\n",
      "Train: Epoch [7], Batch [402/938], Loss: 0.649634599685669\n",
      "Train: Epoch [7], Batch [403/938], Loss: 0.5344940423965454\n",
      "Train: Epoch [7], Batch [404/938], Loss: 1.0061016082763672\n",
      "Train: Epoch [7], Batch [405/938], Loss: 0.5734155178070068\n",
      "Train: Epoch [7], Batch [406/938], Loss: 0.4794262647628784\n",
      "Train: Epoch [7], Batch [407/938], Loss: 0.5521622896194458\n",
      "Train: Epoch [7], Batch [408/938], Loss: 0.37316593527793884\n",
      "Train: Epoch [7], Batch [409/938], Loss: 0.4452004134654999\n",
      "Train: Epoch [7], Batch [410/938], Loss: 0.4946970045566559\n",
      "Train: Epoch [7], Batch [411/938], Loss: 0.5384863615036011\n",
      "Train: Epoch [7], Batch [412/938], Loss: 0.5739984512329102\n",
      "Train: Epoch [7], Batch [413/938], Loss: 0.7714560031890869\n",
      "Train: Epoch [7], Batch [414/938], Loss: 0.7020691633224487\n",
      "Train: Epoch [7], Batch [415/938], Loss: 0.4107871651649475\n",
      "Train: Epoch [7], Batch [416/938], Loss: 0.582974910736084\n",
      "Train: Epoch [7], Batch [417/938], Loss: 0.6965368986129761\n",
      "Train: Epoch [7], Batch [418/938], Loss: 0.7420084476470947\n",
      "Train: Epoch [7], Batch [419/938], Loss: 1.0089479684829712\n",
      "Train: Epoch [7], Batch [420/938], Loss: 0.6614513993263245\n",
      "Train: Epoch [7], Batch [421/938], Loss: 0.6468578577041626\n",
      "Train: Epoch [7], Batch [422/938], Loss: 0.747400164604187\n",
      "Train: Epoch [7], Batch [423/938], Loss: 0.616638720035553\n",
      "Train: Epoch [7], Batch [424/938], Loss: 0.6316336393356323\n",
      "Train: Epoch [7], Batch [425/938], Loss: 0.6318082809448242\n",
      "Train: Epoch [7], Batch [426/938], Loss: 0.5626446604728699\n",
      "Train: Epoch [7], Batch [427/938], Loss: 0.6251499056816101\n",
      "Train: Epoch [7], Batch [428/938], Loss: 0.5912869572639465\n",
      "Train: Epoch [7], Batch [429/938], Loss: 0.499637246131897\n",
      "Train: Epoch [7], Batch [430/938], Loss: 0.6178883910179138\n",
      "Train: Epoch [7], Batch [431/938], Loss: 0.6149083971977234\n",
      "Train: Epoch [7], Batch [432/938], Loss: 0.4560222327709198\n",
      "Train: Epoch [7], Batch [433/938], Loss: 0.6666088104248047\n",
      "Train: Epoch [7], Batch [434/938], Loss: 0.5089318752288818\n",
      "Train: Epoch [7], Batch [435/938], Loss: 0.420718789100647\n",
      "Train: Epoch [7], Batch [436/938], Loss: 0.5011656284332275\n",
      "Train: Epoch [7], Batch [437/938], Loss: 0.3258174657821655\n",
      "Train: Epoch [7], Batch [438/938], Loss: 0.7643983960151672\n",
      "Train: Epoch [7], Batch [439/938], Loss: 0.501879096031189\n",
      "Train: Epoch [7], Batch [440/938], Loss: 0.6120943427085876\n",
      "Train: Epoch [7], Batch [441/938], Loss: 0.5577769875526428\n",
      "Train: Epoch [7], Batch [442/938], Loss: 0.5843467712402344\n",
      "Train: Epoch [7], Batch [443/938], Loss: 0.7293249368667603\n",
      "Train: Epoch [7], Batch [444/938], Loss: 0.6959137916564941\n",
      "Train: Epoch [7], Batch [445/938], Loss: 0.7863068580627441\n",
      "Train: Epoch [7], Batch [446/938], Loss: 0.9334268569946289\n",
      "Train: Epoch [7], Batch [447/938], Loss: 0.5091139078140259\n",
      "Train: Epoch [7], Batch [448/938], Loss: 0.4203377068042755\n",
      "Train: Epoch [7], Batch [449/938], Loss: 0.7512743473052979\n",
      "Train: Epoch [7], Batch [450/938], Loss: 0.5838937759399414\n",
      "Train: Epoch [7], Batch [451/938], Loss: 0.5976034998893738\n",
      "Train: Epoch [7], Batch [452/938], Loss: 0.6648799777030945\n",
      "Train: Epoch [7], Batch [453/938], Loss: 0.40215182304382324\n",
      "Train: Epoch [7], Batch [454/938], Loss: 1.002516269683838\n",
      "Train: Epoch [7], Batch [455/938], Loss: 0.6358005404472351\n",
      "Train: Epoch [7], Batch [456/938], Loss: 0.6242616176605225\n",
      "Train: Epoch [7], Batch [457/938], Loss: 0.6737332940101624\n",
      "Train: Epoch [7], Batch [458/938], Loss: 0.46673935651779175\n",
      "Train: Epoch [7], Batch [459/938], Loss: 0.5065368413925171\n",
      "Train: Epoch [7], Batch [460/938], Loss: 0.5487552285194397\n",
      "Train: Epoch [7], Batch [461/938], Loss: 0.4550917148590088\n",
      "Train: Epoch [7], Batch [462/938], Loss: 0.6843808889389038\n",
      "Train: Epoch [7], Batch [463/938], Loss: 0.7934659719467163\n",
      "Train: Epoch [7], Batch [464/938], Loss: 0.634587824344635\n",
      "Train: Epoch [7], Batch [465/938], Loss: 0.6328240633010864\n",
      "Train: Epoch [7], Batch [466/938], Loss: 0.5782175064086914\n",
      "Train: Epoch [7], Batch [467/938], Loss: 0.5280569791793823\n",
      "Train: Epoch [7], Batch [468/938], Loss: 0.44528135657310486\n",
      "Train: Epoch [7], Batch [469/938], Loss: 0.698523759841919\n",
      "Train: Epoch [7], Batch [470/938], Loss: 0.5619207620620728\n",
      "Train: Epoch [7], Batch [471/938], Loss: 0.6300926208496094\n",
      "Train: Epoch [7], Batch [472/938], Loss: 0.6206415891647339\n",
      "Train: Epoch [7], Batch [473/938], Loss: 0.5811516046524048\n",
      "Train: Epoch [7], Batch [474/938], Loss: 0.6790375709533691\n",
      "Train: Epoch [7], Batch [475/938], Loss: 0.5682612061500549\n",
      "Train: Epoch [7], Batch [476/938], Loss: 0.6575567722320557\n",
      "Train: Epoch [7], Batch [477/938], Loss: 0.5644360184669495\n",
      "Train: Epoch [7], Batch [478/938], Loss: 0.6501068472862244\n",
      "Train: Epoch [7], Batch [479/938], Loss: 0.5815422534942627\n",
      "Train: Epoch [7], Batch [480/938], Loss: 0.5166472792625427\n",
      "Train: Epoch [7], Batch [481/938], Loss: 0.6529405117034912\n",
      "Train: Epoch [7], Batch [482/938], Loss: 0.8892948031425476\n",
      "Train: Epoch [7], Batch [483/938], Loss: 0.7567013502120972\n",
      "Train: Epoch [7], Batch [484/938], Loss: 0.6844501495361328\n",
      "Train: Epoch [7], Batch [485/938], Loss: 0.7968093156814575\n",
      "Train: Epoch [7], Batch [486/938], Loss: 0.6337034106254578\n",
      "Train: Epoch [7], Batch [487/938], Loss: 0.6886992454528809\n",
      "Train: Epoch [7], Batch [488/938], Loss: 0.6518226861953735\n",
      "Train: Epoch [7], Batch [489/938], Loss: 0.41582366824150085\n",
      "Train: Epoch [7], Batch [490/938], Loss: 0.45550674200057983\n",
      "Train: Epoch [7], Batch [491/938], Loss: 0.564889669418335\n",
      "Train: Epoch [7], Batch [492/938], Loss: 0.6281534433364868\n",
      "Train: Epoch [7], Batch [493/938], Loss: 0.5553142428398132\n",
      "Train: Epoch [7], Batch [494/938], Loss: 0.5778063535690308\n",
      "Train: Epoch [7], Batch [495/938], Loss: 0.6300176978111267\n",
      "Train: Epoch [7], Batch [496/938], Loss: 0.5891738533973694\n",
      "Train: Epoch [7], Batch [497/938], Loss: 0.32968372106552124\n",
      "Train: Epoch [7], Batch [498/938], Loss: 0.5476095676422119\n",
      "Train: Epoch [7], Batch [499/938], Loss: 0.41242146492004395\n",
      "Train: Epoch [7], Batch [500/938], Loss: 0.5208195447921753\n",
      "Train: Epoch [7], Batch [501/938], Loss: 0.6069207191467285\n",
      "Train: Epoch [7], Batch [502/938], Loss: 0.4339103102684021\n",
      "Train: Epoch [7], Batch [503/938], Loss: 0.5610071420669556\n",
      "Train: Epoch [7], Batch [504/938], Loss: 0.6724892854690552\n",
      "Train: Epoch [7], Batch [505/938], Loss: 0.8690598607063293\n",
      "Train: Epoch [7], Batch [506/938], Loss: 0.5259573459625244\n",
      "Train: Epoch [7], Batch [507/938], Loss: 0.7633934020996094\n",
      "Train: Epoch [7], Batch [508/938], Loss: 0.5886420011520386\n",
      "Train: Epoch [7], Batch [509/938], Loss: 0.728073000907898\n",
      "Train: Epoch [7], Batch [510/938], Loss: 0.7161161303520203\n",
      "Train: Epoch [7], Batch [511/938], Loss: 0.7481433153152466\n",
      "Train: Epoch [7], Batch [512/938], Loss: 0.5659841299057007\n",
      "Train: Epoch [7], Batch [513/938], Loss: 0.5796374082565308\n",
      "Train: Epoch [7], Batch [514/938], Loss: 0.516654908657074\n",
      "Train: Epoch [7], Batch [515/938], Loss: 0.4545876085758209\n",
      "Train: Epoch [7], Batch [516/938], Loss: 0.5156295299530029\n",
      "Train: Epoch [7], Batch [517/938], Loss: 0.6633419990539551\n",
      "Train: Epoch [7], Batch [518/938], Loss: 0.8554651737213135\n",
      "Train: Epoch [7], Batch [519/938], Loss: 0.6316825747489929\n",
      "Train: Epoch [7], Batch [520/938], Loss: 0.720984697341919\n",
      "Train: Epoch [7], Batch [521/938], Loss: 0.5301830768585205\n",
      "Train: Epoch [7], Batch [522/938], Loss: 0.5816354751586914\n",
      "Train: Epoch [7], Batch [523/938], Loss: 0.5556278228759766\n",
      "Train: Epoch [7], Batch [524/938], Loss: 0.5260993242263794\n",
      "Train: Epoch [7], Batch [525/938], Loss: 0.6637451648712158\n",
      "Train: Epoch [7], Batch [526/938], Loss: 0.6411328911781311\n",
      "Train: Epoch [7], Batch [527/938], Loss: 0.5360726714134216\n",
      "Train: Epoch [7], Batch [528/938], Loss: 0.4288206398487091\n",
      "Train: Epoch [7], Batch [529/938], Loss: 0.5672406554222107\n",
      "Train: Epoch [7], Batch [530/938], Loss: 0.5371172428131104\n",
      "Train: Epoch [7], Batch [531/938], Loss: 0.5699467062950134\n",
      "Train: Epoch [7], Batch [532/938], Loss: 0.6342737674713135\n",
      "Train: Epoch [7], Batch [533/938], Loss: 0.6291964054107666\n",
      "Train: Epoch [7], Batch [534/938], Loss: 0.8099435567855835\n",
      "Train: Epoch [7], Batch [535/938], Loss: 0.7570177316665649\n",
      "Train: Epoch [7], Batch [536/938], Loss: 0.44076764583587646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [7], Batch [537/938], Loss: 0.5159618258476257\n",
      "Train: Epoch [7], Batch [538/938], Loss: 0.5500773191452026\n",
      "Train: Epoch [7], Batch [539/938], Loss: 0.5841164588928223\n",
      "Train: Epoch [7], Batch [540/938], Loss: 0.6674628257751465\n",
      "Train: Epoch [7], Batch [541/938], Loss: 0.6141875982284546\n",
      "Train: Epoch [7], Batch [542/938], Loss: 0.6859927177429199\n",
      "Train: Epoch [7], Batch [543/938], Loss: 0.5119455456733704\n",
      "Train: Epoch [7], Batch [544/938], Loss: 0.5747052431106567\n",
      "Train: Epoch [7], Batch [545/938], Loss: 0.7908152937889099\n",
      "Train: Epoch [7], Batch [546/938], Loss: 0.4741707444190979\n",
      "Train: Epoch [7], Batch [547/938], Loss: 0.6775400638580322\n",
      "Train: Epoch [7], Batch [548/938], Loss: 0.6064777374267578\n",
      "Train: Epoch [7], Batch [549/938], Loss: 0.507632851600647\n",
      "Train: Epoch [7], Batch [550/938], Loss: 0.5315001606941223\n",
      "Train: Epoch [7], Batch [551/938], Loss: 0.5902708768844604\n",
      "Train: Epoch [7], Batch [552/938], Loss: 0.6172017455101013\n",
      "Train: Epoch [7], Batch [553/938], Loss: 0.5862821340560913\n",
      "Train: Epoch [7], Batch [554/938], Loss: 0.5544484257698059\n",
      "Train: Epoch [7], Batch [555/938], Loss: 0.5037704110145569\n",
      "Train: Epoch [7], Batch [556/938], Loss: 0.5768251419067383\n",
      "Train: Epoch [7], Batch [557/938], Loss: 0.5952175855636597\n",
      "Train: Epoch [7], Batch [558/938], Loss: 0.8143609762191772\n",
      "Train: Epoch [7], Batch [559/938], Loss: 0.5769491195678711\n",
      "Train: Epoch [7], Batch [560/938], Loss: 0.6193814277648926\n",
      "Train: Epoch [7], Batch [561/938], Loss: 0.5834457278251648\n",
      "Train: Epoch [7], Batch [562/938], Loss: 0.6143848896026611\n",
      "Train: Epoch [7], Batch [563/938], Loss: 0.6325425505638123\n",
      "Train: Epoch [7], Batch [564/938], Loss: 0.6475921273231506\n",
      "Train: Epoch [7], Batch [565/938], Loss: 0.5419970154762268\n",
      "Train: Epoch [7], Batch [566/938], Loss: 0.6532827615737915\n",
      "Train: Epoch [7], Batch [567/938], Loss: 0.597042977809906\n",
      "Train: Epoch [7], Batch [568/938], Loss: 0.6042722463607788\n",
      "Train: Epoch [7], Batch [569/938], Loss: 0.6538506746292114\n",
      "Train: Epoch [7], Batch [570/938], Loss: 0.395822137594223\n",
      "Train: Epoch [7], Batch [571/938], Loss: 0.5488935708999634\n",
      "Train: Epoch [7], Batch [572/938], Loss: 0.35283684730529785\n",
      "Train: Epoch [7], Batch [573/938], Loss: 0.5133457779884338\n",
      "Train: Epoch [7], Batch [574/938], Loss: 0.5087045431137085\n",
      "Train: Epoch [7], Batch [575/938], Loss: 0.6709250807762146\n",
      "Train: Epoch [7], Batch [576/938], Loss: 0.5992079973220825\n",
      "Train: Epoch [7], Batch [577/938], Loss: 0.5205600261688232\n",
      "Train: Epoch [7], Batch [578/938], Loss: 0.5473394989967346\n",
      "Train: Epoch [7], Batch [579/938], Loss: 0.576524019241333\n",
      "Train: Epoch [7], Batch [580/938], Loss: 0.5624741315841675\n",
      "Train: Epoch [7], Batch [581/938], Loss: 0.45142364501953125\n",
      "Train: Epoch [7], Batch [582/938], Loss: 0.8463308811187744\n",
      "Train: Epoch [7], Batch [583/938], Loss: 0.49456384778022766\n",
      "Train: Epoch [7], Batch [584/938], Loss: 0.4403080344200134\n",
      "Train: Epoch [7], Batch [585/938], Loss: 0.709764301776886\n",
      "Train: Epoch [7], Batch [586/938], Loss: 0.5664410591125488\n",
      "Train: Epoch [7], Batch [587/938], Loss: 0.649905800819397\n",
      "Train: Epoch [7], Batch [588/938], Loss: 0.4423474073410034\n",
      "Train: Epoch [7], Batch [589/938], Loss: 0.5247745513916016\n",
      "Train: Epoch [7], Batch [590/938], Loss: 0.7142543196678162\n",
      "Train: Epoch [7], Batch [591/938], Loss: 0.7213143110275269\n",
      "Train: Epoch [7], Batch [592/938], Loss: 0.5407668352127075\n",
      "Train: Epoch [7], Batch [593/938], Loss: 0.6551883220672607\n",
      "Train: Epoch [7], Batch [594/938], Loss: 0.6237121224403381\n",
      "Train: Epoch [7], Batch [595/938], Loss: 0.6065381169319153\n",
      "Train: Epoch [7], Batch [596/938], Loss: 0.7338335514068604\n",
      "Train: Epoch [7], Batch [597/938], Loss: 0.5651724338531494\n",
      "Train: Epoch [7], Batch [598/938], Loss: 0.5902190804481506\n",
      "Train: Epoch [7], Batch [599/938], Loss: 0.4398646950721741\n",
      "Train: Epoch [7], Batch [600/938], Loss: 0.6515804529190063\n",
      "Train: Epoch [7], Batch [601/938], Loss: 0.6935157179832458\n",
      "Train: Epoch [7], Batch [602/938], Loss: 0.5553745627403259\n",
      "Train: Epoch [7], Batch [603/938], Loss: 0.470078706741333\n",
      "Train: Epoch [7], Batch [604/938], Loss: 0.6264798045158386\n",
      "Train: Epoch [7], Batch [605/938], Loss: 0.35010379552841187\n",
      "Train: Epoch [7], Batch [606/938], Loss: 0.6489752531051636\n",
      "Train: Epoch [7], Batch [607/938], Loss: 0.6628551483154297\n",
      "Train: Epoch [7], Batch [608/938], Loss: 0.6723290681838989\n",
      "Train: Epoch [7], Batch [609/938], Loss: 0.521765410900116\n",
      "Train: Epoch [7], Batch [610/938], Loss: 0.43950849771499634\n",
      "Train: Epoch [7], Batch [611/938], Loss: 0.6671476364135742\n",
      "Train: Epoch [7], Batch [612/938], Loss: 0.5890108346939087\n",
      "Train: Epoch [7], Batch [613/938], Loss: 0.6952680349349976\n",
      "Train: Epoch [7], Batch [614/938], Loss: 0.7470313310623169\n",
      "Train: Epoch [7], Batch [615/938], Loss: 0.44878897070884705\n",
      "Train: Epoch [7], Batch [616/938], Loss: 0.7575908899307251\n",
      "Train: Epoch [7], Batch [617/938], Loss: 0.6821624040603638\n",
      "Train: Epoch [7], Batch [618/938], Loss: 0.638903796672821\n",
      "Train: Epoch [7], Batch [619/938], Loss: 0.4946732521057129\n",
      "Train: Epoch [7], Batch [620/938], Loss: 0.6888465285301208\n",
      "Train: Epoch [7], Batch [621/938], Loss: 0.6520060300827026\n",
      "Train: Epoch [7], Batch [622/938], Loss: 0.6402046084403992\n",
      "Train: Epoch [7], Batch [623/938], Loss: 0.5879157781600952\n",
      "Train: Epoch [7], Batch [624/938], Loss: 0.671661913394928\n",
      "Train: Epoch [7], Batch [625/938], Loss: 0.5649908781051636\n",
      "Train: Epoch [7], Batch [626/938], Loss: 0.5134753584861755\n",
      "Train: Epoch [7], Batch [627/938], Loss: 0.5704220533370972\n",
      "Train: Epoch [7], Batch [628/938], Loss: 0.8154878616333008\n",
      "Train: Epoch [7], Batch [629/938], Loss: 0.7142171859741211\n",
      "Train: Epoch [7], Batch [630/938], Loss: 0.7328075766563416\n",
      "Train: Epoch [7], Batch [631/938], Loss: 0.6522929668426514\n",
      "Train: Epoch [7], Batch [632/938], Loss: 0.45886683464050293\n",
      "Train: Epoch [7], Batch [633/938], Loss: 0.8373966217041016\n",
      "Train: Epoch [7], Batch [634/938], Loss: 0.6199840307235718\n",
      "Train: Epoch [7], Batch [635/938], Loss: 0.5431955456733704\n",
      "Train: Epoch [7], Batch [636/938], Loss: 0.6338149309158325\n",
      "Train: Epoch [7], Batch [637/938], Loss: 0.6289452314376831\n",
      "Train: Epoch [7], Batch [638/938], Loss: 0.5349224209785461\n",
      "Train: Epoch [7], Batch [639/938], Loss: 0.5345763564109802\n",
      "Train: Epoch [7], Batch [640/938], Loss: 0.4874030649662018\n",
      "Train: Epoch [7], Batch [641/938], Loss: 0.8524870276451111\n",
      "Train: Epoch [7], Batch [642/938], Loss: 0.39782363176345825\n",
      "Train: Epoch [7], Batch [643/938], Loss: 0.4856818616390228\n",
      "Train: Epoch [7], Batch [644/938], Loss: 0.5244566202163696\n",
      "Train: Epoch [7], Batch [645/938], Loss: 0.600406289100647\n",
      "Train: Epoch [7], Batch [646/938], Loss: 0.6905755996704102\n",
      "Train: Epoch [7], Batch [647/938], Loss: 0.617820143699646\n",
      "Train: Epoch [7], Batch [648/938], Loss: 0.38843411207199097\n",
      "Train: Epoch [7], Batch [649/938], Loss: 0.625532865524292\n",
      "Train: Epoch [7], Batch [650/938], Loss: 0.5322061777114868\n",
      "Train: Epoch [7], Batch [651/938], Loss: 0.5612475872039795\n",
      "Train: Epoch [7], Batch [652/938], Loss: 0.698209822177887\n",
      "Train: Epoch [7], Batch [653/938], Loss: 0.7632420659065247\n",
      "Train: Epoch [7], Batch [654/938], Loss: 0.8216254711151123\n",
      "Train: Epoch [7], Batch [655/938], Loss: 0.6541699171066284\n",
      "Train: Epoch [7], Batch [656/938], Loss: 0.6795970797538757\n",
      "Train: Epoch [7], Batch [657/938], Loss: 0.49085569381713867\n",
      "Train: Epoch [7], Batch [658/938], Loss: 0.5741621851921082\n",
      "Train: Epoch [7], Batch [659/938], Loss: 0.46970582008361816\n",
      "Train: Epoch [7], Batch [660/938], Loss: 0.6992380619049072\n",
      "Train: Epoch [7], Batch [661/938], Loss: 0.5008395910263062\n",
      "Train: Epoch [7], Batch [662/938], Loss: 0.5557631254196167\n",
      "Train: Epoch [7], Batch [663/938], Loss: 0.5280200242996216\n",
      "Train: Epoch [7], Batch [664/938], Loss: 0.5956988334655762\n",
      "Train: Epoch [7], Batch [665/938], Loss: 0.6375497579574585\n",
      "Train: Epoch [7], Batch [666/938], Loss: 0.8887830972671509\n",
      "Train: Epoch [7], Batch [667/938], Loss: 0.5124089121818542\n",
      "Train: Epoch [7], Batch [668/938], Loss: 0.5449835658073425\n",
      "Train: Epoch [7], Batch [669/938], Loss: 0.4356451630592346\n",
      "Train: Epoch [7], Batch [670/938], Loss: 0.6152116656303406\n",
      "Train: Epoch [7], Batch [671/938], Loss: 0.40643423795700073\n",
      "Train: Epoch [7], Batch [672/938], Loss: 0.8419991731643677\n",
      "Train: Epoch [7], Batch [673/938], Loss: 0.6207034587860107\n",
      "Train: Epoch [7], Batch [674/938], Loss: 0.7170251607894897\n",
      "Train: Epoch [7], Batch [675/938], Loss: 0.7301098108291626\n",
      "Train: Epoch [7], Batch [676/938], Loss: 0.5884799957275391\n",
      "Train: Epoch [7], Batch [677/938], Loss: 0.357464075088501\n",
      "Train: Epoch [7], Batch [678/938], Loss: 0.4873763620853424\n",
      "Train: Epoch [7], Batch [679/938], Loss: 0.6788092851638794\n",
      "Train: Epoch [7], Batch [680/938], Loss: 0.5579025149345398\n",
      "Train: Epoch [7], Batch [681/938], Loss: 0.558447003364563\n",
      "Train: Epoch [7], Batch [682/938], Loss: 0.5006208419799805\n",
      "Train: Epoch [7], Batch [683/938], Loss: 0.6764800548553467\n",
      "Train: Epoch [7], Batch [684/938], Loss: 0.6200333833694458\n",
      "Train: Epoch [7], Batch [685/938], Loss: 0.6710616946220398\n",
      "Train: Epoch [7], Batch [686/938], Loss: 0.6343706250190735\n",
      "Train: Epoch [7], Batch [687/938], Loss: 0.7002909183502197\n",
      "Train: Epoch [7], Batch [688/938], Loss: 0.7272593975067139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [7], Batch [689/938], Loss: 0.3749212920665741\n",
      "Train: Epoch [7], Batch [690/938], Loss: 0.5562953948974609\n",
      "Train: Epoch [7], Batch [691/938], Loss: 0.466681569814682\n",
      "Train: Epoch [7], Batch [692/938], Loss: 0.5465986728668213\n",
      "Train: Epoch [7], Batch [693/938], Loss: 0.5274732112884521\n",
      "Train: Epoch [7], Batch [694/938], Loss: 0.5456806421279907\n",
      "Train: Epoch [7], Batch [695/938], Loss: 0.5822185277938843\n",
      "Train: Epoch [7], Batch [696/938], Loss: 0.5813935399055481\n",
      "Train: Epoch [7], Batch [697/938], Loss: 0.6953097581863403\n",
      "Train: Epoch [7], Batch [698/938], Loss: 0.7458820939064026\n",
      "Train: Epoch [7], Batch [699/938], Loss: 0.5124543905258179\n",
      "Train: Epoch [7], Batch [700/938], Loss: 0.6209371089935303\n",
      "Train: Epoch [7], Batch [701/938], Loss: 0.5485909581184387\n",
      "Train: Epoch [7], Batch [702/938], Loss: 0.5772684812545776\n",
      "Train: Epoch [7], Batch [703/938], Loss: 0.5637959241867065\n",
      "Train: Epoch [7], Batch [704/938], Loss: 0.639579713344574\n",
      "Train: Epoch [7], Batch [705/938], Loss: 0.5435832738876343\n",
      "Train: Epoch [7], Batch [706/938], Loss: 0.5542083978652954\n",
      "Train: Epoch [7], Batch [707/938], Loss: 0.6301639080047607\n",
      "Train: Epoch [7], Batch [708/938], Loss: 0.6553800106048584\n",
      "Train: Epoch [7], Batch [709/938], Loss: 0.569998025894165\n",
      "Train: Epoch [7], Batch [710/938], Loss: 0.7259056568145752\n",
      "Train: Epoch [7], Batch [711/938], Loss: 0.49096518754959106\n",
      "Train: Epoch [7], Batch [712/938], Loss: 0.6342877149581909\n",
      "Train: Epoch [7], Batch [713/938], Loss: 0.7048310041427612\n",
      "Train: Epoch [7], Batch [714/938], Loss: 0.5726370811462402\n",
      "Train: Epoch [7], Batch [715/938], Loss: 0.4609982669353485\n",
      "Train: Epoch [7], Batch [716/938], Loss: 0.5620960593223572\n",
      "Train: Epoch [7], Batch [717/938], Loss: 0.5068020820617676\n",
      "Train: Epoch [7], Batch [718/938], Loss: 0.5096868872642517\n",
      "Train: Epoch [7], Batch [719/938], Loss: 0.5937307476997375\n",
      "Train: Epoch [7], Batch [720/938], Loss: 0.5530340671539307\n",
      "Train: Epoch [7], Batch [721/938], Loss: 0.700548529624939\n",
      "Train: Epoch [7], Batch [722/938], Loss: 0.7193779349327087\n",
      "Train: Epoch [7], Batch [723/938], Loss: 0.3934141993522644\n",
      "Train: Epoch [7], Batch [724/938], Loss: 0.5068772435188293\n",
      "Train: Epoch [7], Batch [725/938], Loss: 0.5888059735298157\n",
      "Train: Epoch [7], Batch [726/938], Loss: 0.6247617602348328\n",
      "Train: Epoch [7], Batch [727/938], Loss: 0.513877272605896\n",
      "Train: Epoch [7], Batch [728/938], Loss: 0.49405595660209656\n",
      "Train: Epoch [7], Batch [729/938], Loss: 0.7393085956573486\n",
      "Train: Epoch [7], Batch [730/938], Loss: 0.5253466367721558\n",
      "Train: Epoch [7], Batch [731/938], Loss: 0.6846402883529663\n",
      "Train: Epoch [7], Batch [732/938], Loss: 0.5093887448310852\n",
      "Train: Epoch [7], Batch [733/938], Loss: 0.329021692276001\n",
      "Train: Epoch [7], Batch [734/938], Loss: 0.7641814947128296\n",
      "Train: Epoch [7], Batch [735/938], Loss: 0.8438060283660889\n",
      "Train: Epoch [7], Batch [736/938], Loss: 0.3878192901611328\n",
      "Train: Epoch [7], Batch [737/938], Loss: 0.540259838104248\n",
      "Train: Epoch [7], Batch [738/938], Loss: 0.6472020149230957\n",
      "Train: Epoch [7], Batch [739/938], Loss: 0.5030144453048706\n",
      "Train: Epoch [7], Batch [740/938], Loss: 0.5766086578369141\n",
      "Train: Epoch [7], Batch [741/938], Loss: 0.46934133768081665\n",
      "Train: Epoch [7], Batch [742/938], Loss: 0.692283570766449\n",
      "Train: Epoch [7], Batch [743/938], Loss: 0.7400569319725037\n",
      "Train: Epoch [7], Batch [744/938], Loss: 0.7832707166671753\n",
      "Train: Epoch [7], Batch [745/938], Loss: 0.8673063516616821\n",
      "Train: Epoch [7], Batch [746/938], Loss: 0.5868252515792847\n",
      "Train: Epoch [7], Batch [747/938], Loss: 0.4804353713989258\n",
      "Train: Epoch [7], Batch [748/938], Loss: 0.5797150135040283\n",
      "Train: Epoch [7], Batch [749/938], Loss: 0.46567869186401367\n",
      "Train: Epoch [7], Batch [750/938], Loss: 0.4709548056125641\n",
      "Train: Epoch [7], Batch [751/938], Loss: 0.6061494946479797\n",
      "Train: Epoch [7], Batch [752/938], Loss: 0.5592667460441589\n",
      "Train: Epoch [7], Batch [753/938], Loss: 0.7591763734817505\n",
      "Train: Epoch [7], Batch [754/938], Loss: 0.4897957146167755\n",
      "Train: Epoch [7], Batch [755/938], Loss: 0.5844454169273376\n",
      "Train: Epoch [7], Batch [756/938], Loss: 0.6083903908729553\n",
      "Train: Epoch [7], Batch [757/938], Loss: 0.6984750032424927\n",
      "Train: Epoch [7], Batch [758/938], Loss: 0.5964015126228333\n",
      "Train: Epoch [7], Batch [759/938], Loss: 0.5611972808837891\n",
      "Train: Epoch [7], Batch [760/938], Loss: 0.479633629322052\n",
      "Train: Epoch [7], Batch [761/938], Loss: 0.768387496471405\n",
      "Train: Epoch [7], Batch [762/938], Loss: 0.4247899353504181\n",
      "Train: Epoch [7], Batch [763/938], Loss: 0.41747474670410156\n",
      "Train: Epoch [7], Batch [764/938], Loss: 0.5361247062683105\n",
      "Train: Epoch [7], Batch [765/938], Loss: 0.734390377998352\n",
      "Train: Epoch [7], Batch [766/938], Loss: 0.41198450326919556\n",
      "Train: Epoch [7], Batch [767/938], Loss: 0.5841827988624573\n",
      "Train: Epoch [7], Batch [768/938], Loss: 0.7172712087631226\n",
      "Train: Epoch [7], Batch [769/938], Loss: 0.528986930847168\n",
      "Train: Epoch [7], Batch [770/938], Loss: 0.4599100649356842\n",
      "Train: Epoch [7], Batch [771/938], Loss: 0.5282489061355591\n",
      "Train: Epoch [7], Batch [772/938], Loss: 0.4994792938232422\n",
      "Train: Epoch [7], Batch [773/938], Loss: 0.5458548069000244\n",
      "Train: Epoch [7], Batch [774/938], Loss: 0.4210854768753052\n",
      "Train: Epoch [7], Batch [775/938], Loss: 0.506458044052124\n",
      "Train: Epoch [7], Batch [776/938], Loss: 0.608875036239624\n",
      "Train: Epoch [7], Batch [777/938], Loss: 0.5964300632476807\n",
      "Train: Epoch [7], Batch [778/938], Loss: 0.5823990702629089\n",
      "Train: Epoch [7], Batch [779/938], Loss: 0.3693026900291443\n",
      "Train: Epoch [7], Batch [780/938], Loss: 0.4722431004047394\n",
      "Train: Epoch [7], Batch [781/938], Loss: 0.6449302434921265\n",
      "Train: Epoch [7], Batch [782/938], Loss: 0.6386263370513916\n",
      "Train: Epoch [7], Batch [783/938], Loss: 0.4582768380641937\n",
      "Train: Epoch [7], Batch [784/938], Loss: 0.6038033962249756\n",
      "Train: Epoch [7], Batch [785/938], Loss: 0.5339528322219849\n",
      "Train: Epoch [7], Batch [786/938], Loss: 0.7902861833572388\n",
      "Train: Epoch [7], Batch [787/938], Loss: 0.45816588401794434\n",
      "Train: Epoch [7], Batch [788/938], Loss: 0.7763830423355103\n",
      "Train: Epoch [7], Batch [789/938], Loss: 0.5629172325134277\n",
      "Train: Epoch [7], Batch [790/938], Loss: 0.602698564529419\n",
      "Train: Epoch [7], Batch [791/938], Loss: 0.559669017791748\n",
      "Train: Epoch [7], Batch [792/938], Loss: 0.714544415473938\n",
      "Train: Epoch [7], Batch [793/938], Loss: 0.55880206823349\n",
      "Train: Epoch [7], Batch [794/938], Loss: 0.6544344425201416\n",
      "Train: Epoch [7], Batch [795/938], Loss: 0.6095584034919739\n",
      "Train: Epoch [7], Batch [796/938], Loss: 0.6508192420005798\n",
      "Train: Epoch [7], Batch [797/938], Loss: 0.5678386092185974\n",
      "Train: Epoch [7], Batch [798/938], Loss: 0.5755647420883179\n",
      "Train: Epoch [7], Batch [799/938], Loss: 0.4689972400665283\n",
      "Train: Epoch [7], Batch [800/938], Loss: 0.6754381060600281\n",
      "Train: Epoch [7], Batch [801/938], Loss: 0.6831226944923401\n",
      "Train: Epoch [7], Batch [802/938], Loss: 0.7303152084350586\n",
      "Train: Epoch [7], Batch [803/938], Loss: 0.5461825132369995\n",
      "Train: Epoch [7], Batch [804/938], Loss: 0.7969831228256226\n",
      "Train: Epoch [7], Batch [805/938], Loss: 0.6185463070869446\n",
      "Train: Epoch [7], Batch [806/938], Loss: 0.753246009349823\n",
      "Train: Epoch [7], Batch [807/938], Loss: 0.6057862639427185\n",
      "Train: Epoch [7], Batch [808/938], Loss: 0.610512375831604\n",
      "Train: Epoch [7], Batch [809/938], Loss: 0.6639124155044556\n",
      "Train: Epoch [7], Batch [810/938], Loss: 0.6348029971122742\n",
      "Train: Epoch [7], Batch [811/938], Loss: 0.5163270831108093\n",
      "Train: Epoch [7], Batch [812/938], Loss: 0.5661998987197876\n",
      "Train: Epoch [7], Batch [813/938], Loss: 0.6065713167190552\n",
      "Train: Epoch [7], Batch [814/938], Loss: 0.7029657363891602\n",
      "Train: Epoch [7], Batch [815/938], Loss: 0.5419575572013855\n",
      "Train: Epoch [7], Batch [816/938], Loss: 0.5408579111099243\n",
      "Train: Epoch [7], Batch [817/938], Loss: 0.6430842280387878\n",
      "Train: Epoch [7], Batch [818/938], Loss: 0.4222950339317322\n",
      "Train: Epoch [7], Batch [819/938], Loss: 0.5518105030059814\n",
      "Train: Epoch [7], Batch [820/938], Loss: 0.6845484972000122\n",
      "Train: Epoch [7], Batch [821/938], Loss: 0.44364506006240845\n",
      "Train: Epoch [7], Batch [822/938], Loss: 0.578969419002533\n",
      "Train: Epoch [7], Batch [823/938], Loss: 0.5216363668441772\n",
      "Train: Epoch [7], Batch [824/938], Loss: 0.6709088087081909\n",
      "Train: Epoch [7], Batch [825/938], Loss: 0.7151491641998291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [7], Batch [826/938], Loss: 0.4653726816177368\n",
      "Train: Epoch [7], Batch [827/938], Loss: 0.49021655321121216\n",
      "Train: Epoch [7], Batch [828/938], Loss: 0.6219425201416016\n",
      "Train: Epoch [7], Batch [829/938], Loss: 0.6554852724075317\n",
      "Train: Epoch [7], Batch [830/938], Loss: 0.41819703578948975\n",
      "Train: Epoch [7], Batch [831/938], Loss: 0.5670698881149292\n",
      "Train: Epoch [7], Batch [832/938], Loss: 0.6991423964500427\n",
      "Train: Epoch [7], Batch [833/938], Loss: 0.4304872155189514\n",
      "Train: Epoch [7], Batch [834/938], Loss: 0.6246827840805054\n",
      "Train: Epoch [7], Batch [835/938], Loss: 0.5600528717041016\n",
      "Train: Epoch [7], Batch [836/938], Loss: 0.5321616530418396\n",
      "Train: Epoch [7], Batch [837/938], Loss: 0.4801701605319977\n",
      "Train: Epoch [7], Batch [838/938], Loss: 0.6457788348197937\n",
      "Train: Epoch [7], Batch [839/938], Loss: 0.6863257884979248\n",
      "Train: Epoch [7], Batch [840/938], Loss: 0.646513819694519\n",
      "Train: Epoch [7], Batch [841/938], Loss: 0.7566976547241211\n",
      "Train: Epoch [7], Batch [842/938], Loss: 0.6177206039428711\n",
      "Train: Epoch [7], Batch [843/938], Loss: 0.5334276556968689\n",
      "Train: Epoch [7], Batch [844/938], Loss: 0.4833860397338867\n",
      "Train: Epoch [7], Batch [845/938], Loss: 0.5363423824310303\n",
      "Train: Epoch [7], Batch [846/938], Loss: 0.8122140169143677\n",
      "Train: Epoch [7], Batch [847/938], Loss: 0.6320247054100037\n",
      "Train: Epoch [7], Batch [848/938], Loss: 0.7206776142120361\n",
      "Train: Epoch [7], Batch [849/938], Loss: 0.7959009408950806\n",
      "Train: Epoch [7], Batch [850/938], Loss: 0.6476629972457886\n",
      "Train: Epoch [7], Batch [851/938], Loss: 0.44794294238090515\n",
      "Train: Epoch [7], Batch [852/938], Loss: 0.4656227231025696\n",
      "Train: Epoch [7], Batch [853/938], Loss: 0.5492618680000305\n",
      "Train: Epoch [7], Batch [854/938], Loss: 0.6697801351547241\n",
      "Train: Epoch [7], Batch [855/938], Loss: 0.5604451894760132\n",
      "Train: Epoch [7], Batch [856/938], Loss: 0.4804690480232239\n",
      "Train: Epoch [7], Batch [857/938], Loss: 0.4821215271949768\n",
      "Train: Epoch [7], Batch [858/938], Loss: 0.49157220125198364\n",
      "Train: Epoch [7], Batch [859/938], Loss: 0.38617363572120667\n",
      "Train: Epoch [7], Batch [860/938], Loss: 0.5253265500068665\n",
      "Train: Epoch [7], Batch [861/938], Loss: 0.9562576413154602\n",
      "Train: Epoch [7], Batch [862/938], Loss: 0.6772297620773315\n",
      "Train: Epoch [7], Batch [863/938], Loss: 0.5522294640541077\n",
      "Train: Epoch [7], Batch [864/938], Loss: 0.43075117468833923\n",
      "Train: Epoch [7], Batch [865/938], Loss: 0.6345518231391907\n",
      "Train: Epoch [7], Batch [866/938], Loss: 0.4919288456439972\n",
      "Train: Epoch [7], Batch [867/938], Loss: 0.6329623460769653\n",
      "Train: Epoch [7], Batch [868/938], Loss: 0.5848904848098755\n",
      "Train: Epoch [7], Batch [869/938], Loss: 0.46410512924194336\n",
      "Train: Epoch [7], Batch [870/938], Loss: 0.4796026349067688\n",
      "Train: Epoch [7], Batch [871/938], Loss: 0.673386812210083\n",
      "Train: Epoch [7], Batch [872/938], Loss: 0.5803714990615845\n",
      "Train: Epoch [7], Batch [873/938], Loss: 0.7853419780731201\n",
      "Train: Epoch [7], Batch [874/938], Loss: 0.6584888696670532\n",
      "Train: Epoch [7], Batch [875/938], Loss: 0.5109876394271851\n",
      "Train: Epoch [7], Batch [876/938], Loss: 0.9733121395111084\n",
      "Train: Epoch [7], Batch [877/938], Loss: 0.7556927800178528\n",
      "Train: Epoch [7], Batch [878/938], Loss: 0.5299928188323975\n",
      "Train: Epoch [7], Batch [879/938], Loss: 0.5435348749160767\n",
      "Train: Epoch [7], Batch [880/938], Loss: 0.5258573293685913\n",
      "Train: Epoch [7], Batch [881/938], Loss: 0.6251932978630066\n",
      "Train: Epoch [7], Batch [882/938], Loss: 0.555831253528595\n",
      "Train: Epoch [7], Batch [883/938], Loss: 0.5891357660293579\n",
      "Train: Epoch [7], Batch [884/938], Loss: 0.7744684219360352\n",
      "Train: Epoch [7], Batch [885/938], Loss: 0.6423194408416748\n",
      "Train: Epoch [7], Batch [886/938], Loss: 0.7807929515838623\n",
      "Train: Epoch [7], Batch [887/938], Loss: 0.6990792751312256\n",
      "Train: Epoch [7], Batch [888/938], Loss: 0.6426331996917725\n",
      "Train: Epoch [7], Batch [889/938], Loss: 0.6787541508674622\n",
      "Train: Epoch [7], Batch [890/938], Loss: 0.6527508497238159\n",
      "Train: Epoch [7], Batch [891/938], Loss: 0.5978156328201294\n",
      "Train: Epoch [7], Batch [892/938], Loss: 0.6834661960601807\n",
      "Train: Epoch [7], Batch [893/938], Loss: 0.5794358253479004\n",
      "Train: Epoch [7], Batch [894/938], Loss: 0.6325613260269165\n",
      "Train: Epoch [7], Batch [895/938], Loss: 0.6752124428749084\n",
      "Train: Epoch [7], Batch [896/938], Loss: 0.7080811262130737\n",
      "Train: Epoch [7], Batch [897/938], Loss: 0.6016207933425903\n",
      "Train: Epoch [7], Batch [898/938], Loss: 0.6686531901359558\n",
      "Train: Epoch [7], Batch [899/938], Loss: 0.5636962652206421\n",
      "Train: Epoch [7], Batch [900/938], Loss: 0.4449921250343323\n",
      "Train: Epoch [7], Batch [901/938], Loss: 0.6504052877426147\n",
      "Train: Epoch [7], Batch [902/938], Loss: 0.4656952917575836\n",
      "Train: Epoch [7], Batch [903/938], Loss: 0.5486035943031311\n",
      "Train: Epoch [7], Batch [904/938], Loss: 0.6332906484603882\n",
      "Train: Epoch [7], Batch [905/938], Loss: 0.5808605551719666\n",
      "Train: Epoch [7], Batch [906/938], Loss: 0.6072618961334229\n",
      "Train: Epoch [7], Batch [907/938], Loss: 0.5910658836364746\n",
      "Train: Epoch [7], Batch [908/938], Loss: 0.6505391597747803\n",
      "Train: Epoch [7], Batch [909/938], Loss: 0.5926681756973267\n",
      "Train: Epoch [7], Batch [910/938], Loss: 0.6323715448379517\n",
      "Train: Epoch [7], Batch [911/938], Loss: 0.7416356801986694\n",
      "Train: Epoch [7], Batch [912/938], Loss: 0.5379082560539246\n",
      "Train: Epoch [7], Batch [913/938], Loss: 0.39005669951438904\n",
      "Train: Epoch [7], Batch [914/938], Loss: 0.6654322743415833\n",
      "Train: Epoch [7], Batch [915/938], Loss: 0.5312818884849548\n",
      "Train: Epoch [7], Batch [916/938], Loss: 0.53435879945755\n",
      "Train: Epoch [7], Batch [917/938], Loss: 0.44888949394226074\n",
      "Train: Epoch [7], Batch [918/938], Loss: 0.6829739809036255\n",
      "Train: Epoch [7], Batch [919/938], Loss: 0.7232121229171753\n",
      "Train: Epoch [7], Batch [920/938], Loss: 0.5691996812820435\n",
      "Train: Epoch [7], Batch [921/938], Loss: 0.7037021517753601\n",
      "Train: Epoch [7], Batch [922/938], Loss: 0.6532188057899475\n",
      "Train: Epoch [7], Batch [923/938], Loss: 0.5541643500328064\n",
      "Train: Epoch [7], Batch [924/938], Loss: 0.5293107032775879\n",
      "Train: Epoch [7], Batch [925/938], Loss: 0.7130335569381714\n",
      "Train: Epoch [7], Batch [926/938], Loss: 0.4717157483100891\n",
      "Train: Epoch [7], Batch [927/938], Loss: 0.8343502879142761\n",
      "Train: Epoch [7], Batch [928/938], Loss: 0.6681033372879028\n",
      "Train: Epoch [7], Batch [929/938], Loss: 0.726978063583374\n",
      "Train: Epoch [7], Batch [930/938], Loss: 0.4662442207336426\n",
      "Train: Epoch [7], Batch [931/938], Loss: 0.7510024309158325\n",
      "Train: Epoch [7], Batch [932/938], Loss: 0.7098145484924316\n",
      "Train: Epoch [7], Batch [933/938], Loss: 0.5723292827606201\n",
      "Train: Epoch [7], Batch [934/938], Loss: 0.49554264545440674\n",
      "Train: Epoch [7], Batch [935/938], Loss: 0.5147438645362854\n",
      "Train: Epoch [7], Batch [936/938], Loss: 0.6386204957962036\n",
      "Train: Epoch [7], Batch [937/938], Loss: 0.689549446105957\n",
      "Train: Epoch [7], Batch [938/938], Loss: 0.48230743408203125\n",
      "Accuracy of train set: 0.7852333333333333\n",
      "Validation: Epoch [7], Batch [1/938], Loss: 0.6912224888801575\n",
      "Validation: Epoch [7], Batch [2/938], Loss: 0.46944135427474976\n",
      "Validation: Epoch [7], Batch [3/938], Loss: 0.8896428346633911\n",
      "Validation: Epoch [7], Batch [4/938], Loss: 0.648105263710022\n",
      "Validation: Epoch [7], Batch [5/938], Loss: 0.4642101228237152\n",
      "Validation: Epoch [7], Batch [6/938], Loss: 0.7147175073623657\n",
      "Validation: Epoch [7], Batch [7/938], Loss: 0.5797298550605774\n",
      "Validation: Epoch [7], Batch [8/938], Loss: 0.6330076456069946\n",
      "Validation: Epoch [7], Batch [9/938], Loss: 0.5287838578224182\n",
      "Validation: Epoch [7], Batch [10/938], Loss: 0.49316519498825073\n",
      "Validation: Epoch [7], Batch [11/938], Loss: 0.6143072843551636\n",
      "Validation: Epoch [7], Batch [12/938], Loss: 0.4487769603729248\n",
      "Validation: Epoch [7], Batch [13/938], Loss: 0.40820690989494324\n",
      "Validation: Epoch [7], Batch [14/938], Loss: 0.4967919588088989\n",
      "Validation: Epoch [7], Batch [15/938], Loss: 0.6229022145271301\n",
      "Validation: Epoch [7], Batch [16/938], Loss: 0.6165285110473633\n",
      "Validation: Epoch [7], Batch [17/938], Loss: 0.6105338335037231\n",
      "Validation: Epoch [7], Batch [18/938], Loss: 0.4986332058906555\n",
      "Validation: Epoch [7], Batch [19/938], Loss: 0.5930396318435669\n",
      "Validation: Epoch [7], Batch [20/938], Loss: 0.5394941568374634\n",
      "Validation: Epoch [7], Batch [21/938], Loss: 0.5835656523704529\n",
      "Validation: Epoch [7], Batch [22/938], Loss: 0.554969072341919\n",
      "Validation: Epoch [7], Batch [23/938], Loss: 0.6822666525840759\n",
      "Validation: Epoch [7], Batch [24/938], Loss: 0.5577300786972046\n",
      "Validation: Epoch [7], Batch [25/938], Loss: 0.4607919156551361\n",
      "Validation: Epoch [7], Batch [26/938], Loss: 0.8525403738021851\n",
      "Validation: Epoch [7], Batch [27/938], Loss: 0.6208221316337585\n",
      "Validation: Epoch [7], Batch [28/938], Loss: 0.7769629955291748\n",
      "Validation: Epoch [7], Batch [29/938], Loss: 0.5470024943351746\n",
      "Validation: Epoch [7], Batch [30/938], Loss: 0.5767791271209717\n",
      "Validation: Epoch [7], Batch [31/938], Loss: 0.4881889224052429\n",
      "Validation: Epoch [7], Batch [32/938], Loss: 0.6664243340492249\n",
      "Validation: Epoch [7], Batch [33/938], Loss: 0.5189005732536316\n",
      "Validation: Epoch [7], Batch [34/938], Loss: 0.4694555103778839\n",
      "Validation: Epoch [7], Batch [35/938], Loss: 0.4484989047050476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [36/938], Loss: 0.6806079149246216\n",
      "Validation: Epoch [7], Batch [37/938], Loss: 0.5443356037139893\n",
      "Validation: Epoch [7], Batch [38/938], Loss: 0.5097302794456482\n",
      "Validation: Epoch [7], Batch [39/938], Loss: 0.46695977449417114\n",
      "Validation: Epoch [7], Batch [40/938], Loss: 0.6729112863540649\n",
      "Validation: Epoch [7], Batch [41/938], Loss: 0.813569188117981\n",
      "Validation: Epoch [7], Batch [42/938], Loss: 0.717566967010498\n",
      "Validation: Epoch [7], Batch [43/938], Loss: 0.7273862957954407\n",
      "Validation: Epoch [7], Batch [44/938], Loss: 0.5113398432731628\n",
      "Validation: Epoch [7], Batch [45/938], Loss: 0.5566204786300659\n",
      "Validation: Epoch [7], Batch [46/938], Loss: 0.6426212787628174\n",
      "Validation: Epoch [7], Batch [47/938], Loss: 0.8090113401412964\n",
      "Validation: Epoch [7], Batch [48/938], Loss: 0.5570312142372131\n",
      "Validation: Epoch [7], Batch [49/938], Loss: 0.7416073083877563\n",
      "Validation: Epoch [7], Batch [50/938], Loss: 0.62184739112854\n",
      "Validation: Epoch [7], Batch [51/938], Loss: 0.6370657682418823\n",
      "Validation: Epoch [7], Batch [52/938], Loss: 0.4887654781341553\n",
      "Validation: Epoch [7], Batch [53/938], Loss: 0.6296538710594177\n",
      "Validation: Epoch [7], Batch [54/938], Loss: 0.855338454246521\n",
      "Validation: Epoch [7], Batch [55/938], Loss: 0.7176661491394043\n",
      "Validation: Epoch [7], Batch [56/938], Loss: 0.6617798209190369\n",
      "Validation: Epoch [7], Batch [57/938], Loss: 0.6503902077674866\n",
      "Validation: Epoch [7], Batch [58/938], Loss: 0.6879698038101196\n",
      "Validation: Epoch [7], Batch [59/938], Loss: 0.6719024181365967\n",
      "Validation: Epoch [7], Batch [60/938], Loss: 0.6132571697235107\n",
      "Validation: Epoch [7], Batch [61/938], Loss: 0.6802374124526978\n",
      "Validation: Epoch [7], Batch [62/938], Loss: 0.5026838779449463\n",
      "Validation: Epoch [7], Batch [63/938], Loss: 0.7414791584014893\n",
      "Validation: Epoch [7], Batch [64/938], Loss: 0.674454927444458\n",
      "Validation: Epoch [7], Batch [65/938], Loss: 0.6924025416374207\n",
      "Validation: Epoch [7], Batch [66/938], Loss: 0.410284161567688\n",
      "Validation: Epoch [7], Batch [67/938], Loss: 0.6170424818992615\n",
      "Validation: Epoch [7], Batch [68/938], Loss: 0.5362444519996643\n",
      "Validation: Epoch [7], Batch [69/938], Loss: 0.5084166526794434\n",
      "Validation: Epoch [7], Batch [70/938], Loss: 0.4261881113052368\n",
      "Validation: Epoch [7], Batch [71/938], Loss: 0.5365023016929626\n",
      "Validation: Epoch [7], Batch [72/938], Loss: 0.5370234251022339\n",
      "Validation: Epoch [7], Batch [73/938], Loss: 0.4819551706314087\n",
      "Validation: Epoch [7], Batch [74/938], Loss: 0.4498347342014313\n",
      "Validation: Epoch [7], Batch [75/938], Loss: 0.7746863961219788\n",
      "Validation: Epoch [7], Batch [76/938], Loss: 0.6975030303001404\n",
      "Validation: Epoch [7], Batch [77/938], Loss: 0.7724078893661499\n",
      "Validation: Epoch [7], Batch [78/938], Loss: 0.6121339797973633\n",
      "Validation: Epoch [7], Batch [79/938], Loss: 0.7903850078582764\n",
      "Validation: Epoch [7], Batch [80/938], Loss: 0.506268322467804\n",
      "Validation: Epoch [7], Batch [81/938], Loss: 0.3907666802406311\n",
      "Validation: Epoch [7], Batch [82/938], Loss: 0.5705950260162354\n",
      "Validation: Epoch [7], Batch [83/938], Loss: 0.640346109867096\n",
      "Validation: Epoch [7], Batch [84/938], Loss: 0.5772228240966797\n",
      "Validation: Epoch [7], Batch [85/938], Loss: 0.5597469806671143\n",
      "Validation: Epoch [7], Batch [86/938], Loss: 0.3792479634284973\n",
      "Validation: Epoch [7], Batch [87/938], Loss: 0.7221061587333679\n",
      "Validation: Epoch [7], Batch [88/938], Loss: 0.5741484761238098\n",
      "Validation: Epoch [7], Batch [89/938], Loss: 0.6172125339508057\n",
      "Validation: Epoch [7], Batch [90/938], Loss: 0.43060311675071716\n",
      "Validation: Epoch [7], Batch [91/938], Loss: 0.4480098485946655\n",
      "Validation: Epoch [7], Batch [92/938], Loss: 0.4669371247291565\n",
      "Validation: Epoch [7], Batch [93/938], Loss: 0.6151505708694458\n",
      "Validation: Epoch [7], Batch [94/938], Loss: 0.6436219811439514\n",
      "Validation: Epoch [7], Batch [95/938], Loss: 0.6477700471878052\n",
      "Validation: Epoch [7], Batch [96/938], Loss: 0.619046688079834\n",
      "Validation: Epoch [7], Batch [97/938], Loss: 0.5790628790855408\n",
      "Validation: Epoch [7], Batch [98/938], Loss: 0.6298503875732422\n",
      "Validation: Epoch [7], Batch [99/938], Loss: 0.6502116322517395\n",
      "Validation: Epoch [7], Batch [100/938], Loss: 0.5512517690658569\n",
      "Validation: Epoch [7], Batch [101/938], Loss: 0.6406624913215637\n",
      "Validation: Epoch [7], Batch [102/938], Loss: 0.4811248779296875\n",
      "Validation: Epoch [7], Batch [103/938], Loss: 0.45147889852523804\n",
      "Validation: Epoch [7], Batch [104/938], Loss: 0.4006907343864441\n",
      "Validation: Epoch [7], Batch [105/938], Loss: 0.5726170539855957\n",
      "Validation: Epoch [7], Batch [106/938], Loss: 0.5900039672851562\n",
      "Validation: Epoch [7], Batch [107/938], Loss: 0.5426473021507263\n",
      "Validation: Epoch [7], Batch [108/938], Loss: 0.461151659488678\n",
      "Validation: Epoch [7], Batch [109/938], Loss: 0.7477907538414001\n",
      "Validation: Epoch [7], Batch [110/938], Loss: 0.6769866943359375\n",
      "Validation: Epoch [7], Batch [111/938], Loss: 0.627925455570221\n",
      "Validation: Epoch [7], Batch [112/938], Loss: 0.6312873363494873\n",
      "Validation: Epoch [7], Batch [113/938], Loss: 0.6002964377403259\n",
      "Validation: Epoch [7], Batch [114/938], Loss: 0.4850482940673828\n",
      "Validation: Epoch [7], Batch [115/938], Loss: 0.694907546043396\n",
      "Validation: Epoch [7], Batch [116/938], Loss: 0.4993758201599121\n",
      "Validation: Epoch [7], Batch [117/938], Loss: 0.5924660563468933\n",
      "Validation: Epoch [7], Batch [118/938], Loss: 0.7425144910812378\n",
      "Validation: Epoch [7], Batch [119/938], Loss: 0.7375131845474243\n",
      "Validation: Epoch [7], Batch [120/938], Loss: 0.5592588782310486\n",
      "Validation: Epoch [7], Batch [121/938], Loss: 0.6389963626861572\n",
      "Validation: Epoch [7], Batch [122/938], Loss: 0.6326316595077515\n",
      "Validation: Epoch [7], Batch [123/938], Loss: 0.6710054278373718\n",
      "Validation: Epoch [7], Batch [124/938], Loss: 0.5710038542747498\n",
      "Validation: Epoch [7], Batch [125/938], Loss: 0.5431547164916992\n",
      "Validation: Epoch [7], Batch [126/938], Loss: 0.5225318074226379\n",
      "Validation: Epoch [7], Batch [127/938], Loss: 0.7877771258354187\n",
      "Validation: Epoch [7], Batch [128/938], Loss: 0.726162314414978\n",
      "Validation: Epoch [7], Batch [129/938], Loss: 0.5722270607948303\n",
      "Validation: Epoch [7], Batch [130/938], Loss: 0.5288600921630859\n",
      "Validation: Epoch [7], Batch [131/938], Loss: 0.6509948372840881\n",
      "Validation: Epoch [7], Batch [132/938], Loss: 0.49755871295928955\n",
      "Validation: Epoch [7], Batch [133/938], Loss: 0.544316828250885\n",
      "Validation: Epoch [7], Batch [134/938], Loss: 0.5007181167602539\n",
      "Validation: Epoch [7], Batch [135/938], Loss: 0.7361457943916321\n",
      "Validation: Epoch [7], Batch [136/938], Loss: 0.6807733178138733\n",
      "Validation: Epoch [7], Batch [137/938], Loss: 0.4870559573173523\n",
      "Validation: Epoch [7], Batch [138/938], Loss: 0.47694164514541626\n",
      "Validation: Epoch [7], Batch [139/938], Loss: 0.5383883714675903\n",
      "Validation: Epoch [7], Batch [140/938], Loss: 0.5483025908470154\n",
      "Validation: Epoch [7], Batch [141/938], Loss: 0.4760797619819641\n",
      "Validation: Epoch [7], Batch [142/938], Loss: 0.49105605483055115\n",
      "Validation: Epoch [7], Batch [143/938], Loss: 0.530574381351471\n",
      "Validation: Epoch [7], Batch [144/938], Loss: 0.5206727981567383\n",
      "Validation: Epoch [7], Batch [145/938], Loss: 0.6471197009086609\n",
      "Validation: Epoch [7], Batch [146/938], Loss: 0.47792649269104004\n",
      "Validation: Epoch [7], Batch [147/938], Loss: 0.4511312246322632\n",
      "Validation: Epoch [7], Batch [148/938], Loss: 0.6868414282798767\n",
      "Validation: Epoch [7], Batch [149/938], Loss: 0.6814091205596924\n",
      "Validation: Epoch [7], Batch [150/938], Loss: 0.47824740409851074\n",
      "Validation: Epoch [7], Batch [151/938], Loss: 0.5483856201171875\n",
      "Validation: Epoch [7], Batch [152/938], Loss: 0.3789249062538147\n",
      "Validation: Epoch [7], Batch [153/938], Loss: 0.5726105570793152\n",
      "Validation: Epoch [7], Batch [154/938], Loss: 0.4659159481525421\n",
      "Validation: Epoch [7], Batch [155/938], Loss: 0.47215771675109863\n",
      "Validation: Epoch [7], Batch [156/938], Loss: 0.4105686843395233\n",
      "Validation: Epoch [7], Batch [157/938], Loss: 0.5229194164276123\n",
      "Validation: Epoch [7], Batch [158/938], Loss: 0.48921141028404236\n",
      "Validation: Epoch [7], Batch [159/938], Loss: 0.8015275001525879\n",
      "Validation: Epoch [7], Batch [160/938], Loss: 0.44688335061073303\n",
      "Validation: Epoch [7], Batch [161/938], Loss: 0.46260762214660645\n",
      "Validation: Epoch [7], Batch [162/938], Loss: 0.4127003252506256\n",
      "Validation: Epoch [7], Batch [163/938], Loss: 0.5629957318305969\n",
      "Validation: Epoch [7], Batch [164/938], Loss: 0.4538015127182007\n",
      "Validation: Epoch [7], Batch [165/938], Loss: 0.6112078428268433\n",
      "Validation: Epoch [7], Batch [166/938], Loss: 0.5004178881645203\n",
      "Validation: Epoch [7], Batch [167/938], Loss: 0.49494507908821106\n",
      "Validation: Epoch [7], Batch [168/938], Loss: 0.8085260391235352\n",
      "Validation: Epoch [7], Batch [169/938], Loss: 0.6081307530403137\n",
      "Validation: Epoch [7], Batch [170/938], Loss: 0.7599489688873291\n",
      "Validation: Epoch [7], Batch [171/938], Loss: 0.4184127748012543\n",
      "Validation: Epoch [7], Batch [172/938], Loss: 0.5897761583328247\n",
      "Validation: Epoch [7], Batch [173/938], Loss: 0.48558974266052246\n",
      "Validation: Epoch [7], Batch [174/938], Loss: 0.5002124905586243\n",
      "Validation: Epoch [7], Batch [175/938], Loss: 0.5644631385803223\n",
      "Validation: Epoch [7], Batch [176/938], Loss: 0.36583518981933594\n",
      "Validation: Epoch [7], Batch [177/938], Loss: 0.5469224452972412\n",
      "Validation: Epoch [7], Batch [178/938], Loss: 0.6137241125106812\n",
      "Validation: Epoch [7], Batch [179/938], Loss: 0.4235861003398895\n",
      "Validation: Epoch [7], Batch [180/938], Loss: 0.6922600269317627\n",
      "Validation: Epoch [7], Batch [181/938], Loss: 0.426103413105011\n",
      "Validation: Epoch [7], Batch [182/938], Loss: 0.6559719443321228\n",
      "Validation: Epoch [7], Batch [183/938], Loss: 0.7436188459396362\n",
      "Validation: Epoch [7], Batch [184/938], Loss: 0.4762882590293884\n",
      "Validation: Epoch [7], Batch [185/938], Loss: 0.526389479637146\n",
      "Validation: Epoch [7], Batch [186/938], Loss: 0.6613141894340515\n",
      "Validation: Epoch [7], Batch [187/938], Loss: 0.5431088209152222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [188/938], Loss: 0.5199739933013916\n",
      "Validation: Epoch [7], Batch [189/938], Loss: 0.38648849725723267\n",
      "Validation: Epoch [7], Batch [190/938], Loss: 0.6389118432998657\n",
      "Validation: Epoch [7], Batch [191/938], Loss: 0.6036777496337891\n",
      "Validation: Epoch [7], Batch [192/938], Loss: 0.42917126417160034\n",
      "Validation: Epoch [7], Batch [193/938], Loss: 0.5408347249031067\n",
      "Validation: Epoch [7], Batch [194/938], Loss: 0.5633540153503418\n",
      "Validation: Epoch [7], Batch [195/938], Loss: 0.5734876394271851\n",
      "Validation: Epoch [7], Batch [196/938], Loss: 0.33513858914375305\n",
      "Validation: Epoch [7], Batch [197/938], Loss: 0.4822278618812561\n",
      "Validation: Epoch [7], Batch [198/938], Loss: 0.5036415457725525\n",
      "Validation: Epoch [7], Batch [199/938], Loss: 0.5681244134902954\n",
      "Validation: Epoch [7], Batch [200/938], Loss: 0.5954047441482544\n",
      "Validation: Epoch [7], Batch [201/938], Loss: 0.6015018224716187\n",
      "Validation: Epoch [7], Batch [202/938], Loss: 0.37625449895858765\n",
      "Validation: Epoch [7], Batch [203/938], Loss: 0.5497626066207886\n",
      "Validation: Epoch [7], Batch [204/938], Loss: 0.38383305072784424\n",
      "Validation: Epoch [7], Batch [205/938], Loss: 0.6848382353782654\n",
      "Validation: Epoch [7], Batch [206/938], Loss: 0.6636558175086975\n",
      "Validation: Epoch [7], Batch [207/938], Loss: 0.38908082246780396\n",
      "Validation: Epoch [7], Batch [208/938], Loss: 0.47818467020988464\n",
      "Validation: Epoch [7], Batch [209/938], Loss: 0.49718767404556274\n",
      "Validation: Epoch [7], Batch [210/938], Loss: 0.5747985243797302\n",
      "Validation: Epoch [7], Batch [211/938], Loss: 0.5365440845489502\n",
      "Validation: Epoch [7], Batch [212/938], Loss: 0.6174608469009399\n",
      "Validation: Epoch [7], Batch [213/938], Loss: 0.6716248989105225\n",
      "Validation: Epoch [7], Batch [214/938], Loss: 0.5441872477531433\n",
      "Validation: Epoch [7], Batch [215/938], Loss: 0.6073131561279297\n",
      "Validation: Epoch [7], Batch [216/938], Loss: 0.5592161417007446\n",
      "Validation: Epoch [7], Batch [217/938], Loss: 0.805124819278717\n",
      "Validation: Epoch [7], Batch [218/938], Loss: 0.6597522497177124\n",
      "Validation: Epoch [7], Batch [219/938], Loss: 0.5185911059379578\n",
      "Validation: Epoch [7], Batch [220/938], Loss: 0.5687982439994812\n",
      "Validation: Epoch [7], Batch [221/938], Loss: 0.6746602058410645\n",
      "Validation: Epoch [7], Batch [222/938], Loss: 0.6234683990478516\n",
      "Validation: Epoch [7], Batch [223/938], Loss: 0.4916318655014038\n",
      "Validation: Epoch [7], Batch [224/938], Loss: 0.7113999128341675\n",
      "Validation: Epoch [7], Batch [225/938], Loss: 0.6229557991027832\n",
      "Validation: Epoch [7], Batch [226/938], Loss: 0.8270602226257324\n",
      "Validation: Epoch [7], Batch [227/938], Loss: 0.3564518690109253\n",
      "Validation: Epoch [7], Batch [228/938], Loss: 0.5355933904647827\n",
      "Validation: Epoch [7], Batch [229/938], Loss: 0.6127467751502991\n",
      "Validation: Epoch [7], Batch [230/938], Loss: 0.46580421924591064\n",
      "Validation: Epoch [7], Batch [231/938], Loss: 0.5224196910858154\n",
      "Validation: Epoch [7], Batch [232/938], Loss: 0.6270418763160706\n",
      "Validation: Epoch [7], Batch [233/938], Loss: 0.7622740864753723\n",
      "Validation: Epoch [7], Batch [234/938], Loss: 0.6629384160041809\n",
      "Validation: Epoch [7], Batch [235/938], Loss: 0.6933664083480835\n",
      "Validation: Epoch [7], Batch [236/938], Loss: 0.7614316940307617\n",
      "Validation: Epoch [7], Batch [237/938], Loss: 0.6673579216003418\n",
      "Validation: Epoch [7], Batch [238/938], Loss: 0.8976389169692993\n",
      "Validation: Epoch [7], Batch [239/938], Loss: 0.5687310695648193\n",
      "Validation: Epoch [7], Batch [240/938], Loss: 0.4955107569694519\n",
      "Validation: Epoch [7], Batch [241/938], Loss: 0.5411453247070312\n",
      "Validation: Epoch [7], Batch [242/938], Loss: 0.584365963935852\n",
      "Validation: Epoch [7], Batch [243/938], Loss: 0.5377271175384521\n",
      "Validation: Epoch [7], Batch [244/938], Loss: 0.3565441370010376\n",
      "Validation: Epoch [7], Batch [245/938], Loss: 0.6587637066841125\n",
      "Validation: Epoch [7], Batch [246/938], Loss: 0.6175496578216553\n",
      "Validation: Epoch [7], Batch [247/938], Loss: 0.7273236513137817\n",
      "Validation: Epoch [7], Batch [248/938], Loss: 0.5196985006332397\n",
      "Validation: Epoch [7], Batch [249/938], Loss: 0.5423170328140259\n",
      "Validation: Epoch [7], Batch [250/938], Loss: 0.7206814885139465\n",
      "Validation: Epoch [7], Batch [251/938], Loss: 0.79936683177948\n",
      "Validation: Epoch [7], Batch [252/938], Loss: 0.5323606133460999\n",
      "Validation: Epoch [7], Batch [253/938], Loss: 0.5895931124687195\n",
      "Validation: Epoch [7], Batch [254/938], Loss: 0.5995247960090637\n",
      "Validation: Epoch [7], Batch [255/938], Loss: 0.8631824254989624\n",
      "Validation: Epoch [7], Batch [256/938], Loss: 0.6198575496673584\n",
      "Validation: Epoch [7], Batch [257/938], Loss: 0.6038626432418823\n",
      "Validation: Epoch [7], Batch [258/938], Loss: 0.2827940583229065\n",
      "Validation: Epoch [7], Batch [259/938], Loss: 0.5735982656478882\n",
      "Validation: Epoch [7], Batch [260/938], Loss: 0.4541948437690735\n",
      "Validation: Epoch [7], Batch [261/938], Loss: 0.542020320892334\n",
      "Validation: Epoch [7], Batch [262/938], Loss: 0.44403770565986633\n",
      "Validation: Epoch [7], Batch [263/938], Loss: 0.5406186580657959\n",
      "Validation: Epoch [7], Batch [264/938], Loss: 1.0131089687347412\n",
      "Validation: Epoch [7], Batch [265/938], Loss: 0.7586522698402405\n",
      "Validation: Epoch [7], Batch [266/938], Loss: 0.6371601819992065\n",
      "Validation: Epoch [7], Batch [267/938], Loss: 0.5219086408615112\n",
      "Validation: Epoch [7], Batch [268/938], Loss: 0.5021985769271851\n",
      "Validation: Epoch [7], Batch [269/938], Loss: 0.4750986099243164\n",
      "Validation: Epoch [7], Batch [270/938], Loss: 0.5675079226493835\n",
      "Validation: Epoch [7], Batch [271/938], Loss: 0.5377836227416992\n",
      "Validation: Epoch [7], Batch [272/938], Loss: 0.3559335172176361\n",
      "Validation: Epoch [7], Batch [273/938], Loss: 0.6281923055648804\n",
      "Validation: Epoch [7], Batch [274/938], Loss: 0.5640959739685059\n",
      "Validation: Epoch [7], Batch [275/938], Loss: 0.6139134168624878\n",
      "Validation: Epoch [7], Batch [276/938], Loss: 0.6359886527061462\n",
      "Validation: Epoch [7], Batch [277/938], Loss: 0.6196719408035278\n",
      "Validation: Epoch [7], Batch [278/938], Loss: 0.6552464365959167\n",
      "Validation: Epoch [7], Batch [279/938], Loss: 0.5288779735565186\n",
      "Validation: Epoch [7], Batch [280/938], Loss: 0.6217548847198486\n",
      "Validation: Epoch [7], Batch [281/938], Loss: 0.4855465888977051\n",
      "Validation: Epoch [7], Batch [282/938], Loss: 0.6150490641593933\n",
      "Validation: Epoch [7], Batch [283/938], Loss: 0.5923483371734619\n",
      "Validation: Epoch [7], Batch [284/938], Loss: 0.7309975624084473\n",
      "Validation: Epoch [7], Batch [285/938], Loss: 0.43397146463394165\n",
      "Validation: Epoch [7], Batch [286/938], Loss: 0.577273964881897\n",
      "Validation: Epoch [7], Batch [287/938], Loss: 0.7161716222763062\n",
      "Validation: Epoch [7], Batch [288/938], Loss: 0.5211598873138428\n",
      "Validation: Epoch [7], Batch [289/938], Loss: 0.5988336801528931\n",
      "Validation: Epoch [7], Batch [290/938], Loss: 0.5139161348342896\n",
      "Validation: Epoch [7], Batch [291/938], Loss: 0.6041155457496643\n",
      "Validation: Epoch [7], Batch [292/938], Loss: 0.4810238182544708\n",
      "Validation: Epoch [7], Batch [293/938], Loss: 0.5545620918273926\n",
      "Validation: Epoch [7], Batch [294/938], Loss: 0.4924875497817993\n",
      "Validation: Epoch [7], Batch [295/938], Loss: 0.5881857872009277\n",
      "Validation: Epoch [7], Batch [296/938], Loss: 0.35725876688957214\n",
      "Validation: Epoch [7], Batch [297/938], Loss: 0.6676939725875854\n",
      "Validation: Epoch [7], Batch [298/938], Loss: 0.5575678944587708\n",
      "Validation: Epoch [7], Batch [299/938], Loss: 0.7120013236999512\n",
      "Validation: Epoch [7], Batch [300/938], Loss: 0.8133645057678223\n",
      "Validation: Epoch [7], Batch [301/938], Loss: 0.5554412007331848\n",
      "Validation: Epoch [7], Batch [302/938], Loss: 0.6125249862670898\n",
      "Validation: Epoch [7], Batch [303/938], Loss: 0.7304115295410156\n",
      "Validation: Epoch [7], Batch [304/938], Loss: 0.4544643759727478\n",
      "Validation: Epoch [7], Batch [305/938], Loss: 0.661176323890686\n",
      "Validation: Epoch [7], Batch [306/938], Loss: 0.6165195107460022\n",
      "Validation: Epoch [7], Batch [307/938], Loss: 0.5723388195037842\n",
      "Validation: Epoch [7], Batch [308/938], Loss: 0.6454176306724548\n",
      "Validation: Epoch [7], Batch [309/938], Loss: 0.5876604318618774\n",
      "Validation: Epoch [7], Batch [310/938], Loss: 0.49750250577926636\n",
      "Validation: Epoch [7], Batch [311/938], Loss: 0.6440227627754211\n",
      "Validation: Epoch [7], Batch [312/938], Loss: 0.5155916213989258\n",
      "Validation: Epoch [7], Batch [313/938], Loss: 0.5005884170532227\n",
      "Validation: Epoch [7], Batch [314/938], Loss: 0.5841981172561646\n",
      "Validation: Epoch [7], Batch [315/938], Loss: 0.5371718406677246\n",
      "Validation: Epoch [7], Batch [316/938], Loss: 0.6973210573196411\n",
      "Validation: Epoch [7], Batch [317/938], Loss: 0.5185344219207764\n",
      "Validation: Epoch [7], Batch [318/938], Loss: 0.5606073141098022\n",
      "Validation: Epoch [7], Batch [319/938], Loss: 0.7210633158683777\n",
      "Validation: Epoch [7], Batch [320/938], Loss: 0.4653252363204956\n",
      "Validation: Epoch [7], Batch [321/938], Loss: 0.5881897807121277\n",
      "Validation: Epoch [7], Batch [322/938], Loss: 0.6558756828308105\n",
      "Validation: Epoch [7], Batch [323/938], Loss: 0.5867947936058044\n",
      "Validation: Epoch [7], Batch [324/938], Loss: 0.41661760210990906\n",
      "Validation: Epoch [7], Batch [325/938], Loss: 0.7695050239562988\n",
      "Validation: Epoch [7], Batch [326/938], Loss: 0.5079724788665771\n",
      "Validation: Epoch [7], Batch [327/938], Loss: 0.8110437393188477\n",
      "Validation: Epoch [7], Batch [328/938], Loss: 0.571871280670166\n",
      "Validation: Epoch [7], Batch [329/938], Loss: 0.4902638792991638\n",
      "Validation: Epoch [7], Batch [330/938], Loss: 0.545075535774231\n",
      "Validation: Epoch [7], Batch [331/938], Loss: 0.5584132671356201\n",
      "Validation: Epoch [7], Batch [332/938], Loss: 0.713470995426178\n",
      "Validation: Epoch [7], Batch [333/938], Loss: 0.6486278772354126\n",
      "Validation: Epoch [7], Batch [334/938], Loss: 0.7955242395401001\n",
      "Validation: Epoch [7], Batch [335/938], Loss: 0.6121591329574585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [336/938], Loss: 0.6368604898452759\n",
      "Validation: Epoch [7], Batch [337/938], Loss: 0.5571357011795044\n",
      "Validation: Epoch [7], Batch [338/938], Loss: 0.602922260761261\n",
      "Validation: Epoch [7], Batch [339/938], Loss: 0.6058574318885803\n",
      "Validation: Epoch [7], Batch [340/938], Loss: 0.7703244090080261\n",
      "Validation: Epoch [7], Batch [341/938], Loss: 0.5523385405540466\n",
      "Validation: Epoch [7], Batch [342/938], Loss: 0.571269154548645\n",
      "Validation: Epoch [7], Batch [343/938], Loss: 0.628280520439148\n",
      "Validation: Epoch [7], Batch [344/938], Loss: 0.3629702031612396\n",
      "Validation: Epoch [7], Batch [345/938], Loss: 0.6548119783401489\n",
      "Validation: Epoch [7], Batch [346/938], Loss: 0.8034059405326843\n",
      "Validation: Epoch [7], Batch [347/938], Loss: 0.6285159587860107\n",
      "Validation: Epoch [7], Batch [348/938], Loss: 0.6645904183387756\n",
      "Validation: Epoch [7], Batch [349/938], Loss: 0.5030494928359985\n",
      "Validation: Epoch [7], Batch [350/938], Loss: 0.6421571969985962\n",
      "Validation: Epoch [7], Batch [351/938], Loss: 0.7931621670722961\n",
      "Validation: Epoch [7], Batch [352/938], Loss: 0.5857423543930054\n",
      "Validation: Epoch [7], Batch [353/938], Loss: 0.682736873626709\n",
      "Validation: Epoch [7], Batch [354/938], Loss: 0.609512448310852\n",
      "Validation: Epoch [7], Batch [355/938], Loss: 0.5584093332290649\n",
      "Validation: Epoch [7], Batch [356/938], Loss: 0.5524641871452332\n",
      "Validation: Epoch [7], Batch [357/938], Loss: 0.5965400338172913\n",
      "Validation: Epoch [7], Batch [358/938], Loss: 0.654560387134552\n",
      "Validation: Epoch [7], Batch [359/938], Loss: 0.4279300570487976\n",
      "Validation: Epoch [7], Batch [360/938], Loss: 0.6566712856292725\n",
      "Validation: Epoch [7], Batch [361/938], Loss: 0.5905999541282654\n",
      "Validation: Epoch [7], Batch [362/938], Loss: 0.6097928285598755\n",
      "Validation: Epoch [7], Batch [363/938], Loss: 0.5093376636505127\n",
      "Validation: Epoch [7], Batch [364/938], Loss: 0.6569586396217346\n",
      "Validation: Epoch [7], Batch [365/938], Loss: 0.4784972369670868\n",
      "Validation: Epoch [7], Batch [366/938], Loss: 0.9146906733512878\n",
      "Validation: Epoch [7], Batch [367/938], Loss: 0.9548065066337585\n",
      "Validation: Epoch [7], Batch [368/938], Loss: 0.5990456342697144\n",
      "Validation: Epoch [7], Batch [369/938], Loss: 0.6092973947525024\n",
      "Validation: Epoch [7], Batch [370/938], Loss: 0.6112080812454224\n",
      "Validation: Epoch [7], Batch [371/938], Loss: 0.4849444031715393\n",
      "Validation: Epoch [7], Batch [372/938], Loss: 0.5371094942092896\n",
      "Validation: Epoch [7], Batch [373/938], Loss: 0.3502900004386902\n",
      "Validation: Epoch [7], Batch [374/938], Loss: 0.8596271276473999\n",
      "Validation: Epoch [7], Batch [375/938], Loss: 0.567345917224884\n",
      "Validation: Epoch [7], Batch [376/938], Loss: 0.696770966053009\n",
      "Validation: Epoch [7], Batch [377/938], Loss: 0.5646102428436279\n",
      "Validation: Epoch [7], Batch [378/938], Loss: 0.4829500615596771\n",
      "Validation: Epoch [7], Batch [379/938], Loss: 0.40314704179763794\n",
      "Validation: Epoch [7], Batch [380/938], Loss: 0.37938398122787476\n",
      "Validation: Epoch [7], Batch [381/938], Loss: 0.5760382413864136\n",
      "Validation: Epoch [7], Batch [382/938], Loss: 0.6210678815841675\n",
      "Validation: Epoch [7], Batch [383/938], Loss: 0.5240894556045532\n",
      "Validation: Epoch [7], Batch [384/938], Loss: 0.6473410129547119\n",
      "Validation: Epoch [7], Batch [385/938], Loss: 0.8211959600448608\n",
      "Validation: Epoch [7], Batch [386/938], Loss: 0.6302723288536072\n",
      "Validation: Epoch [7], Batch [387/938], Loss: 0.5348036289215088\n",
      "Validation: Epoch [7], Batch [388/938], Loss: 0.50771564245224\n",
      "Validation: Epoch [7], Batch [389/938], Loss: 0.6005901098251343\n",
      "Validation: Epoch [7], Batch [390/938], Loss: 0.536055326461792\n",
      "Validation: Epoch [7], Batch [391/938], Loss: 0.5193809270858765\n",
      "Validation: Epoch [7], Batch [392/938], Loss: 0.5253127813339233\n",
      "Validation: Epoch [7], Batch [393/938], Loss: 0.7525272369384766\n",
      "Validation: Epoch [7], Batch [394/938], Loss: 0.6511633992195129\n",
      "Validation: Epoch [7], Batch [395/938], Loss: 0.6133575439453125\n",
      "Validation: Epoch [7], Batch [396/938], Loss: 0.7200559377670288\n",
      "Validation: Epoch [7], Batch [397/938], Loss: 0.5798813700675964\n",
      "Validation: Epoch [7], Batch [398/938], Loss: 0.5898616313934326\n",
      "Validation: Epoch [7], Batch [399/938], Loss: 0.573531985282898\n",
      "Validation: Epoch [7], Batch [400/938], Loss: 0.6209633350372314\n",
      "Validation: Epoch [7], Batch [401/938], Loss: 0.5553319454193115\n",
      "Validation: Epoch [7], Batch [402/938], Loss: 0.6757098436355591\n",
      "Validation: Epoch [7], Batch [403/938], Loss: 0.47107401490211487\n",
      "Validation: Epoch [7], Batch [404/938], Loss: 0.512315034866333\n",
      "Validation: Epoch [7], Batch [405/938], Loss: 0.5949735045433044\n",
      "Validation: Epoch [7], Batch [406/938], Loss: 0.5624390840530396\n",
      "Validation: Epoch [7], Batch [407/938], Loss: 0.5274948477745056\n",
      "Validation: Epoch [7], Batch [408/938], Loss: 0.5878859162330627\n",
      "Validation: Epoch [7], Batch [409/938], Loss: 0.5949274897575378\n",
      "Validation: Epoch [7], Batch [410/938], Loss: 0.4672204256057739\n",
      "Validation: Epoch [7], Batch [411/938], Loss: 0.5394173860549927\n",
      "Validation: Epoch [7], Batch [412/938], Loss: 0.687953531742096\n",
      "Validation: Epoch [7], Batch [413/938], Loss: 0.6749975085258484\n",
      "Validation: Epoch [7], Batch [414/938], Loss: 0.5522692203521729\n",
      "Validation: Epoch [7], Batch [415/938], Loss: 0.8361201882362366\n",
      "Validation: Epoch [7], Batch [416/938], Loss: 0.42152079939842224\n",
      "Validation: Epoch [7], Batch [417/938], Loss: 0.6090222001075745\n",
      "Validation: Epoch [7], Batch [418/938], Loss: 0.33900296688079834\n",
      "Validation: Epoch [7], Batch [419/938], Loss: 0.6798538565635681\n",
      "Validation: Epoch [7], Batch [420/938], Loss: 0.4106125831604004\n",
      "Validation: Epoch [7], Batch [421/938], Loss: 0.5780307650566101\n",
      "Validation: Epoch [7], Batch [422/938], Loss: 0.6243854761123657\n",
      "Validation: Epoch [7], Batch [423/938], Loss: 0.39755362272262573\n",
      "Validation: Epoch [7], Batch [424/938], Loss: 0.6564372777938843\n",
      "Validation: Epoch [7], Batch [425/938], Loss: 0.5418382287025452\n",
      "Validation: Epoch [7], Batch [426/938], Loss: 0.590469241142273\n",
      "Validation: Epoch [7], Batch [427/938], Loss: 0.4763435125350952\n",
      "Validation: Epoch [7], Batch [428/938], Loss: 0.46041569113731384\n",
      "Validation: Epoch [7], Batch [429/938], Loss: 0.5293401479721069\n",
      "Validation: Epoch [7], Batch [430/938], Loss: 0.8385151624679565\n",
      "Validation: Epoch [7], Batch [431/938], Loss: 0.47268742322921753\n",
      "Validation: Epoch [7], Batch [432/938], Loss: 0.4559745192527771\n",
      "Validation: Epoch [7], Batch [433/938], Loss: 0.5450387001037598\n",
      "Validation: Epoch [7], Batch [434/938], Loss: 0.6328855752944946\n",
      "Validation: Epoch [7], Batch [435/938], Loss: 0.5438651442527771\n",
      "Validation: Epoch [7], Batch [436/938], Loss: 0.46222060918807983\n",
      "Validation: Epoch [7], Batch [437/938], Loss: 0.6140753626823425\n",
      "Validation: Epoch [7], Batch [438/938], Loss: 0.4881010353565216\n",
      "Validation: Epoch [7], Batch [439/938], Loss: 0.4356731176376343\n",
      "Validation: Epoch [7], Batch [440/938], Loss: 0.48416462540626526\n",
      "Validation: Epoch [7], Batch [441/938], Loss: 0.49450063705444336\n",
      "Validation: Epoch [7], Batch [442/938], Loss: 0.637877345085144\n",
      "Validation: Epoch [7], Batch [443/938], Loss: 0.7329341173171997\n",
      "Validation: Epoch [7], Batch [444/938], Loss: 0.5452620983123779\n",
      "Validation: Epoch [7], Batch [445/938], Loss: 0.4861028492450714\n",
      "Validation: Epoch [7], Batch [446/938], Loss: 0.5559526681900024\n",
      "Validation: Epoch [7], Batch [447/938], Loss: 0.7196491956710815\n",
      "Validation: Epoch [7], Batch [448/938], Loss: 0.6796150207519531\n",
      "Validation: Epoch [7], Batch [449/938], Loss: 0.44933274388313293\n",
      "Validation: Epoch [7], Batch [450/938], Loss: 0.5086153149604797\n",
      "Validation: Epoch [7], Batch [451/938], Loss: 0.6512348651885986\n",
      "Validation: Epoch [7], Batch [452/938], Loss: 0.6685927510261536\n",
      "Validation: Epoch [7], Batch [453/938], Loss: 0.531777024269104\n",
      "Validation: Epoch [7], Batch [454/938], Loss: 0.5100191831588745\n",
      "Validation: Epoch [7], Batch [455/938], Loss: 0.7021098136901855\n",
      "Validation: Epoch [7], Batch [456/938], Loss: 0.6401165723800659\n",
      "Validation: Epoch [7], Batch [457/938], Loss: 0.6674662828445435\n",
      "Validation: Epoch [7], Batch [458/938], Loss: 0.6242838501930237\n",
      "Validation: Epoch [7], Batch [459/938], Loss: 0.8281897902488708\n",
      "Validation: Epoch [7], Batch [460/938], Loss: 0.6923559904098511\n",
      "Validation: Epoch [7], Batch [461/938], Loss: 0.6924386024475098\n",
      "Validation: Epoch [7], Batch [462/938], Loss: 0.5492768883705139\n",
      "Validation: Epoch [7], Batch [463/938], Loss: 0.536518931388855\n",
      "Validation: Epoch [7], Batch [464/938], Loss: 0.6820704340934753\n",
      "Validation: Epoch [7], Batch [465/938], Loss: 0.8884942531585693\n",
      "Validation: Epoch [7], Batch [466/938], Loss: 0.5657184720039368\n",
      "Validation: Epoch [7], Batch [467/938], Loss: 0.6491880416870117\n",
      "Validation: Epoch [7], Batch [468/938], Loss: 0.4366278052330017\n",
      "Validation: Epoch [7], Batch [469/938], Loss: 0.6092398166656494\n",
      "Validation: Epoch [7], Batch [470/938], Loss: 0.6737126111984253\n",
      "Validation: Epoch [7], Batch [471/938], Loss: 0.5268350839614868\n",
      "Validation: Epoch [7], Batch [472/938], Loss: 0.587158739566803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [473/938], Loss: 0.4233114719390869\n",
      "Validation: Epoch [7], Batch [474/938], Loss: 0.6816929578781128\n",
      "Validation: Epoch [7], Batch [475/938], Loss: 0.5579419136047363\n",
      "Validation: Epoch [7], Batch [476/938], Loss: 0.6233677864074707\n",
      "Validation: Epoch [7], Batch [477/938], Loss: 0.4138563871383667\n",
      "Validation: Epoch [7], Batch [478/938], Loss: 0.699354887008667\n",
      "Validation: Epoch [7], Batch [479/938], Loss: 0.5796707272529602\n",
      "Validation: Epoch [7], Batch [480/938], Loss: 0.6620376706123352\n",
      "Validation: Epoch [7], Batch [481/938], Loss: 0.5119081735610962\n",
      "Validation: Epoch [7], Batch [482/938], Loss: 0.9243544340133667\n",
      "Validation: Epoch [7], Batch [483/938], Loss: 0.46934494376182556\n",
      "Validation: Epoch [7], Batch [484/938], Loss: 0.585134744644165\n",
      "Validation: Epoch [7], Batch [485/938], Loss: 0.7768639326095581\n",
      "Validation: Epoch [7], Batch [486/938], Loss: 0.6745794415473938\n",
      "Validation: Epoch [7], Batch [487/938], Loss: 0.46215981245040894\n",
      "Validation: Epoch [7], Batch [488/938], Loss: 0.5298669338226318\n",
      "Validation: Epoch [7], Batch [489/938], Loss: 0.44608935713768005\n",
      "Validation: Epoch [7], Batch [490/938], Loss: 0.6985094547271729\n",
      "Validation: Epoch [7], Batch [491/938], Loss: 0.5410401821136475\n",
      "Validation: Epoch [7], Batch [492/938], Loss: 0.7029769420623779\n",
      "Validation: Epoch [7], Batch [493/938], Loss: 0.5779585242271423\n",
      "Validation: Epoch [7], Batch [494/938], Loss: 0.655363917350769\n",
      "Validation: Epoch [7], Batch [495/938], Loss: 0.6315386891365051\n",
      "Validation: Epoch [7], Batch [496/938], Loss: 0.5407254695892334\n",
      "Validation: Epoch [7], Batch [497/938], Loss: 0.43953657150268555\n",
      "Validation: Epoch [7], Batch [498/938], Loss: 0.5176929235458374\n",
      "Validation: Epoch [7], Batch [499/938], Loss: 0.46610572934150696\n",
      "Validation: Epoch [7], Batch [500/938], Loss: 0.714766800403595\n",
      "Validation: Epoch [7], Batch [501/938], Loss: 0.5743719339370728\n",
      "Validation: Epoch [7], Batch [502/938], Loss: 0.5793017148971558\n",
      "Validation: Epoch [7], Batch [503/938], Loss: 0.5274041891098022\n",
      "Validation: Epoch [7], Batch [504/938], Loss: 0.7953788042068481\n",
      "Validation: Epoch [7], Batch [505/938], Loss: 0.6349987387657166\n",
      "Validation: Epoch [7], Batch [506/938], Loss: 0.5840771794319153\n",
      "Validation: Epoch [7], Batch [507/938], Loss: 0.5892733335494995\n",
      "Validation: Epoch [7], Batch [508/938], Loss: 0.4751162528991699\n",
      "Validation: Epoch [7], Batch [509/938], Loss: 0.5112414956092834\n",
      "Validation: Epoch [7], Batch [510/938], Loss: 0.6009279489517212\n",
      "Validation: Epoch [7], Batch [511/938], Loss: 0.7764638066291809\n",
      "Validation: Epoch [7], Batch [512/938], Loss: 0.6730087399482727\n",
      "Validation: Epoch [7], Batch [513/938], Loss: 0.6053979992866516\n",
      "Validation: Epoch [7], Batch [514/938], Loss: 0.6370573043823242\n",
      "Validation: Epoch [7], Batch [515/938], Loss: 0.5238221883773804\n",
      "Validation: Epoch [7], Batch [516/938], Loss: 0.6389506459236145\n",
      "Validation: Epoch [7], Batch [517/938], Loss: 0.7706832885742188\n",
      "Validation: Epoch [7], Batch [518/938], Loss: 0.5290732979774475\n",
      "Validation: Epoch [7], Batch [519/938], Loss: 0.6447803378105164\n",
      "Validation: Epoch [7], Batch [520/938], Loss: 0.5130308866500854\n",
      "Validation: Epoch [7], Batch [521/938], Loss: 0.5698032379150391\n",
      "Validation: Epoch [7], Batch [522/938], Loss: 0.5141703486442566\n",
      "Validation: Epoch [7], Batch [523/938], Loss: 0.37620672583580017\n",
      "Validation: Epoch [7], Batch [524/938], Loss: 0.5761059522628784\n",
      "Validation: Epoch [7], Batch [525/938], Loss: 0.623661994934082\n",
      "Validation: Epoch [7], Batch [526/938], Loss: 0.746590256690979\n",
      "Validation: Epoch [7], Batch [527/938], Loss: 0.616180956363678\n",
      "Validation: Epoch [7], Batch [528/938], Loss: 0.664399266242981\n",
      "Validation: Epoch [7], Batch [529/938], Loss: 0.6759685277938843\n",
      "Validation: Epoch [7], Batch [530/938], Loss: 0.5844542980194092\n",
      "Validation: Epoch [7], Batch [531/938], Loss: 0.5061088800430298\n",
      "Validation: Epoch [7], Batch [532/938], Loss: 0.4792158007621765\n",
      "Validation: Epoch [7], Batch [533/938], Loss: 0.6769511699676514\n",
      "Validation: Epoch [7], Batch [534/938], Loss: 0.5712260603904724\n",
      "Validation: Epoch [7], Batch [535/938], Loss: 0.6396539807319641\n",
      "Validation: Epoch [7], Batch [536/938], Loss: 0.4992982745170593\n",
      "Validation: Epoch [7], Batch [537/938], Loss: 0.553715705871582\n",
      "Validation: Epoch [7], Batch [538/938], Loss: 0.5234034061431885\n",
      "Validation: Epoch [7], Batch [539/938], Loss: 0.49935922026634216\n",
      "Validation: Epoch [7], Batch [540/938], Loss: 0.6893953084945679\n",
      "Validation: Epoch [7], Batch [541/938], Loss: 0.6914839744567871\n",
      "Validation: Epoch [7], Batch [542/938], Loss: 0.49772346019744873\n",
      "Validation: Epoch [7], Batch [543/938], Loss: 0.5994354486465454\n",
      "Validation: Epoch [7], Batch [544/938], Loss: 0.6025724411010742\n",
      "Validation: Epoch [7], Batch [545/938], Loss: 0.49749284982681274\n",
      "Validation: Epoch [7], Batch [546/938], Loss: 0.5150399208068848\n",
      "Validation: Epoch [7], Batch [547/938], Loss: 0.4385206401348114\n",
      "Validation: Epoch [7], Batch [548/938], Loss: 0.6629480123519897\n",
      "Validation: Epoch [7], Batch [549/938], Loss: 0.5958124399185181\n",
      "Validation: Epoch [7], Batch [550/938], Loss: 0.5241280794143677\n",
      "Validation: Epoch [7], Batch [551/938], Loss: 0.6723942756652832\n",
      "Validation: Epoch [7], Batch [552/938], Loss: 0.6764776706695557\n",
      "Validation: Epoch [7], Batch [553/938], Loss: 0.3897528052330017\n",
      "Validation: Epoch [7], Batch [554/938], Loss: 0.5928137898445129\n",
      "Validation: Epoch [7], Batch [555/938], Loss: 0.7028703689575195\n",
      "Validation: Epoch [7], Batch [556/938], Loss: 0.5027070641517639\n",
      "Validation: Epoch [7], Batch [557/938], Loss: 0.7069871425628662\n",
      "Validation: Epoch [7], Batch [558/938], Loss: 0.6537798643112183\n",
      "Validation: Epoch [7], Batch [559/938], Loss: 0.4309026002883911\n",
      "Validation: Epoch [7], Batch [560/938], Loss: 0.5544714331626892\n",
      "Validation: Epoch [7], Batch [561/938], Loss: 0.7056926488876343\n",
      "Validation: Epoch [7], Batch [562/938], Loss: 0.6878999471664429\n",
      "Validation: Epoch [7], Batch [563/938], Loss: 0.5647305846214294\n",
      "Validation: Epoch [7], Batch [564/938], Loss: 0.6569393873214722\n",
      "Validation: Epoch [7], Batch [565/938], Loss: 0.4598601460456848\n",
      "Validation: Epoch [7], Batch [566/938], Loss: 0.4904855489730835\n",
      "Validation: Epoch [7], Batch [567/938], Loss: 0.5255587697029114\n",
      "Validation: Epoch [7], Batch [568/938], Loss: 0.6950331926345825\n",
      "Validation: Epoch [7], Batch [569/938], Loss: 0.5816303491592407\n",
      "Validation: Epoch [7], Batch [570/938], Loss: 0.6628618240356445\n",
      "Validation: Epoch [7], Batch [571/938], Loss: 0.5652322769165039\n",
      "Validation: Epoch [7], Batch [572/938], Loss: 0.3861919343471527\n",
      "Validation: Epoch [7], Batch [573/938], Loss: 0.5370979309082031\n",
      "Validation: Epoch [7], Batch [574/938], Loss: 0.7251174449920654\n",
      "Validation: Epoch [7], Batch [575/938], Loss: 0.3628681004047394\n",
      "Validation: Epoch [7], Batch [576/938], Loss: 0.49497124552726746\n",
      "Validation: Epoch [7], Batch [577/938], Loss: 0.5511943697929382\n",
      "Validation: Epoch [7], Batch [578/938], Loss: 0.5631756782531738\n",
      "Validation: Epoch [7], Batch [579/938], Loss: 0.43580204248428345\n",
      "Validation: Epoch [7], Batch [580/938], Loss: 0.4148755371570587\n",
      "Validation: Epoch [7], Batch [581/938], Loss: 0.42020946741104126\n",
      "Validation: Epoch [7], Batch [582/938], Loss: 0.6423925161361694\n",
      "Validation: Epoch [7], Batch [583/938], Loss: 0.5809565782546997\n",
      "Validation: Epoch [7], Batch [584/938], Loss: 0.6896646618843079\n",
      "Validation: Epoch [7], Batch [585/938], Loss: 0.6282877922058105\n",
      "Validation: Epoch [7], Batch [586/938], Loss: 0.5452173352241516\n",
      "Validation: Epoch [7], Batch [587/938], Loss: 0.7339531183242798\n",
      "Validation: Epoch [7], Batch [588/938], Loss: 0.47266528010368347\n",
      "Validation: Epoch [7], Batch [589/938], Loss: 0.40179795026779175\n",
      "Validation: Epoch [7], Batch [590/938], Loss: 0.4957314133644104\n",
      "Validation: Epoch [7], Batch [591/938], Loss: 0.6391850709915161\n",
      "Validation: Epoch [7], Batch [592/938], Loss: 0.6343787908554077\n",
      "Validation: Epoch [7], Batch [593/938], Loss: 0.8196501731872559\n",
      "Validation: Epoch [7], Batch [594/938], Loss: 0.5975903272628784\n",
      "Validation: Epoch [7], Batch [595/938], Loss: 0.5869324207305908\n",
      "Validation: Epoch [7], Batch [596/938], Loss: 0.686637282371521\n",
      "Validation: Epoch [7], Batch [597/938], Loss: 0.475307822227478\n",
      "Validation: Epoch [7], Batch [598/938], Loss: 0.5551906228065491\n",
      "Validation: Epoch [7], Batch [599/938], Loss: 0.43193504214286804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [600/938], Loss: 0.4851192533969879\n",
      "Validation: Epoch [7], Batch [601/938], Loss: 0.581092357635498\n",
      "Validation: Epoch [7], Batch [602/938], Loss: 0.5059249401092529\n",
      "Validation: Epoch [7], Batch [603/938], Loss: 0.4552711248397827\n",
      "Validation: Epoch [7], Batch [604/938], Loss: 0.4805726706981659\n",
      "Validation: Epoch [7], Batch [605/938], Loss: 0.7415295839309692\n",
      "Validation: Epoch [7], Batch [606/938], Loss: 0.6836431622505188\n",
      "Validation: Epoch [7], Batch [607/938], Loss: 0.889107882976532\n",
      "Validation: Epoch [7], Batch [608/938], Loss: 0.6662514209747314\n",
      "Validation: Epoch [7], Batch [609/938], Loss: 0.49112045764923096\n",
      "Validation: Epoch [7], Batch [610/938], Loss: 0.595218300819397\n",
      "Validation: Epoch [7], Batch [611/938], Loss: 0.40048131346702576\n",
      "Validation: Epoch [7], Batch [612/938], Loss: 0.5762024521827698\n",
      "Validation: Epoch [7], Batch [613/938], Loss: 0.45808589458465576\n",
      "Validation: Epoch [7], Batch [614/938], Loss: 0.4453343152999878\n",
      "Validation: Epoch [7], Batch [615/938], Loss: 0.34868234395980835\n",
      "Validation: Epoch [7], Batch [616/938], Loss: 1.0258686542510986\n",
      "Validation: Epoch [7], Batch [617/938], Loss: 0.5421425104141235\n",
      "Validation: Epoch [7], Batch [618/938], Loss: 0.7612261772155762\n",
      "Validation: Epoch [7], Batch [619/938], Loss: 0.44899487495422363\n",
      "Validation: Epoch [7], Batch [620/938], Loss: 0.5637337565422058\n",
      "Validation: Epoch [7], Batch [621/938], Loss: 0.5017768144607544\n",
      "Validation: Epoch [7], Batch [622/938], Loss: 0.6122055053710938\n",
      "Validation: Epoch [7], Batch [623/938], Loss: 0.6427690982818604\n",
      "Validation: Epoch [7], Batch [624/938], Loss: 0.3699532151222229\n",
      "Validation: Epoch [7], Batch [625/938], Loss: 0.5612601041793823\n",
      "Validation: Epoch [7], Batch [626/938], Loss: 0.6188784837722778\n",
      "Validation: Epoch [7], Batch [627/938], Loss: 0.633090615272522\n",
      "Validation: Epoch [7], Batch [628/938], Loss: 0.6370692849159241\n",
      "Validation: Epoch [7], Batch [629/938], Loss: 0.6874750852584839\n",
      "Validation: Epoch [7], Batch [630/938], Loss: 0.618151068687439\n",
      "Validation: Epoch [7], Batch [631/938], Loss: 0.44904911518096924\n",
      "Validation: Epoch [7], Batch [632/938], Loss: 0.7037148475646973\n",
      "Validation: Epoch [7], Batch [633/938], Loss: 0.49332326650619507\n",
      "Validation: Epoch [7], Batch [634/938], Loss: 0.7762203216552734\n",
      "Validation: Epoch [7], Batch [635/938], Loss: 0.6282049417495728\n",
      "Validation: Epoch [7], Batch [636/938], Loss: 0.3732267916202545\n",
      "Validation: Epoch [7], Batch [637/938], Loss: 0.5210472345352173\n",
      "Validation: Epoch [7], Batch [638/938], Loss: 0.5652217864990234\n",
      "Validation: Epoch [7], Batch [639/938], Loss: 0.7297656536102295\n",
      "Validation: Epoch [7], Batch [640/938], Loss: 0.7273907661437988\n",
      "Validation: Epoch [7], Batch [641/938], Loss: 0.5293417572975159\n",
      "Validation: Epoch [7], Batch [642/938], Loss: 0.7041674852371216\n",
      "Validation: Epoch [7], Batch [643/938], Loss: 0.5927451848983765\n",
      "Validation: Epoch [7], Batch [644/938], Loss: 0.6314655542373657\n",
      "Validation: Epoch [7], Batch [645/938], Loss: 0.7581961154937744\n",
      "Validation: Epoch [7], Batch [646/938], Loss: 0.4492592513561249\n",
      "Validation: Epoch [7], Batch [647/938], Loss: 0.48275843262672424\n",
      "Validation: Epoch [7], Batch [648/938], Loss: 0.5544997453689575\n",
      "Validation: Epoch [7], Batch [649/938], Loss: 0.5072835683822632\n",
      "Validation: Epoch [7], Batch [650/938], Loss: 0.44693896174430847\n",
      "Validation: Epoch [7], Batch [651/938], Loss: 0.4522780179977417\n",
      "Validation: Epoch [7], Batch [652/938], Loss: 0.5473513007164001\n",
      "Validation: Epoch [7], Batch [653/938], Loss: 0.3293560743331909\n",
      "Validation: Epoch [7], Batch [654/938], Loss: 0.6218858957290649\n",
      "Validation: Epoch [7], Batch [655/938], Loss: 0.5254268050193787\n",
      "Validation: Epoch [7], Batch [656/938], Loss: 0.5670357346534729\n",
      "Validation: Epoch [7], Batch [657/938], Loss: 0.6756974458694458\n",
      "Validation: Epoch [7], Batch [658/938], Loss: 0.5790375471115112\n",
      "Validation: Epoch [7], Batch [659/938], Loss: 0.43904727697372437\n",
      "Validation: Epoch [7], Batch [660/938], Loss: 0.5791528224945068\n",
      "Validation: Epoch [7], Batch [661/938], Loss: 0.4549458622932434\n",
      "Validation: Epoch [7], Batch [662/938], Loss: 0.605414867401123\n",
      "Validation: Epoch [7], Batch [663/938], Loss: 0.3193426728248596\n",
      "Validation: Epoch [7], Batch [664/938], Loss: 0.5492534637451172\n",
      "Validation: Epoch [7], Batch [665/938], Loss: 0.5325026512145996\n",
      "Validation: Epoch [7], Batch [666/938], Loss: 0.42961883544921875\n",
      "Validation: Epoch [7], Batch [667/938], Loss: 0.5899033546447754\n",
      "Validation: Epoch [7], Batch [668/938], Loss: 0.8555092811584473\n",
      "Validation: Epoch [7], Batch [669/938], Loss: 0.8353524804115295\n",
      "Validation: Epoch [7], Batch [670/938], Loss: 0.5968419909477234\n",
      "Validation: Epoch [7], Batch [671/938], Loss: 0.449930340051651\n",
      "Validation: Epoch [7], Batch [672/938], Loss: 0.6353060007095337\n",
      "Validation: Epoch [7], Batch [673/938], Loss: 0.4947640895843506\n",
      "Validation: Epoch [7], Batch [674/938], Loss: 0.7118394374847412\n",
      "Validation: Epoch [7], Batch [675/938], Loss: 0.6602379083633423\n",
      "Validation: Epoch [7], Batch [676/938], Loss: 0.6621522903442383\n",
      "Validation: Epoch [7], Batch [677/938], Loss: 0.48074808716773987\n",
      "Validation: Epoch [7], Batch [678/938], Loss: 0.6491860151290894\n",
      "Validation: Epoch [7], Batch [679/938], Loss: 0.5409237146377563\n",
      "Validation: Epoch [7], Batch [680/938], Loss: 0.7475801706314087\n",
      "Validation: Epoch [7], Batch [681/938], Loss: 0.5082054138183594\n",
      "Validation: Epoch [7], Batch [682/938], Loss: 0.6622317433357239\n",
      "Validation: Epoch [7], Batch [683/938], Loss: 0.5708560347557068\n",
      "Validation: Epoch [7], Batch [684/938], Loss: 0.6364617347717285\n",
      "Validation: Epoch [7], Batch [685/938], Loss: 0.3715706467628479\n",
      "Validation: Epoch [7], Batch [686/938], Loss: 0.5473726987838745\n",
      "Validation: Epoch [7], Batch [687/938], Loss: 0.5552166700363159\n",
      "Validation: Epoch [7], Batch [688/938], Loss: 0.6016926765441895\n",
      "Validation: Epoch [7], Batch [689/938], Loss: 0.5059888958930969\n",
      "Validation: Epoch [7], Batch [690/938], Loss: 0.6054270267486572\n",
      "Validation: Epoch [7], Batch [691/938], Loss: 0.7282570004463196\n",
      "Validation: Epoch [7], Batch [692/938], Loss: 0.503354549407959\n",
      "Validation: Epoch [7], Batch [693/938], Loss: 0.6992756128311157\n",
      "Validation: Epoch [7], Batch [694/938], Loss: 0.7061573266983032\n",
      "Validation: Epoch [7], Batch [695/938], Loss: 0.5427149534225464\n",
      "Validation: Epoch [7], Batch [696/938], Loss: 0.6010716557502747\n",
      "Validation: Epoch [7], Batch [697/938], Loss: 0.6222121715545654\n",
      "Validation: Epoch [7], Batch [698/938], Loss: 0.5955691933631897\n",
      "Validation: Epoch [7], Batch [699/938], Loss: 0.8106863498687744\n",
      "Validation: Epoch [7], Batch [700/938], Loss: 0.46236228942871094\n",
      "Validation: Epoch [7], Batch [701/938], Loss: 0.5327131748199463\n",
      "Validation: Epoch [7], Batch [702/938], Loss: 0.5335650444030762\n",
      "Validation: Epoch [7], Batch [703/938], Loss: 0.478112131357193\n",
      "Validation: Epoch [7], Batch [704/938], Loss: 0.6929668188095093\n",
      "Validation: Epoch [7], Batch [705/938], Loss: 0.4666477143764496\n",
      "Validation: Epoch [7], Batch [706/938], Loss: 0.5677040219306946\n",
      "Validation: Epoch [7], Batch [707/938], Loss: 0.48064491152763367\n",
      "Validation: Epoch [7], Batch [708/938], Loss: 0.5175354480743408\n",
      "Validation: Epoch [7], Batch [709/938], Loss: 0.6427236795425415\n",
      "Validation: Epoch [7], Batch [710/938], Loss: 0.7007879614830017\n",
      "Validation: Epoch [7], Batch [711/938], Loss: 0.6263631582260132\n",
      "Validation: Epoch [7], Batch [712/938], Loss: 0.748355507850647\n",
      "Validation: Epoch [7], Batch [713/938], Loss: 0.5048023462295532\n",
      "Validation: Epoch [7], Batch [714/938], Loss: 0.6611841917037964\n",
      "Validation: Epoch [7], Batch [715/938], Loss: 0.46296828985214233\n",
      "Validation: Epoch [7], Batch [716/938], Loss: 0.6699381470680237\n",
      "Validation: Epoch [7], Batch [717/938], Loss: 0.7340359091758728\n",
      "Validation: Epoch [7], Batch [718/938], Loss: 0.5619211196899414\n",
      "Validation: Epoch [7], Batch [719/938], Loss: 0.5554234385490417\n",
      "Validation: Epoch [7], Batch [720/938], Loss: 0.4672141671180725\n",
      "Validation: Epoch [7], Batch [721/938], Loss: 0.6923198699951172\n",
      "Validation: Epoch [7], Batch [722/938], Loss: 0.745995819568634\n",
      "Validation: Epoch [7], Batch [723/938], Loss: 0.6084817051887512\n",
      "Validation: Epoch [7], Batch [724/938], Loss: 0.5305356979370117\n",
      "Validation: Epoch [7], Batch [725/938], Loss: 0.5816344618797302\n",
      "Validation: Epoch [7], Batch [726/938], Loss: 0.6846828460693359\n",
      "Validation: Epoch [7], Batch [727/938], Loss: 0.7833296060562134\n",
      "Validation: Epoch [7], Batch [728/938], Loss: 0.5997560024261475\n",
      "Validation: Epoch [7], Batch [729/938], Loss: 0.513924241065979\n",
      "Validation: Epoch [7], Batch [730/938], Loss: 0.5364718437194824\n",
      "Validation: Epoch [7], Batch [731/938], Loss: 0.6339490413665771\n",
      "Validation: Epoch [7], Batch [732/938], Loss: 0.5628914833068848\n",
      "Validation: Epoch [7], Batch [733/938], Loss: 0.4826575219631195\n",
      "Validation: Epoch [7], Batch [734/938], Loss: 0.7046926021575928\n",
      "Validation: Epoch [7], Batch [735/938], Loss: 0.7086358070373535\n",
      "Validation: Epoch [7], Batch [736/938], Loss: 0.7326368093490601\n",
      "Validation: Epoch [7], Batch [737/938], Loss: 0.36504727602005005\n",
      "Validation: Epoch [7], Batch [738/938], Loss: 0.7473275661468506\n",
      "Validation: Epoch [7], Batch [739/938], Loss: 0.507038950920105\n",
      "Validation: Epoch [7], Batch [740/938], Loss: 0.6432657241821289\n",
      "Validation: Epoch [7], Batch [741/938], Loss: 0.5995410680770874\n",
      "Validation: Epoch [7], Batch [742/938], Loss: 0.5950762629508972\n",
      "Validation: Epoch [7], Batch [743/938], Loss: 0.49037012457847595\n",
      "Validation: Epoch [7], Batch [744/938], Loss: 0.6696816682815552\n",
      "Validation: Epoch [7], Batch [745/938], Loss: 0.713050901889801\n",
      "Validation: Epoch [7], Batch [746/938], Loss: 0.5504526495933533\n",
      "Validation: Epoch [7], Batch [747/938], Loss: 0.6088229417800903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [748/938], Loss: 0.5690163373947144\n",
      "Validation: Epoch [7], Batch [749/938], Loss: 0.5427771210670471\n",
      "Validation: Epoch [7], Batch [750/938], Loss: 0.5824130773544312\n",
      "Validation: Epoch [7], Batch [751/938], Loss: 1.041464924812317\n",
      "Validation: Epoch [7], Batch [752/938], Loss: 0.7225138545036316\n",
      "Validation: Epoch [7], Batch [753/938], Loss: 0.638008713722229\n",
      "Validation: Epoch [7], Batch [754/938], Loss: 0.5895483493804932\n",
      "Validation: Epoch [7], Batch [755/938], Loss: 0.7115368247032166\n",
      "Validation: Epoch [7], Batch [756/938], Loss: 0.713028073310852\n",
      "Validation: Epoch [7], Batch [757/938], Loss: 0.5289525389671326\n",
      "Validation: Epoch [7], Batch [758/938], Loss: 0.5658032298088074\n",
      "Validation: Epoch [7], Batch [759/938], Loss: 0.7049663662910461\n",
      "Validation: Epoch [7], Batch [760/938], Loss: 0.7195852994918823\n",
      "Validation: Epoch [7], Batch [761/938], Loss: 0.8318639397621155\n",
      "Validation: Epoch [7], Batch [762/938], Loss: 0.5077560544013977\n",
      "Validation: Epoch [7], Batch [763/938], Loss: 0.6920236349105835\n",
      "Validation: Epoch [7], Batch [764/938], Loss: 0.6045236587524414\n",
      "Validation: Epoch [7], Batch [765/938], Loss: 0.4443938136100769\n",
      "Validation: Epoch [7], Batch [766/938], Loss: 0.6855749487876892\n",
      "Validation: Epoch [7], Batch [767/938], Loss: 0.4887542128562927\n",
      "Validation: Epoch [7], Batch [768/938], Loss: 0.6059910655021667\n",
      "Validation: Epoch [7], Batch [769/938], Loss: 0.6701899766921997\n",
      "Validation: Epoch [7], Batch [770/938], Loss: 0.6193008422851562\n",
      "Validation: Epoch [7], Batch [771/938], Loss: 0.7589029669761658\n",
      "Validation: Epoch [7], Batch [772/938], Loss: 0.6702198386192322\n",
      "Validation: Epoch [7], Batch [773/938], Loss: 0.5802735686302185\n",
      "Validation: Epoch [7], Batch [774/938], Loss: 0.4890081286430359\n",
      "Validation: Epoch [7], Batch [775/938], Loss: 0.6542692184448242\n",
      "Validation: Epoch [7], Batch [776/938], Loss: 0.5063704252243042\n",
      "Validation: Epoch [7], Batch [777/938], Loss: 0.6781005859375\n",
      "Validation: Epoch [7], Batch [778/938], Loss: 0.5538542866706848\n",
      "Validation: Epoch [7], Batch [779/938], Loss: 0.8833792805671692\n",
      "Validation: Epoch [7], Batch [780/938], Loss: 0.45027339458465576\n",
      "Validation: Epoch [7], Batch [781/938], Loss: 0.557157039642334\n",
      "Validation: Epoch [7], Batch [782/938], Loss: 0.4322611689567566\n",
      "Validation: Epoch [7], Batch [783/938], Loss: 0.40198850631713867\n",
      "Validation: Epoch [7], Batch [784/938], Loss: 0.5968050956726074\n",
      "Validation: Epoch [7], Batch [785/938], Loss: 0.6316808462142944\n",
      "Validation: Epoch [7], Batch [786/938], Loss: 0.5508450865745544\n",
      "Validation: Epoch [7], Batch [787/938], Loss: 0.5157612562179565\n",
      "Validation: Epoch [7], Batch [788/938], Loss: 0.6564874649047852\n",
      "Validation: Epoch [7], Batch [789/938], Loss: 0.6202203631401062\n",
      "Validation: Epoch [7], Batch [790/938], Loss: 0.6078755855560303\n",
      "Validation: Epoch [7], Batch [791/938], Loss: 0.4551406502723694\n",
      "Validation: Epoch [7], Batch [792/938], Loss: 0.5888794660568237\n",
      "Validation: Epoch [7], Batch [793/938], Loss: 0.6550456881523132\n",
      "Validation: Epoch [7], Batch [794/938], Loss: 0.6973142623901367\n",
      "Validation: Epoch [7], Batch [795/938], Loss: 0.8248517513275146\n",
      "Validation: Epoch [7], Batch [796/938], Loss: 0.7227036952972412\n",
      "Validation: Epoch [7], Batch [797/938], Loss: 0.4703097939491272\n",
      "Validation: Epoch [7], Batch [798/938], Loss: 0.4740574359893799\n",
      "Validation: Epoch [7], Batch [799/938], Loss: 0.6430290937423706\n",
      "Validation: Epoch [7], Batch [800/938], Loss: 0.6761621236801147\n",
      "Validation: Epoch [7], Batch [801/938], Loss: 0.6233189105987549\n",
      "Validation: Epoch [7], Batch [802/938], Loss: 0.5764400959014893\n",
      "Validation: Epoch [7], Batch [803/938], Loss: 0.6207407712936401\n",
      "Validation: Epoch [7], Batch [804/938], Loss: 0.4702059328556061\n",
      "Validation: Epoch [7], Batch [805/938], Loss: 0.7846280336380005\n",
      "Validation: Epoch [7], Batch [806/938], Loss: 0.6540111303329468\n",
      "Validation: Epoch [7], Batch [807/938], Loss: 0.396838903427124\n",
      "Validation: Epoch [7], Batch [808/938], Loss: 0.4345957338809967\n",
      "Validation: Epoch [7], Batch [809/938], Loss: 0.7730242013931274\n",
      "Validation: Epoch [7], Batch [810/938], Loss: 0.5764481425285339\n",
      "Validation: Epoch [7], Batch [811/938], Loss: 0.5485107898712158\n",
      "Validation: Epoch [7], Batch [812/938], Loss: 0.3732430338859558\n",
      "Validation: Epoch [7], Batch [813/938], Loss: 0.49631667137145996\n",
      "Validation: Epoch [7], Batch [814/938], Loss: 0.6731332540512085\n",
      "Validation: Epoch [7], Batch [815/938], Loss: 0.6667284369468689\n",
      "Validation: Epoch [7], Batch [816/938], Loss: 0.5871881246566772\n",
      "Validation: Epoch [7], Batch [817/938], Loss: 0.3892862796783447\n",
      "Validation: Epoch [7], Batch [818/938], Loss: 0.36653751134872437\n",
      "Validation: Epoch [7], Batch [819/938], Loss: 0.4509107172489166\n",
      "Validation: Epoch [7], Batch [820/938], Loss: 0.7169007062911987\n",
      "Validation: Epoch [7], Batch [821/938], Loss: 0.4951091706752777\n",
      "Validation: Epoch [7], Batch [822/938], Loss: 0.45135563611984253\n",
      "Validation: Epoch [7], Batch [823/938], Loss: 0.4090745151042938\n",
      "Validation: Epoch [7], Batch [824/938], Loss: 0.6525222659111023\n",
      "Validation: Epoch [7], Batch [825/938], Loss: 0.7184231877326965\n",
      "Validation: Epoch [7], Batch [826/938], Loss: 0.4952091872692108\n",
      "Validation: Epoch [7], Batch [827/938], Loss: 0.5161729454994202\n",
      "Validation: Epoch [7], Batch [828/938], Loss: 0.5630404949188232\n",
      "Validation: Epoch [7], Batch [829/938], Loss: 0.686530590057373\n",
      "Validation: Epoch [7], Batch [830/938], Loss: 0.5564771890640259\n",
      "Validation: Epoch [7], Batch [831/938], Loss: 0.6533799171447754\n",
      "Validation: Epoch [7], Batch [832/938], Loss: 0.6064457297325134\n",
      "Validation: Epoch [7], Batch [833/938], Loss: 0.6035933494567871\n",
      "Validation: Epoch [7], Batch [834/938], Loss: 0.6601325273513794\n",
      "Validation: Epoch [7], Batch [835/938], Loss: 0.6934565305709839\n",
      "Validation: Epoch [7], Batch [836/938], Loss: 0.6342682242393494\n",
      "Validation: Epoch [7], Batch [837/938], Loss: 0.4759417772293091\n",
      "Validation: Epoch [7], Batch [838/938], Loss: 0.5554311871528625\n",
      "Validation: Epoch [7], Batch [839/938], Loss: 0.40740931034088135\n",
      "Validation: Epoch [7], Batch [840/938], Loss: 0.5925679802894592\n",
      "Validation: Epoch [7], Batch [841/938], Loss: 0.4957137703895569\n",
      "Validation: Epoch [7], Batch [842/938], Loss: 0.5194778442382812\n",
      "Validation: Epoch [7], Batch [843/938], Loss: 0.5295454859733582\n",
      "Validation: Epoch [7], Batch [844/938], Loss: 0.8748475909233093\n",
      "Validation: Epoch [7], Batch [845/938], Loss: 0.5914824604988098\n",
      "Validation: Epoch [7], Batch [846/938], Loss: 0.7747838497161865\n",
      "Validation: Epoch [7], Batch [847/938], Loss: 0.4813370406627655\n",
      "Validation: Epoch [7], Batch [848/938], Loss: 0.5655323266983032\n",
      "Validation: Epoch [7], Batch [849/938], Loss: 0.5497480630874634\n",
      "Validation: Epoch [7], Batch [850/938], Loss: 0.5664453506469727\n",
      "Validation: Epoch [7], Batch [851/938], Loss: 0.6392884254455566\n",
      "Validation: Epoch [7], Batch [852/938], Loss: 0.43705666065216064\n",
      "Validation: Epoch [7], Batch [853/938], Loss: 0.5825125575065613\n",
      "Validation: Epoch [7], Batch [854/938], Loss: 0.5262277126312256\n",
      "Validation: Epoch [7], Batch [855/938], Loss: 0.6611319780349731\n",
      "Validation: Epoch [7], Batch [856/938], Loss: 0.40057173371315\n",
      "Validation: Epoch [7], Batch [857/938], Loss: 0.6405226588249207\n",
      "Validation: Epoch [7], Batch [858/938], Loss: 0.5963789224624634\n",
      "Validation: Epoch [7], Batch [859/938], Loss: 0.7860773205757141\n",
      "Validation: Epoch [7], Batch [860/938], Loss: 0.6788069009780884\n",
      "Validation: Epoch [7], Batch [861/938], Loss: 0.7166907787322998\n",
      "Validation: Epoch [7], Batch [862/938], Loss: 0.5753879547119141\n",
      "Validation: Epoch [7], Batch [863/938], Loss: 0.6927536725997925\n",
      "Validation: Epoch [7], Batch [864/938], Loss: 0.7398366928100586\n",
      "Validation: Epoch [7], Batch [865/938], Loss: 0.6117328405380249\n",
      "Validation: Epoch [7], Batch [866/938], Loss: 0.748522162437439\n",
      "Validation: Epoch [7], Batch [867/938], Loss: 0.47637122869491577\n",
      "Validation: Epoch [7], Batch [868/938], Loss: 0.593463659286499\n",
      "Validation: Epoch [7], Batch [869/938], Loss: 0.7131357192993164\n",
      "Validation: Epoch [7], Batch [870/938], Loss: 0.668179988861084\n",
      "Validation: Epoch [7], Batch [871/938], Loss: 0.7163657546043396\n",
      "Validation: Epoch [7], Batch [872/938], Loss: 0.5339077115058899\n",
      "Validation: Epoch [7], Batch [873/938], Loss: 0.4932914674282074\n",
      "Validation: Epoch [7], Batch [874/938], Loss: 0.7016004920005798\n",
      "Validation: Epoch [7], Batch [875/938], Loss: 0.6207841634750366\n",
      "Validation: Epoch [7], Batch [876/938], Loss: 0.4635697901248932\n",
      "Validation: Epoch [7], Batch [877/938], Loss: 0.7907963991165161\n",
      "Validation: Epoch [7], Batch [878/938], Loss: 0.4456927180290222\n",
      "Validation: Epoch [7], Batch [879/938], Loss: 0.40195566415786743\n",
      "Validation: Epoch [7], Batch [880/938], Loss: 0.5551477074623108\n",
      "Validation: Epoch [7], Batch [881/938], Loss: 0.5132426023483276\n",
      "Validation: Epoch [7], Batch [882/938], Loss: 0.5974357724189758\n",
      "Validation: Epoch [7], Batch [883/938], Loss: 0.8114101886749268\n",
      "Validation: Epoch [7], Batch [884/938], Loss: 0.5099304914474487\n",
      "Validation: Epoch [7], Batch [885/938], Loss: 0.589079737663269\n",
      "Validation: Epoch [7], Batch [886/938], Loss: 0.6877695322036743\n",
      "Validation: Epoch [7], Batch [887/938], Loss: 0.5154851675033569\n",
      "Validation: Epoch [7], Batch [888/938], Loss: 0.73125159740448\n",
      "Validation: Epoch [7], Batch [889/938], Loss: 0.553321361541748\n",
      "Validation: Epoch [7], Batch [890/938], Loss: 0.6733093857765198\n",
      "Validation: Epoch [7], Batch [891/938], Loss: 0.6799213290214539\n",
      "Validation: Epoch [7], Batch [892/938], Loss: 0.7707222700119019\n",
      "Validation: Epoch [7], Batch [893/938], Loss: 0.40633049607276917\n",
      "Validation: Epoch [7], Batch [894/938], Loss: 0.5859378576278687\n",
      "Validation: Epoch [7], Batch [895/938], Loss: 0.6852012276649475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [7], Batch [896/938], Loss: 0.5323846340179443\n",
      "Validation: Epoch [7], Batch [897/938], Loss: 0.677051305770874\n",
      "Validation: Epoch [7], Batch [898/938], Loss: 0.4881265163421631\n",
      "Validation: Epoch [7], Batch [899/938], Loss: 0.658549427986145\n",
      "Validation: Epoch [7], Batch [900/938], Loss: 0.7446502447128296\n",
      "Validation: Epoch [7], Batch [901/938], Loss: 0.3970891237258911\n",
      "Validation: Epoch [7], Batch [902/938], Loss: 0.8371068835258484\n",
      "Validation: Epoch [7], Batch [903/938], Loss: 0.6276965141296387\n",
      "Validation: Epoch [7], Batch [904/938], Loss: 0.5349844694137573\n",
      "Validation: Epoch [7], Batch [905/938], Loss: 0.8560437560081482\n",
      "Validation: Epoch [7], Batch [906/938], Loss: 0.6154010891914368\n",
      "Validation: Epoch [7], Batch [907/938], Loss: 0.8127217292785645\n",
      "Validation: Epoch [7], Batch [908/938], Loss: 0.6139616966247559\n",
      "Validation: Epoch [7], Batch [909/938], Loss: 0.6839526891708374\n",
      "Validation: Epoch [7], Batch [910/938], Loss: 0.4469744563102722\n",
      "Validation: Epoch [7], Batch [911/938], Loss: 0.47090718150138855\n",
      "Validation: Epoch [7], Batch [912/938], Loss: 0.47466492652893066\n",
      "Validation: Epoch [7], Batch [913/938], Loss: 0.5222303867340088\n",
      "Validation: Epoch [7], Batch [914/938], Loss: 0.6856186389923096\n",
      "Validation: Epoch [7], Batch [915/938], Loss: 0.408546507358551\n",
      "Validation: Epoch [7], Batch [916/938], Loss: 0.5158326625823975\n",
      "Validation: Epoch [7], Batch [917/938], Loss: 0.4519829750061035\n",
      "Validation: Epoch [7], Batch [918/938], Loss: 0.6990000605583191\n",
      "Validation: Epoch [7], Batch [919/938], Loss: 0.5225580930709839\n",
      "Validation: Epoch [7], Batch [920/938], Loss: 0.6148147583007812\n",
      "Validation: Epoch [7], Batch [921/938], Loss: 0.5292322039604187\n",
      "Validation: Epoch [7], Batch [922/938], Loss: 0.5367148518562317\n",
      "Validation: Epoch [7], Batch [923/938], Loss: 0.8188740015029907\n",
      "Validation: Epoch [7], Batch [924/938], Loss: 0.5487785935401917\n",
      "Validation: Epoch [7], Batch [925/938], Loss: 0.694809079170227\n",
      "Validation: Epoch [7], Batch [926/938], Loss: 0.5991978049278259\n",
      "Validation: Epoch [7], Batch [927/938], Loss: 0.6432398557662964\n",
      "Validation: Epoch [7], Batch [928/938], Loss: 0.5421078205108643\n",
      "Validation: Epoch [7], Batch [929/938], Loss: 0.5978399515151978\n",
      "Validation: Epoch [7], Batch [930/938], Loss: 0.5105748176574707\n",
      "Validation: Epoch [7], Batch [931/938], Loss: 0.7305846214294434\n",
      "Validation: Epoch [7], Batch [932/938], Loss: 0.5385522246360779\n",
      "Validation: Epoch [7], Batch [933/938], Loss: 0.5178702473640442\n",
      "Validation: Epoch [7], Batch [934/938], Loss: 0.6959038972854614\n",
      "Validation: Epoch [7], Batch [935/938], Loss: 0.6389021873474121\n",
      "Validation: Epoch [7], Batch [936/938], Loss: 0.6533966064453125\n",
      "Validation: Epoch [7], Batch [937/938], Loss: 0.4085530638694763\n",
      "Validation: Epoch [7], Batch [938/938], Loss: 0.502138614654541\n",
      "Accuracy of test set: 0.7937\n",
      "Train: Epoch [8], Batch [1/938], Loss: 0.48544177412986755\n",
      "Train: Epoch [8], Batch [2/938], Loss: 0.5430604815483093\n",
      "Train: Epoch [8], Batch [3/938], Loss: 0.7031884789466858\n",
      "Train: Epoch [8], Batch [4/938], Loss: 0.59801185131073\n",
      "Train: Epoch [8], Batch [5/938], Loss: 0.6636871099472046\n",
      "Train: Epoch [8], Batch [6/938], Loss: 0.6436066627502441\n",
      "Train: Epoch [8], Batch [7/938], Loss: 0.41671836376190186\n",
      "Train: Epoch [8], Batch [8/938], Loss: 0.6754894852638245\n",
      "Train: Epoch [8], Batch [9/938], Loss: 0.6858276128768921\n",
      "Train: Epoch [8], Batch [10/938], Loss: 0.564009964466095\n",
      "Train: Epoch [8], Batch [11/938], Loss: 0.5189579725265503\n",
      "Train: Epoch [8], Batch [12/938], Loss: 0.5364557504653931\n",
      "Train: Epoch [8], Batch [13/938], Loss: 0.5502914190292358\n",
      "Train: Epoch [8], Batch [14/938], Loss: 0.47044825553894043\n",
      "Train: Epoch [8], Batch [15/938], Loss: 0.7504594326019287\n",
      "Train: Epoch [8], Batch [16/938], Loss: 0.4945928752422333\n",
      "Train: Epoch [8], Batch [17/938], Loss: 0.6091470718383789\n",
      "Train: Epoch [8], Batch [18/938], Loss: 0.5863648653030396\n",
      "Train: Epoch [8], Batch [19/938], Loss: 0.4478856325149536\n",
      "Train: Epoch [8], Batch [20/938], Loss: 0.746073842048645\n",
      "Train: Epoch [8], Batch [21/938], Loss: 0.6386263966560364\n",
      "Train: Epoch [8], Batch [22/938], Loss: 0.45357468724250793\n",
      "Train: Epoch [8], Batch [23/938], Loss: 0.728794515132904\n",
      "Train: Epoch [8], Batch [24/938], Loss: 0.5452991127967834\n",
      "Train: Epoch [8], Batch [25/938], Loss: 0.43501824140548706\n",
      "Train: Epoch [8], Batch [26/938], Loss: 0.6482842564582825\n",
      "Train: Epoch [8], Batch [27/938], Loss: 0.5357840061187744\n",
      "Train: Epoch [8], Batch [28/938], Loss: 0.5365396738052368\n",
      "Train: Epoch [8], Batch [29/938], Loss: 0.5249466896057129\n",
      "Train: Epoch [8], Batch [30/938], Loss: 0.7383147478103638\n",
      "Train: Epoch [8], Batch [31/938], Loss: 0.43191999197006226\n",
      "Train: Epoch [8], Batch [32/938], Loss: 0.5758655071258545\n",
      "Train: Epoch [8], Batch [33/938], Loss: 0.5672871470451355\n",
      "Train: Epoch [8], Batch [34/938], Loss: 0.6621577739715576\n",
      "Train: Epoch [8], Batch [35/938], Loss: 0.5884138345718384\n",
      "Train: Epoch [8], Batch [36/938], Loss: 0.4678497314453125\n",
      "Train: Epoch [8], Batch [37/938], Loss: 0.6290479898452759\n",
      "Train: Epoch [8], Batch [38/938], Loss: 0.49853235483169556\n",
      "Train: Epoch [8], Batch [39/938], Loss: 0.645936131477356\n",
      "Train: Epoch [8], Batch [40/938], Loss: 0.5881328582763672\n",
      "Train: Epoch [8], Batch [41/938], Loss: 0.4853842854499817\n",
      "Train: Epoch [8], Batch [42/938], Loss: 0.5740084052085876\n",
      "Train: Epoch [8], Batch [43/938], Loss: 0.6527955532073975\n",
      "Train: Epoch [8], Batch [44/938], Loss: 0.6872225999832153\n",
      "Train: Epoch [8], Batch [45/938], Loss: 0.6307591199874878\n",
      "Train: Epoch [8], Batch [46/938], Loss: 0.5191709995269775\n",
      "Train: Epoch [8], Batch [47/938], Loss: 0.7233712673187256\n",
      "Train: Epoch [8], Batch [48/938], Loss: 0.6589242219924927\n",
      "Train: Epoch [8], Batch [49/938], Loss: 0.6423909664154053\n",
      "Train: Epoch [8], Batch [50/938], Loss: 0.44639548659324646\n",
      "Train: Epoch [8], Batch [51/938], Loss: 0.5910099148750305\n",
      "Train: Epoch [8], Batch [52/938], Loss: 0.7291748523712158\n",
      "Train: Epoch [8], Batch [53/938], Loss: 0.6452814340591431\n",
      "Train: Epoch [8], Batch [54/938], Loss: 0.43495994806289673\n",
      "Train: Epoch [8], Batch [55/938], Loss: 0.5854790210723877\n",
      "Train: Epoch [8], Batch [56/938], Loss: 0.5037121772766113\n",
      "Train: Epoch [8], Batch [57/938], Loss: 0.5993103981018066\n",
      "Train: Epoch [8], Batch [58/938], Loss: 0.5891984701156616\n",
      "Train: Epoch [8], Batch [59/938], Loss: 0.6503475904464722\n",
      "Train: Epoch [8], Batch [60/938], Loss: 0.48857131600379944\n",
      "Train: Epoch [8], Batch [61/938], Loss: 0.5884531140327454\n",
      "Train: Epoch [8], Batch [62/938], Loss: 0.4447888135910034\n",
      "Train: Epoch [8], Batch [63/938], Loss: 0.6007611751556396\n",
      "Train: Epoch [8], Batch [64/938], Loss: 0.7942441701889038\n",
      "Train: Epoch [8], Batch [65/938], Loss: 0.37265074253082275\n",
      "Train: Epoch [8], Batch [66/938], Loss: 0.7179412841796875\n",
      "Train: Epoch [8], Batch [67/938], Loss: 0.6788667440414429\n",
      "Train: Epoch [8], Batch [68/938], Loss: 0.5795952081680298\n",
      "Train: Epoch [8], Batch [69/938], Loss: 0.4032691717147827\n",
      "Train: Epoch [8], Batch [70/938], Loss: 0.4850716292858124\n",
      "Train: Epoch [8], Batch [71/938], Loss: 0.6267036199569702\n",
      "Train: Epoch [8], Batch [72/938], Loss: 0.7112354040145874\n",
      "Train: Epoch [8], Batch [73/938], Loss: 0.6495248079299927\n",
      "Train: Epoch [8], Batch [74/938], Loss: 0.49515262246131897\n",
      "Train: Epoch [8], Batch [75/938], Loss: 0.5661669373512268\n",
      "Train: Epoch [8], Batch [76/938], Loss: 0.621910572052002\n",
      "Train: Epoch [8], Batch [77/938], Loss: 0.544731855392456\n",
      "Train: Epoch [8], Batch [78/938], Loss: 0.7329084873199463\n",
      "Train: Epoch [8], Batch [79/938], Loss: 0.6671488285064697\n",
      "Train: Epoch [8], Batch [80/938], Loss: 0.6020532250404358\n",
      "Train: Epoch [8], Batch [81/938], Loss: 0.5539002418518066\n",
      "Train: Epoch [8], Batch [82/938], Loss: 0.41544201970100403\n",
      "Train: Epoch [8], Batch [83/938], Loss: 0.8123994469642639\n",
      "Train: Epoch [8], Batch [84/938], Loss: 0.3992813229560852\n",
      "Train: Epoch [8], Batch [85/938], Loss: 0.7004234790802002\n",
      "Train: Epoch [8], Batch [86/938], Loss: 0.5508363842964172\n",
      "Train: Epoch [8], Batch [87/938], Loss: 0.4878096580505371\n",
      "Train: Epoch [8], Batch [88/938], Loss: 0.8015573620796204\n",
      "Train: Epoch [8], Batch [89/938], Loss: 0.6055065989494324\n",
      "Train: Epoch [8], Batch [90/938], Loss: 0.6948292851448059\n",
      "Train: Epoch [8], Batch [91/938], Loss: 0.6761721968650818\n",
      "Train: Epoch [8], Batch [92/938], Loss: 0.6401231288909912\n",
      "Train: Epoch [8], Batch [93/938], Loss: 0.7601867914199829\n",
      "Train: Epoch [8], Batch [94/938], Loss: 0.6751097440719604\n",
      "Train: Epoch [8], Batch [95/938], Loss: 0.3591177463531494\n",
      "Train: Epoch [8], Batch [96/938], Loss: 0.49221518635749817\n",
      "Train: Epoch [8], Batch [97/938], Loss: 0.5229108333587646\n",
      "Train: Epoch [8], Batch [98/938], Loss: 0.6814383268356323\n",
      "Train: Epoch [8], Batch [99/938], Loss: 0.5063153505325317\n",
      "Train: Epoch [8], Batch [100/938], Loss: 0.7087739706039429\n",
      "Train: Epoch [8], Batch [101/938], Loss: 0.6893277168273926\n",
      "Train: Epoch [8], Batch [102/938], Loss: 0.522033154964447\n",
      "Train: Epoch [8], Batch [103/938], Loss: 0.49456265568733215\n",
      "Train: Epoch [8], Batch [104/938], Loss: 0.45847827196121216\n",
      "Train: Epoch [8], Batch [105/938], Loss: 0.4947800934314728\n",
      "Train: Epoch [8], Batch [106/938], Loss: 0.533088207244873\n",
      "Train: Epoch [8], Batch [107/938], Loss: 0.5349461436271667\n",
      "Train: Epoch [8], Batch [108/938], Loss: 0.6448649168014526\n",
      "Train: Epoch [8], Batch [109/938], Loss: 0.7834004759788513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [8], Batch [110/938], Loss: 0.5448905229568481\n",
      "Train: Epoch [8], Batch [111/938], Loss: 0.6060016751289368\n",
      "Train: Epoch [8], Batch [112/938], Loss: 0.46175265312194824\n",
      "Train: Epoch [8], Batch [113/938], Loss: 0.5675528645515442\n",
      "Train: Epoch [8], Batch [114/938], Loss: 0.42204520106315613\n",
      "Train: Epoch [8], Batch [115/938], Loss: 0.6064020395278931\n",
      "Train: Epoch [8], Batch [116/938], Loss: 0.9153395891189575\n",
      "Train: Epoch [8], Batch [117/938], Loss: 0.6696006059646606\n",
      "Train: Epoch [8], Batch [118/938], Loss: 0.6881639957427979\n",
      "Train: Epoch [8], Batch [119/938], Loss: 0.47852370142936707\n",
      "Train: Epoch [8], Batch [120/938], Loss: 0.6248838901519775\n",
      "Train: Epoch [8], Batch [121/938], Loss: 0.665532648563385\n",
      "Train: Epoch [8], Batch [122/938], Loss: 0.4803036153316498\n",
      "Train: Epoch [8], Batch [123/938], Loss: 0.5632810592651367\n",
      "Train: Epoch [8], Batch [124/938], Loss: 0.6482046842575073\n",
      "Train: Epoch [8], Batch [125/938], Loss: 0.7627366781234741\n",
      "Train: Epoch [8], Batch [126/938], Loss: 0.3770073652267456\n",
      "Train: Epoch [8], Batch [127/938], Loss: 0.5787158608436584\n",
      "Train: Epoch [8], Batch [128/938], Loss: 0.5048715472221375\n",
      "Train: Epoch [8], Batch [129/938], Loss: 0.7432947158813477\n",
      "Train: Epoch [8], Batch [130/938], Loss: 0.7661862373352051\n",
      "Train: Epoch [8], Batch [131/938], Loss: 0.5148313641548157\n",
      "Train: Epoch [8], Batch [132/938], Loss: 0.5033428072929382\n",
      "Train: Epoch [8], Batch [133/938], Loss: 0.6081082820892334\n",
      "Train: Epoch [8], Batch [134/938], Loss: 0.5912332534790039\n",
      "Train: Epoch [8], Batch [135/938], Loss: 0.7783594727516174\n",
      "Train: Epoch [8], Batch [136/938], Loss: 0.5807240009307861\n",
      "Train: Epoch [8], Batch [137/938], Loss: 0.6382269263267517\n",
      "Train: Epoch [8], Batch [138/938], Loss: 0.5016012191772461\n",
      "Train: Epoch [8], Batch [139/938], Loss: 0.7188745141029358\n",
      "Train: Epoch [8], Batch [140/938], Loss: 0.5983269214630127\n",
      "Train: Epoch [8], Batch [141/938], Loss: 0.6874775886535645\n",
      "Train: Epoch [8], Batch [142/938], Loss: 0.7164684534072876\n",
      "Train: Epoch [8], Batch [143/938], Loss: 0.5882046818733215\n",
      "Train: Epoch [8], Batch [144/938], Loss: 0.694101095199585\n",
      "Train: Epoch [8], Batch [145/938], Loss: 0.656822681427002\n",
      "Train: Epoch [8], Batch [146/938], Loss: 0.5359543561935425\n",
      "Train: Epoch [8], Batch [147/938], Loss: 0.6399354338645935\n",
      "Train: Epoch [8], Batch [148/938], Loss: 0.6841632127761841\n",
      "Train: Epoch [8], Batch [149/938], Loss: 0.5469292402267456\n",
      "Train: Epoch [8], Batch [150/938], Loss: 0.7066853642463684\n",
      "Train: Epoch [8], Batch [151/938], Loss: 0.6078991293907166\n",
      "Train: Epoch [8], Batch [152/938], Loss: 0.6219774484634399\n",
      "Train: Epoch [8], Batch [153/938], Loss: 0.8284080028533936\n",
      "Train: Epoch [8], Batch [154/938], Loss: 0.4753533601760864\n",
      "Train: Epoch [8], Batch [155/938], Loss: 0.7146984338760376\n",
      "Train: Epoch [8], Batch [156/938], Loss: 0.6620361804962158\n",
      "Train: Epoch [8], Batch [157/938], Loss: 0.4622828960418701\n",
      "Train: Epoch [8], Batch [158/938], Loss: 0.49921777844429016\n",
      "Train: Epoch [8], Batch [159/938], Loss: 0.4090050458908081\n",
      "Train: Epoch [8], Batch [160/938], Loss: 0.5472763776779175\n",
      "Train: Epoch [8], Batch [161/938], Loss: 0.7603791952133179\n",
      "Train: Epoch [8], Batch [162/938], Loss: 0.5652319192886353\n",
      "Train: Epoch [8], Batch [163/938], Loss: 0.39928528666496277\n",
      "Train: Epoch [8], Batch [164/938], Loss: 0.5130248665809631\n",
      "Train: Epoch [8], Batch [165/938], Loss: 0.5747534036636353\n",
      "Train: Epoch [8], Batch [166/938], Loss: 0.5042852759361267\n",
      "Train: Epoch [8], Batch [167/938], Loss: 0.7538307309150696\n",
      "Train: Epoch [8], Batch [168/938], Loss: 0.573711633682251\n",
      "Train: Epoch [8], Batch [169/938], Loss: 0.6543999314308167\n",
      "Train: Epoch [8], Batch [170/938], Loss: 0.5484424829483032\n",
      "Train: Epoch [8], Batch [171/938], Loss: 0.5539158582687378\n",
      "Train: Epoch [8], Batch [172/938], Loss: 0.44928643107414246\n",
      "Train: Epoch [8], Batch [173/938], Loss: 0.5488758683204651\n",
      "Train: Epoch [8], Batch [174/938], Loss: 0.5191432237625122\n",
      "Train: Epoch [8], Batch [175/938], Loss: 0.5529943108558655\n",
      "Train: Epoch [8], Batch [176/938], Loss: 0.7812813520431519\n",
      "Train: Epoch [8], Batch [177/938], Loss: 0.56758052110672\n",
      "Train: Epoch [8], Batch [178/938], Loss: 0.5630528330802917\n",
      "Train: Epoch [8], Batch [179/938], Loss: 0.5400293469429016\n",
      "Train: Epoch [8], Batch [180/938], Loss: 0.5488996505737305\n",
      "Train: Epoch [8], Batch [181/938], Loss: 0.5535477995872498\n",
      "Train: Epoch [8], Batch [182/938], Loss: 0.5892186164855957\n",
      "Train: Epoch [8], Batch [183/938], Loss: 0.49602872133255005\n",
      "Train: Epoch [8], Batch [184/938], Loss: 0.6244902014732361\n",
      "Train: Epoch [8], Batch [185/938], Loss: 0.46851080656051636\n",
      "Train: Epoch [8], Batch [186/938], Loss: 0.49774956703186035\n",
      "Train: Epoch [8], Batch [187/938], Loss: 0.6514202356338501\n",
      "Train: Epoch [8], Batch [188/938], Loss: 1.0870846509933472\n",
      "Train: Epoch [8], Batch [189/938], Loss: 0.5109593868255615\n",
      "Train: Epoch [8], Batch [190/938], Loss: 0.571144700050354\n",
      "Train: Epoch [8], Batch [191/938], Loss: 0.5068244338035583\n",
      "Train: Epoch [8], Batch [192/938], Loss: 0.5636864304542542\n",
      "Train: Epoch [8], Batch [193/938], Loss: 0.6017314791679382\n",
      "Train: Epoch [8], Batch [194/938], Loss: 0.5689237117767334\n",
      "Train: Epoch [8], Batch [195/938], Loss: 0.6590670347213745\n",
      "Train: Epoch [8], Batch [196/938], Loss: 0.3254638612270355\n",
      "Train: Epoch [8], Batch [197/938], Loss: 0.42870622873306274\n",
      "Train: Epoch [8], Batch [198/938], Loss: 0.40104231238365173\n",
      "Train: Epoch [8], Batch [199/938], Loss: 0.5480321645736694\n",
      "Train: Epoch [8], Batch [200/938], Loss: 0.8111972808837891\n",
      "Train: Epoch [8], Batch [201/938], Loss: 0.558075487613678\n",
      "Train: Epoch [8], Batch [202/938], Loss: 0.6146734952926636\n",
      "Train: Epoch [8], Batch [203/938], Loss: 0.5607386231422424\n",
      "Train: Epoch [8], Batch [204/938], Loss: 0.4199238419532776\n",
      "Train: Epoch [8], Batch [205/938], Loss: 0.49856632947921753\n",
      "Train: Epoch [8], Batch [206/938], Loss: 0.5087147355079651\n",
      "Train: Epoch [8], Batch [207/938], Loss: 0.5644091963768005\n",
      "Train: Epoch [8], Batch [208/938], Loss: 0.6395693421363831\n",
      "Train: Epoch [8], Batch [209/938], Loss: 0.7058862447738647\n",
      "Train: Epoch [8], Batch [210/938], Loss: 0.4757431149482727\n",
      "Train: Epoch [8], Batch [211/938], Loss: 0.6881392002105713\n",
      "Train: Epoch [8], Batch [212/938], Loss: 0.5797002911567688\n",
      "Train: Epoch [8], Batch [213/938], Loss: 0.5694191455841064\n",
      "Train: Epoch [8], Batch [214/938], Loss: 0.5534511804580688\n",
      "Train: Epoch [8], Batch [215/938], Loss: 0.5404766201972961\n",
      "Train: Epoch [8], Batch [216/938], Loss: 0.5262129306793213\n",
      "Train: Epoch [8], Batch [217/938], Loss: 0.4807645380496979\n",
      "Train: Epoch [8], Batch [218/938], Loss: 0.6624257564544678\n",
      "Train: Epoch [8], Batch [219/938], Loss: 0.489382266998291\n",
      "Train: Epoch [8], Batch [220/938], Loss: 0.49244195222854614\n",
      "Train: Epoch [8], Batch [221/938], Loss: 0.5464822053909302\n",
      "Train: Epoch [8], Batch [222/938], Loss: 0.45186305046081543\n",
      "Train: Epoch [8], Batch [223/938], Loss: 0.5771538019180298\n",
      "Train: Epoch [8], Batch [224/938], Loss: 0.6167027354240417\n",
      "Train: Epoch [8], Batch [225/938], Loss: 0.4762325584888458\n",
      "Train: Epoch [8], Batch [226/938], Loss: 0.40578538179397583\n",
      "Train: Epoch [8], Batch [227/938], Loss: 0.5908791422843933\n",
      "Train: Epoch [8], Batch [228/938], Loss: 0.49555203318595886\n",
      "Train: Epoch [8], Batch [229/938], Loss: 0.428070604801178\n",
      "Train: Epoch [8], Batch [230/938], Loss: 0.7016608715057373\n",
      "Train: Epoch [8], Batch [231/938], Loss: 0.6406575441360474\n",
      "Train: Epoch [8], Batch [232/938], Loss: 0.5655426979064941\n",
      "Train: Epoch [8], Batch [233/938], Loss: 0.4488265812397003\n",
      "Train: Epoch [8], Batch [234/938], Loss: 0.5582711696624756\n",
      "Train: Epoch [8], Batch [235/938], Loss: 0.49696317315101624\n",
      "Train: Epoch [8], Batch [236/938], Loss: 0.5361489653587341\n",
      "Train: Epoch [8], Batch [237/938], Loss: 0.597960889339447\n",
      "Train: Epoch [8], Batch [238/938], Loss: 0.3241421580314636\n",
      "Train: Epoch [8], Batch [239/938], Loss: 0.5237885117530823\n",
      "Train: Epoch [8], Batch [240/938], Loss: 0.49199408292770386\n",
      "Train: Epoch [8], Batch [241/938], Loss: 0.6654860973358154\n",
      "Train: Epoch [8], Batch [242/938], Loss: 0.6018093824386597\n",
      "Train: Epoch [8], Batch [243/938], Loss: 0.5859527587890625\n",
      "Train: Epoch [8], Batch [244/938], Loss: 0.5985327959060669\n",
      "Train: Epoch [8], Batch [245/938], Loss: 0.35464349389076233\n",
      "Train: Epoch [8], Batch [246/938], Loss: 0.5067023634910583\n",
      "Train: Epoch [8], Batch [247/938], Loss: 0.5707422494888306\n",
      "Train: Epoch [8], Batch [248/938], Loss: 0.6408756971359253\n",
      "Train: Epoch [8], Batch [249/938], Loss: 0.6716533899307251\n",
      "Train: Epoch [8], Batch [250/938], Loss: 0.6477808356285095\n",
      "Train: Epoch [8], Batch [251/938], Loss: 0.5392229557037354\n",
      "Train: Epoch [8], Batch [252/938], Loss: 0.5159387588500977\n",
      "Train: Epoch [8], Batch [253/938], Loss: 0.6559003591537476\n",
      "Train: Epoch [8], Batch [254/938], Loss: 0.5870926380157471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [8], Batch [255/938], Loss: 0.4874689280986786\n",
      "Train: Epoch [8], Batch [256/938], Loss: 0.7982051372528076\n",
      "Train: Epoch [8], Batch [257/938], Loss: 0.7895578145980835\n",
      "Train: Epoch [8], Batch [258/938], Loss: 0.6002472639083862\n",
      "Train: Epoch [8], Batch [259/938], Loss: 0.5422636270523071\n",
      "Train: Epoch [8], Batch [260/938], Loss: 0.40979692339897156\n",
      "Train: Epoch [8], Batch [261/938], Loss: 0.6209535598754883\n",
      "Train: Epoch [8], Batch [262/938], Loss: 0.6589608192443848\n",
      "Train: Epoch [8], Batch [263/938], Loss: 0.3491249978542328\n",
      "Train: Epoch [8], Batch [264/938], Loss: 0.6025007367134094\n",
      "Train: Epoch [8], Batch [265/938], Loss: 0.626776933670044\n",
      "Train: Epoch [8], Batch [266/938], Loss: 0.7226226329803467\n",
      "Train: Epoch [8], Batch [267/938], Loss: 0.749169647693634\n",
      "Train: Epoch [8], Batch [268/938], Loss: 0.5971877574920654\n",
      "Train: Epoch [8], Batch [269/938], Loss: 0.6026612520217896\n",
      "Train: Epoch [8], Batch [270/938], Loss: 0.7228572368621826\n",
      "Train: Epoch [8], Batch [271/938], Loss: 0.5381802916526794\n",
      "Train: Epoch [8], Batch [272/938], Loss: 0.8765667676925659\n",
      "Train: Epoch [8], Batch [273/938], Loss: 0.6658238172531128\n",
      "Train: Epoch [8], Batch [274/938], Loss: 0.3992386758327484\n",
      "Train: Epoch [8], Batch [275/938], Loss: 0.3568044602870941\n",
      "Train: Epoch [8], Batch [276/938], Loss: 0.5963842272758484\n",
      "Train: Epoch [8], Batch [277/938], Loss: 0.46470603346824646\n",
      "Train: Epoch [8], Batch [278/938], Loss: 0.5751619338989258\n",
      "Train: Epoch [8], Batch [279/938], Loss: 0.5867358446121216\n",
      "Train: Epoch [8], Batch [280/938], Loss: 0.47358131408691406\n",
      "Train: Epoch [8], Batch [281/938], Loss: 0.5689108967781067\n",
      "Train: Epoch [8], Batch [282/938], Loss: 0.5035169124603271\n",
      "Train: Epoch [8], Batch [283/938], Loss: 0.6786393523216248\n",
      "Train: Epoch [8], Batch [284/938], Loss: 0.5619099140167236\n",
      "Train: Epoch [8], Batch [285/938], Loss: 0.6262106895446777\n",
      "Train: Epoch [8], Batch [286/938], Loss: 0.5877598524093628\n",
      "Train: Epoch [8], Batch [287/938], Loss: 0.5337370038032532\n",
      "Train: Epoch [8], Batch [288/938], Loss: 0.6592073440551758\n",
      "Train: Epoch [8], Batch [289/938], Loss: 0.4843161702156067\n",
      "Train: Epoch [8], Batch [290/938], Loss: 0.5732277631759644\n",
      "Train: Epoch [8], Batch [291/938], Loss: 0.6913156509399414\n",
      "Train: Epoch [8], Batch [292/938], Loss: 0.6412902474403381\n",
      "Train: Epoch [8], Batch [293/938], Loss: 0.766720712184906\n",
      "Train: Epoch [8], Batch [294/938], Loss: 0.5188447833061218\n",
      "Train: Epoch [8], Batch [295/938], Loss: 0.5499206781387329\n",
      "Train: Epoch [8], Batch [296/938], Loss: 0.6648726463317871\n",
      "Train: Epoch [8], Batch [297/938], Loss: 0.5117921829223633\n",
      "Train: Epoch [8], Batch [298/938], Loss: 0.4998443126678467\n",
      "Train: Epoch [8], Batch [299/938], Loss: 0.583755373954773\n",
      "Train: Epoch [8], Batch [300/938], Loss: 0.519073486328125\n",
      "Train: Epoch [8], Batch [301/938], Loss: 0.5223901271820068\n",
      "Train: Epoch [8], Batch [302/938], Loss: 0.694949746131897\n",
      "Train: Epoch [8], Batch [303/938], Loss: 0.4499185383319855\n",
      "Train: Epoch [8], Batch [304/938], Loss: 0.7314926385879517\n",
      "Train: Epoch [8], Batch [305/938], Loss: 0.7038868069648743\n",
      "Train: Epoch [8], Batch [306/938], Loss: 0.6257754564285278\n",
      "Train: Epoch [8], Batch [307/938], Loss: 0.737136721611023\n",
      "Train: Epoch [8], Batch [308/938], Loss: 0.5871986150741577\n",
      "Train: Epoch [8], Batch [309/938], Loss: 0.3868986964225769\n",
      "Train: Epoch [8], Batch [310/938], Loss: 0.805542528629303\n",
      "Train: Epoch [8], Batch [311/938], Loss: 0.5461859107017517\n",
      "Train: Epoch [8], Batch [312/938], Loss: 0.8331112265586853\n",
      "Train: Epoch [8], Batch [313/938], Loss: 0.8191367983818054\n",
      "Train: Epoch [8], Batch [314/938], Loss: 0.605589747428894\n",
      "Train: Epoch [8], Batch [315/938], Loss: 0.5560802221298218\n",
      "Train: Epoch [8], Batch [316/938], Loss: 0.4925154447555542\n",
      "Train: Epoch [8], Batch [317/938], Loss: 0.6275465488433838\n",
      "Train: Epoch [8], Batch [318/938], Loss: 0.5903252363204956\n",
      "Train: Epoch [8], Batch [319/938], Loss: 0.9733392000198364\n",
      "Train: Epoch [8], Batch [320/938], Loss: 0.7006502747535706\n",
      "Train: Epoch [8], Batch [321/938], Loss: 0.5986785888671875\n",
      "Train: Epoch [8], Batch [322/938], Loss: 0.7771086692810059\n",
      "Train: Epoch [8], Batch [323/938], Loss: 0.4763995409011841\n",
      "Train: Epoch [8], Batch [324/938], Loss: 0.6061482429504395\n",
      "Train: Epoch [8], Batch [325/938], Loss: 0.6825305223464966\n",
      "Train: Epoch [8], Batch [326/938], Loss: 0.6247385740280151\n",
      "Train: Epoch [8], Batch [327/938], Loss: 0.5736898183822632\n",
      "Train: Epoch [8], Batch [328/938], Loss: 0.3591744303703308\n",
      "Train: Epoch [8], Batch [329/938], Loss: 0.691835880279541\n",
      "Train: Epoch [8], Batch [330/938], Loss: 0.42008644342422485\n",
      "Train: Epoch [8], Batch [331/938], Loss: 0.5383464694023132\n",
      "Train: Epoch [8], Batch [332/938], Loss: 0.5489333868026733\n",
      "Train: Epoch [8], Batch [333/938], Loss: 0.575417697429657\n",
      "Train: Epoch [8], Batch [334/938], Loss: 0.49163126945495605\n",
      "Train: Epoch [8], Batch [335/938], Loss: 0.596854031085968\n",
      "Train: Epoch [8], Batch [336/938], Loss: 0.854354977607727\n",
      "Train: Epoch [8], Batch [337/938], Loss: 0.611635684967041\n",
      "Train: Epoch [8], Batch [338/938], Loss: 0.6145281791687012\n",
      "Train: Epoch [8], Batch [339/938], Loss: 0.4528733789920807\n",
      "Train: Epoch [8], Batch [340/938], Loss: 0.45026350021362305\n",
      "Train: Epoch [8], Batch [341/938], Loss: 0.4445487856864929\n",
      "Train: Epoch [8], Batch [342/938], Loss: 0.5295138359069824\n",
      "Train: Epoch [8], Batch [343/938], Loss: 0.7083925604820251\n",
      "Train: Epoch [8], Batch [344/938], Loss: 0.5947877168655396\n",
      "Train: Epoch [8], Batch [345/938], Loss: 0.5956671237945557\n",
      "Train: Epoch [8], Batch [346/938], Loss: 0.5407882928848267\n",
      "Train: Epoch [8], Batch [347/938], Loss: 0.4485219717025757\n",
      "Train: Epoch [8], Batch [348/938], Loss: 0.6019030809402466\n",
      "Train: Epoch [8], Batch [349/938], Loss: 0.48633289337158203\n",
      "Train: Epoch [8], Batch [350/938], Loss: 0.665562629699707\n",
      "Train: Epoch [8], Batch [351/938], Loss: 0.5865579843521118\n",
      "Train: Epoch [8], Batch [352/938], Loss: 0.5041567087173462\n",
      "Train: Epoch [8], Batch [353/938], Loss: 0.618675172328949\n",
      "Train: Epoch [8], Batch [354/938], Loss: 0.6458992958068848\n",
      "Train: Epoch [8], Batch [355/938], Loss: 0.5768435597419739\n",
      "Train: Epoch [8], Batch [356/938], Loss: 0.28811490535736084\n",
      "Train: Epoch [8], Batch [357/938], Loss: 0.4729658365249634\n",
      "Train: Epoch [8], Batch [358/938], Loss: 0.7683541178703308\n",
      "Train: Epoch [8], Batch [359/938], Loss: 0.5302298069000244\n",
      "Train: Epoch [8], Batch [360/938], Loss: 0.6627124547958374\n",
      "Train: Epoch [8], Batch [361/938], Loss: 0.6010205149650574\n",
      "Train: Epoch [8], Batch [362/938], Loss: 0.4506451487541199\n",
      "Train: Epoch [8], Batch [363/938], Loss: 0.5671665072441101\n",
      "Train: Epoch [8], Batch [364/938], Loss: 0.6279899477958679\n",
      "Train: Epoch [8], Batch [365/938], Loss: 0.637819766998291\n",
      "Train: Epoch [8], Batch [366/938], Loss: 0.5141103863716125\n",
      "Train: Epoch [8], Batch [367/938], Loss: 0.5311625003814697\n",
      "Train: Epoch [8], Batch [368/938], Loss: 0.5818207263946533\n",
      "Train: Epoch [8], Batch [369/938], Loss: 0.5256028175354004\n",
      "Train: Epoch [8], Batch [370/938], Loss: 0.516514778137207\n",
      "Train: Epoch [8], Batch [371/938], Loss: 0.6353896856307983\n",
      "Train: Epoch [8], Batch [372/938], Loss: 0.694517970085144\n",
      "Train: Epoch [8], Batch [373/938], Loss: 0.7030326128005981\n",
      "Train: Epoch [8], Batch [374/938], Loss: 0.49386435747146606\n",
      "Train: Epoch [8], Batch [375/938], Loss: 0.5014861822128296\n",
      "Train: Epoch [8], Batch [376/938], Loss: 0.5583572387695312\n",
      "Train: Epoch [8], Batch [377/938], Loss: 0.5791150331497192\n",
      "Train: Epoch [8], Batch [378/938], Loss: 0.6507488489151001\n",
      "Train: Epoch [8], Batch [379/938], Loss: 0.4836370646953583\n",
      "Train: Epoch [8], Batch [380/938], Loss: 0.578887939453125\n",
      "Train: Epoch [8], Batch [381/938], Loss: 0.5823870897293091\n",
      "Train: Epoch [8], Batch [382/938], Loss: 0.7863166332244873\n",
      "Train: Epoch [8], Batch [383/938], Loss: 0.6867579221725464\n",
      "Train: Epoch [8], Batch [384/938], Loss: 0.4082316756248474\n",
      "Train: Epoch [8], Batch [385/938], Loss: 0.524872899055481\n",
      "Train: Epoch [8], Batch [386/938], Loss: 0.6747405529022217\n",
      "Train: Epoch [8], Batch [387/938], Loss: 0.5282657146453857\n",
      "Train: Epoch [8], Batch [388/938], Loss: 0.7455825805664062\n",
      "Train: Epoch [8], Batch [389/938], Loss: 0.49316492676734924\n",
      "Train: Epoch [8], Batch [390/938], Loss: 0.4777665138244629\n",
      "Train: Epoch [8], Batch [391/938], Loss: 0.8508895635604858\n",
      "Train: Epoch [8], Batch [392/938], Loss: 0.636825680732727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [8], Batch [393/938], Loss: 0.45792973041534424\n",
      "Train: Epoch [8], Batch [394/938], Loss: 0.4612295925617218\n",
      "Train: Epoch [8], Batch [395/938], Loss: 0.8648428916931152\n",
      "Train: Epoch [8], Batch [396/938], Loss: 0.6388287544250488\n",
      "Train: Epoch [8], Batch [397/938], Loss: 0.5677913427352905\n",
      "Train: Epoch [8], Batch [398/938], Loss: 0.7476761937141418\n",
      "Train: Epoch [8], Batch [399/938], Loss: 0.5121399164199829\n",
      "Train: Epoch [8], Batch [400/938], Loss: 0.5884177684783936\n",
      "Train: Epoch [8], Batch [401/938], Loss: 0.8275627493858337\n",
      "Train: Epoch [8], Batch [402/938], Loss: 0.7066202163696289\n",
      "Train: Epoch [8], Batch [403/938], Loss: 0.3241427540779114\n",
      "Train: Epoch [8], Batch [404/938], Loss: 0.5393521189689636\n",
      "Train: Epoch [8], Batch [405/938], Loss: 0.5648733377456665\n",
      "Train: Epoch [8], Batch [406/938], Loss: 0.6169944405555725\n",
      "Train: Epoch [8], Batch [407/938], Loss: 0.7912262082099915\n",
      "Train: Epoch [8], Batch [408/938], Loss: 0.6463741064071655\n",
      "Train: Epoch [8], Batch [409/938], Loss: 0.4460816979408264\n",
      "Train: Epoch [8], Batch [410/938], Loss: 0.5868146419525146\n",
      "Train: Epoch [8], Batch [411/938], Loss: 0.6485048532485962\n",
      "Train: Epoch [8], Batch [412/938], Loss: 0.4212496280670166\n",
      "Train: Epoch [8], Batch [413/938], Loss: 0.46190348267555237\n",
      "Train: Epoch [8], Batch [414/938], Loss: 0.4084784984588623\n",
      "Train: Epoch [8], Batch [415/938], Loss: 0.8151812553405762\n",
      "Train: Epoch [8], Batch [416/938], Loss: 0.610089898109436\n",
      "Train: Epoch [8], Batch [417/938], Loss: 0.6055725812911987\n",
      "Train: Epoch [8], Batch [418/938], Loss: 0.5320634245872498\n",
      "Train: Epoch [8], Batch [419/938], Loss: 0.7415289878845215\n",
      "Train: Epoch [8], Batch [420/938], Loss: 0.6006519794464111\n",
      "Train: Epoch [8], Batch [421/938], Loss: 0.4591338634490967\n",
      "Train: Epoch [8], Batch [422/938], Loss: 0.42590463161468506\n",
      "Train: Epoch [8], Batch [423/938], Loss: 0.41286996006965637\n",
      "Train: Epoch [8], Batch [424/938], Loss: 0.45372089743614197\n",
      "Train: Epoch [8], Batch [425/938], Loss: 0.6327134370803833\n",
      "Train: Epoch [8], Batch [426/938], Loss: 0.5742660760879517\n",
      "Train: Epoch [8], Batch [427/938], Loss: 0.5632612109184265\n",
      "Train: Epoch [8], Batch [428/938], Loss: 0.5719131231307983\n",
      "Train: Epoch [8], Batch [429/938], Loss: 0.4662531614303589\n",
      "Train: Epoch [8], Batch [430/938], Loss: 0.4962864816188812\n",
      "Train: Epoch [8], Batch [431/938], Loss: 0.4398435950279236\n",
      "Train: Epoch [8], Batch [432/938], Loss: 0.7271609306335449\n",
      "Train: Epoch [8], Batch [433/938], Loss: 0.46107837557792664\n",
      "Train: Epoch [8], Batch [434/938], Loss: 0.5866063833236694\n",
      "Train: Epoch [8], Batch [435/938], Loss: 0.5908664464950562\n",
      "Train: Epoch [8], Batch [436/938], Loss: 0.5883232355117798\n",
      "Train: Epoch [8], Batch [437/938], Loss: 0.5598031878471375\n",
      "Train: Epoch [8], Batch [438/938], Loss: 0.47548621892929077\n",
      "Train: Epoch [8], Batch [439/938], Loss: 0.6473616361618042\n",
      "Train: Epoch [8], Batch [440/938], Loss: 0.451823890209198\n",
      "Train: Epoch [8], Batch [441/938], Loss: 0.43282550573349\n",
      "Train: Epoch [8], Batch [442/938], Loss: 0.6223105192184448\n",
      "Train: Epoch [8], Batch [443/938], Loss: 0.4240194261074066\n",
      "Train: Epoch [8], Batch [444/938], Loss: 0.42035335302352905\n",
      "Train: Epoch [8], Batch [445/938], Loss: 0.39631763100624084\n",
      "Train: Epoch [8], Batch [446/938], Loss: 0.691624641418457\n",
      "Train: Epoch [8], Batch [447/938], Loss: 0.5779082775115967\n",
      "Train: Epoch [8], Batch [448/938], Loss: 0.5412415862083435\n",
      "Train: Epoch [8], Batch [449/938], Loss: 0.6690540313720703\n",
      "Train: Epoch [8], Batch [450/938], Loss: 0.7344512343406677\n",
      "Train: Epoch [8], Batch [451/938], Loss: 0.8859518766403198\n",
      "Train: Epoch [8], Batch [452/938], Loss: 0.7982344031333923\n",
      "Train: Epoch [8], Batch [453/938], Loss: 0.6929850578308105\n",
      "Train: Epoch [8], Batch [454/938], Loss: 0.49366265535354614\n",
      "Train: Epoch [8], Batch [455/938], Loss: 0.7000195980072021\n",
      "Train: Epoch [8], Batch [456/938], Loss: 0.5297009944915771\n",
      "Train: Epoch [8], Batch [457/938], Loss: 0.5449798107147217\n",
      "Train: Epoch [8], Batch [458/938], Loss: 0.42375752329826355\n",
      "Train: Epoch [8], Batch [459/938], Loss: 0.48905348777770996\n",
      "Train: Epoch [8], Batch [460/938], Loss: 0.47229212522506714\n",
      "Train: Epoch [8], Batch [461/938], Loss: 0.7059491872787476\n",
      "Train: Epoch [8], Batch [462/938], Loss: 0.6156882047653198\n",
      "Train: Epoch [8], Batch [463/938], Loss: 0.5423446297645569\n",
      "Train: Epoch [8], Batch [464/938], Loss: 0.6286530494689941\n",
      "Train: Epoch [8], Batch [465/938], Loss: 0.4721153676509857\n",
      "Train: Epoch [8], Batch [466/938], Loss: 0.6528940200805664\n",
      "Train: Epoch [8], Batch [467/938], Loss: 0.8329328298568726\n",
      "Train: Epoch [8], Batch [468/938], Loss: 0.6608504056930542\n",
      "Train: Epoch [8], Batch [469/938], Loss: 0.5685918927192688\n",
      "Train: Epoch [8], Batch [470/938], Loss: 0.6496251821517944\n",
      "Train: Epoch [8], Batch [471/938], Loss: 0.5889748334884644\n",
      "Train: Epoch [8], Batch [472/938], Loss: 0.41969969868659973\n",
      "Train: Epoch [8], Batch [473/938], Loss: 0.6226248741149902\n",
      "Train: Epoch [8], Batch [474/938], Loss: 0.6405965089797974\n",
      "Train: Epoch [8], Batch [475/938], Loss: 0.6396071910858154\n",
      "Train: Epoch [8], Batch [476/938], Loss: 0.6042814254760742\n",
      "Train: Epoch [8], Batch [477/938], Loss: 0.5399622917175293\n",
      "Train: Epoch [8], Batch [478/938], Loss: 0.4720080494880676\n",
      "Train: Epoch [8], Batch [479/938], Loss: 0.4778664708137512\n",
      "Train: Epoch [8], Batch [480/938], Loss: 0.6283981800079346\n",
      "Train: Epoch [8], Batch [481/938], Loss: 0.5118305683135986\n",
      "Train: Epoch [8], Batch [482/938], Loss: 0.49229463934898376\n",
      "Train: Epoch [8], Batch [483/938], Loss: 0.48183587193489075\n",
      "Train: Epoch [8], Batch [484/938], Loss: 0.5225300788879395\n",
      "Train: Epoch [8], Batch [485/938], Loss: 0.4910692870616913\n",
      "Train: Epoch [8], Batch [486/938], Loss: 0.5909863114356995\n",
      "Train: Epoch [8], Batch [487/938], Loss: 0.5201022624969482\n",
      "Train: Epoch [8], Batch [488/938], Loss: 0.46558207273483276\n",
      "Train: Epoch [8], Batch [489/938], Loss: 0.5752973556518555\n",
      "Train: Epoch [8], Batch [490/938], Loss: 0.48747432231903076\n",
      "Train: Epoch [8], Batch [491/938], Loss: 0.5424010753631592\n",
      "Train: Epoch [8], Batch [492/938], Loss: 0.637076735496521\n",
      "Train: Epoch [8], Batch [493/938], Loss: 0.4473187029361725\n",
      "Train: Epoch [8], Batch [494/938], Loss: 0.4681004285812378\n",
      "Train: Epoch [8], Batch [495/938], Loss: 0.5416320562362671\n",
      "Train: Epoch [8], Batch [496/938], Loss: 0.5454915165901184\n",
      "Train: Epoch [8], Batch [497/938], Loss: 0.3999013900756836\n",
      "Train: Epoch [8], Batch [498/938], Loss: 0.5997532606124878\n",
      "Train: Epoch [8], Batch [499/938], Loss: 0.3841201066970825\n",
      "Train: Epoch [8], Batch [500/938], Loss: 0.43526512384414673\n",
      "Train: Epoch [8], Batch [501/938], Loss: 0.5303850173950195\n",
      "Train: Epoch [8], Batch [502/938], Loss: 0.4734940230846405\n",
      "Train: Epoch [8], Batch [503/938], Loss: 0.5066314935684204\n",
      "Train: Epoch [8], Batch [504/938], Loss: 0.3849136531352997\n",
      "Train: Epoch [8], Batch [505/938], Loss: 0.5331648588180542\n",
      "Train: Epoch [8], Batch [506/938], Loss: 0.5108153820037842\n",
      "Train: Epoch [8], Batch [507/938], Loss: 0.5451467633247375\n",
      "Train: Epoch [8], Batch [508/938], Loss: 0.749173641204834\n",
      "Train: Epoch [8], Batch [509/938], Loss: 0.41273701190948486\n",
      "Train: Epoch [8], Batch [510/938], Loss: 0.493984580039978\n",
      "Train: Epoch [8], Batch [511/938], Loss: 0.5014816522598267\n",
      "Train: Epoch [8], Batch [512/938], Loss: 0.4772227108478546\n",
      "Train: Epoch [8], Batch [513/938], Loss: 0.5112385749816895\n",
      "Train: Epoch [8], Batch [514/938], Loss: 0.7065333724021912\n",
      "Train: Epoch [8], Batch [515/938], Loss: 0.6469140648841858\n",
      "Train: Epoch [8], Batch [516/938], Loss: 0.47981274127960205\n",
      "Train: Epoch [8], Batch [517/938], Loss: 0.8113658428192139\n",
      "Train: Epoch [8], Batch [518/938], Loss: 0.45087873935699463\n",
      "Train: Epoch [8], Batch [519/938], Loss: 0.8713386058807373\n",
      "Train: Epoch [8], Batch [520/938], Loss: 0.44777658581733704\n",
      "Train: Epoch [8], Batch [521/938], Loss: 0.5741499066352844\n",
      "Train: Epoch [8], Batch [522/938], Loss: 0.7225075960159302\n",
      "Train: Epoch [8], Batch [523/938], Loss: 0.6714510917663574\n",
      "Train: Epoch [8], Batch [524/938], Loss: 0.6130144596099854\n",
      "Train: Epoch [8], Batch [525/938], Loss: 0.6274309158325195\n",
      "Train: Epoch [8], Batch [526/938], Loss: 0.42353805899620056\n",
      "Train: Epoch [8], Batch [527/938], Loss: 0.5551087856292725\n",
      "Train: Epoch [8], Batch [528/938], Loss: 0.5027182698249817\n",
      "Train: Epoch [8], Batch [529/938], Loss: 0.5093092918395996\n",
      "Train: Epoch [8], Batch [530/938], Loss: 0.5570281744003296\n",
      "Train: Epoch [8], Batch [531/938], Loss: 0.6167348623275757\n",
      "Train: Epoch [8], Batch [532/938], Loss: 0.9041258096694946\n",
      "Train: Epoch [8], Batch [533/938], Loss: 0.6794044971466064\n",
      "Train: Epoch [8], Batch [534/938], Loss: 0.5361309051513672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [8], Batch [535/938], Loss: 0.5139563083648682\n",
      "Train: Epoch [8], Batch [536/938], Loss: 0.5986345410346985\n",
      "Train: Epoch [8], Batch [537/938], Loss: 0.48375576734542847\n",
      "Train: Epoch [8], Batch [538/938], Loss: 0.6458859443664551\n",
      "Train: Epoch [8], Batch [539/938], Loss: 0.5094637870788574\n",
      "Train: Epoch [8], Batch [540/938], Loss: 0.5089404582977295\n",
      "Train: Epoch [8], Batch [541/938], Loss: 0.4852285385131836\n",
      "Train: Epoch [8], Batch [542/938], Loss: 0.45190343260765076\n",
      "Train: Epoch [8], Batch [543/938], Loss: 0.7210645079612732\n",
      "Train: Epoch [8], Batch [544/938], Loss: 0.383401095867157\n",
      "Train: Epoch [8], Batch [545/938], Loss: 0.5799920558929443\n",
      "Train: Epoch [8], Batch [546/938], Loss: 0.5438323616981506\n",
      "Train: Epoch [8], Batch [547/938], Loss: 0.7626199722290039\n",
      "Train: Epoch [8], Batch [548/938], Loss: 0.7791612148284912\n",
      "Train: Epoch [8], Batch [549/938], Loss: 0.5627985000610352\n",
      "Train: Epoch [8], Batch [550/938], Loss: 0.6256629824638367\n",
      "Train: Epoch [8], Batch [551/938], Loss: 0.596814751625061\n",
      "Train: Epoch [8], Batch [552/938], Loss: 0.5344607830047607\n",
      "Train: Epoch [8], Batch [553/938], Loss: 0.8848605155944824\n",
      "Train: Epoch [8], Batch [554/938], Loss: 0.5624337792396545\n",
      "Train: Epoch [8], Batch [555/938], Loss: 0.697293758392334\n",
      "Train: Epoch [8], Batch [556/938], Loss: 0.6887323260307312\n",
      "Train: Epoch [8], Batch [557/938], Loss: 0.44019365310668945\n",
      "Train: Epoch [8], Batch [558/938], Loss: 0.5763500928878784\n",
      "Train: Epoch [8], Batch [559/938], Loss: 0.5718262195587158\n",
      "Train: Epoch [8], Batch [560/938], Loss: 0.4265296459197998\n",
      "Train: Epoch [8], Batch [561/938], Loss: 0.39629459381103516\n",
      "Train: Epoch [8], Batch [562/938], Loss: 0.38181787729263306\n",
      "Train: Epoch [8], Batch [563/938], Loss: 0.7404184937477112\n",
      "Train: Epoch [8], Batch [564/938], Loss: 0.6270660758018494\n",
      "Train: Epoch [8], Batch [565/938], Loss: 0.5747248530387878\n",
      "Train: Epoch [8], Batch [566/938], Loss: 0.6335850954055786\n",
      "Train: Epoch [8], Batch [567/938], Loss: 0.717593789100647\n",
      "Train: Epoch [8], Batch [568/938], Loss: 0.5944424867630005\n",
      "Train: Epoch [8], Batch [569/938], Loss: 0.47800469398498535\n",
      "Train: Epoch [8], Batch [570/938], Loss: 0.5890229940414429\n",
      "Train: Epoch [8], Batch [571/938], Loss: 0.4567042589187622\n",
      "Train: Epoch [8], Batch [572/938], Loss: 0.48296383023262024\n",
      "Train: Epoch [8], Batch [573/938], Loss: 0.5841280221939087\n",
      "Train: Epoch [8], Batch [574/938], Loss: 0.5280810594558716\n",
      "Train: Epoch [8], Batch [575/938], Loss: 0.8395175933837891\n",
      "Train: Epoch [8], Batch [576/938], Loss: 0.7411659955978394\n",
      "Train: Epoch [8], Batch [577/938], Loss: 0.6791914701461792\n",
      "Train: Epoch [8], Batch [578/938], Loss: 0.5180898904800415\n",
      "Train: Epoch [8], Batch [579/938], Loss: 0.5425388813018799\n",
      "Train: Epoch [8], Batch [580/938], Loss: 0.5946965217590332\n",
      "Train: Epoch [8], Batch [581/938], Loss: 0.5957876443862915\n",
      "Train: Epoch [8], Batch [582/938], Loss: 0.4383399188518524\n",
      "Train: Epoch [8], Batch [583/938], Loss: 0.5534399747848511\n",
      "Train: Epoch [8], Batch [584/938], Loss: 0.4690128266811371\n",
      "Train: Epoch [8], Batch [585/938], Loss: 0.5299772620201111\n",
      "Train: Epoch [8], Batch [586/938], Loss: 0.5613117218017578\n",
      "Train: Epoch [8], Batch [587/938], Loss: 0.644422709941864\n",
      "Train: Epoch [8], Batch [588/938], Loss: 0.4929228127002716\n",
      "Train: Epoch [8], Batch [589/938], Loss: 0.669757068157196\n",
      "Train: Epoch [8], Batch [590/938], Loss: 0.4331478178501129\n",
      "Train: Epoch [8], Batch [591/938], Loss: 0.5303242206573486\n",
      "Train: Epoch [8], Batch [592/938], Loss: 0.465948224067688\n",
      "Train: Epoch [8], Batch [593/938], Loss: 0.6211349368095398\n",
      "Train: Epoch [8], Batch [594/938], Loss: 0.49630820751190186\n",
      "Train: Epoch [8], Batch [595/938], Loss: 0.6644866466522217\n",
      "Train: Epoch [8], Batch [596/938], Loss: 0.8031589984893799\n",
      "Train: Epoch [8], Batch [597/938], Loss: 0.7694329619407654\n",
      "Train: Epoch [8], Batch [598/938], Loss: 0.749799370765686\n",
      "Train: Epoch [8], Batch [599/938], Loss: 0.629628598690033\n",
      "Train: Epoch [8], Batch [600/938], Loss: 0.5517272353172302\n",
      "Train: Epoch [8], Batch [601/938], Loss: 0.42403125762939453\n",
      "Train: Epoch [8], Batch [602/938], Loss: 0.4510178864002228\n",
      "Train: Epoch [8], Batch [603/938], Loss: 0.8187217116355896\n",
      "Train: Epoch [8], Batch [604/938], Loss: 0.6956444382667542\n",
      "Train: Epoch [8], Batch [605/938], Loss: 0.45717740058898926\n",
      "Train: Epoch [8], Batch [606/938], Loss: 0.4029412865638733\n",
      "Train: Epoch [8], Batch [607/938], Loss: 0.6801026463508606\n",
      "Train: Epoch [8], Batch [608/938], Loss: 0.4184766411781311\n",
      "Train: Epoch [8], Batch [609/938], Loss: 0.5914453268051147\n",
      "Train: Epoch [8], Batch [610/938], Loss: 0.6368953585624695\n",
      "Train: Epoch [8], Batch [611/938], Loss: 0.39146947860717773\n",
      "Train: Epoch [8], Batch [612/938], Loss: 0.6438592672348022\n",
      "Train: Epoch [8], Batch [613/938], Loss: 0.6124019622802734\n",
      "Train: Epoch [8], Batch [614/938], Loss: 0.3517557382583618\n",
      "Train: Epoch [8], Batch [615/938], Loss: 0.48978689312934875\n",
      "Train: Epoch [8], Batch [616/938], Loss: 0.43677958846092224\n",
      "Train: Epoch [8], Batch [617/938], Loss: 0.5096787214279175\n",
      "Train: Epoch [8], Batch [618/938], Loss: 0.4003618359565735\n",
      "Train: Epoch [8], Batch [619/938], Loss: 0.5861282348632812\n",
      "Train: Epoch [8], Batch [620/938], Loss: 0.4841412305831909\n",
      "Train: Epoch [8], Batch [621/938], Loss: 0.5347575545310974\n",
      "Train: Epoch [8], Batch [622/938], Loss: 0.46876826882362366\n",
      "Train: Epoch [8], Batch [623/938], Loss: 0.3361052870750427\n",
      "Train: Epoch [8], Batch [624/938], Loss: 0.5347920060157776\n",
      "Train: Epoch [8], Batch [625/938], Loss: 0.41062939167022705\n",
      "Train: Epoch [8], Batch [626/938], Loss: 0.7517999410629272\n",
      "Train: Epoch [8], Batch [627/938], Loss: 0.8121890425682068\n",
      "Train: Epoch [8], Batch [628/938], Loss: 0.5036428570747375\n",
      "Train: Epoch [8], Batch [629/938], Loss: 0.7306414842605591\n",
      "Train: Epoch [8], Batch [630/938], Loss: 0.583915114402771\n",
      "Train: Epoch [8], Batch [631/938], Loss: 0.8970913290977478\n",
      "Train: Epoch [8], Batch [632/938], Loss: 0.5974172949790955\n",
      "Train: Epoch [8], Batch [633/938], Loss: 0.588058590888977\n",
      "Train: Epoch [8], Batch [634/938], Loss: 0.5585355758666992\n",
      "Train: Epoch [8], Batch [635/938], Loss: 0.7018296718597412\n",
      "Train: Epoch [8], Batch [636/938], Loss: 0.7991001605987549\n",
      "Train: Epoch [8], Batch [637/938], Loss: 0.4881289005279541\n",
      "Train: Epoch [8], Batch [638/938], Loss: 0.6606000661849976\n",
      "Train: Epoch [8], Batch [639/938], Loss: 0.43960535526275635\n",
      "Train: Epoch [8], Batch [640/938], Loss: 0.5179411172866821\n",
      "Train: Epoch [8], Batch [641/938], Loss: 0.5062522888183594\n",
      "Train: Epoch [8], Batch [642/938], Loss: 0.8388301134109497\n",
      "Train: Epoch [8], Batch [643/938], Loss: 0.5783368945121765\n",
      "Train: Epoch [8], Batch [644/938], Loss: 0.5550796389579773\n",
      "Train: Epoch [8], Batch [645/938], Loss: 0.6097716689109802\n",
      "Train: Epoch [8], Batch [646/938], Loss: 0.5795236825942993\n",
      "Train: Epoch [8], Batch [647/938], Loss: 0.6886076927185059\n",
      "Train: Epoch [8], Batch [648/938], Loss: 0.566641092300415\n",
      "Train: Epoch [8], Batch [649/938], Loss: 0.5231451988220215\n",
      "Train: Epoch [8], Batch [650/938], Loss: 0.7384621500968933\n",
      "Train: Epoch [8], Batch [651/938], Loss: 0.4712008833885193\n",
      "Train: Epoch [8], Batch [652/938], Loss: 0.4392513632774353\n",
      "Train: Epoch [8], Batch [653/938], Loss: 0.46009084582328796\n",
      "Train: Epoch [8], Batch [654/938], Loss: 0.5766892433166504\n",
      "Train: Epoch [8], Batch [655/938], Loss: 0.4083973169326782\n",
      "Train: Epoch [8], Batch [656/938], Loss: 0.6524621248245239\n",
      "Train: Epoch [8], Batch [657/938], Loss: 0.5892523527145386\n",
      "Train: Epoch [8], Batch [658/938], Loss: 0.46907612681388855\n",
      "Train: Epoch [8], Batch [659/938], Loss: 0.4241206645965576\n",
      "Train: Epoch [8], Batch [660/938], Loss: 0.47325825691223145\n",
      "Train: Epoch [8], Batch [661/938], Loss: 0.7333157062530518\n",
      "Train: Epoch [8], Batch [662/938], Loss: 0.6348934173583984\n",
      "Train: Epoch [8], Batch [663/938], Loss: 0.5984662771224976\n",
      "Train: Epoch [8], Batch [664/938], Loss: 0.6554436683654785\n",
      "Train: Epoch [8], Batch [665/938], Loss: 0.5662992000579834\n",
      "Train: Epoch [8], Batch [666/938], Loss: 0.49180901050567627\n",
      "Train: Epoch [8], Batch [667/938], Loss: 0.34259146451950073\n",
      "Train: Epoch [8], Batch [668/938], Loss: 0.4369391202926636\n",
      "Train: Epoch [8], Batch [669/938], Loss: 0.637249767780304\n",
      "Train: Epoch [8], Batch [670/938], Loss: 0.6378619074821472\n",
      "Train: Epoch [8], Batch [671/938], Loss: 0.4216342866420746\n",
      "Train: Epoch [8], Batch [672/938], Loss: 0.5772542953491211\n",
      "Train: Epoch [8], Batch [673/938], Loss: 0.6436952352523804\n",
      "Train: Epoch [8], Batch [674/938], Loss: 0.605843186378479\n",
      "Train: Epoch [8], Batch [675/938], Loss: 0.6660656929016113\n",
      "Train: Epoch [8], Batch [676/938], Loss: 0.5239840745925903\n",
      "Train: Epoch [8], Batch [677/938], Loss: 0.5280176401138306\n",
      "Train: Epoch [8], Batch [678/938], Loss: 0.7824491262435913\n",
      "Train: Epoch [8], Batch [679/938], Loss: 0.7754193544387817\n",
      "Train: Epoch [8], Batch [680/938], Loss: 0.5350881218910217\n",
      "Train: Epoch [8], Batch [681/938], Loss: 0.6189989447593689\n",
      "Train: Epoch [8], Batch [682/938], Loss: 0.5326313376426697\n",
      "Train: Epoch [8], Batch [683/938], Loss: 0.7651350498199463\n",
      "Train: Epoch [8], Batch [684/938], Loss: 0.5042316317558289\n",
      "Train: Epoch [8], Batch [685/938], Loss: 0.5250705480575562\n",
      "Train: Epoch [8], Batch [686/938], Loss: 0.5129767656326294\n",
      "Train: Epoch [8], Batch [687/938], Loss: 0.6772591471672058\n",
      "Train: Epoch [8], Batch [688/938], Loss: 0.5801058411598206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [8], Batch [689/938], Loss: 0.45951327681541443\n",
      "Train: Epoch [8], Batch [690/938], Loss: 0.6213323473930359\n",
      "Train: Epoch [8], Batch [691/938], Loss: 0.6035242080688477\n",
      "Train: Epoch [8], Batch [692/938], Loss: 0.645000696182251\n",
      "Train: Epoch [8], Batch [693/938], Loss: 0.559411883354187\n",
      "Train: Epoch [8], Batch [694/938], Loss: 0.5579026341438293\n",
      "Train: Epoch [8], Batch [695/938], Loss: 0.5319108366966248\n",
      "Train: Epoch [8], Batch [696/938], Loss: 0.7070670127868652\n",
      "Train: Epoch [8], Batch [697/938], Loss: 0.46881186962127686\n",
      "Train: Epoch [8], Batch [698/938], Loss: 0.4864369332790375\n",
      "Train: Epoch [8], Batch [699/938], Loss: 0.48148030042648315\n",
      "Train: Epoch [8], Batch [700/938], Loss: 0.4838848114013672\n",
      "Train: Epoch [8], Batch [701/938], Loss: 0.6383978128433228\n",
      "Train: Epoch [8], Batch [702/938], Loss: 0.8388198614120483\n",
      "Train: Epoch [8], Batch [703/938], Loss: 0.6909364461898804\n",
      "Train: Epoch [8], Batch [704/938], Loss: 0.7236208915710449\n",
      "Train: Epoch [8], Batch [705/938], Loss: 0.6085652112960815\n",
      "Train: Epoch [8], Batch [706/938], Loss: 0.636803388595581\n",
      "Train: Epoch [8], Batch [707/938], Loss: 0.5529899597167969\n",
      "Train: Epoch [8], Batch [708/938], Loss: 0.7048369646072388\n",
      "Train: Epoch [8], Batch [709/938], Loss: 0.5724301338195801\n",
      "Train: Epoch [8], Batch [710/938], Loss: 0.5363660454750061\n",
      "Train: Epoch [8], Batch [711/938], Loss: 0.4470241665840149\n",
      "Train: Epoch [8], Batch [712/938], Loss: 0.5594967603683472\n",
      "Train: Epoch [8], Batch [713/938], Loss: 0.48235687613487244\n",
      "Train: Epoch [8], Batch [714/938], Loss: 0.599665641784668\n",
      "Train: Epoch [8], Batch [715/938], Loss: 0.6874785423278809\n",
      "Train: Epoch [8], Batch [716/938], Loss: 0.7551879286766052\n",
      "Train: Epoch [8], Batch [717/938], Loss: 0.5330847501754761\n",
      "Train: Epoch [8], Batch [718/938], Loss: 0.44535794854164124\n",
      "Train: Epoch [8], Batch [719/938], Loss: 0.5726727247238159\n",
      "Train: Epoch [8], Batch [720/938], Loss: 0.551782488822937\n",
      "Train: Epoch [8], Batch [721/938], Loss: 0.7550774216651917\n",
      "Train: Epoch [8], Batch [722/938], Loss: 0.35053735971450806\n",
      "Train: Epoch [8], Batch [723/938], Loss: 0.4782400131225586\n",
      "Train: Epoch [8], Batch [724/938], Loss: 0.5308904647827148\n",
      "Train: Epoch [8], Batch [725/938], Loss: 0.5472672581672668\n",
      "Train: Epoch [8], Batch [726/938], Loss: 0.6284011602401733\n",
      "Train: Epoch [8], Batch [727/938], Loss: 0.4596640467643738\n",
      "Train: Epoch [8], Batch [728/938], Loss: 0.3678976595401764\n",
      "Train: Epoch [8], Batch [729/938], Loss: 0.43522417545318604\n",
      "Train: Epoch [8], Batch [730/938], Loss: 0.44996052980422974\n",
      "Train: Epoch [8], Batch [731/938], Loss: 0.7718977332115173\n",
      "Train: Epoch [8], Batch [732/938], Loss: 0.5850790143013\n",
      "Train: Epoch [8], Batch [733/938], Loss: 0.7072920799255371\n",
      "Train: Epoch [8], Batch [734/938], Loss: 0.540523886680603\n",
      "Train: Epoch [8], Batch [735/938], Loss: 0.6225468516349792\n",
      "Train: Epoch [8], Batch [736/938], Loss: 0.7510790824890137\n",
      "Train: Epoch [8], Batch [737/938], Loss: 0.6403594613075256\n",
      "Train: Epoch [8], Batch [738/938], Loss: 0.4459882974624634\n",
      "Train: Epoch [8], Batch [739/938], Loss: 0.8788384795188904\n",
      "Train: Epoch [8], Batch [740/938], Loss: 0.618468165397644\n",
      "Train: Epoch [8], Batch [741/938], Loss: 0.42749255895614624\n",
      "Train: Epoch [8], Batch [742/938], Loss: 0.579969048500061\n",
      "Train: Epoch [8], Batch [743/938], Loss: 0.6987467408180237\n",
      "Train: Epoch [8], Batch [744/938], Loss: 0.42537498474121094\n",
      "Train: Epoch [8], Batch [745/938], Loss: 0.5636692047119141\n",
      "Train: Epoch [8], Batch [746/938], Loss: 0.7424905896186829\n",
      "Train: Epoch [8], Batch [747/938], Loss: 0.8191907405853271\n",
      "Train: Epoch [8], Batch [748/938], Loss: 0.6532978415489197\n",
      "Train: Epoch [8], Batch [749/938], Loss: 0.6541178822517395\n",
      "Train: Epoch [8], Batch [750/938], Loss: 0.4585808515548706\n",
      "Train: Epoch [8], Batch [751/938], Loss: 0.6444073915481567\n",
      "Train: Epoch [8], Batch [752/938], Loss: 0.7106095552444458\n",
      "Train: Epoch [8], Batch [753/938], Loss: 0.4745030403137207\n",
      "Train: Epoch [8], Batch [754/938], Loss: 0.7176147699356079\n",
      "Train: Epoch [8], Batch [755/938], Loss: 0.806249737739563\n",
      "Train: Epoch [8], Batch [756/938], Loss: 0.7332794070243835\n",
      "Train: Epoch [8], Batch [757/938], Loss: 0.5622907876968384\n",
      "Train: Epoch [8], Batch [758/938], Loss: 0.6042221188545227\n",
      "Train: Epoch [8], Batch [759/938], Loss: 0.5219594240188599\n",
      "Train: Epoch [8], Batch [760/938], Loss: 0.6627144813537598\n",
      "Train: Epoch [8], Batch [761/938], Loss: 0.7834564447402954\n",
      "Train: Epoch [8], Batch [762/938], Loss: 0.5411679148674011\n",
      "Train: Epoch [8], Batch [763/938], Loss: 0.7385482788085938\n",
      "Train: Epoch [8], Batch [764/938], Loss: 0.6065800189971924\n",
      "Train: Epoch [8], Batch [765/938], Loss: 0.6119953393936157\n",
      "Train: Epoch [8], Batch [766/938], Loss: 0.6671246290206909\n",
      "Train: Epoch [8], Batch [767/938], Loss: 0.4103373885154724\n",
      "Train: Epoch [8], Batch [768/938], Loss: 0.8690589666366577\n",
      "Train: Epoch [8], Batch [769/938], Loss: 0.43572741746902466\n",
      "Train: Epoch [8], Batch [770/938], Loss: 0.5427420139312744\n",
      "Train: Epoch [8], Batch [771/938], Loss: 0.6131466031074524\n",
      "Train: Epoch [8], Batch [772/938], Loss: 0.6028000116348267\n",
      "Train: Epoch [8], Batch [773/938], Loss: 0.46810388565063477\n",
      "Train: Epoch [8], Batch [774/938], Loss: 0.46062517166137695\n",
      "Train: Epoch [8], Batch [775/938], Loss: 0.35207805037498474\n",
      "Train: Epoch [8], Batch [776/938], Loss: 0.6714444756507874\n",
      "Train: Epoch [8], Batch [777/938], Loss: 0.6447349786758423\n",
      "Train: Epoch [8], Batch [778/938], Loss: 0.6101914048194885\n",
      "Train: Epoch [8], Batch [779/938], Loss: 0.7671058773994446\n",
      "Train: Epoch [8], Batch [780/938], Loss: 0.6423630714416504\n",
      "Train: Epoch [8], Batch [781/938], Loss: 0.4017041325569153\n",
      "Train: Epoch [8], Batch [782/938], Loss: 0.5014252662658691\n",
      "Train: Epoch [8], Batch [783/938], Loss: 0.7890483736991882\n",
      "Train: Epoch [8], Batch [784/938], Loss: 0.6826264262199402\n",
      "Train: Epoch [8], Batch [785/938], Loss: 0.8051116466522217\n",
      "Train: Epoch [8], Batch [786/938], Loss: 0.7129894495010376\n",
      "Train: Epoch [8], Batch [787/938], Loss: 0.6678948998451233\n",
      "Train: Epoch [8], Batch [788/938], Loss: 0.49765193462371826\n",
      "Train: Epoch [8], Batch [789/938], Loss: 0.5654091238975525\n",
      "Train: Epoch [8], Batch [790/938], Loss: 0.5084319114685059\n",
      "Train: Epoch [8], Batch [791/938], Loss: 0.5124005675315857\n",
      "Train: Epoch [8], Batch [792/938], Loss: 0.5250110030174255\n",
      "Train: Epoch [8], Batch [793/938], Loss: 0.5895897150039673\n",
      "Train: Epoch [8], Batch [794/938], Loss: 0.6407831907272339\n",
      "Train: Epoch [8], Batch [795/938], Loss: 0.5769920349121094\n",
      "Train: Epoch [8], Batch [796/938], Loss: 0.6286571621894836\n",
      "Train: Epoch [8], Batch [797/938], Loss: 0.45003435015678406\n",
      "Train: Epoch [8], Batch [798/938], Loss: 0.42547500133514404\n",
      "Train: Epoch [8], Batch [799/938], Loss: 0.6252123117446899\n",
      "Train: Epoch [8], Batch [800/938], Loss: 0.6177094578742981\n",
      "Train: Epoch [8], Batch [801/938], Loss: 0.5849719047546387\n",
      "Train: Epoch [8], Batch [802/938], Loss: 0.7270396947860718\n",
      "Train: Epoch [8], Batch [803/938], Loss: 0.6209505796432495\n",
      "Train: Epoch [8], Batch [804/938], Loss: 0.4728168547153473\n",
      "Train: Epoch [8], Batch [805/938], Loss: 0.5655987858772278\n",
      "Train: Epoch [8], Batch [806/938], Loss: 0.4726622998714447\n",
      "Train: Epoch [8], Batch [807/938], Loss: 0.6785054206848145\n",
      "Train: Epoch [8], Batch [808/938], Loss: 0.3873414993286133\n",
      "Train: Epoch [8], Batch [809/938], Loss: 0.8273242712020874\n",
      "Train: Epoch [8], Batch [810/938], Loss: 0.45767953991889954\n",
      "Train: Epoch [8], Batch [811/938], Loss: 0.5447434186935425\n",
      "Train: Epoch [8], Batch [812/938], Loss: 0.636157751083374\n",
      "Train: Epoch [8], Batch [813/938], Loss: 0.48609739542007446\n",
      "Train: Epoch [8], Batch [814/938], Loss: 0.4586196839809418\n",
      "Train: Epoch [8], Batch [815/938], Loss: 0.5474669933319092\n",
      "Train: Epoch [8], Batch [816/938], Loss: 0.48056358098983765\n",
      "Train: Epoch [8], Batch [817/938], Loss: 0.43094968795776367\n",
      "Train: Epoch [8], Batch [818/938], Loss: 0.4300217032432556\n",
      "Train: Epoch [8], Batch [819/938], Loss: 0.805198073387146\n",
      "Train: Epoch [8], Batch [820/938], Loss: 0.439031183719635\n",
      "Train: Epoch [8], Batch [821/938], Loss: 0.4145537316799164\n",
      "Train: Epoch [8], Batch [822/938], Loss: 0.6792488098144531\n",
      "Train: Epoch [8], Batch [823/938], Loss: 0.6198672652244568\n",
      "Train: Epoch [8], Batch [824/938], Loss: 0.4852311909198761\n",
      "Train: Epoch [8], Batch [825/938], Loss: 0.583045482635498\n",
      "Train: Epoch [8], Batch [826/938], Loss: 0.40036332607269287\n",
      "Train: Epoch [8], Batch [827/938], Loss: 0.49121904373168945\n",
      "Train: Epoch [8], Batch [828/938], Loss: 0.5538880228996277\n",
      "Train: Epoch [8], Batch [829/938], Loss: 0.5365275144577026\n",
      "Train: Epoch [8], Batch [830/938], Loss: 0.4974583089351654\n",
      "Train: Epoch [8], Batch [831/938], Loss: 0.706855833530426\n",
      "Train: Epoch [8], Batch [832/938], Loss: 0.47224700450897217\n",
      "Train: Epoch [8], Batch [833/938], Loss: 0.5838916301727295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [8], Batch [834/938], Loss: 0.5380349159240723\n",
      "Train: Epoch [8], Batch [835/938], Loss: 0.6152399778366089\n",
      "Train: Epoch [8], Batch [836/938], Loss: 0.4835386872291565\n",
      "Train: Epoch [8], Batch [837/938], Loss: 0.5990777015686035\n",
      "Train: Epoch [8], Batch [838/938], Loss: 0.5467582941055298\n",
      "Train: Epoch [8], Batch [839/938], Loss: 0.41028690338134766\n",
      "Train: Epoch [8], Batch [840/938], Loss: 0.5783824920654297\n",
      "Train: Epoch [8], Batch [841/938], Loss: 0.7721067667007446\n",
      "Train: Epoch [8], Batch [842/938], Loss: 0.5674501657485962\n",
      "Train: Epoch [8], Batch [843/938], Loss: 1.0052008628845215\n",
      "Train: Epoch [8], Batch [844/938], Loss: 0.48826271295547485\n",
      "Train: Epoch [8], Batch [845/938], Loss: 0.4306861162185669\n",
      "Train: Epoch [8], Batch [846/938], Loss: 0.7345271706581116\n",
      "Train: Epoch [8], Batch [847/938], Loss: 0.616574764251709\n",
      "Train: Epoch [8], Batch [848/938], Loss: 0.49728623032569885\n",
      "Train: Epoch [8], Batch [849/938], Loss: 0.4342227280139923\n",
      "Train: Epoch [8], Batch [850/938], Loss: 0.6626941561698914\n",
      "Train: Epoch [8], Batch [851/938], Loss: 0.45862841606140137\n",
      "Train: Epoch [8], Batch [852/938], Loss: 0.5146973133087158\n",
      "Train: Epoch [8], Batch [853/938], Loss: 0.79218590259552\n",
      "Train: Epoch [8], Batch [854/938], Loss: 0.5890183448791504\n",
      "Train: Epoch [8], Batch [855/938], Loss: 0.47911733388900757\n",
      "Train: Epoch [8], Batch [856/938], Loss: 0.5484099388122559\n",
      "Train: Epoch [8], Batch [857/938], Loss: 0.47890138626098633\n",
      "Train: Epoch [8], Batch [858/938], Loss: 0.5351886749267578\n",
      "Train: Epoch [8], Batch [859/938], Loss: 0.4611336290836334\n",
      "Train: Epoch [8], Batch [860/938], Loss: 0.747317910194397\n",
      "Train: Epoch [8], Batch [861/938], Loss: 0.6136232614517212\n",
      "Train: Epoch [8], Batch [862/938], Loss: 0.6376932859420776\n",
      "Train: Epoch [8], Batch [863/938], Loss: 0.39631974697113037\n",
      "Train: Epoch [8], Batch [864/938], Loss: 0.48128193616867065\n",
      "Train: Epoch [8], Batch [865/938], Loss: 0.7883493900299072\n",
      "Train: Epoch [8], Batch [866/938], Loss: 0.6248089075088501\n",
      "Train: Epoch [8], Batch [867/938], Loss: 0.48448503017425537\n",
      "Train: Epoch [8], Batch [868/938], Loss: 0.4685303866863251\n",
      "Train: Epoch [8], Batch [869/938], Loss: 0.395004004240036\n",
      "Train: Epoch [8], Batch [870/938], Loss: 0.32621461153030396\n",
      "Train: Epoch [8], Batch [871/938], Loss: 0.6858112215995789\n",
      "Train: Epoch [8], Batch [872/938], Loss: 0.7356257438659668\n",
      "Train: Epoch [8], Batch [873/938], Loss: 0.5477103590965271\n",
      "Train: Epoch [8], Batch [874/938], Loss: 0.6969021558761597\n",
      "Train: Epoch [8], Batch [875/938], Loss: 0.8061351776123047\n",
      "Train: Epoch [8], Batch [876/938], Loss: 0.5803480744361877\n",
      "Train: Epoch [8], Batch [877/938], Loss: 0.5188342332839966\n",
      "Train: Epoch [8], Batch [878/938], Loss: 0.4272117018699646\n",
      "Train: Epoch [8], Batch [879/938], Loss: 0.534250020980835\n",
      "Train: Epoch [8], Batch [880/938], Loss: 0.3659319281578064\n",
      "Train: Epoch [8], Batch [881/938], Loss: 0.4722852110862732\n",
      "Train: Epoch [8], Batch [882/938], Loss: 0.5060489773750305\n",
      "Train: Epoch [8], Batch [883/938], Loss: 0.6247831583023071\n",
      "Train: Epoch [8], Batch [884/938], Loss: 0.669176459312439\n",
      "Train: Epoch [8], Batch [885/938], Loss: 0.7908585667610168\n",
      "Train: Epoch [8], Batch [886/938], Loss: 0.6876658201217651\n",
      "Train: Epoch [8], Batch [887/938], Loss: 0.7885004878044128\n",
      "Train: Epoch [8], Batch [888/938], Loss: 0.6198894381523132\n",
      "Train: Epoch [8], Batch [889/938], Loss: 0.531762421131134\n",
      "Train: Epoch [8], Batch [890/938], Loss: 0.49807649850845337\n",
      "Train: Epoch [8], Batch [891/938], Loss: 0.6055498123168945\n",
      "Train: Epoch [8], Batch [892/938], Loss: 0.4544217586517334\n",
      "Train: Epoch [8], Batch [893/938], Loss: 0.7006711959838867\n",
      "Train: Epoch [8], Batch [894/938], Loss: 0.5207990407943726\n",
      "Train: Epoch [8], Batch [895/938], Loss: 0.5443814396858215\n",
      "Train: Epoch [8], Batch [896/938], Loss: 0.5561084747314453\n",
      "Train: Epoch [8], Batch [897/938], Loss: 0.5477781295776367\n",
      "Train: Epoch [8], Batch [898/938], Loss: 0.5151271820068359\n",
      "Train: Epoch [8], Batch [899/938], Loss: 0.4605146646499634\n",
      "Train: Epoch [8], Batch [900/938], Loss: 0.5247517824172974\n",
      "Train: Epoch [8], Batch [901/938], Loss: 0.47463300824165344\n",
      "Train: Epoch [8], Batch [902/938], Loss: 0.648159384727478\n",
      "Train: Epoch [8], Batch [903/938], Loss: 0.3433073163032532\n",
      "Train: Epoch [8], Batch [904/938], Loss: 0.6310304403305054\n",
      "Train: Epoch [8], Batch [905/938], Loss: 0.45488351583480835\n",
      "Train: Epoch [8], Batch [906/938], Loss: 0.44332990050315857\n",
      "Train: Epoch [8], Batch [907/938], Loss: 0.4356861114501953\n",
      "Train: Epoch [8], Batch [908/938], Loss: 0.45419055223464966\n",
      "Train: Epoch [8], Batch [909/938], Loss: 0.5704920291900635\n",
      "Train: Epoch [8], Batch [910/938], Loss: 0.5992581844329834\n",
      "Train: Epoch [8], Batch [911/938], Loss: 0.6271218061447144\n",
      "Train: Epoch [8], Batch [912/938], Loss: 0.6095108985900879\n",
      "Train: Epoch [8], Batch [913/938], Loss: 0.33567681908607483\n",
      "Train: Epoch [8], Batch [914/938], Loss: 0.5524739027023315\n",
      "Train: Epoch [8], Batch [915/938], Loss: 0.5466470718383789\n",
      "Train: Epoch [8], Batch [916/938], Loss: 0.44724327325820923\n",
      "Train: Epoch [8], Batch [917/938], Loss: 0.5055805444717407\n",
      "Train: Epoch [8], Batch [918/938], Loss: 0.7442082166671753\n",
      "Train: Epoch [8], Batch [919/938], Loss: 0.3670666217803955\n",
      "Train: Epoch [8], Batch [920/938], Loss: 0.4611474275588989\n",
      "Train: Epoch [8], Batch [921/938], Loss: 0.8487249612808228\n",
      "Train: Epoch [8], Batch [922/938], Loss: 0.636439859867096\n",
      "Train: Epoch [8], Batch [923/938], Loss: 0.5044548511505127\n",
      "Train: Epoch [8], Batch [924/938], Loss: 0.5456334352493286\n",
      "Train: Epoch [8], Batch [925/938], Loss: 0.4694487154483795\n",
      "Train: Epoch [8], Batch [926/938], Loss: 0.4414759576320648\n",
      "Train: Epoch [8], Batch [927/938], Loss: 0.5258402824401855\n",
      "Train: Epoch [8], Batch [928/938], Loss: 0.5176838636398315\n",
      "Train: Epoch [8], Batch [929/938], Loss: 0.5012258887290955\n",
      "Train: Epoch [8], Batch [930/938], Loss: 0.5605728030204773\n",
      "Train: Epoch [8], Batch [931/938], Loss: 0.55594801902771\n",
      "Train: Epoch [8], Batch [932/938], Loss: 0.4447295665740967\n",
      "Train: Epoch [8], Batch [933/938], Loss: 0.41407686471939087\n",
      "Train: Epoch [8], Batch [934/938], Loss: 0.6936414837837219\n",
      "Train: Epoch [8], Batch [935/938], Loss: 0.46137893199920654\n",
      "Train: Epoch [8], Batch [936/938], Loss: 0.6394921541213989\n",
      "Train: Epoch [8], Batch [937/938], Loss: 0.6008176803588867\n",
      "Train: Epoch [8], Batch [938/938], Loss: 0.636939287185669\n",
      "Accuracy of train set: 0.7964333333333333\n",
      "Validation: Epoch [8], Batch [1/938], Loss: 0.5819952487945557\n",
      "Validation: Epoch [8], Batch [2/938], Loss: 0.5529691576957703\n",
      "Validation: Epoch [8], Batch [3/938], Loss: 0.53645920753479\n",
      "Validation: Epoch [8], Batch [4/938], Loss: 0.6798529624938965\n",
      "Validation: Epoch [8], Batch [5/938], Loss: 0.5019544363021851\n",
      "Validation: Epoch [8], Batch [6/938], Loss: 0.5386749505996704\n",
      "Validation: Epoch [8], Batch [7/938], Loss: 0.4880857765674591\n",
      "Validation: Epoch [8], Batch [8/938], Loss: 0.7032240629196167\n",
      "Validation: Epoch [8], Batch [9/938], Loss: 0.3405055105686188\n",
      "Validation: Epoch [8], Batch [10/938], Loss: 0.7547645568847656\n",
      "Validation: Epoch [8], Batch [11/938], Loss: 0.5085561871528625\n",
      "Validation: Epoch [8], Batch [12/938], Loss: 0.5901852250099182\n",
      "Validation: Epoch [8], Batch [13/938], Loss: 0.5323150157928467\n",
      "Validation: Epoch [8], Batch [14/938], Loss: 0.6307418346405029\n",
      "Validation: Epoch [8], Batch [15/938], Loss: 0.6151711940765381\n",
      "Validation: Epoch [8], Batch [16/938], Loss: 0.5118656158447266\n",
      "Validation: Epoch [8], Batch [17/938], Loss: 0.37539172172546387\n",
      "Validation: Epoch [8], Batch [18/938], Loss: 0.5939452648162842\n",
      "Validation: Epoch [8], Batch [19/938], Loss: 0.6843940019607544\n",
      "Validation: Epoch [8], Batch [20/938], Loss: 0.6312214732170105\n",
      "Validation: Epoch [8], Batch [21/938], Loss: 0.629217267036438\n",
      "Validation: Epoch [8], Batch [22/938], Loss: 0.5963858366012573\n",
      "Validation: Epoch [8], Batch [23/938], Loss: 0.5253279805183411\n",
      "Validation: Epoch [8], Batch [24/938], Loss: 0.517333984375\n",
      "Validation: Epoch [8], Batch [25/938], Loss: 0.7255575656890869\n",
      "Validation: Epoch [8], Batch [26/938], Loss: 0.38058626651763916\n",
      "Validation: Epoch [8], Batch [27/938], Loss: 0.41707733273506165\n",
      "Validation: Epoch [8], Batch [28/938], Loss: 0.5955100059509277\n",
      "Validation: Epoch [8], Batch [29/938], Loss: 0.6434916257858276\n",
      "Validation: Epoch [8], Batch [30/938], Loss: 0.6323821544647217\n",
      "Validation: Epoch [8], Batch [31/938], Loss: 0.47149842977523804\n",
      "Validation: Epoch [8], Batch [32/938], Loss: 0.658212423324585\n",
      "Validation: Epoch [8], Batch [33/938], Loss: 0.6155809164047241\n",
      "Validation: Epoch [8], Batch [34/938], Loss: 0.5650322437286377\n",
      "Validation: Epoch [8], Batch [35/938], Loss: 0.6787159442901611\n",
      "Validation: Epoch [8], Batch [36/938], Loss: 0.7434947490692139\n",
      "Validation: Epoch [8], Batch [37/938], Loss: 0.43130818009376526\n",
      "Validation: Epoch [8], Batch [38/938], Loss: 0.6010314226150513\n",
      "Validation: Epoch [8], Batch [39/938], Loss: 0.5740299224853516\n",
      "Validation: Epoch [8], Batch [40/938], Loss: 0.9474863409996033\n",
      "Validation: Epoch [8], Batch [41/938], Loss: 0.5443776845932007\n",
      "Validation: Epoch [8], Batch [42/938], Loss: 0.6383442878723145\n",
      "Validation: Epoch [8], Batch [43/938], Loss: 0.5009506940841675\n",
      "Validation: Epoch [8], Batch [44/938], Loss: 0.4352322816848755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [45/938], Loss: 0.36735373735427856\n",
      "Validation: Epoch [8], Batch [46/938], Loss: 0.641260027885437\n",
      "Validation: Epoch [8], Batch [47/938], Loss: 0.6748148202896118\n",
      "Validation: Epoch [8], Batch [48/938], Loss: 0.4636611044406891\n",
      "Validation: Epoch [8], Batch [49/938], Loss: 0.6510059833526611\n",
      "Validation: Epoch [8], Batch [50/938], Loss: 0.45855146646499634\n",
      "Validation: Epoch [8], Batch [51/938], Loss: 0.6319311857223511\n",
      "Validation: Epoch [8], Batch [52/938], Loss: 0.45582130551338196\n",
      "Validation: Epoch [8], Batch [53/938], Loss: 0.723595142364502\n",
      "Validation: Epoch [8], Batch [54/938], Loss: 0.6161577701568604\n",
      "Validation: Epoch [8], Batch [55/938], Loss: 0.6697611212730408\n",
      "Validation: Epoch [8], Batch [56/938], Loss: 0.6639914512634277\n",
      "Validation: Epoch [8], Batch [57/938], Loss: 0.516359269618988\n",
      "Validation: Epoch [8], Batch [58/938], Loss: 0.570523738861084\n",
      "Validation: Epoch [8], Batch [59/938], Loss: 0.6998259425163269\n",
      "Validation: Epoch [8], Batch [60/938], Loss: 0.5236482620239258\n",
      "Validation: Epoch [8], Batch [61/938], Loss: 0.42406001687049866\n",
      "Validation: Epoch [8], Batch [62/938], Loss: 0.6023920774459839\n",
      "Validation: Epoch [8], Batch [63/938], Loss: 0.516089141368866\n",
      "Validation: Epoch [8], Batch [64/938], Loss: 0.6732851266860962\n",
      "Validation: Epoch [8], Batch [65/938], Loss: 0.5854537487030029\n",
      "Validation: Epoch [8], Batch [66/938], Loss: 0.5685006380081177\n",
      "Validation: Epoch [8], Batch [67/938], Loss: 0.4658544063568115\n",
      "Validation: Epoch [8], Batch [68/938], Loss: 0.4724992513656616\n",
      "Validation: Epoch [8], Batch [69/938], Loss: 0.5006505250930786\n",
      "Validation: Epoch [8], Batch [70/938], Loss: 0.7781860828399658\n",
      "Validation: Epoch [8], Batch [71/938], Loss: 0.6840958595275879\n",
      "Validation: Epoch [8], Batch [72/938], Loss: 0.5547137260437012\n",
      "Validation: Epoch [8], Batch [73/938], Loss: 0.6954423785209656\n",
      "Validation: Epoch [8], Batch [74/938], Loss: 0.6519889831542969\n",
      "Validation: Epoch [8], Batch [75/938], Loss: 0.671665370464325\n",
      "Validation: Epoch [8], Batch [76/938], Loss: 0.6367886066436768\n",
      "Validation: Epoch [8], Batch [77/938], Loss: 0.370221346616745\n",
      "Validation: Epoch [8], Batch [78/938], Loss: 0.6919255256652832\n",
      "Validation: Epoch [8], Batch [79/938], Loss: 0.5863714218139648\n",
      "Validation: Epoch [8], Batch [80/938], Loss: 0.5495961308479309\n",
      "Validation: Epoch [8], Batch [81/938], Loss: 0.6534188985824585\n",
      "Validation: Epoch [8], Batch [82/938], Loss: 0.3531530797481537\n",
      "Validation: Epoch [8], Batch [83/938], Loss: 0.6595839262008667\n",
      "Validation: Epoch [8], Batch [84/938], Loss: 0.5657243132591248\n",
      "Validation: Epoch [8], Batch [85/938], Loss: 0.5714688301086426\n",
      "Validation: Epoch [8], Batch [86/938], Loss: 0.564433217048645\n",
      "Validation: Epoch [8], Batch [87/938], Loss: 0.5618723034858704\n",
      "Validation: Epoch [8], Batch [88/938], Loss: 0.43544837832450867\n",
      "Validation: Epoch [8], Batch [89/938], Loss: 0.6271126866340637\n",
      "Validation: Epoch [8], Batch [90/938], Loss: 0.7321871519088745\n",
      "Validation: Epoch [8], Batch [91/938], Loss: 0.5861804485321045\n",
      "Validation: Epoch [8], Batch [92/938], Loss: 0.5510521531105042\n",
      "Validation: Epoch [8], Batch [93/938], Loss: 0.5218110680580139\n",
      "Validation: Epoch [8], Batch [94/938], Loss: 0.7424954175949097\n",
      "Validation: Epoch [8], Batch [95/938], Loss: 0.5651528835296631\n",
      "Validation: Epoch [8], Batch [96/938], Loss: 0.4120819568634033\n",
      "Validation: Epoch [8], Batch [97/938], Loss: 0.4578644335269928\n",
      "Validation: Epoch [8], Batch [98/938], Loss: 0.6116975545883179\n",
      "Validation: Epoch [8], Batch [99/938], Loss: 0.7635650038719177\n",
      "Validation: Epoch [8], Batch [100/938], Loss: 0.7158786654472351\n",
      "Validation: Epoch [8], Batch [101/938], Loss: 0.4698391854763031\n",
      "Validation: Epoch [8], Batch [102/938], Loss: 0.4436609745025635\n",
      "Validation: Epoch [8], Batch [103/938], Loss: 0.5121428370475769\n",
      "Validation: Epoch [8], Batch [104/938], Loss: 0.7338230609893799\n",
      "Validation: Epoch [8], Batch [105/938], Loss: 0.5657865405082703\n",
      "Validation: Epoch [8], Batch [106/938], Loss: 0.5431656837463379\n",
      "Validation: Epoch [8], Batch [107/938], Loss: 0.5141589641571045\n",
      "Validation: Epoch [8], Batch [108/938], Loss: 0.6154452562332153\n",
      "Validation: Epoch [8], Batch [109/938], Loss: 0.4410145580768585\n",
      "Validation: Epoch [8], Batch [110/938], Loss: 0.46855631470680237\n",
      "Validation: Epoch [8], Batch [111/938], Loss: 0.43189090490341187\n",
      "Validation: Epoch [8], Batch [112/938], Loss: 0.6017532348632812\n",
      "Validation: Epoch [8], Batch [113/938], Loss: 0.600084662437439\n",
      "Validation: Epoch [8], Batch [114/938], Loss: 0.49135905504226685\n",
      "Validation: Epoch [8], Batch [115/938], Loss: 0.48937392234802246\n",
      "Validation: Epoch [8], Batch [116/938], Loss: 0.5134952068328857\n",
      "Validation: Epoch [8], Batch [117/938], Loss: 0.6604557633399963\n",
      "Validation: Epoch [8], Batch [118/938], Loss: 0.5352108478546143\n",
      "Validation: Epoch [8], Batch [119/938], Loss: 0.5134319067001343\n",
      "Validation: Epoch [8], Batch [120/938], Loss: 0.5705176591873169\n",
      "Validation: Epoch [8], Batch [121/938], Loss: 0.6303117275238037\n",
      "Validation: Epoch [8], Batch [122/938], Loss: 0.6735172271728516\n",
      "Validation: Epoch [8], Batch [123/938], Loss: 0.4650423526763916\n",
      "Validation: Epoch [8], Batch [124/938], Loss: 0.5945358872413635\n",
      "Validation: Epoch [8], Batch [125/938], Loss: 0.43912723660469055\n",
      "Validation: Epoch [8], Batch [126/938], Loss: 0.5889065265655518\n",
      "Validation: Epoch [8], Batch [127/938], Loss: 0.5474704504013062\n",
      "Validation: Epoch [8], Batch [128/938], Loss: 0.6211117506027222\n",
      "Validation: Epoch [8], Batch [129/938], Loss: 0.5436955094337463\n",
      "Validation: Epoch [8], Batch [130/938], Loss: 0.4892871081829071\n",
      "Validation: Epoch [8], Batch [131/938], Loss: 0.4852461814880371\n",
      "Validation: Epoch [8], Batch [132/938], Loss: 0.5472581386566162\n",
      "Validation: Epoch [8], Batch [133/938], Loss: 0.49893468618392944\n",
      "Validation: Epoch [8], Batch [134/938], Loss: 0.5239076614379883\n",
      "Validation: Epoch [8], Batch [135/938], Loss: 0.575717031955719\n",
      "Validation: Epoch [8], Batch [136/938], Loss: 0.48988890647888184\n",
      "Validation: Epoch [8], Batch [137/938], Loss: 0.6148719787597656\n",
      "Validation: Epoch [8], Batch [138/938], Loss: 0.685435950756073\n",
      "Validation: Epoch [8], Batch [139/938], Loss: 0.4627154767513275\n",
      "Validation: Epoch [8], Batch [140/938], Loss: 0.4648982882499695\n",
      "Validation: Epoch [8], Batch [141/938], Loss: 0.6880499124526978\n",
      "Validation: Epoch [8], Batch [142/938], Loss: 0.8003091812133789\n",
      "Validation: Epoch [8], Batch [143/938], Loss: 0.6614122986793518\n",
      "Validation: Epoch [8], Batch [144/938], Loss: 0.41582468152046204\n",
      "Validation: Epoch [8], Batch [145/938], Loss: 0.6400213837623596\n",
      "Validation: Epoch [8], Batch [146/938], Loss: 0.4831816554069519\n",
      "Validation: Epoch [8], Batch [147/938], Loss: 0.6103169918060303\n",
      "Validation: Epoch [8], Batch [148/938], Loss: 0.5321928262710571\n",
      "Validation: Epoch [8], Batch [149/938], Loss: 0.5919215679168701\n",
      "Validation: Epoch [8], Batch [150/938], Loss: 0.5553164482116699\n",
      "Validation: Epoch [8], Batch [151/938], Loss: 0.7655353546142578\n",
      "Validation: Epoch [8], Batch [152/938], Loss: 0.5290918350219727\n",
      "Validation: Epoch [8], Batch [153/938], Loss: 0.7301555275917053\n",
      "Validation: Epoch [8], Batch [154/938], Loss: 0.5178517699241638\n",
      "Validation: Epoch [8], Batch [155/938], Loss: 0.7682055234909058\n",
      "Validation: Epoch [8], Batch [156/938], Loss: 0.5408411622047424\n",
      "Validation: Epoch [8], Batch [157/938], Loss: 0.5606682300567627\n",
      "Validation: Epoch [8], Batch [158/938], Loss: 0.482208251953125\n",
      "Validation: Epoch [8], Batch [159/938], Loss: 0.7263759970664978\n",
      "Validation: Epoch [8], Batch [160/938], Loss: 0.5134738683700562\n",
      "Validation: Epoch [8], Batch [161/938], Loss: 0.4005964398384094\n",
      "Validation: Epoch [8], Batch [162/938], Loss: 0.435138076543808\n",
      "Validation: Epoch [8], Batch [163/938], Loss: 0.6174761652946472\n",
      "Validation: Epoch [8], Batch [164/938], Loss: 0.6370900869369507\n",
      "Validation: Epoch [8], Batch [165/938], Loss: 0.4061165452003479\n",
      "Validation: Epoch [8], Batch [166/938], Loss: 0.4825018644332886\n",
      "Validation: Epoch [8], Batch [167/938], Loss: 0.5619604587554932\n",
      "Validation: Epoch [8], Batch [168/938], Loss: 0.6458972692489624\n",
      "Validation: Epoch [8], Batch [169/938], Loss: 0.5979892015457153\n",
      "Validation: Epoch [8], Batch [170/938], Loss: 0.6324870586395264\n",
      "Validation: Epoch [8], Batch [171/938], Loss: 0.628038763999939\n",
      "Validation: Epoch [8], Batch [172/938], Loss: 0.5364093780517578\n",
      "Validation: Epoch [8], Batch [173/938], Loss: 0.4625451862812042\n",
      "Validation: Epoch [8], Batch [174/938], Loss: 0.6041353940963745\n",
      "Validation: Epoch [8], Batch [175/938], Loss: 0.4027344584465027\n",
      "Validation: Epoch [8], Batch [176/938], Loss: 0.524005651473999\n",
      "Validation: Epoch [8], Batch [177/938], Loss: 0.5868334770202637\n",
      "Validation: Epoch [8], Batch [178/938], Loss: 0.5873635411262512\n",
      "Validation: Epoch [8], Batch [179/938], Loss: 0.5147722959518433\n",
      "Validation: Epoch [8], Batch [180/938], Loss: 0.5116416215896606\n",
      "Validation: Epoch [8], Batch [181/938], Loss: 0.6016687154769897\n",
      "Validation: Epoch [8], Batch [182/938], Loss: 0.6044095158576965\n",
      "Validation: Epoch [8], Batch [183/938], Loss: 0.49669593572616577\n",
      "Validation: Epoch [8], Batch [184/938], Loss: 0.49716639518737793\n",
      "Validation: Epoch [8], Batch [185/938], Loss: 0.350358247756958\n",
      "Validation: Epoch [8], Batch [186/938], Loss: 0.5615509748458862\n",
      "Validation: Epoch [8], Batch [187/938], Loss: 0.4445781707763672\n",
      "Validation: Epoch [8], Batch [188/938], Loss: 0.6118319630622864\n",
      "Validation: Epoch [8], Batch [189/938], Loss: 0.508842408657074\n",
      "Validation: Epoch [8], Batch [190/938], Loss: 0.5834929943084717\n",
      "Validation: Epoch [8], Batch [191/938], Loss: 0.6116253137588501\n",
      "Validation: Epoch [8], Batch [192/938], Loss: 0.5201749205589294\n",
      "Validation: Epoch [8], Batch [193/938], Loss: 0.6877269148826599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [194/938], Loss: 0.6129183769226074\n",
      "Validation: Epoch [8], Batch [195/938], Loss: 0.35432636737823486\n",
      "Validation: Epoch [8], Batch [196/938], Loss: 0.9112995862960815\n",
      "Validation: Epoch [8], Batch [197/938], Loss: 0.5327643156051636\n",
      "Validation: Epoch [8], Batch [198/938], Loss: 0.526807963848114\n",
      "Validation: Epoch [8], Batch [199/938], Loss: 0.4508257508277893\n",
      "Validation: Epoch [8], Batch [200/938], Loss: 0.44220224022865295\n",
      "Validation: Epoch [8], Batch [201/938], Loss: 0.5394898056983948\n",
      "Validation: Epoch [8], Batch [202/938], Loss: 0.48479804396629333\n",
      "Validation: Epoch [8], Batch [203/938], Loss: 0.4502844214439392\n",
      "Validation: Epoch [8], Batch [204/938], Loss: 0.47032630443573\n",
      "Validation: Epoch [8], Batch [205/938], Loss: 0.6579514741897583\n",
      "Validation: Epoch [8], Batch [206/938], Loss: 0.5114513039588928\n",
      "Validation: Epoch [8], Batch [207/938], Loss: 0.7738932967185974\n",
      "Validation: Epoch [8], Batch [208/938], Loss: 0.4526408314704895\n",
      "Validation: Epoch [8], Batch [209/938], Loss: 0.5378402471542358\n",
      "Validation: Epoch [8], Batch [210/938], Loss: 0.5966227054595947\n",
      "Validation: Epoch [8], Batch [211/938], Loss: 0.45566168427467346\n",
      "Validation: Epoch [8], Batch [212/938], Loss: 0.8364514112472534\n",
      "Validation: Epoch [8], Batch [213/938], Loss: 0.6173517107963562\n",
      "Validation: Epoch [8], Batch [214/938], Loss: 0.6169811487197876\n",
      "Validation: Epoch [8], Batch [215/938], Loss: 0.540648341178894\n",
      "Validation: Epoch [8], Batch [216/938], Loss: 0.5420818328857422\n",
      "Validation: Epoch [8], Batch [217/938], Loss: 0.7320462465286255\n",
      "Validation: Epoch [8], Batch [218/938], Loss: 0.6124672293663025\n",
      "Validation: Epoch [8], Batch [219/938], Loss: 0.6850009560585022\n",
      "Validation: Epoch [8], Batch [220/938], Loss: 0.7683239579200745\n",
      "Validation: Epoch [8], Batch [221/938], Loss: 0.5275009274482727\n",
      "Validation: Epoch [8], Batch [222/938], Loss: 0.5996251106262207\n",
      "Validation: Epoch [8], Batch [223/938], Loss: 0.48136693239212036\n",
      "Validation: Epoch [8], Batch [224/938], Loss: 0.5791394114494324\n",
      "Validation: Epoch [8], Batch [225/938], Loss: 0.6253910064697266\n",
      "Validation: Epoch [8], Batch [226/938], Loss: 0.47411325573921204\n",
      "Validation: Epoch [8], Batch [227/938], Loss: 0.555513858795166\n",
      "Validation: Epoch [8], Batch [228/938], Loss: 0.7262324094772339\n",
      "Validation: Epoch [8], Batch [229/938], Loss: 0.537436842918396\n",
      "Validation: Epoch [8], Batch [230/938], Loss: 0.5622362494468689\n",
      "Validation: Epoch [8], Batch [231/938], Loss: 0.5712348222732544\n",
      "Validation: Epoch [8], Batch [232/938], Loss: 0.5735890865325928\n",
      "Validation: Epoch [8], Batch [233/938], Loss: 0.4887866675853729\n",
      "Validation: Epoch [8], Batch [234/938], Loss: 0.6767794489860535\n",
      "Validation: Epoch [8], Batch [235/938], Loss: 0.4694385528564453\n",
      "Validation: Epoch [8], Batch [236/938], Loss: 0.6698927283287048\n",
      "Validation: Epoch [8], Batch [237/938], Loss: 0.49760401248931885\n",
      "Validation: Epoch [8], Batch [238/938], Loss: 0.5136675834655762\n",
      "Validation: Epoch [8], Batch [239/938], Loss: 0.4116489589214325\n",
      "Validation: Epoch [8], Batch [240/938], Loss: 0.5249698162078857\n",
      "Validation: Epoch [8], Batch [241/938], Loss: 0.5826317667961121\n",
      "Validation: Epoch [8], Batch [242/938], Loss: 0.47626006603240967\n",
      "Validation: Epoch [8], Batch [243/938], Loss: 0.47461575269699097\n",
      "Validation: Epoch [8], Batch [244/938], Loss: 0.5791505575180054\n",
      "Validation: Epoch [8], Batch [245/938], Loss: 0.46644771099090576\n",
      "Validation: Epoch [8], Batch [246/938], Loss: 0.4637376070022583\n",
      "Validation: Epoch [8], Batch [247/938], Loss: 0.41574525833129883\n",
      "Validation: Epoch [8], Batch [248/938], Loss: 0.4790559709072113\n",
      "Validation: Epoch [8], Batch [249/938], Loss: 0.6902177929878235\n",
      "Validation: Epoch [8], Batch [250/938], Loss: 0.4703689515590668\n",
      "Validation: Epoch [8], Batch [251/938], Loss: 0.7659383416175842\n",
      "Validation: Epoch [8], Batch [252/938], Loss: 0.5214791297912598\n",
      "Validation: Epoch [8], Batch [253/938], Loss: 0.5363434553146362\n",
      "Validation: Epoch [8], Batch [254/938], Loss: 0.5066413879394531\n",
      "Validation: Epoch [8], Batch [255/938], Loss: 0.5388180017471313\n",
      "Validation: Epoch [8], Batch [256/938], Loss: 0.6101386547088623\n",
      "Validation: Epoch [8], Batch [257/938], Loss: 0.6119790077209473\n",
      "Validation: Epoch [8], Batch [258/938], Loss: 0.7381734251976013\n",
      "Validation: Epoch [8], Batch [259/938], Loss: 0.6372723579406738\n",
      "Validation: Epoch [8], Batch [260/938], Loss: 0.8600820899009705\n",
      "Validation: Epoch [8], Batch [261/938], Loss: 0.5111510753631592\n",
      "Validation: Epoch [8], Batch [262/938], Loss: 0.7135093212127686\n",
      "Validation: Epoch [8], Batch [263/938], Loss: 0.5768066644668579\n",
      "Validation: Epoch [8], Batch [264/938], Loss: 0.6860082149505615\n",
      "Validation: Epoch [8], Batch [265/938], Loss: 0.39991846680641174\n",
      "Validation: Epoch [8], Batch [266/938], Loss: 0.6497207880020142\n",
      "Validation: Epoch [8], Batch [267/938], Loss: 0.5414441823959351\n",
      "Validation: Epoch [8], Batch [268/938], Loss: 0.4854464530944824\n",
      "Validation: Epoch [8], Batch [269/938], Loss: 0.4447680413722992\n",
      "Validation: Epoch [8], Batch [270/938], Loss: 0.5839996933937073\n",
      "Validation: Epoch [8], Batch [271/938], Loss: 0.4980274736881256\n",
      "Validation: Epoch [8], Batch [272/938], Loss: 0.5763206481933594\n",
      "Validation: Epoch [8], Batch [273/938], Loss: 0.5582783222198486\n",
      "Validation: Epoch [8], Batch [274/938], Loss: 0.543798565864563\n",
      "Validation: Epoch [8], Batch [275/938], Loss: 0.5176538228988647\n",
      "Validation: Epoch [8], Batch [276/938], Loss: 0.5174554586410522\n",
      "Validation: Epoch [8], Batch [277/938], Loss: 0.678188681602478\n",
      "Validation: Epoch [8], Batch [278/938], Loss: 0.7562842965126038\n",
      "Validation: Epoch [8], Batch [279/938], Loss: 0.6432754397392273\n",
      "Validation: Epoch [8], Batch [280/938], Loss: 0.7164008617401123\n",
      "Validation: Epoch [8], Batch [281/938], Loss: 0.5210987329483032\n",
      "Validation: Epoch [8], Batch [282/938], Loss: 0.47345155477523804\n",
      "Validation: Epoch [8], Batch [283/938], Loss: 0.5899967551231384\n",
      "Validation: Epoch [8], Batch [284/938], Loss: 0.5982980132102966\n",
      "Validation: Epoch [8], Batch [285/938], Loss: 0.4956623315811157\n",
      "Validation: Epoch [8], Batch [286/938], Loss: 0.3973139524459839\n",
      "Validation: Epoch [8], Batch [287/938], Loss: 0.3508814573287964\n",
      "Validation: Epoch [8], Batch [288/938], Loss: 0.6446049809455872\n",
      "Validation: Epoch [8], Batch [289/938], Loss: 0.4948548674583435\n",
      "Validation: Epoch [8], Batch [290/938], Loss: 0.7639168500900269\n",
      "Validation: Epoch [8], Batch [291/938], Loss: 0.592360258102417\n",
      "Validation: Epoch [8], Batch [292/938], Loss: 0.5664979219436646\n",
      "Validation: Epoch [8], Batch [293/938], Loss: 0.5746780037879944\n",
      "Validation: Epoch [8], Batch [294/938], Loss: 0.6483573317527771\n",
      "Validation: Epoch [8], Batch [295/938], Loss: 0.5904185771942139\n",
      "Validation: Epoch [8], Batch [296/938], Loss: 0.47975608706474304\n",
      "Validation: Epoch [8], Batch [297/938], Loss: 0.5192117094993591\n",
      "Validation: Epoch [8], Batch [298/938], Loss: 0.4906696081161499\n",
      "Validation: Epoch [8], Batch [299/938], Loss: 0.5535718202590942\n",
      "Validation: Epoch [8], Batch [300/938], Loss: 0.7037150859832764\n",
      "Validation: Epoch [8], Batch [301/938], Loss: 0.4477406144142151\n",
      "Validation: Epoch [8], Batch [302/938], Loss: 0.4809378981590271\n",
      "Validation: Epoch [8], Batch [303/938], Loss: 0.6247622966766357\n",
      "Validation: Epoch [8], Batch [304/938], Loss: 0.6177482604980469\n",
      "Validation: Epoch [8], Batch [305/938], Loss: 0.6310515999794006\n",
      "Validation: Epoch [8], Batch [306/938], Loss: 0.5062138438224792\n",
      "Validation: Epoch [8], Batch [307/938], Loss: 0.7390294671058655\n",
      "Validation: Epoch [8], Batch [308/938], Loss: 0.5883561372756958\n",
      "Validation: Epoch [8], Batch [309/938], Loss: 0.5821797847747803\n",
      "Validation: Epoch [8], Batch [310/938], Loss: 0.45933637022972107\n",
      "Validation: Epoch [8], Batch [311/938], Loss: 0.7101100087165833\n",
      "Validation: Epoch [8], Batch [312/938], Loss: 0.5405460596084595\n",
      "Validation: Epoch [8], Batch [313/938], Loss: 0.42128628492355347\n",
      "Validation: Epoch [8], Batch [314/938], Loss: 0.44659483432769775\n",
      "Validation: Epoch [8], Batch [315/938], Loss: 0.8123292326927185\n",
      "Validation: Epoch [8], Batch [316/938], Loss: 0.5727527141571045\n",
      "Validation: Epoch [8], Batch [317/938], Loss: 0.5260904431343079\n",
      "Validation: Epoch [8], Batch [318/938], Loss: 0.35903680324554443\n",
      "Validation: Epoch [8], Batch [319/938], Loss: 0.4922383427619934\n",
      "Validation: Epoch [8], Batch [320/938], Loss: 0.5598961114883423\n",
      "Validation: Epoch [8], Batch [321/938], Loss: 0.7243837714195251\n",
      "Validation: Epoch [8], Batch [322/938], Loss: 0.6088817119598389\n",
      "Validation: Epoch [8], Batch [323/938], Loss: 0.5064059495925903\n",
      "Validation: Epoch [8], Batch [324/938], Loss: 0.5225549340248108\n",
      "Validation: Epoch [8], Batch [325/938], Loss: 0.9033076763153076\n",
      "Validation: Epoch [8], Batch [326/938], Loss: 0.7435321807861328\n",
      "Validation: Epoch [8], Batch [327/938], Loss: 0.5549603700637817\n",
      "Validation: Epoch [8], Batch [328/938], Loss: 0.4780677855014801\n",
      "Validation: Epoch [8], Batch [329/938], Loss: 0.520024299621582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [330/938], Loss: 0.5016313791275024\n",
      "Validation: Epoch [8], Batch [331/938], Loss: 0.5850716829299927\n",
      "Validation: Epoch [8], Batch [332/938], Loss: 0.4235803186893463\n",
      "Validation: Epoch [8], Batch [333/938], Loss: 0.5118018388748169\n",
      "Validation: Epoch [8], Batch [334/938], Loss: 0.5985665321350098\n",
      "Validation: Epoch [8], Batch [335/938], Loss: 0.8149683475494385\n",
      "Validation: Epoch [8], Batch [336/938], Loss: 0.5615682005882263\n",
      "Validation: Epoch [8], Batch [337/938], Loss: 0.6389304399490356\n",
      "Validation: Epoch [8], Batch [338/938], Loss: 0.584822416305542\n",
      "Validation: Epoch [8], Batch [339/938], Loss: 0.6007614135742188\n",
      "Validation: Epoch [8], Batch [340/938], Loss: 0.4839079976081848\n",
      "Validation: Epoch [8], Batch [341/938], Loss: 0.600654125213623\n",
      "Validation: Epoch [8], Batch [342/938], Loss: 0.7046055197715759\n",
      "Validation: Epoch [8], Batch [343/938], Loss: 0.3440719246864319\n",
      "Validation: Epoch [8], Batch [344/938], Loss: 0.6054509282112122\n",
      "Validation: Epoch [8], Batch [345/938], Loss: 0.6116957664489746\n",
      "Validation: Epoch [8], Batch [346/938], Loss: 0.5059385299682617\n",
      "Validation: Epoch [8], Batch [347/938], Loss: 0.5949748754501343\n",
      "Validation: Epoch [8], Batch [348/938], Loss: 0.4480096995830536\n",
      "Validation: Epoch [8], Batch [349/938], Loss: 0.543606162071228\n",
      "Validation: Epoch [8], Batch [350/938], Loss: 0.732797384262085\n",
      "Validation: Epoch [8], Batch [351/938], Loss: 0.569379985332489\n",
      "Validation: Epoch [8], Batch [352/938], Loss: 0.5067758560180664\n",
      "Validation: Epoch [8], Batch [353/938], Loss: 0.4977717399597168\n",
      "Validation: Epoch [8], Batch [354/938], Loss: 0.42304542660713196\n",
      "Validation: Epoch [8], Batch [355/938], Loss: 0.448625385761261\n",
      "Validation: Epoch [8], Batch [356/938], Loss: 0.6233993768692017\n",
      "Validation: Epoch [8], Batch [357/938], Loss: 0.4683758020401001\n",
      "Validation: Epoch [8], Batch [358/938], Loss: 0.7044852375984192\n",
      "Validation: Epoch [8], Batch [359/938], Loss: 0.5297876000404358\n",
      "Validation: Epoch [8], Batch [360/938], Loss: 0.5959059000015259\n",
      "Validation: Epoch [8], Batch [361/938], Loss: 0.46134185791015625\n",
      "Validation: Epoch [8], Batch [362/938], Loss: 0.6224908828735352\n",
      "Validation: Epoch [8], Batch [363/938], Loss: 0.7073301076889038\n",
      "Validation: Epoch [8], Batch [364/938], Loss: 0.5553156733512878\n",
      "Validation: Epoch [8], Batch [365/938], Loss: 0.8433377742767334\n",
      "Validation: Epoch [8], Batch [366/938], Loss: 0.5240747928619385\n",
      "Validation: Epoch [8], Batch [367/938], Loss: 0.46812349557876587\n",
      "Validation: Epoch [8], Batch [368/938], Loss: 0.5182080268859863\n",
      "Validation: Epoch [8], Batch [369/938], Loss: 0.6791170835494995\n",
      "Validation: Epoch [8], Batch [370/938], Loss: 0.6388393044471741\n",
      "Validation: Epoch [8], Batch [371/938], Loss: 0.7144453525543213\n",
      "Validation: Epoch [8], Batch [372/938], Loss: 0.5701836943626404\n",
      "Validation: Epoch [8], Batch [373/938], Loss: 0.5968958139419556\n",
      "Validation: Epoch [8], Batch [374/938], Loss: 0.5447961091995239\n",
      "Validation: Epoch [8], Batch [375/938], Loss: 0.6878510117530823\n",
      "Validation: Epoch [8], Batch [376/938], Loss: 0.5188858509063721\n",
      "Validation: Epoch [8], Batch [377/938], Loss: 0.6264628171920776\n",
      "Validation: Epoch [8], Batch [378/938], Loss: 0.5297975540161133\n",
      "Validation: Epoch [8], Batch [379/938], Loss: 0.6211187243461609\n",
      "Validation: Epoch [8], Batch [380/938], Loss: 0.490786612033844\n",
      "Validation: Epoch [8], Batch [381/938], Loss: 0.8464006185531616\n",
      "Validation: Epoch [8], Batch [382/938], Loss: 0.6391579508781433\n",
      "Validation: Epoch [8], Batch [383/938], Loss: 0.6338903903961182\n",
      "Validation: Epoch [8], Batch [384/938], Loss: 0.49350249767303467\n",
      "Validation: Epoch [8], Batch [385/938], Loss: 0.6893196105957031\n",
      "Validation: Epoch [8], Batch [386/938], Loss: 0.5420258045196533\n",
      "Validation: Epoch [8], Batch [387/938], Loss: 0.7942764163017273\n",
      "Validation: Epoch [8], Batch [388/938], Loss: 0.6027979850769043\n",
      "Validation: Epoch [8], Batch [389/938], Loss: 0.6245357990264893\n",
      "Validation: Epoch [8], Batch [390/938], Loss: 0.5430249571800232\n",
      "Validation: Epoch [8], Batch [391/938], Loss: 0.3763555586338043\n",
      "Validation: Epoch [8], Batch [392/938], Loss: 0.4950926899909973\n",
      "Validation: Epoch [8], Batch [393/938], Loss: 0.6250522136688232\n",
      "Validation: Epoch [8], Batch [394/938], Loss: 0.7234718203544617\n",
      "Validation: Epoch [8], Batch [395/938], Loss: 0.5301405191421509\n",
      "Validation: Epoch [8], Batch [396/938], Loss: 0.6414591073989868\n",
      "Validation: Epoch [8], Batch [397/938], Loss: 0.465991735458374\n",
      "Validation: Epoch [8], Batch [398/938], Loss: 0.4550158381462097\n",
      "Validation: Epoch [8], Batch [399/938], Loss: 0.8755967020988464\n",
      "Validation: Epoch [8], Batch [400/938], Loss: 0.44962236285209656\n",
      "Validation: Epoch [8], Batch [401/938], Loss: 0.5369842052459717\n",
      "Validation: Epoch [8], Batch [402/938], Loss: 0.6766959428787231\n",
      "Validation: Epoch [8], Batch [403/938], Loss: 0.5565692782402039\n",
      "Validation: Epoch [8], Batch [404/938], Loss: 0.5494821071624756\n",
      "Validation: Epoch [8], Batch [405/938], Loss: 0.6881859302520752\n",
      "Validation: Epoch [8], Batch [406/938], Loss: 0.5298148989677429\n",
      "Validation: Epoch [8], Batch [407/938], Loss: 0.5496258735656738\n",
      "Validation: Epoch [8], Batch [408/938], Loss: 0.546915590763092\n",
      "Validation: Epoch [8], Batch [409/938], Loss: 0.6602612137794495\n",
      "Validation: Epoch [8], Batch [410/938], Loss: 0.6459433436393738\n",
      "Validation: Epoch [8], Batch [411/938], Loss: 0.4084041118621826\n",
      "Validation: Epoch [8], Batch [412/938], Loss: 0.683125376701355\n",
      "Validation: Epoch [8], Batch [413/938], Loss: 0.5351338386535645\n",
      "Validation: Epoch [8], Batch [414/938], Loss: 0.4387419819831848\n",
      "Validation: Epoch [8], Batch [415/938], Loss: 0.5924162864685059\n",
      "Validation: Epoch [8], Batch [416/938], Loss: 0.6015697121620178\n",
      "Validation: Epoch [8], Batch [417/938], Loss: 0.5314900875091553\n",
      "Validation: Epoch [8], Batch [418/938], Loss: 0.6751299500465393\n",
      "Validation: Epoch [8], Batch [419/938], Loss: 0.6187487840652466\n",
      "Validation: Epoch [8], Batch [420/938], Loss: 0.4143986701965332\n",
      "Validation: Epoch [8], Batch [421/938], Loss: 0.6434476375579834\n",
      "Validation: Epoch [8], Batch [422/938], Loss: 0.596336841583252\n",
      "Validation: Epoch [8], Batch [423/938], Loss: 0.5566262006759644\n",
      "Validation: Epoch [8], Batch [424/938], Loss: 0.5593234896659851\n",
      "Validation: Epoch [8], Batch [425/938], Loss: 0.6553958058357239\n",
      "Validation: Epoch [8], Batch [426/938], Loss: 0.6738442182540894\n",
      "Validation: Epoch [8], Batch [427/938], Loss: 0.6026524305343628\n",
      "Validation: Epoch [8], Batch [428/938], Loss: 0.8590433597564697\n",
      "Validation: Epoch [8], Batch [429/938], Loss: 0.6289507746696472\n",
      "Validation: Epoch [8], Batch [430/938], Loss: 0.6275365352630615\n",
      "Validation: Epoch [8], Batch [431/938], Loss: 0.4944484829902649\n",
      "Validation: Epoch [8], Batch [432/938], Loss: 0.4648057222366333\n",
      "Validation: Epoch [8], Batch [433/938], Loss: 0.39196476340293884\n",
      "Validation: Epoch [8], Batch [434/938], Loss: 0.7462115287780762\n",
      "Validation: Epoch [8], Batch [435/938], Loss: 0.5386943221092224\n",
      "Validation: Epoch [8], Batch [436/938], Loss: 0.6033445000648499\n",
      "Validation: Epoch [8], Batch [437/938], Loss: 0.49528563022613525\n",
      "Validation: Epoch [8], Batch [438/938], Loss: 0.6620309948921204\n",
      "Validation: Epoch [8], Batch [439/938], Loss: 0.448153555393219\n",
      "Validation: Epoch [8], Batch [440/938], Loss: 0.5025162696838379\n",
      "Validation: Epoch [8], Batch [441/938], Loss: 0.46430033445358276\n",
      "Validation: Epoch [8], Batch [442/938], Loss: 0.4683447778224945\n",
      "Validation: Epoch [8], Batch [443/938], Loss: 0.5965262651443481\n",
      "Validation: Epoch [8], Batch [444/938], Loss: 0.5164156556129456\n",
      "Validation: Epoch [8], Batch [445/938], Loss: 0.7123072147369385\n",
      "Validation: Epoch [8], Batch [446/938], Loss: 0.7318780422210693\n",
      "Validation: Epoch [8], Batch [447/938], Loss: 0.5333032011985779\n",
      "Validation: Epoch [8], Batch [448/938], Loss: 0.5382694005966187\n",
      "Validation: Epoch [8], Batch [449/938], Loss: 0.8901413083076477\n",
      "Validation: Epoch [8], Batch [450/938], Loss: 0.5472050905227661\n",
      "Validation: Epoch [8], Batch [451/938], Loss: 0.5892174243927002\n",
      "Validation: Epoch [8], Batch [452/938], Loss: 0.5735549926757812\n",
      "Validation: Epoch [8], Batch [453/938], Loss: 0.6097774505615234\n",
      "Validation: Epoch [8], Batch [454/938], Loss: 0.6557985544204712\n",
      "Validation: Epoch [8], Batch [455/938], Loss: 0.5306671857833862\n",
      "Validation: Epoch [8], Batch [456/938], Loss: 0.435741126537323\n",
      "Validation: Epoch [8], Batch [457/938], Loss: 0.4989418387413025\n",
      "Validation: Epoch [8], Batch [458/938], Loss: 0.6579532623291016\n",
      "Validation: Epoch [8], Batch [459/938], Loss: 0.5647891759872437\n",
      "Validation: Epoch [8], Batch [460/938], Loss: 0.5571129322052002\n",
      "Validation: Epoch [8], Batch [461/938], Loss: 0.5103852152824402\n",
      "Validation: Epoch [8], Batch [462/938], Loss: 0.6358556747436523\n",
      "Validation: Epoch [8], Batch [463/938], Loss: 0.5207521915435791\n",
      "Validation: Epoch [8], Batch [464/938], Loss: 0.5221042633056641\n",
      "Validation: Epoch [8], Batch [465/938], Loss: 0.6458336710929871\n",
      "Validation: Epoch [8], Batch [466/938], Loss: 0.7004615068435669\n",
      "Validation: Epoch [8], Batch [467/938], Loss: 0.6566133499145508\n",
      "Validation: Epoch [8], Batch [468/938], Loss: 0.5582436323165894\n",
      "Validation: Epoch [8], Batch [469/938], Loss: 0.6706633567810059\n",
      "Validation: Epoch [8], Batch [470/938], Loss: 0.5158662796020508\n",
      "Validation: Epoch [8], Batch [471/938], Loss: 0.6777657866477966\n",
      "Validation: Epoch [8], Batch [472/938], Loss: 0.6515148282051086\n",
      "Validation: Epoch [8], Batch [473/938], Loss: 0.5016775727272034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [474/938], Loss: 0.6913593411445618\n",
      "Validation: Epoch [8], Batch [475/938], Loss: 0.6633998155593872\n",
      "Validation: Epoch [8], Batch [476/938], Loss: 0.5519220232963562\n",
      "Validation: Epoch [8], Batch [477/938], Loss: 0.9249267578125\n",
      "Validation: Epoch [8], Batch [478/938], Loss: 0.5457233786582947\n",
      "Validation: Epoch [8], Batch [479/938], Loss: 0.7187544107437134\n",
      "Validation: Epoch [8], Batch [480/938], Loss: 0.4504423141479492\n",
      "Validation: Epoch [8], Batch [481/938], Loss: 0.6456845998764038\n",
      "Validation: Epoch [8], Batch [482/938], Loss: 0.49038416147232056\n",
      "Validation: Epoch [8], Batch [483/938], Loss: 0.6484739184379578\n",
      "Validation: Epoch [8], Batch [484/938], Loss: 0.45479458570480347\n",
      "Validation: Epoch [8], Batch [485/938], Loss: 0.549004077911377\n",
      "Validation: Epoch [8], Batch [486/938], Loss: 0.5074906349182129\n",
      "Validation: Epoch [8], Batch [487/938], Loss: 0.6590322852134705\n",
      "Validation: Epoch [8], Batch [488/938], Loss: 0.5171593427658081\n",
      "Validation: Epoch [8], Batch [489/938], Loss: 0.49300265312194824\n",
      "Validation: Epoch [8], Batch [490/938], Loss: 0.5415226221084595\n",
      "Validation: Epoch [8], Batch [491/938], Loss: 0.48490670323371887\n",
      "Validation: Epoch [8], Batch [492/938], Loss: 0.5072003602981567\n",
      "Validation: Epoch [8], Batch [493/938], Loss: 0.5223006010055542\n",
      "Validation: Epoch [8], Batch [494/938], Loss: 0.5012273788452148\n",
      "Validation: Epoch [8], Batch [495/938], Loss: 0.6626002788543701\n",
      "Validation: Epoch [8], Batch [496/938], Loss: 0.6401090621948242\n",
      "Validation: Epoch [8], Batch [497/938], Loss: 0.47176122665405273\n",
      "Validation: Epoch [8], Batch [498/938], Loss: 0.4892420172691345\n",
      "Validation: Epoch [8], Batch [499/938], Loss: 0.6287081241607666\n",
      "Validation: Epoch [8], Batch [500/938], Loss: 0.587714433670044\n",
      "Validation: Epoch [8], Batch [501/938], Loss: 0.5614702701568604\n",
      "Validation: Epoch [8], Batch [502/938], Loss: 0.4720669984817505\n",
      "Validation: Epoch [8], Batch [503/938], Loss: 0.6354964375495911\n",
      "Validation: Epoch [8], Batch [504/938], Loss: 0.5345611572265625\n",
      "Validation: Epoch [8], Batch [505/938], Loss: 0.6945890188217163\n",
      "Validation: Epoch [8], Batch [506/938], Loss: 0.834460973739624\n",
      "Validation: Epoch [8], Batch [507/938], Loss: 0.4350132346153259\n",
      "Validation: Epoch [8], Batch [508/938], Loss: 0.5979788303375244\n",
      "Validation: Epoch [8], Batch [509/938], Loss: 0.5181201100349426\n",
      "Validation: Epoch [8], Batch [510/938], Loss: 0.5426434278488159\n",
      "Validation: Epoch [8], Batch [511/938], Loss: 0.5421732664108276\n",
      "Validation: Epoch [8], Batch [512/938], Loss: 0.6796131134033203\n",
      "Validation: Epoch [8], Batch [513/938], Loss: 0.7771321535110474\n",
      "Validation: Epoch [8], Batch [514/938], Loss: 0.559164822101593\n",
      "Validation: Epoch [8], Batch [515/938], Loss: 0.7702598571777344\n",
      "Validation: Epoch [8], Batch [516/938], Loss: 0.6553856134414673\n",
      "Validation: Epoch [8], Batch [517/938], Loss: 0.5283974409103394\n",
      "Validation: Epoch [8], Batch [518/938], Loss: 0.671383261680603\n",
      "Validation: Epoch [8], Batch [519/938], Loss: 0.45736807584762573\n",
      "Validation: Epoch [8], Batch [520/938], Loss: 0.6809730529785156\n",
      "Validation: Epoch [8], Batch [521/938], Loss: 0.6242949962615967\n",
      "Validation: Epoch [8], Batch [522/938], Loss: 0.46667176485061646\n",
      "Validation: Epoch [8], Batch [523/938], Loss: 0.521284282207489\n",
      "Validation: Epoch [8], Batch [524/938], Loss: 0.5153704881668091\n",
      "Validation: Epoch [8], Batch [525/938], Loss: 0.8047739267349243\n",
      "Validation: Epoch [8], Batch [526/938], Loss: 0.4647657871246338\n",
      "Validation: Epoch [8], Batch [527/938], Loss: 0.38683027029037476\n",
      "Validation: Epoch [8], Batch [528/938], Loss: 0.4724667966365814\n",
      "Validation: Epoch [8], Batch [529/938], Loss: 0.5345080494880676\n",
      "Validation: Epoch [8], Batch [530/938], Loss: 0.6881290078163147\n",
      "Validation: Epoch [8], Batch [531/938], Loss: 0.5592741966247559\n",
      "Validation: Epoch [8], Batch [532/938], Loss: 0.4768673777580261\n",
      "Validation: Epoch [8], Batch [533/938], Loss: 0.532233476638794\n",
      "Validation: Epoch [8], Batch [534/938], Loss: 0.5973665714263916\n",
      "Validation: Epoch [8], Batch [535/938], Loss: 0.5057551264762878\n",
      "Validation: Epoch [8], Batch [536/938], Loss: 0.7084741592407227\n",
      "Validation: Epoch [8], Batch [537/938], Loss: 0.691943883895874\n",
      "Validation: Epoch [8], Batch [538/938], Loss: 0.5295711755752563\n",
      "Validation: Epoch [8], Batch [539/938], Loss: 0.5990475416183472\n",
      "Validation: Epoch [8], Batch [540/938], Loss: 0.6031070947647095\n",
      "Validation: Epoch [8], Batch [541/938], Loss: 0.4869304895401001\n",
      "Validation: Epoch [8], Batch [542/938], Loss: 0.3620496392250061\n",
      "Validation: Epoch [8], Batch [543/938], Loss: 0.4998316764831543\n",
      "Validation: Epoch [8], Batch [544/938], Loss: 0.6408147215843201\n",
      "Validation: Epoch [8], Batch [545/938], Loss: 0.44905543327331543\n",
      "Validation: Epoch [8], Batch [546/938], Loss: 0.4755421280860901\n",
      "Validation: Epoch [8], Batch [547/938], Loss: 0.42019134759902954\n",
      "Validation: Epoch [8], Batch [548/938], Loss: 0.7539182305335999\n",
      "Validation: Epoch [8], Batch [549/938], Loss: 0.4485844373703003\n",
      "Validation: Epoch [8], Batch [550/938], Loss: 0.6576862335205078\n",
      "Validation: Epoch [8], Batch [551/938], Loss: 0.4757923483848572\n",
      "Validation: Epoch [8], Batch [552/938], Loss: 0.48515650629997253\n",
      "Validation: Epoch [8], Batch [553/938], Loss: 0.5364527702331543\n",
      "Validation: Epoch [8], Batch [554/938], Loss: 0.6028779745101929\n",
      "Validation: Epoch [8], Batch [555/938], Loss: 0.37500283122062683\n",
      "Validation: Epoch [8], Batch [556/938], Loss: 0.4713570177555084\n",
      "Validation: Epoch [8], Batch [557/938], Loss: 0.5868799686431885\n",
      "Validation: Epoch [8], Batch [558/938], Loss: 0.6795644164085388\n",
      "Validation: Epoch [8], Batch [559/938], Loss: 0.7665470242500305\n",
      "Validation: Epoch [8], Batch [560/938], Loss: 0.6799064874649048\n",
      "Validation: Epoch [8], Batch [561/938], Loss: 0.5673831701278687\n",
      "Validation: Epoch [8], Batch [562/938], Loss: 0.5940479636192322\n",
      "Validation: Epoch [8], Batch [563/938], Loss: 0.5986023545265198\n",
      "Validation: Epoch [8], Batch [564/938], Loss: 0.3937150239944458\n",
      "Validation: Epoch [8], Batch [565/938], Loss: 0.5759063959121704\n",
      "Validation: Epoch [8], Batch [566/938], Loss: 0.331765353679657\n",
      "Validation: Epoch [8], Batch [567/938], Loss: 0.6441683769226074\n",
      "Validation: Epoch [8], Batch [568/938], Loss: 0.4824296236038208\n",
      "Validation: Epoch [8], Batch [569/938], Loss: 0.5618000030517578\n",
      "Validation: Epoch [8], Batch [570/938], Loss: 0.6578497886657715\n",
      "Validation: Epoch [8], Batch [571/938], Loss: 0.5773949027061462\n",
      "Validation: Epoch [8], Batch [572/938], Loss: 0.49800586700439453\n",
      "Validation: Epoch [8], Batch [573/938], Loss: 0.5795585513114929\n",
      "Validation: Epoch [8], Batch [574/938], Loss: 0.6301325559616089\n",
      "Validation: Epoch [8], Batch [575/938], Loss: 0.4877678453922272\n",
      "Validation: Epoch [8], Batch [576/938], Loss: 0.823143720626831\n",
      "Validation: Epoch [8], Batch [577/938], Loss: 0.5467069745063782\n",
      "Validation: Epoch [8], Batch [578/938], Loss: 0.6000856161117554\n",
      "Validation: Epoch [8], Batch [579/938], Loss: 0.5729097723960876\n",
      "Validation: Epoch [8], Batch [580/938], Loss: 0.5982973575592041\n",
      "Validation: Epoch [8], Batch [581/938], Loss: 0.5383985638618469\n",
      "Validation: Epoch [8], Batch [582/938], Loss: 0.6582106351852417\n",
      "Validation: Epoch [8], Batch [583/938], Loss: 0.8491129279136658\n",
      "Validation: Epoch [8], Batch [584/938], Loss: 0.48481786251068115\n",
      "Validation: Epoch [8], Batch [585/938], Loss: 0.47131645679473877\n",
      "Validation: Epoch [8], Batch [586/938], Loss: 0.4686542749404907\n",
      "Validation: Epoch [8], Batch [587/938], Loss: 0.7151999473571777\n",
      "Validation: Epoch [8], Batch [588/938], Loss: 0.5083309412002563\n",
      "Validation: Epoch [8], Batch [589/938], Loss: 0.5360294580459595\n",
      "Validation: Epoch [8], Batch [590/938], Loss: 0.4565085768699646\n",
      "Validation: Epoch [8], Batch [591/938], Loss: 0.6517032980918884\n",
      "Validation: Epoch [8], Batch [592/938], Loss: 0.4853042960166931\n",
      "Validation: Epoch [8], Batch [593/938], Loss: 0.49093544483184814\n",
      "Validation: Epoch [8], Batch [594/938], Loss: 0.4846231937408447\n",
      "Validation: Epoch [8], Batch [595/938], Loss: 0.3522394597530365\n",
      "Validation: Epoch [8], Batch [596/938], Loss: 0.4897482693195343\n",
      "Validation: Epoch [8], Batch [597/938], Loss: 0.7342388033866882\n",
      "Validation: Epoch [8], Batch [598/938], Loss: 0.5090926885604858\n",
      "Validation: Epoch [8], Batch [599/938], Loss: 0.508766770362854\n",
      "Validation: Epoch [8], Batch [600/938], Loss: 0.7030952572822571\n",
      "Validation: Epoch [8], Batch [601/938], Loss: 0.45935508608818054\n",
      "Validation: Epoch [8], Batch [602/938], Loss: 0.3919302225112915\n",
      "Validation: Epoch [8], Batch [603/938], Loss: 0.543899416923523\n",
      "Validation: Epoch [8], Batch [604/938], Loss: 0.7067248821258545\n",
      "Validation: Epoch [8], Batch [605/938], Loss: 0.46889790892601013\n",
      "Validation: Epoch [8], Batch [606/938], Loss: 0.4899073541164398\n",
      "Validation: Epoch [8], Batch [607/938], Loss: 0.633062481880188\n",
      "Validation: Epoch [8], Batch [608/938], Loss: 0.5730429887771606\n",
      "Validation: Epoch [8], Batch [609/938], Loss: 0.7551171779632568\n",
      "Validation: Epoch [8], Batch [610/938], Loss: 0.6980631351470947\n",
      "Validation: Epoch [8], Batch [611/938], Loss: 0.45919281244277954\n",
      "Validation: Epoch [8], Batch [612/938], Loss: 0.5003436207771301\n",
      "Validation: Epoch [8], Batch [613/938], Loss: 0.5782647132873535\n",
      "Validation: Epoch [8], Batch [614/938], Loss: 0.4984017014503479\n",
      "Validation: Epoch [8], Batch [615/938], Loss: 0.7067341804504395\n",
      "Validation: Epoch [8], Batch [616/938], Loss: 0.6568933725357056\n",
      "Validation: Epoch [8], Batch [617/938], Loss: 0.6073590517044067\n",
      "Validation: Epoch [8], Batch [618/938], Loss: 0.4563160240650177\n",
      "Validation: Epoch [8], Batch [619/938], Loss: 0.48643532395362854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [620/938], Loss: 0.5347998738288879\n",
      "Validation: Epoch [8], Batch [621/938], Loss: 0.5374981760978699\n",
      "Validation: Epoch [8], Batch [622/938], Loss: 0.6086175441741943\n",
      "Validation: Epoch [8], Batch [623/938], Loss: 0.5173768997192383\n",
      "Validation: Epoch [8], Batch [624/938], Loss: 0.5719034671783447\n",
      "Validation: Epoch [8], Batch [625/938], Loss: 0.5644528269767761\n",
      "Validation: Epoch [8], Batch [626/938], Loss: 0.494992733001709\n",
      "Validation: Epoch [8], Batch [627/938], Loss: 0.469891220331192\n",
      "Validation: Epoch [8], Batch [628/938], Loss: 0.41165000200271606\n",
      "Validation: Epoch [8], Batch [629/938], Loss: 0.5856794118881226\n",
      "Validation: Epoch [8], Batch [630/938], Loss: 0.83735591173172\n",
      "Validation: Epoch [8], Batch [631/938], Loss: 0.6350385546684265\n",
      "Validation: Epoch [8], Batch [632/938], Loss: 0.6201947927474976\n",
      "Validation: Epoch [8], Batch [633/938], Loss: 0.5042991638183594\n",
      "Validation: Epoch [8], Batch [634/938], Loss: 0.6386717557907104\n",
      "Validation: Epoch [8], Batch [635/938], Loss: 0.6868661046028137\n",
      "Validation: Epoch [8], Batch [636/938], Loss: 0.5687553882598877\n",
      "Validation: Epoch [8], Batch [637/938], Loss: 0.4427611529827118\n",
      "Validation: Epoch [8], Batch [638/938], Loss: 0.4365793466567993\n",
      "Validation: Epoch [8], Batch [639/938], Loss: 0.6917381882667542\n",
      "Validation: Epoch [8], Batch [640/938], Loss: 0.5337842702865601\n",
      "Validation: Epoch [8], Batch [641/938], Loss: 0.47849079966545105\n",
      "Validation: Epoch [8], Batch [642/938], Loss: 0.5159565806388855\n",
      "Validation: Epoch [8], Batch [643/938], Loss: 0.5994929671287537\n",
      "Validation: Epoch [8], Batch [644/938], Loss: 0.6954755783081055\n",
      "Validation: Epoch [8], Batch [645/938], Loss: 0.8674951195716858\n",
      "Validation: Epoch [8], Batch [646/938], Loss: 0.5233336687088013\n",
      "Validation: Epoch [8], Batch [647/938], Loss: 0.5118962526321411\n",
      "Validation: Epoch [8], Batch [648/938], Loss: 0.9183385372161865\n",
      "Validation: Epoch [8], Batch [649/938], Loss: 0.7714497447013855\n",
      "Validation: Epoch [8], Batch [650/938], Loss: 0.49073362350463867\n",
      "Validation: Epoch [8], Batch [651/938], Loss: 0.6792895793914795\n",
      "Validation: Epoch [8], Batch [652/938], Loss: 0.6410203576087952\n",
      "Validation: Epoch [8], Batch [653/938], Loss: 0.751700758934021\n",
      "Validation: Epoch [8], Batch [654/938], Loss: 0.5156040191650391\n",
      "Validation: Epoch [8], Batch [655/938], Loss: 0.31671541929244995\n",
      "Validation: Epoch [8], Batch [656/938], Loss: 0.5387499332427979\n",
      "Validation: Epoch [8], Batch [657/938], Loss: 0.6217941641807556\n",
      "Validation: Epoch [8], Batch [658/938], Loss: 0.5054097175598145\n",
      "Validation: Epoch [8], Batch [659/938], Loss: 0.5035549402236938\n",
      "Validation: Epoch [8], Batch [660/938], Loss: 0.5307859182357788\n",
      "Validation: Epoch [8], Batch [661/938], Loss: 0.45784860849380493\n",
      "Validation: Epoch [8], Batch [662/938], Loss: 0.9361579418182373\n",
      "Validation: Epoch [8], Batch [663/938], Loss: 0.5343468189239502\n",
      "Validation: Epoch [8], Batch [664/938], Loss: 0.6233961582183838\n",
      "Validation: Epoch [8], Batch [665/938], Loss: 0.5989638566970825\n",
      "Validation: Epoch [8], Batch [666/938], Loss: 0.6635607481002808\n",
      "Validation: Epoch [8], Batch [667/938], Loss: 0.4984554350376129\n",
      "Validation: Epoch [8], Batch [668/938], Loss: 0.49599573016166687\n",
      "Validation: Epoch [8], Batch [669/938], Loss: 0.9432331323623657\n",
      "Validation: Epoch [8], Batch [670/938], Loss: 0.5349069237709045\n",
      "Validation: Epoch [8], Batch [671/938], Loss: 0.6500830054283142\n",
      "Validation: Epoch [8], Batch [672/938], Loss: 0.5940930843353271\n",
      "Validation: Epoch [8], Batch [673/938], Loss: 0.5556986331939697\n",
      "Validation: Epoch [8], Batch [674/938], Loss: 0.6378294229507446\n",
      "Validation: Epoch [8], Batch [675/938], Loss: 0.4461559057235718\n",
      "Validation: Epoch [8], Batch [676/938], Loss: 0.6334984302520752\n",
      "Validation: Epoch [8], Batch [677/938], Loss: 0.4432985484600067\n",
      "Validation: Epoch [8], Batch [678/938], Loss: 0.7386610507965088\n",
      "Validation: Epoch [8], Batch [679/938], Loss: 0.5734220147132874\n",
      "Validation: Epoch [8], Batch [680/938], Loss: 0.5960986614227295\n",
      "Validation: Epoch [8], Batch [681/938], Loss: 0.7579212188720703\n",
      "Validation: Epoch [8], Batch [682/938], Loss: 0.5191344022750854\n",
      "Validation: Epoch [8], Batch [683/938], Loss: 0.4898754954338074\n",
      "Validation: Epoch [8], Batch [684/938], Loss: 0.42299890518188477\n",
      "Validation: Epoch [8], Batch [685/938], Loss: 0.425224632024765\n",
      "Validation: Epoch [8], Batch [686/938], Loss: 0.6343479752540588\n",
      "Validation: Epoch [8], Batch [687/938], Loss: 0.6880210638046265\n",
      "Validation: Epoch [8], Batch [688/938], Loss: 0.8763443231582642\n",
      "Validation: Epoch [8], Batch [689/938], Loss: 0.5569975972175598\n",
      "Validation: Epoch [8], Batch [690/938], Loss: 0.4810643196105957\n",
      "Validation: Epoch [8], Batch [691/938], Loss: 0.797878086566925\n",
      "Validation: Epoch [8], Batch [692/938], Loss: 0.6082624197006226\n",
      "Validation: Epoch [8], Batch [693/938], Loss: 0.5992671251296997\n",
      "Validation: Epoch [8], Batch [694/938], Loss: 0.4308776557445526\n",
      "Validation: Epoch [8], Batch [695/938], Loss: 0.6750783920288086\n",
      "Validation: Epoch [8], Batch [696/938], Loss: 0.9718018770217896\n",
      "Validation: Epoch [8], Batch [697/938], Loss: 0.45487743616104126\n",
      "Validation: Epoch [8], Batch [698/938], Loss: 0.766661524772644\n",
      "Validation: Epoch [8], Batch [699/938], Loss: 0.4730873703956604\n",
      "Validation: Epoch [8], Batch [700/938], Loss: 0.47962188720703125\n",
      "Validation: Epoch [8], Batch [701/938], Loss: 0.7987780570983887\n",
      "Validation: Epoch [8], Batch [702/938], Loss: 0.4843154847621918\n",
      "Validation: Epoch [8], Batch [703/938], Loss: 0.6056915521621704\n",
      "Validation: Epoch [8], Batch [704/938], Loss: 0.6678804755210876\n",
      "Validation: Epoch [8], Batch [705/938], Loss: 0.7077317237854004\n",
      "Validation: Epoch [8], Batch [706/938], Loss: 0.31581783294677734\n",
      "Validation: Epoch [8], Batch [707/938], Loss: 0.4550410509109497\n",
      "Validation: Epoch [8], Batch [708/938], Loss: 0.5083768367767334\n",
      "Validation: Epoch [8], Batch [709/938], Loss: 0.6907610893249512\n",
      "Validation: Epoch [8], Batch [710/938], Loss: 0.6767743825912476\n",
      "Validation: Epoch [8], Batch [711/938], Loss: 0.4846719801425934\n",
      "Validation: Epoch [8], Batch [712/938], Loss: 0.654088020324707\n",
      "Validation: Epoch [8], Batch [713/938], Loss: 0.6440149545669556\n",
      "Validation: Epoch [8], Batch [714/938], Loss: 0.3586040139198303\n",
      "Validation: Epoch [8], Batch [715/938], Loss: 0.4128858149051666\n",
      "Validation: Epoch [8], Batch [716/938], Loss: 0.7240349650382996\n",
      "Validation: Epoch [8], Batch [717/938], Loss: 0.7087463140487671\n",
      "Validation: Epoch [8], Batch [718/938], Loss: 0.46900174021720886\n",
      "Validation: Epoch [8], Batch [719/938], Loss: 0.743706226348877\n",
      "Validation: Epoch [8], Batch [720/938], Loss: 0.6197866201400757\n",
      "Validation: Epoch [8], Batch [721/938], Loss: 0.5579239130020142\n",
      "Validation: Epoch [8], Batch [722/938], Loss: 0.4488386809825897\n",
      "Validation: Epoch [8], Batch [723/938], Loss: 0.520048975944519\n",
      "Validation: Epoch [8], Batch [724/938], Loss: 0.4826296865940094\n",
      "Validation: Epoch [8], Batch [725/938], Loss: 0.5447655916213989\n",
      "Validation: Epoch [8], Batch [726/938], Loss: 0.6570760011672974\n",
      "Validation: Epoch [8], Batch [727/938], Loss: 0.761082649230957\n",
      "Validation: Epoch [8], Batch [728/938], Loss: 0.5215544700622559\n",
      "Validation: Epoch [8], Batch [729/938], Loss: 0.5866730809211731\n",
      "Validation: Epoch [8], Batch [730/938], Loss: 0.6018748879432678\n",
      "Validation: Epoch [8], Batch [731/938], Loss: 0.5056033134460449\n",
      "Validation: Epoch [8], Batch [732/938], Loss: 0.7136939167976379\n",
      "Validation: Epoch [8], Batch [733/938], Loss: 0.5002223253250122\n",
      "Validation: Epoch [8], Batch [734/938], Loss: 0.664855420589447\n",
      "Validation: Epoch [8], Batch [735/938], Loss: 0.5084612369537354\n",
      "Validation: Epoch [8], Batch [736/938], Loss: 0.5802547931671143\n",
      "Validation: Epoch [8], Batch [737/938], Loss: 0.6216850280761719\n",
      "Validation: Epoch [8], Batch [738/938], Loss: 0.5562002062797546\n",
      "Validation: Epoch [8], Batch [739/938], Loss: 0.6097134351730347\n",
      "Validation: Epoch [8], Batch [740/938], Loss: 0.7143648862838745\n",
      "Validation: Epoch [8], Batch [741/938], Loss: 0.643607497215271\n",
      "Validation: Epoch [8], Batch [742/938], Loss: 0.6252757906913757\n",
      "Validation: Epoch [8], Batch [743/938], Loss: 0.8648301362991333\n",
      "Validation: Epoch [8], Batch [744/938], Loss: 0.4959721565246582\n",
      "Validation: Epoch [8], Batch [745/938], Loss: 0.44202256202697754\n",
      "Validation: Epoch [8], Batch [746/938], Loss: 0.4196798503398895\n",
      "Validation: Epoch [8], Batch [747/938], Loss: 0.9024176597595215\n",
      "Validation: Epoch [8], Batch [748/938], Loss: 0.3685716986656189\n",
      "Validation: Epoch [8], Batch [749/938], Loss: 0.6472551822662354\n",
      "Validation: Epoch [8], Batch [750/938], Loss: 0.6440014839172363\n",
      "Validation: Epoch [8], Batch [751/938], Loss: 0.8163616061210632\n",
      "Validation: Epoch [8], Batch [752/938], Loss: 0.6475611925125122\n",
      "Validation: Epoch [8], Batch [753/938], Loss: 0.5296918749809265\n",
      "Validation: Epoch [8], Batch [754/938], Loss: 0.5855975151062012\n",
      "Validation: Epoch [8], Batch [755/938], Loss: 0.3881375193595886\n",
      "Validation: Epoch [8], Batch [756/938], Loss: 0.5888597965240479\n",
      "Validation: Epoch [8], Batch [757/938], Loss: 0.44097957015037537\n",
      "Validation: Epoch [8], Batch [758/938], Loss: 0.529850959777832\n",
      "Validation: Epoch [8], Batch [759/938], Loss: 0.5816670656204224\n",
      "Validation: Epoch [8], Batch [760/938], Loss: 0.5151940584182739\n",
      "Validation: Epoch [8], Batch [761/938], Loss: 0.7078592777252197\n",
      "Validation: Epoch [8], Batch [762/938], Loss: 0.5527603030204773\n",
      "Validation: Epoch [8], Batch [763/938], Loss: 0.41604432463645935\n",
      "Validation: Epoch [8], Batch [764/938], Loss: 0.6134563684463501\n",
      "Validation: Epoch [8], Batch [765/938], Loss: 0.5882766842842102\n",
      "Validation: Epoch [8], Batch [766/938], Loss: 0.508165717124939\n",
      "Validation: Epoch [8], Batch [767/938], Loss: 0.5011162757873535\n",
      "Validation: Epoch [8], Batch [768/938], Loss: 0.7076554298400879\n",
      "Validation: Epoch [8], Batch [769/938], Loss: 0.5947992205619812\n",
      "Validation: Epoch [8], Batch [770/938], Loss: 0.6435232162475586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [771/938], Loss: 0.3594966530799866\n",
      "Validation: Epoch [8], Batch [772/938], Loss: 0.4819485545158386\n",
      "Validation: Epoch [8], Batch [773/938], Loss: 0.7133779525756836\n",
      "Validation: Epoch [8], Batch [774/938], Loss: 0.3738945722579956\n",
      "Validation: Epoch [8], Batch [775/938], Loss: 0.6537107229232788\n",
      "Validation: Epoch [8], Batch [776/938], Loss: 0.726710319519043\n",
      "Validation: Epoch [8], Batch [777/938], Loss: 0.4674813747406006\n",
      "Validation: Epoch [8], Batch [778/938], Loss: 0.6292926073074341\n",
      "Validation: Epoch [8], Batch [779/938], Loss: 0.5631769895553589\n",
      "Validation: Epoch [8], Batch [780/938], Loss: 0.4661337733268738\n",
      "Validation: Epoch [8], Batch [781/938], Loss: 0.5402955412864685\n",
      "Validation: Epoch [8], Batch [782/938], Loss: 0.6071621179580688\n",
      "Validation: Epoch [8], Batch [783/938], Loss: 0.4206739664077759\n",
      "Validation: Epoch [8], Batch [784/938], Loss: 0.8716151118278503\n",
      "Validation: Epoch [8], Batch [785/938], Loss: 0.426334410905838\n",
      "Validation: Epoch [8], Batch [786/938], Loss: 0.4652208685874939\n",
      "Validation: Epoch [8], Batch [787/938], Loss: 0.5473117232322693\n",
      "Validation: Epoch [8], Batch [788/938], Loss: 0.4297904968261719\n",
      "Validation: Epoch [8], Batch [789/938], Loss: 0.6168794631958008\n",
      "Validation: Epoch [8], Batch [790/938], Loss: 0.6316878795623779\n",
      "Validation: Epoch [8], Batch [791/938], Loss: 0.41400012373924255\n",
      "Validation: Epoch [8], Batch [792/938], Loss: 0.6268401145935059\n",
      "Validation: Epoch [8], Batch [793/938], Loss: 0.7036058306694031\n",
      "Validation: Epoch [8], Batch [794/938], Loss: 0.6006869077682495\n",
      "Validation: Epoch [8], Batch [795/938], Loss: 0.7376806139945984\n",
      "Validation: Epoch [8], Batch [796/938], Loss: 0.5330274105072021\n",
      "Validation: Epoch [8], Batch [797/938], Loss: 0.5762564539909363\n",
      "Validation: Epoch [8], Batch [798/938], Loss: 0.36532583832740784\n",
      "Validation: Epoch [8], Batch [799/938], Loss: 0.5810272097587585\n",
      "Validation: Epoch [8], Batch [800/938], Loss: 0.5737448334693909\n",
      "Validation: Epoch [8], Batch [801/938], Loss: 0.39031484723091125\n",
      "Validation: Epoch [8], Batch [802/938], Loss: 0.5963318347930908\n",
      "Validation: Epoch [8], Batch [803/938], Loss: 0.7171988487243652\n",
      "Validation: Epoch [8], Batch [804/938], Loss: 0.908906877040863\n",
      "Validation: Epoch [8], Batch [805/938], Loss: 0.7031044960021973\n",
      "Validation: Epoch [8], Batch [806/938], Loss: 0.5439556837081909\n",
      "Validation: Epoch [8], Batch [807/938], Loss: 0.7010362148284912\n",
      "Validation: Epoch [8], Batch [808/938], Loss: 0.5471799373626709\n",
      "Validation: Epoch [8], Batch [809/938], Loss: 0.41455790400505066\n",
      "Validation: Epoch [8], Batch [810/938], Loss: 0.4552091360092163\n",
      "Validation: Epoch [8], Batch [811/938], Loss: 0.545586347579956\n",
      "Validation: Epoch [8], Batch [812/938], Loss: 0.5993293523788452\n",
      "Validation: Epoch [8], Batch [813/938], Loss: 0.5206112861633301\n",
      "Validation: Epoch [8], Batch [814/938], Loss: 0.4814175069332123\n",
      "Validation: Epoch [8], Batch [815/938], Loss: 0.44302645325660706\n",
      "Validation: Epoch [8], Batch [816/938], Loss: 0.5682935118675232\n",
      "Validation: Epoch [8], Batch [817/938], Loss: 0.7784931659698486\n",
      "Validation: Epoch [8], Batch [818/938], Loss: 0.43533462285995483\n",
      "Validation: Epoch [8], Batch [819/938], Loss: 0.43116018176078796\n",
      "Validation: Epoch [8], Batch [820/938], Loss: 0.543062686920166\n",
      "Validation: Epoch [8], Batch [821/938], Loss: 0.6214845180511475\n",
      "Validation: Epoch [8], Batch [822/938], Loss: 0.6034417152404785\n",
      "Validation: Epoch [8], Batch [823/938], Loss: 0.48924142122268677\n",
      "Validation: Epoch [8], Batch [824/938], Loss: 0.586390495300293\n",
      "Validation: Epoch [8], Batch [825/938], Loss: 0.5180339813232422\n",
      "Validation: Epoch [8], Batch [826/938], Loss: 0.6554176807403564\n",
      "Validation: Epoch [8], Batch [827/938], Loss: 0.6554067134857178\n",
      "Validation: Epoch [8], Batch [828/938], Loss: 0.618136465549469\n",
      "Validation: Epoch [8], Batch [829/938], Loss: 0.5290961861610413\n",
      "Validation: Epoch [8], Batch [830/938], Loss: 0.5217894911766052\n",
      "Validation: Epoch [8], Batch [831/938], Loss: 0.4645056128501892\n",
      "Validation: Epoch [8], Batch [832/938], Loss: 0.5961268544197083\n",
      "Validation: Epoch [8], Batch [833/938], Loss: 0.5604337453842163\n",
      "Validation: Epoch [8], Batch [834/938], Loss: 0.6362249255180359\n",
      "Validation: Epoch [8], Batch [835/938], Loss: 0.38016295433044434\n",
      "Validation: Epoch [8], Batch [836/938], Loss: 0.6710458993911743\n",
      "Validation: Epoch [8], Batch [837/938], Loss: 0.7847074270248413\n",
      "Validation: Epoch [8], Batch [838/938], Loss: 0.7122353315353394\n",
      "Validation: Epoch [8], Batch [839/938], Loss: 0.48217839002609253\n",
      "Validation: Epoch [8], Batch [840/938], Loss: 0.49520987272262573\n",
      "Validation: Epoch [8], Batch [841/938], Loss: 0.5130150318145752\n",
      "Validation: Epoch [8], Batch [842/938], Loss: 0.4558557868003845\n",
      "Validation: Epoch [8], Batch [843/938], Loss: 0.5394407510757446\n",
      "Validation: Epoch [8], Batch [844/938], Loss: 0.35244959592819214\n",
      "Validation: Epoch [8], Batch [845/938], Loss: 0.45883628726005554\n",
      "Validation: Epoch [8], Batch [846/938], Loss: 0.5408573150634766\n",
      "Validation: Epoch [8], Batch [847/938], Loss: 0.5948362350463867\n",
      "Validation: Epoch [8], Batch [848/938], Loss: 0.6239697933197021\n",
      "Validation: Epoch [8], Batch [849/938], Loss: 0.4625145196914673\n",
      "Validation: Epoch [8], Batch [850/938], Loss: 0.6279805898666382\n",
      "Validation: Epoch [8], Batch [851/938], Loss: 0.7343274354934692\n",
      "Validation: Epoch [8], Batch [852/938], Loss: 0.7638819813728333\n",
      "Validation: Epoch [8], Batch [853/938], Loss: 0.6016133427619934\n",
      "Validation: Epoch [8], Batch [854/938], Loss: 0.779116153717041\n",
      "Validation: Epoch [8], Batch [855/938], Loss: 0.580432653427124\n",
      "Validation: Epoch [8], Batch [856/938], Loss: 0.6742399334907532\n",
      "Validation: Epoch [8], Batch [857/938], Loss: 0.5195229053497314\n",
      "Validation: Epoch [8], Batch [858/938], Loss: 0.5502065420150757\n",
      "Validation: Epoch [8], Batch [859/938], Loss: 0.5415719151496887\n",
      "Validation: Epoch [8], Batch [860/938], Loss: 0.857814610004425\n",
      "Validation: Epoch [8], Batch [861/938], Loss: 0.4960060715675354\n",
      "Validation: Epoch [8], Batch [862/938], Loss: 0.5530043244361877\n",
      "Validation: Epoch [8], Batch [863/938], Loss: 0.735530436038971\n",
      "Validation: Epoch [8], Batch [864/938], Loss: 0.5265636444091797\n",
      "Validation: Epoch [8], Batch [865/938], Loss: 0.5670233964920044\n",
      "Validation: Epoch [8], Batch [866/938], Loss: 0.8462004065513611\n",
      "Validation: Epoch [8], Batch [867/938], Loss: 0.773452639579773\n",
      "Validation: Epoch [8], Batch [868/938], Loss: 0.8114327192306519\n",
      "Validation: Epoch [8], Batch [869/938], Loss: 0.5157729387283325\n",
      "Validation: Epoch [8], Batch [870/938], Loss: 0.4513254463672638\n",
      "Validation: Epoch [8], Batch [871/938], Loss: 0.5496336817741394\n",
      "Validation: Epoch [8], Batch [872/938], Loss: 0.5631890296936035\n",
      "Validation: Epoch [8], Batch [873/938], Loss: 0.4520484507083893\n",
      "Validation: Epoch [8], Batch [874/938], Loss: 0.6782499551773071\n",
      "Validation: Epoch [8], Batch [875/938], Loss: 0.5077191591262817\n",
      "Validation: Epoch [8], Batch [876/938], Loss: 0.4324186444282532\n",
      "Validation: Epoch [8], Batch [877/938], Loss: 0.4782291650772095\n",
      "Validation: Epoch [8], Batch [878/938], Loss: 0.5552007555961609\n",
      "Validation: Epoch [8], Batch [879/938], Loss: 0.48643556237220764\n",
      "Validation: Epoch [8], Batch [880/938], Loss: 0.5560123920440674\n",
      "Validation: Epoch [8], Batch [881/938], Loss: 0.7492592334747314\n",
      "Validation: Epoch [8], Batch [882/938], Loss: 0.7683314681053162\n",
      "Validation: Epoch [8], Batch [883/938], Loss: 0.38014036417007446\n",
      "Validation: Epoch [8], Batch [884/938], Loss: 0.3910888433456421\n",
      "Validation: Epoch [8], Batch [885/938], Loss: 0.6883646845817566\n",
      "Validation: Epoch [8], Batch [886/938], Loss: 0.5431157350540161\n",
      "Validation: Epoch [8], Batch [887/938], Loss: 0.44746360182762146\n",
      "Validation: Epoch [8], Batch [888/938], Loss: 0.45712152123451233\n",
      "Validation: Epoch [8], Batch [889/938], Loss: 0.4775678217411041\n",
      "Validation: Epoch [8], Batch [890/938], Loss: 0.4792558252811432\n",
      "Validation: Epoch [8], Batch [891/938], Loss: 0.6592015027999878\n",
      "Validation: Epoch [8], Batch [892/938], Loss: 0.5512688159942627\n",
      "Validation: Epoch [8], Batch [893/938], Loss: 0.7258924245834351\n",
      "Validation: Epoch [8], Batch [894/938], Loss: 0.45916178822517395\n",
      "Validation: Epoch [8], Batch [895/938], Loss: 0.4877614676952362\n",
      "Validation: Epoch [8], Batch [896/938], Loss: 0.7506985068321228\n",
      "Validation: Epoch [8], Batch [897/938], Loss: 0.6328930854797363\n",
      "Validation: Epoch [8], Batch [898/938], Loss: 0.5463029146194458\n",
      "Validation: Epoch [8], Batch [899/938], Loss: 0.6069589853286743\n",
      "Validation: Epoch [8], Batch [900/938], Loss: 0.5398380160331726\n",
      "Validation: Epoch [8], Batch [901/938], Loss: 0.5428804159164429\n",
      "Validation: Epoch [8], Batch [902/938], Loss: 0.590213418006897\n",
      "Validation: Epoch [8], Batch [903/938], Loss: 0.48587101697921753\n",
      "Validation: Epoch [8], Batch [904/938], Loss: 0.5535954236984253\n",
      "Validation: Epoch [8], Batch [905/938], Loss: 0.832419753074646\n",
      "Validation: Epoch [8], Batch [906/938], Loss: 0.6125815510749817\n",
      "Validation: Epoch [8], Batch [907/938], Loss: 0.5083963871002197\n",
      "Validation: Epoch [8], Batch [908/938], Loss: 0.5471615791320801\n",
      "Validation: Epoch [8], Batch [909/938], Loss: 0.40710318088531494\n",
      "Validation: Epoch [8], Batch [910/938], Loss: 0.4368949234485626\n",
      "Validation: Epoch [8], Batch [911/938], Loss: 0.5238549113273621\n",
      "Validation: Epoch [8], Batch [912/938], Loss: 0.590180516242981\n",
      "Validation: Epoch [8], Batch [913/938], Loss: 0.5242639780044556\n",
      "Validation: Epoch [8], Batch [914/938], Loss: 0.7415425777435303\n",
      "Validation: Epoch [8], Batch [915/938], Loss: 0.5960053205490112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [8], Batch [916/938], Loss: 0.5908908247947693\n",
      "Validation: Epoch [8], Batch [917/938], Loss: 0.6116880178451538\n",
      "Validation: Epoch [8], Batch [918/938], Loss: 0.5679460763931274\n",
      "Validation: Epoch [8], Batch [919/938], Loss: 0.5856536626815796\n",
      "Validation: Epoch [8], Batch [920/938], Loss: 0.4809558689594269\n",
      "Validation: Epoch [8], Batch [921/938], Loss: 0.8716312646865845\n",
      "Validation: Epoch [8], Batch [922/938], Loss: 0.5878221988677979\n",
      "Validation: Epoch [8], Batch [923/938], Loss: 0.495326429605484\n",
      "Validation: Epoch [8], Batch [924/938], Loss: 0.5146664381027222\n",
      "Validation: Epoch [8], Batch [925/938], Loss: 0.49619558453559875\n",
      "Validation: Epoch [8], Batch [926/938], Loss: 0.6064482927322388\n",
      "Validation: Epoch [8], Batch [927/938], Loss: 0.7032601833343506\n",
      "Validation: Epoch [8], Batch [928/938], Loss: 0.5012686848640442\n",
      "Validation: Epoch [8], Batch [929/938], Loss: 0.2754223942756653\n",
      "Validation: Epoch [8], Batch [930/938], Loss: 0.6060070991516113\n",
      "Validation: Epoch [8], Batch [931/938], Loss: 0.38738834857940674\n",
      "Validation: Epoch [8], Batch [932/938], Loss: 0.6287381649017334\n",
      "Validation: Epoch [8], Batch [933/938], Loss: 0.7215477228164673\n",
      "Validation: Epoch [8], Batch [934/938], Loss: 0.6070716381072998\n",
      "Validation: Epoch [8], Batch [935/938], Loss: 0.5085403919219971\n",
      "Validation: Epoch [8], Batch [936/938], Loss: 0.5909814834594727\n",
      "Validation: Epoch [8], Batch [937/938], Loss: 0.5733004808425903\n",
      "Validation: Epoch [8], Batch [938/938], Loss: 0.9124884605407715\n",
      "Accuracy of test set: 0.7932\n",
      "Train: Epoch [9], Batch [1/938], Loss: 0.5117659568786621\n",
      "Train: Epoch [9], Batch [2/938], Loss: 0.40119773149490356\n",
      "Train: Epoch [9], Batch [3/938], Loss: 0.523781955242157\n",
      "Train: Epoch [9], Batch [4/938], Loss: 0.5009605884552002\n",
      "Train: Epoch [9], Batch [5/938], Loss: 0.6380518674850464\n",
      "Train: Epoch [9], Batch [6/938], Loss: 0.6717087030410767\n",
      "Train: Epoch [9], Batch [7/938], Loss: 0.486771821975708\n",
      "Train: Epoch [9], Batch [8/938], Loss: 0.5534528493881226\n",
      "Train: Epoch [9], Batch [9/938], Loss: 0.549004077911377\n",
      "Train: Epoch [9], Batch [10/938], Loss: 0.5291875600814819\n",
      "Train: Epoch [9], Batch [11/938], Loss: 0.7212624549865723\n",
      "Train: Epoch [9], Batch [12/938], Loss: 0.6052558422088623\n",
      "Train: Epoch [9], Batch [13/938], Loss: 0.591122567653656\n",
      "Train: Epoch [9], Batch [14/938], Loss: 0.5728271007537842\n",
      "Train: Epoch [9], Batch [15/938], Loss: 0.4129512906074524\n",
      "Train: Epoch [9], Batch [16/938], Loss: 0.5418897867202759\n",
      "Train: Epoch [9], Batch [17/938], Loss: 0.6198029518127441\n",
      "Train: Epoch [9], Batch [18/938], Loss: 0.5454005599021912\n",
      "Train: Epoch [9], Batch [19/938], Loss: 0.4480946362018585\n",
      "Train: Epoch [9], Batch [20/938], Loss: 0.64020174741745\n",
      "Train: Epoch [9], Batch [21/938], Loss: 0.47362929582595825\n",
      "Train: Epoch [9], Batch [22/938], Loss: 0.6526790261268616\n",
      "Train: Epoch [9], Batch [23/938], Loss: 0.5574888586997986\n",
      "Train: Epoch [9], Batch [24/938], Loss: 0.4555743932723999\n",
      "Train: Epoch [9], Batch [25/938], Loss: 0.34364622831344604\n",
      "Train: Epoch [9], Batch [26/938], Loss: 0.6166023015975952\n",
      "Train: Epoch [9], Batch [27/938], Loss: 0.5181688070297241\n",
      "Train: Epoch [9], Batch [28/938], Loss: 0.521963894367218\n",
      "Train: Epoch [9], Batch [29/938], Loss: 0.5982811450958252\n",
      "Train: Epoch [9], Batch [30/938], Loss: 0.3504704236984253\n",
      "Train: Epoch [9], Batch [31/938], Loss: 0.5122900605201721\n",
      "Train: Epoch [9], Batch [32/938], Loss: 0.6413881778717041\n",
      "Train: Epoch [9], Batch [33/938], Loss: 0.5252871513366699\n",
      "Train: Epoch [9], Batch [34/938], Loss: 0.6370428800582886\n",
      "Train: Epoch [9], Batch [35/938], Loss: 0.638908863067627\n",
      "Train: Epoch [9], Batch [36/938], Loss: 0.66834956407547\n",
      "Train: Epoch [9], Batch [37/938], Loss: 0.5384924411773682\n",
      "Train: Epoch [9], Batch [38/938], Loss: 0.3795170187950134\n",
      "Train: Epoch [9], Batch [39/938], Loss: 0.5803823471069336\n",
      "Train: Epoch [9], Batch [40/938], Loss: 0.6834769248962402\n",
      "Train: Epoch [9], Batch [41/938], Loss: 0.7150658369064331\n",
      "Train: Epoch [9], Batch [42/938], Loss: 0.5428757667541504\n",
      "Train: Epoch [9], Batch [43/938], Loss: 0.3537942171096802\n",
      "Train: Epoch [9], Batch [44/938], Loss: 0.5263670682907104\n",
      "Train: Epoch [9], Batch [45/938], Loss: 0.7385342121124268\n",
      "Train: Epoch [9], Batch [46/938], Loss: 0.5951604843139648\n",
      "Train: Epoch [9], Batch [47/938], Loss: 0.5185266137123108\n",
      "Train: Epoch [9], Batch [48/938], Loss: 0.7350289821624756\n",
      "Train: Epoch [9], Batch [49/938], Loss: 0.5884362459182739\n",
      "Train: Epoch [9], Batch [50/938], Loss: 0.5812206268310547\n",
      "Train: Epoch [9], Batch [51/938], Loss: 0.7215827703475952\n",
      "Train: Epoch [9], Batch [52/938], Loss: 0.6300054788589478\n",
      "Train: Epoch [9], Batch [53/938], Loss: 0.5511215925216675\n",
      "Train: Epoch [9], Batch [54/938], Loss: 0.8417294025421143\n",
      "Train: Epoch [9], Batch [55/938], Loss: 0.5252623558044434\n",
      "Train: Epoch [9], Batch [56/938], Loss: 0.7110576629638672\n",
      "Train: Epoch [9], Batch [57/938], Loss: 0.5270087718963623\n",
      "Train: Epoch [9], Batch [58/938], Loss: 0.4505034387111664\n",
      "Train: Epoch [9], Batch [59/938], Loss: 0.4902893304824829\n",
      "Train: Epoch [9], Batch [60/938], Loss: 0.36733484268188477\n",
      "Train: Epoch [9], Batch [61/938], Loss: 0.5924524068832397\n",
      "Train: Epoch [9], Batch [62/938], Loss: 0.37047284841537476\n",
      "Train: Epoch [9], Batch [63/938], Loss: 0.5586185455322266\n",
      "Train: Epoch [9], Batch [64/938], Loss: 0.7572681307792664\n",
      "Train: Epoch [9], Batch [65/938], Loss: 0.4587275981903076\n",
      "Train: Epoch [9], Batch [66/938], Loss: 0.6849769353866577\n",
      "Train: Epoch [9], Batch [67/938], Loss: 0.48191359639167786\n",
      "Train: Epoch [9], Batch [68/938], Loss: 0.432145357131958\n",
      "Train: Epoch [9], Batch [69/938], Loss: 0.46306684613227844\n",
      "Train: Epoch [9], Batch [70/938], Loss: 0.5655367374420166\n",
      "Train: Epoch [9], Batch [71/938], Loss: 0.7092392444610596\n",
      "Train: Epoch [9], Batch [72/938], Loss: 0.5169330835342407\n",
      "Train: Epoch [9], Batch [73/938], Loss: 0.49896442890167236\n",
      "Train: Epoch [9], Batch [74/938], Loss: 0.6495621204376221\n",
      "Train: Epoch [9], Batch [75/938], Loss: 0.46428152918815613\n",
      "Train: Epoch [9], Batch [76/938], Loss: 0.43135857582092285\n",
      "Train: Epoch [9], Batch [77/938], Loss: 0.4299025535583496\n",
      "Train: Epoch [9], Batch [78/938], Loss: 0.49395906925201416\n",
      "Train: Epoch [9], Batch [79/938], Loss: 0.4835338592529297\n",
      "Train: Epoch [9], Batch [80/938], Loss: 0.7173582315444946\n",
      "Train: Epoch [9], Batch [81/938], Loss: 0.47496479749679565\n",
      "Train: Epoch [9], Batch [82/938], Loss: 0.5511801242828369\n",
      "Train: Epoch [9], Batch [83/938], Loss: 0.7347577214241028\n",
      "Train: Epoch [9], Batch [84/938], Loss: 0.39700090885162354\n",
      "Train: Epoch [9], Batch [85/938], Loss: 0.44980287551879883\n",
      "Train: Epoch [9], Batch [86/938], Loss: 0.643764317035675\n",
      "Train: Epoch [9], Batch [87/938], Loss: 0.5612227916717529\n",
      "Train: Epoch [9], Batch [88/938], Loss: 0.9029587507247925\n",
      "Train: Epoch [9], Batch [89/938], Loss: 0.5914218425750732\n",
      "Train: Epoch [9], Batch [90/938], Loss: 0.46866941452026367\n",
      "Train: Epoch [9], Batch [91/938], Loss: 0.470245361328125\n",
      "Train: Epoch [9], Batch [92/938], Loss: 0.4370618462562561\n",
      "Train: Epoch [9], Batch [93/938], Loss: 0.6780739426612854\n",
      "Train: Epoch [9], Batch [94/938], Loss: 0.5664398074150085\n",
      "Train: Epoch [9], Batch [95/938], Loss: 0.5688599944114685\n",
      "Train: Epoch [9], Batch [96/938], Loss: 0.45409345626831055\n",
      "Train: Epoch [9], Batch [97/938], Loss: 0.5863511562347412\n",
      "Train: Epoch [9], Batch [98/938], Loss: 0.48044779896736145\n",
      "Train: Epoch [9], Batch [99/938], Loss: 0.44113364815711975\n",
      "Train: Epoch [9], Batch [100/938], Loss: 0.4247642755508423\n",
      "Train: Epoch [9], Batch [101/938], Loss: 0.5588030815124512\n",
      "Train: Epoch [9], Batch [102/938], Loss: 0.7661675214767456\n",
      "Train: Epoch [9], Batch [103/938], Loss: 0.602285623550415\n",
      "Train: Epoch [9], Batch [104/938], Loss: 0.5423644781112671\n",
      "Train: Epoch [9], Batch [105/938], Loss: 0.502807080745697\n",
      "Train: Epoch [9], Batch [106/938], Loss: 0.4930521845817566\n",
      "Train: Epoch [9], Batch [107/938], Loss: 0.5091052651405334\n",
      "Train: Epoch [9], Batch [108/938], Loss: 0.6299772262573242\n",
      "Train: Epoch [9], Batch [109/938], Loss: 0.6941158175468445\n",
      "Train: Epoch [9], Batch [110/938], Loss: 0.6681284308433533\n",
      "Train: Epoch [9], Batch [111/938], Loss: 0.43746668100357056\n",
      "Train: Epoch [9], Batch [112/938], Loss: 0.5081242322921753\n",
      "Train: Epoch [9], Batch [113/938], Loss: 0.4481208920478821\n",
      "Train: Epoch [9], Batch [114/938], Loss: 0.4188483953475952\n",
      "Train: Epoch [9], Batch [115/938], Loss: 0.549201488494873\n",
      "Train: Epoch [9], Batch [116/938], Loss: 0.5812171697616577\n",
      "Train: Epoch [9], Batch [117/938], Loss: 0.5349127054214478\n",
      "Train: Epoch [9], Batch [118/938], Loss: 0.4664686322212219\n",
      "Train: Epoch [9], Batch [119/938], Loss: 0.6017515063285828\n",
      "Train: Epoch [9], Batch [120/938], Loss: 0.5842694640159607\n",
      "Train: Epoch [9], Batch [121/938], Loss: 0.5053964853286743\n",
      "Train: Epoch [9], Batch [122/938], Loss: 0.8151944875717163\n",
      "Train: Epoch [9], Batch [123/938], Loss: 0.43342822790145874\n",
      "Train: Epoch [9], Batch [124/938], Loss: 0.49705445766448975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [9], Batch [125/938], Loss: 0.4927152097225189\n",
      "Train: Epoch [9], Batch [126/938], Loss: 0.4411845803260803\n",
      "Train: Epoch [9], Batch [127/938], Loss: 0.5970549583435059\n",
      "Train: Epoch [9], Batch [128/938], Loss: 0.5426397919654846\n",
      "Train: Epoch [9], Batch [129/938], Loss: 0.5699197053909302\n",
      "Train: Epoch [9], Batch [130/938], Loss: 0.664561927318573\n",
      "Train: Epoch [9], Batch [131/938], Loss: 0.5046146512031555\n",
      "Train: Epoch [9], Batch [132/938], Loss: 0.4794810116291046\n",
      "Train: Epoch [9], Batch [133/938], Loss: 0.491020530462265\n",
      "Train: Epoch [9], Batch [134/938], Loss: 0.5128029584884644\n",
      "Train: Epoch [9], Batch [135/938], Loss: 0.4458029866218567\n",
      "Train: Epoch [9], Batch [136/938], Loss: 0.554298996925354\n",
      "Train: Epoch [9], Batch [137/938], Loss: 0.472256064414978\n",
      "Train: Epoch [9], Batch [138/938], Loss: 0.5437700152397156\n",
      "Train: Epoch [9], Batch [139/938], Loss: 0.6776065230369568\n",
      "Train: Epoch [9], Batch [140/938], Loss: 0.5367889404296875\n",
      "Train: Epoch [9], Batch [141/938], Loss: 0.4714549481868744\n",
      "Train: Epoch [9], Batch [142/938], Loss: 0.5760878324508667\n",
      "Train: Epoch [9], Batch [143/938], Loss: 0.5120059251785278\n",
      "Train: Epoch [9], Batch [144/938], Loss: 0.740224301815033\n",
      "Train: Epoch [9], Batch [145/938], Loss: 0.6092071533203125\n",
      "Train: Epoch [9], Batch [146/938], Loss: 0.4559136927127838\n",
      "Train: Epoch [9], Batch [147/938], Loss: 0.6132729053497314\n",
      "Train: Epoch [9], Batch [148/938], Loss: 0.5368041396141052\n",
      "Train: Epoch [9], Batch [149/938], Loss: 0.5473564863204956\n",
      "Train: Epoch [9], Batch [150/938], Loss: 0.4977957606315613\n",
      "Train: Epoch [9], Batch [151/938], Loss: 0.5939857959747314\n",
      "Train: Epoch [9], Batch [152/938], Loss: 0.9186809062957764\n",
      "Train: Epoch [9], Batch [153/938], Loss: 0.6214220523834229\n",
      "Train: Epoch [9], Batch [154/938], Loss: 0.5312570333480835\n",
      "Train: Epoch [9], Batch [155/938], Loss: 0.43587666749954224\n",
      "Train: Epoch [9], Batch [156/938], Loss: 0.5194272994995117\n",
      "Train: Epoch [9], Batch [157/938], Loss: 0.5772340297698975\n",
      "Train: Epoch [9], Batch [158/938], Loss: 0.6201366186141968\n",
      "Train: Epoch [9], Batch [159/938], Loss: 0.575458824634552\n",
      "Train: Epoch [9], Batch [160/938], Loss: 0.5748956203460693\n",
      "Train: Epoch [9], Batch [161/938], Loss: 0.8388772010803223\n",
      "Train: Epoch [9], Batch [162/938], Loss: 0.6148550510406494\n",
      "Train: Epoch [9], Batch [163/938], Loss: 0.7320148944854736\n",
      "Train: Epoch [9], Batch [164/938], Loss: 0.45644503831863403\n",
      "Train: Epoch [9], Batch [165/938], Loss: 0.5311136245727539\n",
      "Train: Epoch [9], Batch [166/938], Loss: 0.635299801826477\n",
      "Train: Epoch [9], Batch [167/938], Loss: 0.5466224551200867\n",
      "Train: Epoch [9], Batch [168/938], Loss: 0.4126666784286499\n",
      "Train: Epoch [9], Batch [169/938], Loss: 0.6469987630844116\n",
      "Train: Epoch [9], Batch [170/938], Loss: 0.45612800121307373\n",
      "Train: Epoch [9], Batch [171/938], Loss: 0.34469136595726013\n",
      "Train: Epoch [9], Batch [172/938], Loss: 0.6776598691940308\n",
      "Train: Epoch [9], Batch [173/938], Loss: 0.6418439149856567\n",
      "Train: Epoch [9], Batch [174/938], Loss: 0.34454452991485596\n",
      "Train: Epoch [9], Batch [175/938], Loss: 0.6586773991584778\n",
      "Train: Epoch [9], Batch [176/938], Loss: 0.5876186490058899\n",
      "Train: Epoch [9], Batch [177/938], Loss: 0.37125489115715027\n",
      "Train: Epoch [9], Batch [178/938], Loss: 0.5419595837593079\n",
      "Train: Epoch [9], Batch [179/938], Loss: 0.33995962142944336\n",
      "Train: Epoch [9], Batch [180/938], Loss: 0.7880476713180542\n",
      "Train: Epoch [9], Batch [181/938], Loss: 0.6698087453842163\n",
      "Train: Epoch [9], Batch [182/938], Loss: 0.8229886889457703\n",
      "Train: Epoch [9], Batch [183/938], Loss: 0.4699581563472748\n",
      "Train: Epoch [9], Batch [184/938], Loss: 0.40712425112724304\n",
      "Train: Epoch [9], Batch [185/938], Loss: 0.4406895339488983\n",
      "Train: Epoch [9], Batch [186/938], Loss: 0.571096658706665\n",
      "Train: Epoch [9], Batch [187/938], Loss: 0.46264970302581787\n",
      "Train: Epoch [9], Batch [188/938], Loss: 0.6435047388076782\n",
      "Train: Epoch [9], Batch [189/938], Loss: 0.5528331995010376\n",
      "Train: Epoch [9], Batch [190/938], Loss: 0.5645671486854553\n",
      "Train: Epoch [9], Batch [191/938], Loss: 0.6319625377655029\n",
      "Train: Epoch [9], Batch [192/938], Loss: 0.7691686749458313\n",
      "Train: Epoch [9], Batch [193/938], Loss: 0.5146009922027588\n",
      "Train: Epoch [9], Batch [194/938], Loss: 0.4760124385356903\n",
      "Train: Epoch [9], Batch [195/938], Loss: 0.5219248533248901\n",
      "Train: Epoch [9], Batch [196/938], Loss: 0.4895787239074707\n",
      "Train: Epoch [9], Batch [197/938], Loss: 0.8026954531669617\n",
      "Train: Epoch [9], Batch [198/938], Loss: 0.7095382213592529\n",
      "Train: Epoch [9], Batch [199/938], Loss: 0.4933963119983673\n",
      "Train: Epoch [9], Batch [200/938], Loss: 0.5966001749038696\n",
      "Train: Epoch [9], Batch [201/938], Loss: 0.502284824848175\n",
      "Train: Epoch [9], Batch [202/938], Loss: 0.40293169021606445\n",
      "Train: Epoch [9], Batch [203/938], Loss: 0.3874915540218353\n",
      "Train: Epoch [9], Batch [204/938], Loss: 0.7514903545379639\n",
      "Train: Epoch [9], Batch [205/938], Loss: 0.48437777161598206\n",
      "Train: Epoch [9], Batch [206/938], Loss: 0.41601046919822693\n",
      "Train: Epoch [9], Batch [207/938], Loss: 0.47026199102401733\n",
      "Train: Epoch [9], Batch [208/938], Loss: 0.737802267074585\n",
      "Train: Epoch [9], Batch [209/938], Loss: 0.5036212801933289\n",
      "Train: Epoch [9], Batch [210/938], Loss: 0.6190423965454102\n",
      "Train: Epoch [9], Batch [211/938], Loss: 0.4082178771495819\n",
      "Train: Epoch [9], Batch [212/938], Loss: 0.8184401988983154\n",
      "Train: Epoch [9], Batch [213/938], Loss: 0.7206690311431885\n",
      "Train: Epoch [9], Batch [214/938], Loss: 0.5074006915092468\n",
      "Train: Epoch [9], Batch [215/938], Loss: 0.5267654657363892\n",
      "Train: Epoch [9], Batch [216/938], Loss: 0.49466264247894287\n",
      "Train: Epoch [9], Batch [217/938], Loss: 0.41039782762527466\n",
      "Train: Epoch [9], Batch [218/938], Loss: 0.4732203483581543\n",
      "Train: Epoch [9], Batch [219/938], Loss: 0.6368948221206665\n",
      "Train: Epoch [9], Batch [220/938], Loss: 0.4922916293144226\n",
      "Train: Epoch [9], Batch [221/938], Loss: 0.7777066826820374\n",
      "Train: Epoch [9], Batch [222/938], Loss: 0.5340162515640259\n",
      "Train: Epoch [9], Batch [223/938], Loss: 0.6411492824554443\n",
      "Train: Epoch [9], Batch [224/938], Loss: 0.5662088394165039\n",
      "Train: Epoch [9], Batch [225/938], Loss: 0.5035794973373413\n",
      "Train: Epoch [9], Batch [226/938], Loss: 0.5629847645759583\n",
      "Train: Epoch [9], Batch [227/938], Loss: 0.790176510810852\n",
      "Train: Epoch [9], Batch [228/938], Loss: 0.5332434177398682\n",
      "Train: Epoch [9], Batch [229/938], Loss: 0.6384832262992859\n",
      "Train: Epoch [9], Batch [230/938], Loss: 0.6606350541114807\n",
      "Train: Epoch [9], Batch [231/938], Loss: 0.546921968460083\n",
      "Train: Epoch [9], Batch [232/938], Loss: 0.5831118226051331\n",
      "Train: Epoch [9], Batch [233/938], Loss: 0.5875532627105713\n",
      "Train: Epoch [9], Batch [234/938], Loss: 0.7155477404594421\n",
      "Train: Epoch [9], Batch [235/938], Loss: 0.4203418493270874\n",
      "Train: Epoch [9], Batch [236/938], Loss: 0.5532190799713135\n",
      "Train: Epoch [9], Batch [237/938], Loss: 0.6788378953933716\n",
      "Train: Epoch [9], Batch [238/938], Loss: 0.45191487669944763\n",
      "Train: Epoch [9], Batch [239/938], Loss: 0.6980662941932678\n",
      "Train: Epoch [9], Batch [240/938], Loss: 0.5318470001220703\n",
      "Train: Epoch [9], Batch [241/938], Loss: 0.36803674697875977\n",
      "Train: Epoch [9], Batch [242/938], Loss: 0.4145774841308594\n",
      "Train: Epoch [9], Batch [243/938], Loss: 0.6307171583175659\n",
      "Train: Epoch [9], Batch [244/938], Loss: 0.4995831847190857\n",
      "Train: Epoch [9], Batch [245/938], Loss: 0.5419069528579712\n",
      "Train: Epoch [9], Batch [246/938], Loss: 0.5034801959991455\n",
      "Train: Epoch [9], Batch [247/938], Loss: 0.6541519165039062\n",
      "Train: Epoch [9], Batch [248/938], Loss: 0.6663990020751953\n",
      "Train: Epoch [9], Batch [249/938], Loss: 0.4897150993347168\n",
      "Train: Epoch [9], Batch [250/938], Loss: 0.4344429671764374\n",
      "Train: Epoch [9], Batch [251/938], Loss: 0.5162167549133301\n",
      "Train: Epoch [9], Batch [252/938], Loss: 0.6745399236679077\n",
      "Train: Epoch [9], Batch [253/938], Loss: 0.5401030778884888\n",
      "Train: Epoch [9], Batch [254/938], Loss: 0.4487190544605255\n",
      "Train: Epoch [9], Batch [255/938], Loss: 0.6019943952560425\n",
      "Train: Epoch [9], Batch [256/938], Loss: 0.5588788986206055\n",
      "Train: Epoch [9], Batch [257/938], Loss: 0.6076074242591858\n",
      "Train: Epoch [9], Batch [258/938], Loss: 0.5495967268943787\n",
      "Train: Epoch [9], Batch [259/938], Loss: 0.43071484565734863\n",
      "Train: Epoch [9], Batch [260/938], Loss: 0.7359293699264526\n",
      "Train: Epoch [9], Batch [261/938], Loss: 0.48368117213249207\n",
      "Train: Epoch [9], Batch [262/938], Loss: 0.6685633659362793\n",
      "Train: Epoch [9], Batch [263/938], Loss: 0.6277763843536377\n",
      "Train: Epoch [9], Batch [264/938], Loss: 0.7102918028831482\n",
      "Train: Epoch [9], Batch [265/938], Loss: 0.8017092347145081\n",
      "Train: Epoch [9], Batch [266/938], Loss: 0.714808464050293\n",
      "Train: Epoch [9], Batch [267/938], Loss: 0.3870660960674286\n",
      "Train: Epoch [9], Batch [268/938], Loss: 0.5474313497543335\n",
      "Train: Epoch [9], Batch [269/938], Loss: 0.5097581148147583\n",
      "Train: Epoch [9], Batch [270/938], Loss: 0.5749650001525879\n",
      "Train: Epoch [9], Batch [271/938], Loss: 0.3119778037071228\n",
      "Train: Epoch [9], Batch [272/938], Loss: 0.46828752756118774\n",
      "Train: Epoch [9], Batch [273/938], Loss: 0.5668229460716248\n",
      "Train: Epoch [9], Batch [274/938], Loss: 0.688104510307312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [9], Batch [275/938], Loss: 0.43897342681884766\n",
      "Train: Epoch [9], Batch [276/938], Loss: 0.5929332971572876\n",
      "Train: Epoch [9], Batch [277/938], Loss: 0.612790584564209\n",
      "Train: Epoch [9], Batch [278/938], Loss: 0.6870142221450806\n",
      "Train: Epoch [9], Batch [279/938], Loss: 0.5346782803535461\n",
      "Train: Epoch [9], Batch [280/938], Loss: 0.5348992347717285\n",
      "Train: Epoch [9], Batch [281/938], Loss: 0.526665210723877\n",
      "Train: Epoch [9], Batch [282/938], Loss: 0.4249217212200165\n",
      "Train: Epoch [9], Batch [283/938], Loss: 0.5810378789901733\n",
      "Train: Epoch [9], Batch [284/938], Loss: 0.8219443559646606\n",
      "Train: Epoch [9], Batch [285/938], Loss: 0.36105620861053467\n",
      "Train: Epoch [9], Batch [286/938], Loss: 0.5423503518104553\n",
      "Train: Epoch [9], Batch [287/938], Loss: 0.5212589502334595\n",
      "Train: Epoch [9], Batch [288/938], Loss: 0.4710327684879303\n",
      "Train: Epoch [9], Batch [289/938], Loss: 0.6354920864105225\n",
      "Train: Epoch [9], Batch [290/938], Loss: 0.6307068467140198\n",
      "Train: Epoch [9], Batch [291/938], Loss: 0.45019811391830444\n",
      "Train: Epoch [9], Batch [292/938], Loss: 0.5557807087898254\n",
      "Train: Epoch [9], Batch [293/938], Loss: 0.6498221158981323\n",
      "Train: Epoch [9], Batch [294/938], Loss: 0.47192591428756714\n",
      "Train: Epoch [9], Batch [295/938], Loss: 0.5793026685714722\n",
      "Train: Epoch [9], Batch [296/938], Loss: 0.6960939168930054\n",
      "Train: Epoch [9], Batch [297/938], Loss: 0.39523229002952576\n",
      "Train: Epoch [9], Batch [298/938], Loss: 0.5923506021499634\n",
      "Train: Epoch [9], Batch [299/938], Loss: 0.4216308891773224\n",
      "Train: Epoch [9], Batch [300/938], Loss: 0.5928165316581726\n",
      "Train: Epoch [9], Batch [301/938], Loss: 0.4196670651435852\n",
      "Train: Epoch [9], Batch [302/938], Loss: 0.5003180503845215\n",
      "Train: Epoch [9], Batch [303/938], Loss: 0.43202686309814453\n",
      "Train: Epoch [9], Batch [304/938], Loss: 0.3774459958076477\n",
      "Train: Epoch [9], Batch [305/938], Loss: 0.6541659832000732\n",
      "Train: Epoch [9], Batch [306/938], Loss: 0.5037820339202881\n",
      "Train: Epoch [9], Batch [307/938], Loss: 0.5119345188140869\n",
      "Train: Epoch [9], Batch [308/938], Loss: 0.5972506403923035\n",
      "Train: Epoch [9], Batch [309/938], Loss: 0.6066684722900391\n",
      "Train: Epoch [9], Batch [310/938], Loss: 0.668051540851593\n",
      "Train: Epoch [9], Batch [311/938], Loss: 0.6514204740524292\n",
      "Train: Epoch [9], Batch [312/938], Loss: 0.646474301815033\n",
      "Train: Epoch [9], Batch [313/938], Loss: 0.7053666114807129\n",
      "Train: Epoch [9], Batch [314/938], Loss: 0.6203488707542419\n",
      "Train: Epoch [9], Batch [315/938], Loss: 0.6824042201042175\n",
      "Train: Epoch [9], Batch [316/938], Loss: 0.4751102924346924\n",
      "Train: Epoch [9], Batch [317/938], Loss: 0.5027422904968262\n",
      "Train: Epoch [9], Batch [318/938], Loss: 0.7137720584869385\n",
      "Train: Epoch [9], Batch [319/938], Loss: 0.48893287777900696\n",
      "Train: Epoch [9], Batch [320/938], Loss: 0.6049486994743347\n",
      "Train: Epoch [9], Batch [321/938], Loss: 0.4551113247871399\n",
      "Train: Epoch [9], Batch [322/938], Loss: 0.5855083465576172\n",
      "Train: Epoch [9], Batch [323/938], Loss: 0.8001967072486877\n",
      "Train: Epoch [9], Batch [324/938], Loss: 0.7126507759094238\n",
      "Train: Epoch [9], Batch [325/938], Loss: 0.7370375394821167\n",
      "Train: Epoch [9], Batch [326/938], Loss: 0.4040337800979614\n",
      "Train: Epoch [9], Batch [327/938], Loss: 0.8029242753982544\n",
      "Train: Epoch [9], Batch [328/938], Loss: 0.8279715776443481\n",
      "Train: Epoch [9], Batch [329/938], Loss: 0.537106990814209\n",
      "Train: Epoch [9], Batch [330/938], Loss: 0.43106767535209656\n",
      "Train: Epoch [9], Batch [331/938], Loss: 0.5598818063735962\n",
      "Train: Epoch [9], Batch [332/938], Loss: 0.5398881435394287\n",
      "Train: Epoch [9], Batch [333/938], Loss: 0.4469124972820282\n",
      "Train: Epoch [9], Batch [334/938], Loss: 0.5059899687767029\n",
      "Train: Epoch [9], Batch [335/938], Loss: 0.442512571811676\n",
      "Train: Epoch [9], Batch [336/938], Loss: 0.6858365535736084\n",
      "Train: Epoch [9], Batch [337/938], Loss: 0.454654335975647\n",
      "Train: Epoch [9], Batch [338/938], Loss: 0.8308537006378174\n",
      "Train: Epoch [9], Batch [339/938], Loss: 0.5165719389915466\n",
      "Train: Epoch [9], Batch [340/938], Loss: 0.8542040586471558\n",
      "Train: Epoch [9], Batch [341/938], Loss: 0.5312073230743408\n",
      "Train: Epoch [9], Batch [342/938], Loss: 0.5471302270889282\n",
      "Train: Epoch [9], Batch [343/938], Loss: 0.4803754985332489\n",
      "Train: Epoch [9], Batch [344/938], Loss: 0.5561856627464294\n",
      "Train: Epoch [9], Batch [345/938], Loss: 0.4937201142311096\n",
      "Train: Epoch [9], Batch [346/938], Loss: 0.6453474164009094\n",
      "Train: Epoch [9], Batch [347/938], Loss: 0.4715730547904968\n",
      "Train: Epoch [9], Batch [348/938], Loss: 0.5311566591262817\n",
      "Train: Epoch [9], Batch [349/938], Loss: 0.47708314657211304\n",
      "Train: Epoch [9], Batch [350/938], Loss: 0.3597932457923889\n",
      "Train: Epoch [9], Batch [351/938], Loss: 0.435323566198349\n",
      "Train: Epoch [9], Batch [352/938], Loss: 0.545681357383728\n",
      "Train: Epoch [9], Batch [353/938], Loss: 0.45588260889053345\n",
      "Train: Epoch [9], Batch [354/938], Loss: 0.4847440719604492\n",
      "Train: Epoch [9], Batch [355/938], Loss: 0.5911297798156738\n",
      "Train: Epoch [9], Batch [356/938], Loss: 0.5770062208175659\n",
      "Train: Epoch [9], Batch [357/938], Loss: 0.5395848751068115\n",
      "Train: Epoch [9], Batch [358/938], Loss: 0.620908796787262\n",
      "Train: Epoch [9], Batch [359/938], Loss: 0.6667910814285278\n",
      "Train: Epoch [9], Batch [360/938], Loss: 0.43913236260414124\n",
      "Train: Epoch [9], Batch [361/938], Loss: 0.9101009368896484\n",
      "Train: Epoch [9], Batch [362/938], Loss: 0.43105223774909973\n",
      "Train: Epoch [9], Batch [363/938], Loss: 0.48167628049850464\n",
      "Train: Epoch [9], Batch [364/938], Loss: 0.585468053817749\n",
      "Train: Epoch [9], Batch [365/938], Loss: 0.4927958846092224\n",
      "Train: Epoch [9], Batch [366/938], Loss: 0.4595566391944885\n",
      "Train: Epoch [9], Batch [367/938], Loss: 0.490455687046051\n",
      "Train: Epoch [9], Batch [368/938], Loss: 0.7178645730018616\n",
      "Train: Epoch [9], Batch [369/938], Loss: 0.7481949925422668\n",
      "Train: Epoch [9], Batch [370/938], Loss: 0.4674415588378906\n",
      "Train: Epoch [9], Batch [371/938], Loss: 0.7733731865882874\n",
      "Train: Epoch [9], Batch [372/938], Loss: 0.6783071160316467\n",
      "Train: Epoch [9], Batch [373/938], Loss: 0.6019747257232666\n",
      "Train: Epoch [9], Batch [374/938], Loss: 0.7185810804367065\n",
      "Train: Epoch [9], Batch [375/938], Loss: 0.6683918237686157\n",
      "Train: Epoch [9], Batch [376/938], Loss: 0.5998660326004028\n",
      "Train: Epoch [9], Batch [377/938], Loss: 0.33778655529022217\n",
      "Train: Epoch [9], Batch [378/938], Loss: 0.4972892999649048\n",
      "Train: Epoch [9], Batch [379/938], Loss: 0.5314779281616211\n",
      "Train: Epoch [9], Batch [380/938], Loss: 0.6070098876953125\n",
      "Train: Epoch [9], Batch [381/938], Loss: 0.6859203577041626\n",
      "Train: Epoch [9], Batch [382/938], Loss: 0.602210521697998\n",
      "Train: Epoch [9], Batch [383/938], Loss: 0.41219520568847656\n",
      "Train: Epoch [9], Batch [384/938], Loss: 0.38739097118377686\n",
      "Train: Epoch [9], Batch [385/938], Loss: 0.5013280510902405\n",
      "Train: Epoch [9], Batch [386/938], Loss: 0.47789713740348816\n",
      "Train: Epoch [9], Batch [387/938], Loss: 0.544310986995697\n",
      "Train: Epoch [9], Batch [388/938], Loss: 0.6222808361053467\n",
      "Train: Epoch [9], Batch [389/938], Loss: 0.3967108726501465\n",
      "Train: Epoch [9], Batch [390/938], Loss: 0.4139378070831299\n",
      "Train: Epoch [9], Batch [391/938], Loss: 0.46415039896965027\n",
      "Train: Epoch [9], Batch [392/938], Loss: 0.598355770111084\n",
      "Train: Epoch [9], Batch [393/938], Loss: 0.5247563123703003\n",
      "Train: Epoch [9], Batch [394/938], Loss: 0.47780802845954895\n",
      "Train: Epoch [9], Batch [395/938], Loss: 0.6073237657546997\n",
      "Train: Epoch [9], Batch [396/938], Loss: 0.6311725378036499\n",
      "Train: Epoch [9], Batch [397/938], Loss: 0.5083248615264893\n",
      "Train: Epoch [9], Batch [398/938], Loss: 0.6706275939941406\n",
      "Train: Epoch [9], Batch [399/938], Loss: 0.5319231748580933\n",
      "Train: Epoch [9], Batch [400/938], Loss: 0.5143110156059265\n",
      "Train: Epoch [9], Batch [401/938], Loss: 0.3007689118385315\n",
      "Train: Epoch [9], Batch [402/938], Loss: 0.4018164277076721\n",
      "Train: Epoch [9], Batch [403/938], Loss: 0.6160588264465332\n",
      "Train: Epoch [9], Batch [404/938], Loss: 0.6709534525871277\n",
      "Train: Epoch [9], Batch [405/938], Loss: 0.5516671538352966\n",
      "Train: Epoch [9], Batch [406/938], Loss: 0.4900863766670227\n",
      "Train: Epoch [9], Batch [407/938], Loss: 0.44462376832962036\n",
      "Train: Epoch [9], Batch [408/938], Loss: 0.6267290115356445\n",
      "Train: Epoch [9], Batch [409/938], Loss: 0.6160147190093994\n",
      "Train: Epoch [9], Batch [410/938], Loss: 0.5658783912658691\n",
      "Train: Epoch [9], Batch [411/938], Loss: 0.3984629511833191\n",
      "Train: Epoch [9], Batch [412/938], Loss: 0.4237709045410156\n",
      "Train: Epoch [9], Batch [413/938], Loss: 0.7090946435928345\n",
      "Train: Epoch [9], Batch [414/938], Loss: 0.4523313641548157\n",
      "Train: Epoch [9], Batch [415/938], Loss: 0.9267139434814453\n",
      "Train: Epoch [9], Batch [416/938], Loss: 0.5850672125816345\n",
      "Train: Epoch [9], Batch [417/938], Loss: 0.649375319480896\n",
      "Train: Epoch [9], Batch [418/938], Loss: 0.6040757894515991\n",
      "Train: Epoch [9], Batch [419/938], Loss: 0.5671422481536865\n",
      "Train: Epoch [9], Batch [420/938], Loss: 0.4166179299354553\n",
      "Train: Epoch [9], Batch [421/938], Loss: 0.5610736608505249\n",
      "Train: Epoch [9], Batch [422/938], Loss: 0.8064825534820557\n",
      "Train: Epoch [9], Batch [423/938], Loss: 0.45771780610084534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [9], Batch [424/938], Loss: 0.6614707708358765\n",
      "Train: Epoch [9], Batch [425/938], Loss: 0.6179167032241821\n",
      "Train: Epoch [9], Batch [426/938], Loss: 0.6126892566680908\n",
      "Train: Epoch [9], Batch [427/938], Loss: 0.4227692484855652\n",
      "Train: Epoch [9], Batch [428/938], Loss: 0.48502737283706665\n",
      "Train: Epoch [9], Batch [429/938], Loss: 0.6724157333374023\n",
      "Train: Epoch [9], Batch [430/938], Loss: 0.4444539546966553\n",
      "Train: Epoch [9], Batch [431/938], Loss: 0.4887222647666931\n",
      "Train: Epoch [9], Batch [432/938], Loss: 0.6549457311630249\n",
      "Train: Epoch [9], Batch [433/938], Loss: 0.7438530921936035\n",
      "Train: Epoch [9], Batch [434/938], Loss: 0.5558747053146362\n",
      "Train: Epoch [9], Batch [435/938], Loss: 0.50462806224823\n",
      "Train: Epoch [9], Batch [436/938], Loss: 0.5275965929031372\n",
      "Train: Epoch [9], Batch [437/938], Loss: 0.576962947845459\n",
      "Train: Epoch [9], Batch [438/938], Loss: 0.5508118271827698\n",
      "Train: Epoch [9], Batch [439/938], Loss: 0.5128206014633179\n",
      "Train: Epoch [9], Batch [440/938], Loss: 0.5194262266159058\n",
      "Train: Epoch [9], Batch [441/938], Loss: 0.48982328176498413\n",
      "Train: Epoch [9], Batch [442/938], Loss: 0.41426706314086914\n",
      "Train: Epoch [9], Batch [443/938], Loss: 0.560157060623169\n",
      "Train: Epoch [9], Batch [444/938], Loss: 0.36060696840286255\n",
      "Train: Epoch [9], Batch [445/938], Loss: 0.6970746517181396\n",
      "Train: Epoch [9], Batch [446/938], Loss: 0.5544923543930054\n",
      "Train: Epoch [9], Batch [447/938], Loss: 0.5577656030654907\n",
      "Train: Epoch [9], Batch [448/938], Loss: 0.40197843313217163\n",
      "Train: Epoch [9], Batch [449/938], Loss: 0.7339996099472046\n",
      "Train: Epoch [9], Batch [450/938], Loss: 0.6817542314529419\n",
      "Train: Epoch [9], Batch [451/938], Loss: 0.6195572018623352\n",
      "Train: Epoch [9], Batch [452/938], Loss: 0.48850053548812866\n",
      "Train: Epoch [9], Batch [453/938], Loss: 0.7575712203979492\n",
      "Train: Epoch [9], Batch [454/938], Loss: 0.5120326280593872\n",
      "Train: Epoch [9], Batch [455/938], Loss: 0.7744659185409546\n",
      "Train: Epoch [9], Batch [456/938], Loss: 0.6154333353042603\n",
      "Train: Epoch [9], Batch [457/938], Loss: 0.6883347034454346\n",
      "Train: Epoch [9], Batch [458/938], Loss: 0.38607728481292725\n",
      "Train: Epoch [9], Batch [459/938], Loss: 0.5390063524246216\n",
      "Train: Epoch [9], Batch [460/938], Loss: 0.447806715965271\n",
      "Train: Epoch [9], Batch [461/938], Loss: 0.617858350276947\n",
      "Train: Epoch [9], Batch [462/938], Loss: 0.5183368921279907\n",
      "Train: Epoch [9], Batch [463/938], Loss: 0.45087042450904846\n",
      "Train: Epoch [9], Batch [464/938], Loss: 0.4327089190483093\n",
      "Train: Epoch [9], Batch [465/938], Loss: 0.4362879693508148\n",
      "Train: Epoch [9], Batch [466/938], Loss: 0.48662328720092773\n",
      "Train: Epoch [9], Batch [467/938], Loss: 0.5162762403488159\n",
      "Train: Epoch [9], Batch [468/938], Loss: 0.5299898386001587\n",
      "Train: Epoch [9], Batch [469/938], Loss: 0.42270079255104065\n",
      "Train: Epoch [9], Batch [470/938], Loss: 0.2804950773715973\n",
      "Train: Epoch [9], Batch [471/938], Loss: 0.6904022693634033\n",
      "Train: Epoch [9], Batch [472/938], Loss: 0.548077404499054\n",
      "Train: Epoch [9], Batch [473/938], Loss: 0.48667919635772705\n",
      "Train: Epoch [9], Batch [474/938], Loss: 0.5181261301040649\n",
      "Train: Epoch [9], Batch [475/938], Loss: 0.5389333963394165\n",
      "Train: Epoch [9], Batch [476/938], Loss: 0.49057021737098694\n",
      "Train: Epoch [9], Batch [477/938], Loss: 0.543381929397583\n",
      "Train: Epoch [9], Batch [478/938], Loss: 0.6791334748268127\n",
      "Train: Epoch [9], Batch [479/938], Loss: 0.7352395057678223\n",
      "Train: Epoch [9], Batch [480/938], Loss: 0.7028076648712158\n",
      "Train: Epoch [9], Batch [481/938], Loss: 0.5551170110702515\n",
      "Train: Epoch [9], Batch [482/938], Loss: 0.6076605319976807\n",
      "Train: Epoch [9], Batch [483/938], Loss: 0.518845796585083\n",
      "Train: Epoch [9], Batch [484/938], Loss: 0.5907965302467346\n",
      "Train: Epoch [9], Batch [485/938], Loss: 0.5261834859848022\n",
      "Train: Epoch [9], Batch [486/938], Loss: 0.6027981042861938\n",
      "Train: Epoch [9], Batch [487/938], Loss: 0.47615641355514526\n",
      "Train: Epoch [9], Batch [488/938], Loss: 0.7764045596122742\n",
      "Train: Epoch [9], Batch [489/938], Loss: 0.8480887413024902\n",
      "Train: Epoch [9], Batch [490/938], Loss: 0.4773644506931305\n",
      "Train: Epoch [9], Batch [491/938], Loss: 0.5533223748207092\n",
      "Train: Epoch [9], Batch [492/938], Loss: 0.44707897305488586\n",
      "Train: Epoch [9], Batch [493/938], Loss: 0.5224969387054443\n",
      "Train: Epoch [9], Batch [494/938], Loss: 0.43079185485839844\n",
      "Train: Epoch [9], Batch [495/938], Loss: 0.7001867294311523\n",
      "Train: Epoch [9], Batch [496/938], Loss: 0.7188324332237244\n",
      "Train: Epoch [9], Batch [497/938], Loss: 0.5830478668212891\n",
      "Train: Epoch [9], Batch [498/938], Loss: 1.0326777696609497\n",
      "Train: Epoch [9], Batch [499/938], Loss: 0.5573635101318359\n",
      "Train: Epoch [9], Batch [500/938], Loss: 0.5166894197463989\n",
      "Train: Epoch [9], Batch [501/938], Loss: 0.544966459274292\n",
      "Train: Epoch [9], Batch [502/938], Loss: 0.5042639374732971\n",
      "Train: Epoch [9], Batch [503/938], Loss: 0.5746333599090576\n",
      "Train: Epoch [9], Batch [504/938], Loss: 0.5417829751968384\n",
      "Train: Epoch [9], Batch [505/938], Loss: 0.5587924718856812\n",
      "Train: Epoch [9], Batch [506/938], Loss: 0.47519490122795105\n",
      "Train: Epoch [9], Batch [507/938], Loss: 0.3999854326248169\n",
      "Train: Epoch [9], Batch [508/938], Loss: 0.508141279220581\n",
      "Train: Epoch [9], Batch [509/938], Loss: 0.7958961725234985\n",
      "Train: Epoch [9], Batch [510/938], Loss: 0.6624855995178223\n",
      "Train: Epoch [9], Batch [511/938], Loss: 0.4447973370552063\n",
      "Train: Epoch [9], Batch [512/938], Loss: 0.5804054141044617\n",
      "Train: Epoch [9], Batch [513/938], Loss: 0.5458500385284424\n",
      "Train: Epoch [9], Batch [514/938], Loss: 0.4426809549331665\n",
      "Train: Epoch [9], Batch [515/938], Loss: 0.5167885422706604\n",
      "Train: Epoch [9], Batch [516/938], Loss: 0.5472749471664429\n",
      "Train: Epoch [9], Batch [517/938], Loss: 0.5178325176239014\n",
      "Train: Epoch [9], Batch [518/938], Loss: 0.7359760999679565\n",
      "Train: Epoch [9], Batch [519/938], Loss: 0.46292412281036377\n",
      "Train: Epoch [9], Batch [520/938], Loss: 0.5157991647720337\n",
      "Train: Epoch [9], Batch [521/938], Loss: 0.4974622130393982\n",
      "Train: Epoch [9], Batch [522/938], Loss: 0.4413837194442749\n",
      "Train: Epoch [9], Batch [523/938], Loss: 0.4302155375480652\n",
      "Train: Epoch [9], Batch [524/938], Loss: 0.4135016202926636\n",
      "Train: Epoch [9], Batch [525/938], Loss: 0.4183714985847473\n",
      "Train: Epoch [9], Batch [526/938], Loss: 0.5278916358947754\n",
      "Train: Epoch [9], Batch [527/938], Loss: 0.48166459798812866\n",
      "Train: Epoch [9], Batch [528/938], Loss: 0.6026500463485718\n",
      "Train: Epoch [9], Batch [529/938], Loss: 0.4981464445590973\n",
      "Train: Epoch [9], Batch [530/938], Loss: 0.657223641872406\n",
      "Train: Epoch [9], Batch [531/938], Loss: 0.5040671229362488\n",
      "Train: Epoch [9], Batch [532/938], Loss: 0.5044857263565063\n",
      "Train: Epoch [9], Batch [533/938], Loss: 0.7608027458190918\n",
      "Train: Epoch [9], Batch [534/938], Loss: 0.38050907850265503\n",
      "Train: Epoch [9], Batch [535/938], Loss: 0.6134688854217529\n",
      "Train: Epoch [9], Batch [536/938], Loss: 0.6995324492454529\n",
      "Train: Epoch [9], Batch [537/938], Loss: 0.4696047902107239\n",
      "Train: Epoch [9], Batch [538/938], Loss: 0.5635020136833191\n",
      "Train: Epoch [9], Batch [539/938], Loss: 0.6407386660575867\n",
      "Train: Epoch [9], Batch [540/938], Loss: 0.6099221706390381\n",
      "Train: Epoch [9], Batch [541/938], Loss: 0.4516368508338928\n",
      "Train: Epoch [9], Batch [542/938], Loss: 0.44826334714889526\n",
      "Train: Epoch [9], Batch [543/938], Loss: 0.5716813206672668\n",
      "Train: Epoch [9], Batch [544/938], Loss: 0.49821510910987854\n",
      "Train: Epoch [9], Batch [545/938], Loss: 0.46009761095046997\n",
      "Train: Epoch [9], Batch [546/938], Loss: 0.6399188041687012\n",
      "Train: Epoch [9], Batch [547/938], Loss: 0.5752824544906616\n",
      "Train: Epoch [9], Batch [548/938], Loss: 0.5264585018157959\n",
      "Train: Epoch [9], Batch [549/938], Loss: 0.5141115784645081\n",
      "Train: Epoch [9], Batch [550/938], Loss: 0.46778425574302673\n",
      "Train: Epoch [9], Batch [551/938], Loss: 0.5873227715492249\n",
      "Train: Epoch [9], Batch [552/938], Loss: 0.7108141183853149\n",
      "Train: Epoch [9], Batch [553/938], Loss: 0.4439378082752228\n",
      "Train: Epoch [9], Batch [554/938], Loss: 0.437355101108551\n",
      "Train: Epoch [9], Batch [555/938], Loss: 0.566509485244751\n",
      "Train: Epoch [9], Batch [556/938], Loss: 0.4431720972061157\n",
      "Train: Epoch [9], Batch [557/938], Loss: 0.4892856776714325\n",
      "Train: Epoch [9], Batch [558/938], Loss: 0.7454725503921509\n",
      "Train: Epoch [9], Batch [559/938], Loss: 0.37451162934303284\n",
      "Train: Epoch [9], Batch [560/938], Loss: 0.5847609639167786\n",
      "Train: Epoch [9], Batch [561/938], Loss: 0.5144361257553101\n",
      "Train: Epoch [9], Batch [562/938], Loss: 0.5213738083839417\n",
      "Train: Epoch [9], Batch [563/938], Loss: 0.39978957176208496\n",
      "Train: Epoch [9], Batch [564/938], Loss: 0.41663658618927\n",
      "Train: Epoch [9], Batch [565/938], Loss: 0.5967966914176941\n",
      "Train: Epoch [9], Batch [566/938], Loss: 0.5666711330413818\n",
      "Train: Epoch [9], Batch [567/938], Loss: 0.44422709941864014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [9], Batch [568/938], Loss: 0.6814079880714417\n",
      "Train: Epoch [9], Batch [569/938], Loss: 0.5709447860717773\n",
      "Train: Epoch [9], Batch [570/938], Loss: 0.6389527320861816\n",
      "Train: Epoch [9], Batch [571/938], Loss: 0.6619883179664612\n",
      "Train: Epoch [9], Batch [572/938], Loss: 0.443634033203125\n",
      "Train: Epoch [9], Batch [573/938], Loss: 0.44537991285324097\n",
      "Train: Epoch [9], Batch [574/938], Loss: 0.5349984765052795\n",
      "Train: Epoch [9], Batch [575/938], Loss: 0.4736270308494568\n",
      "Train: Epoch [9], Batch [576/938], Loss: 0.4215056300163269\n",
      "Train: Epoch [9], Batch [577/938], Loss: 0.6906288862228394\n",
      "Train: Epoch [9], Batch [578/938], Loss: 0.6408478021621704\n",
      "Train: Epoch [9], Batch [579/938], Loss: 0.46607235074043274\n",
      "Train: Epoch [9], Batch [580/938], Loss: 0.6132410168647766\n",
      "Train: Epoch [9], Batch [581/938], Loss: 0.44923675060272217\n",
      "Train: Epoch [9], Batch [582/938], Loss: 0.5606293678283691\n",
      "Train: Epoch [9], Batch [583/938], Loss: 0.28584083914756775\n",
      "Train: Epoch [9], Batch [584/938], Loss: 0.4792397916316986\n",
      "Train: Epoch [9], Batch [585/938], Loss: 0.5917786359786987\n",
      "Train: Epoch [9], Batch [586/938], Loss: 0.8712937831878662\n",
      "Train: Epoch [9], Batch [587/938], Loss: 0.5060869455337524\n",
      "Train: Epoch [9], Batch [588/938], Loss: 0.5873391032218933\n",
      "Train: Epoch [9], Batch [589/938], Loss: 0.4165775179862976\n",
      "Train: Epoch [9], Batch [590/938], Loss: 0.643787145614624\n",
      "Train: Epoch [9], Batch [591/938], Loss: 0.5009568929672241\n",
      "Train: Epoch [9], Batch [592/938], Loss: 0.47140634059906006\n",
      "Train: Epoch [9], Batch [593/938], Loss: 0.497439980506897\n",
      "Train: Epoch [9], Batch [594/938], Loss: 0.5071403384208679\n",
      "Train: Epoch [9], Batch [595/938], Loss: 0.5063402652740479\n",
      "Train: Epoch [9], Batch [596/938], Loss: 0.703039824962616\n",
      "Train: Epoch [9], Batch [597/938], Loss: 0.5107290744781494\n",
      "Train: Epoch [9], Batch [598/938], Loss: 0.5172027349472046\n",
      "Train: Epoch [9], Batch [599/938], Loss: 0.49271464347839355\n",
      "Train: Epoch [9], Batch [600/938], Loss: 0.6227073669433594\n",
      "Train: Epoch [9], Batch [601/938], Loss: 0.6286428570747375\n",
      "Train: Epoch [9], Batch [602/938], Loss: 0.46325522661209106\n",
      "Train: Epoch [9], Batch [603/938], Loss: 0.6401777267456055\n",
      "Train: Epoch [9], Batch [604/938], Loss: 0.4988693594932556\n",
      "Train: Epoch [9], Batch [605/938], Loss: 0.3799024224281311\n",
      "Train: Epoch [9], Batch [606/938], Loss: 0.7870275974273682\n",
      "Train: Epoch [9], Batch [607/938], Loss: 0.5590657591819763\n",
      "Train: Epoch [9], Batch [608/938], Loss: 0.6429815292358398\n",
      "Train: Epoch [9], Batch [609/938], Loss: 0.5673642158508301\n",
      "Train: Epoch [9], Batch [610/938], Loss: 0.3734772205352783\n",
      "Train: Epoch [9], Batch [611/938], Loss: 0.5485886335372925\n",
      "Train: Epoch [9], Batch [612/938], Loss: 0.5574706792831421\n",
      "Train: Epoch [9], Batch [613/938], Loss: 0.47289201617240906\n",
      "Train: Epoch [9], Batch [614/938], Loss: 0.5880688428878784\n",
      "Train: Epoch [9], Batch [615/938], Loss: 0.3824369013309479\n",
      "Train: Epoch [9], Batch [616/938], Loss: 0.5520209074020386\n",
      "Train: Epoch [9], Batch [617/938], Loss: 0.5850116610527039\n",
      "Train: Epoch [9], Batch [618/938], Loss: 0.6472345590591431\n",
      "Train: Epoch [9], Batch [619/938], Loss: 0.6985329389572144\n",
      "Train: Epoch [9], Batch [620/938], Loss: 0.611578106880188\n",
      "Train: Epoch [9], Batch [621/938], Loss: 0.4067678153514862\n",
      "Train: Epoch [9], Batch [622/938], Loss: 0.42520302534103394\n",
      "Train: Epoch [9], Batch [623/938], Loss: 0.5904680490493774\n",
      "Train: Epoch [9], Batch [624/938], Loss: 0.5495665073394775\n",
      "Train: Epoch [9], Batch [625/938], Loss: 0.4741109609603882\n",
      "Train: Epoch [9], Batch [626/938], Loss: 0.46539703011512756\n",
      "Train: Epoch [9], Batch [627/938], Loss: 0.901951789855957\n",
      "Train: Epoch [9], Batch [628/938], Loss: 0.45194417238235474\n",
      "Train: Epoch [9], Batch [629/938], Loss: 0.5411992073059082\n",
      "Train: Epoch [9], Batch [630/938], Loss: 0.5011129975318909\n",
      "Train: Epoch [9], Batch [631/938], Loss: 0.4789753556251526\n",
      "Train: Epoch [9], Batch [632/938], Loss: 0.5517309904098511\n",
      "Train: Epoch [9], Batch [633/938], Loss: 0.6172077655792236\n",
      "Train: Epoch [9], Batch [634/938], Loss: 0.7881796956062317\n",
      "Train: Epoch [9], Batch [635/938], Loss: 0.5161440968513489\n",
      "Train: Epoch [9], Batch [636/938], Loss: 0.6754468083381653\n",
      "Train: Epoch [9], Batch [637/938], Loss: 0.48435184359550476\n",
      "Train: Epoch [9], Batch [638/938], Loss: 0.40314000844955444\n",
      "Train: Epoch [9], Batch [639/938], Loss: 0.5193469524383545\n",
      "Train: Epoch [9], Batch [640/938], Loss: 0.7594089508056641\n",
      "Train: Epoch [9], Batch [641/938], Loss: 0.5672348737716675\n",
      "Train: Epoch [9], Batch [642/938], Loss: 0.6300776600837708\n",
      "Train: Epoch [9], Batch [643/938], Loss: 0.4213189482688904\n",
      "Train: Epoch [9], Batch [644/938], Loss: 0.49975138902664185\n",
      "Train: Epoch [9], Batch [645/938], Loss: 0.5703890323638916\n",
      "Train: Epoch [9], Batch [646/938], Loss: 0.44875001907348633\n",
      "Train: Epoch [9], Batch [647/938], Loss: 0.4312289357185364\n",
      "Train: Epoch [9], Batch [648/938], Loss: 0.6467864513397217\n",
      "Train: Epoch [9], Batch [649/938], Loss: 0.4870981276035309\n",
      "Train: Epoch [9], Batch [650/938], Loss: 0.37552452087402344\n",
      "Train: Epoch [9], Batch [651/938], Loss: 0.854878306388855\n",
      "Train: Epoch [9], Batch [652/938], Loss: 0.614755392074585\n",
      "Train: Epoch [9], Batch [653/938], Loss: 0.5972245931625366\n",
      "Train: Epoch [9], Batch [654/938], Loss: 0.5229750275611877\n",
      "Train: Epoch [9], Batch [655/938], Loss: 0.46557241678237915\n",
      "Train: Epoch [9], Batch [656/938], Loss: 0.4960954487323761\n",
      "Train: Epoch [9], Batch [657/938], Loss: 0.6619142293930054\n",
      "Train: Epoch [9], Batch [658/938], Loss: 0.639906644821167\n",
      "Train: Epoch [9], Batch [659/938], Loss: 0.4669351875782013\n",
      "Train: Epoch [9], Batch [660/938], Loss: 0.7978609800338745\n",
      "Train: Epoch [9], Batch [661/938], Loss: 0.4339600205421448\n",
      "Train: Epoch [9], Batch [662/938], Loss: 0.5786136984825134\n",
      "Train: Epoch [9], Batch [663/938], Loss: 0.7317281365394592\n",
      "Train: Epoch [9], Batch [664/938], Loss: 0.4524288773536682\n",
      "Train: Epoch [9], Batch [665/938], Loss: 0.5782831907272339\n",
      "Train: Epoch [9], Batch [666/938], Loss: 0.4327090382575989\n",
      "Train: Epoch [9], Batch [667/938], Loss: 0.6507636308670044\n",
      "Train: Epoch [9], Batch [668/938], Loss: 0.8368702530860901\n",
      "Train: Epoch [9], Batch [669/938], Loss: 0.39340370893478394\n",
      "Train: Epoch [9], Batch [670/938], Loss: 0.46345025300979614\n",
      "Train: Epoch [9], Batch [671/938], Loss: 0.5037436485290527\n",
      "Train: Epoch [9], Batch [672/938], Loss: 0.4302220642566681\n",
      "Train: Epoch [9], Batch [673/938], Loss: 0.6201568245887756\n",
      "Train: Epoch [9], Batch [674/938], Loss: 0.4998517632484436\n",
      "Train: Epoch [9], Batch [675/938], Loss: 0.5730338096618652\n",
      "Train: Epoch [9], Batch [676/938], Loss: 0.48904457688331604\n",
      "Train: Epoch [9], Batch [677/938], Loss: 0.4555087685585022\n",
      "Train: Epoch [9], Batch [678/938], Loss: 0.5463764667510986\n",
      "Train: Epoch [9], Batch [679/938], Loss: 0.5436286330223083\n",
      "Train: Epoch [9], Batch [680/938], Loss: 0.4704887866973877\n",
      "Train: Epoch [9], Batch [681/938], Loss: 0.6065492033958435\n",
      "Train: Epoch [9], Batch [682/938], Loss: 0.4476049542427063\n",
      "Train: Epoch [9], Batch [683/938], Loss: 0.32206305861473083\n",
      "Train: Epoch [9], Batch [684/938], Loss: 0.7269628047943115\n",
      "Train: Epoch [9], Batch [685/938], Loss: 0.45711377263069153\n",
      "Train: Epoch [9], Batch [686/938], Loss: 0.4097956418991089\n",
      "Train: Epoch [9], Batch [687/938], Loss: 0.5177855491638184\n",
      "Train: Epoch [9], Batch [688/938], Loss: 0.5245436429977417\n",
      "Train: Epoch [9], Batch [689/938], Loss: 0.662430465221405\n",
      "Train: Epoch [9], Batch [690/938], Loss: 0.5209985375404358\n",
      "Train: Epoch [9], Batch [691/938], Loss: 0.5258102416992188\n",
      "Train: Epoch [9], Batch [692/938], Loss: 0.40924572944641113\n",
      "Train: Epoch [9], Batch [693/938], Loss: 0.6058017611503601\n",
      "Train: Epoch [9], Batch [694/938], Loss: 0.6667085886001587\n",
      "Train: Epoch [9], Batch [695/938], Loss: 0.4485076367855072\n",
      "Train: Epoch [9], Batch [696/938], Loss: 0.4407234191894531\n",
      "Train: Epoch [9], Batch [697/938], Loss: 0.6443721652030945\n",
      "Train: Epoch [9], Batch [698/938], Loss: 0.5038892030715942\n",
      "Train: Epoch [9], Batch [699/938], Loss: 0.7621304988861084\n",
      "Train: Epoch [9], Batch [700/938], Loss: 0.62083899974823\n",
      "Train: Epoch [9], Batch [701/938], Loss: 0.58890700340271\n",
      "Train: Epoch [9], Batch [702/938], Loss: 0.6043292284011841\n",
      "Train: Epoch [9], Batch [703/938], Loss: 0.5327550768852234\n",
      "Train: Epoch [9], Batch [704/938], Loss: 0.4952158033847809\n",
      "Train: Epoch [9], Batch [705/938], Loss: 0.4457119107246399\n",
      "Train: Epoch [9], Batch [706/938], Loss: 0.48131048679351807\n",
      "Train: Epoch [9], Batch [707/938], Loss: 0.5678666830062866\n",
      "Train: Epoch [9], Batch [708/938], Loss: 0.45675399899482727\n",
      "Train: Epoch [9], Batch [709/938], Loss: 0.4649220407009125\n",
      "Train: Epoch [9], Batch [710/938], Loss: 0.607937216758728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [9], Batch [711/938], Loss: 0.7569230198860168\n",
      "Train: Epoch [9], Batch [712/938], Loss: 0.5518531203269958\n",
      "Train: Epoch [9], Batch [713/938], Loss: 0.6059723496437073\n",
      "Train: Epoch [9], Batch [714/938], Loss: 0.6747649312019348\n",
      "Train: Epoch [9], Batch [715/938], Loss: 0.44561466574668884\n",
      "Train: Epoch [9], Batch [716/938], Loss: 0.5078518390655518\n",
      "Train: Epoch [9], Batch [717/938], Loss: 0.9812812805175781\n",
      "Train: Epoch [9], Batch [718/938], Loss: 0.4501509666442871\n",
      "Train: Epoch [9], Batch [719/938], Loss: 0.3782021701335907\n",
      "Train: Epoch [9], Batch [720/938], Loss: 0.5991014242172241\n",
      "Train: Epoch [9], Batch [721/938], Loss: 0.712710440158844\n",
      "Train: Epoch [9], Batch [722/938], Loss: 0.5671225786209106\n",
      "Train: Epoch [9], Batch [723/938], Loss: 0.3760027289390564\n",
      "Train: Epoch [9], Batch [724/938], Loss: 0.4455588459968567\n",
      "Train: Epoch [9], Batch [725/938], Loss: 0.7295454740524292\n",
      "Train: Epoch [9], Batch [726/938], Loss: 0.45763099193573\n",
      "Train: Epoch [9], Batch [727/938], Loss: 0.5333570837974548\n",
      "Train: Epoch [9], Batch [728/938], Loss: 0.6382547616958618\n",
      "Train: Epoch [9], Batch [729/938], Loss: 0.3884046971797943\n",
      "Train: Epoch [9], Batch [730/938], Loss: 0.5742039084434509\n",
      "Train: Epoch [9], Batch [731/938], Loss: 0.4893214702606201\n",
      "Train: Epoch [9], Batch [732/938], Loss: 0.4998302459716797\n",
      "Train: Epoch [9], Batch [733/938], Loss: 0.4263506531715393\n",
      "Train: Epoch [9], Batch [734/938], Loss: 0.3687928318977356\n",
      "Train: Epoch [9], Batch [735/938], Loss: 0.6191246509552002\n",
      "Train: Epoch [9], Batch [736/938], Loss: 0.5027730464935303\n",
      "Train: Epoch [9], Batch [737/938], Loss: 0.6060869693756104\n",
      "Train: Epoch [9], Batch [738/938], Loss: 0.47240954637527466\n",
      "Train: Epoch [9], Batch [739/938], Loss: 0.5091757774353027\n",
      "Train: Epoch [9], Batch [740/938], Loss: 0.7622517347335815\n",
      "Train: Epoch [9], Batch [741/938], Loss: 0.8143979907035828\n",
      "Train: Epoch [9], Batch [742/938], Loss: 0.6721521615982056\n",
      "Train: Epoch [9], Batch [743/938], Loss: 0.42092055082321167\n",
      "Train: Epoch [9], Batch [744/938], Loss: 0.4238702654838562\n",
      "Train: Epoch [9], Batch [745/938], Loss: 0.6844635605812073\n",
      "Train: Epoch [9], Batch [746/938], Loss: 0.454175740480423\n",
      "Train: Epoch [9], Batch [747/938], Loss: 0.5824191570281982\n",
      "Train: Epoch [9], Batch [748/938], Loss: 0.609971284866333\n",
      "Train: Epoch [9], Batch [749/938], Loss: 0.5692999362945557\n",
      "Train: Epoch [9], Batch [750/938], Loss: 0.5857837200164795\n",
      "Train: Epoch [9], Batch [751/938], Loss: 0.6113313436508179\n",
      "Train: Epoch [9], Batch [752/938], Loss: 0.5661267638206482\n",
      "Train: Epoch [9], Batch [753/938], Loss: 0.46017852425575256\n",
      "Train: Epoch [9], Batch [754/938], Loss: 0.47285640239715576\n",
      "Train: Epoch [9], Batch [755/938], Loss: 0.48647579550743103\n",
      "Train: Epoch [9], Batch [756/938], Loss: 0.4452400803565979\n",
      "Train: Epoch [9], Batch [757/938], Loss: 0.5136077404022217\n",
      "Train: Epoch [9], Batch [758/938], Loss: 0.6033393740653992\n",
      "Train: Epoch [9], Batch [759/938], Loss: 0.6174370050430298\n",
      "Train: Epoch [9], Batch [760/938], Loss: 0.43373823165893555\n",
      "Train: Epoch [9], Batch [761/938], Loss: 0.5463463068008423\n",
      "Train: Epoch [9], Batch [762/938], Loss: 0.6555964946746826\n",
      "Train: Epoch [9], Batch [763/938], Loss: 0.4792957603931427\n",
      "Train: Epoch [9], Batch [764/938], Loss: 0.6694415807723999\n",
      "Train: Epoch [9], Batch [765/938], Loss: 0.6611325740814209\n",
      "Train: Epoch [9], Batch [766/938], Loss: 0.35444140434265137\n",
      "Train: Epoch [9], Batch [767/938], Loss: 0.3882750868797302\n",
      "Train: Epoch [9], Batch [768/938], Loss: 0.5902860760688782\n",
      "Train: Epoch [9], Batch [769/938], Loss: 0.7663898468017578\n",
      "Train: Epoch [9], Batch [770/938], Loss: 0.6193594932556152\n",
      "Train: Epoch [9], Batch [771/938], Loss: 0.5912081003189087\n",
      "Train: Epoch [9], Batch [772/938], Loss: 0.49164479970932007\n",
      "Train: Epoch [9], Batch [773/938], Loss: 0.4073270261287689\n",
      "Train: Epoch [9], Batch [774/938], Loss: 0.49512434005737305\n",
      "Train: Epoch [9], Batch [775/938], Loss: 0.34952405095100403\n",
      "Train: Epoch [9], Batch [776/938], Loss: 0.538676381111145\n",
      "Train: Epoch [9], Batch [777/938], Loss: 0.4597829282283783\n",
      "Train: Epoch [9], Batch [778/938], Loss: 0.5056741237640381\n",
      "Train: Epoch [9], Batch [779/938], Loss: 0.4873266816139221\n",
      "Train: Epoch [9], Batch [780/938], Loss: 0.7016714811325073\n",
      "Train: Epoch [9], Batch [781/938], Loss: 0.791598379611969\n",
      "Train: Epoch [9], Batch [782/938], Loss: 0.7200255393981934\n",
      "Train: Epoch [9], Batch [783/938], Loss: 0.7728147506713867\n",
      "Train: Epoch [9], Batch [784/938], Loss: 0.5101535320281982\n",
      "Train: Epoch [9], Batch [785/938], Loss: 0.49487805366516113\n",
      "Train: Epoch [9], Batch [786/938], Loss: 0.42311349511146545\n",
      "Train: Epoch [9], Batch [787/938], Loss: 0.5728751420974731\n",
      "Train: Epoch [9], Batch [788/938], Loss: 0.4603222906589508\n",
      "Train: Epoch [9], Batch [789/938], Loss: 0.6194140911102295\n",
      "Train: Epoch [9], Batch [790/938], Loss: 0.5568351745605469\n",
      "Train: Epoch [9], Batch [791/938], Loss: 0.5114853978157043\n",
      "Train: Epoch [9], Batch [792/938], Loss: 0.5429205894470215\n",
      "Train: Epoch [9], Batch [793/938], Loss: 0.4802819788455963\n",
      "Train: Epoch [9], Batch [794/938], Loss: 0.5664862394332886\n",
      "Train: Epoch [9], Batch [795/938], Loss: 0.3227108120918274\n",
      "Train: Epoch [9], Batch [796/938], Loss: 0.47646564245224\n",
      "Train: Epoch [9], Batch [797/938], Loss: 0.5290567874908447\n",
      "Train: Epoch [9], Batch [798/938], Loss: 0.5733864307403564\n",
      "Train: Epoch [9], Batch [799/938], Loss: 0.7173283696174622\n",
      "Train: Epoch [9], Batch [800/938], Loss: 0.7563841342926025\n",
      "Train: Epoch [9], Batch [801/938], Loss: 0.47476792335510254\n",
      "Train: Epoch [9], Batch [802/938], Loss: 0.5813422203063965\n",
      "Train: Epoch [9], Batch [803/938], Loss: 0.4952343702316284\n",
      "Train: Epoch [9], Batch [804/938], Loss: 0.5828441381454468\n",
      "Train: Epoch [9], Batch [805/938], Loss: 0.4234601855278015\n",
      "Train: Epoch [9], Batch [806/938], Loss: 0.6356968879699707\n",
      "Train: Epoch [9], Batch [807/938], Loss: 0.3486540913581848\n",
      "Train: Epoch [9], Batch [808/938], Loss: 0.6186045408248901\n",
      "Train: Epoch [9], Batch [809/938], Loss: 0.5441296100616455\n",
      "Train: Epoch [9], Batch [810/938], Loss: 0.6428402662277222\n",
      "Train: Epoch [9], Batch [811/938], Loss: 0.6224104166030884\n",
      "Train: Epoch [9], Batch [812/938], Loss: 0.6758747100830078\n",
      "Train: Epoch [9], Batch [813/938], Loss: 0.5800312757492065\n",
      "Train: Epoch [9], Batch [814/938], Loss: 0.3946625590324402\n",
      "Train: Epoch [9], Batch [815/938], Loss: 0.6878432631492615\n",
      "Train: Epoch [9], Batch [816/938], Loss: 0.5041589736938477\n",
      "Train: Epoch [9], Batch [817/938], Loss: 0.5081967115402222\n",
      "Train: Epoch [9], Batch [818/938], Loss: 0.49251312017440796\n",
      "Train: Epoch [9], Batch [819/938], Loss: 0.5178106427192688\n",
      "Train: Epoch [9], Batch [820/938], Loss: 0.5328410267829895\n",
      "Train: Epoch [9], Batch [821/938], Loss: 0.4344463348388672\n",
      "Train: Epoch [9], Batch [822/938], Loss: 0.4135361909866333\n",
      "Train: Epoch [9], Batch [823/938], Loss: 0.5651916265487671\n",
      "Train: Epoch [9], Batch [824/938], Loss: 0.5307164192199707\n",
      "Train: Epoch [9], Batch [825/938], Loss: 0.42592549324035645\n",
      "Train: Epoch [9], Batch [826/938], Loss: 0.6711207032203674\n",
      "Train: Epoch [9], Batch [827/938], Loss: 0.5323776006698608\n",
      "Train: Epoch [9], Batch [828/938], Loss: 0.5293322801589966\n",
      "Train: Epoch [9], Batch [829/938], Loss: 0.4651199281215668\n",
      "Train: Epoch [9], Batch [830/938], Loss: 0.7418104410171509\n",
      "Train: Epoch [9], Batch [831/938], Loss: 0.4618135392665863\n",
      "Train: Epoch [9], Batch [832/938], Loss: 0.6701157689094543\n",
      "Train: Epoch [9], Batch [833/938], Loss: 0.5168215036392212\n",
      "Train: Epoch [9], Batch [834/938], Loss: 0.8270554542541504\n",
      "Train: Epoch [9], Batch [835/938], Loss: 0.5843138098716736\n",
      "Train: Epoch [9], Batch [836/938], Loss: 0.5335251688957214\n",
      "Train: Epoch [9], Batch [837/938], Loss: 0.6072292327880859\n",
      "Train: Epoch [9], Batch [838/938], Loss: 0.5671292543411255\n",
      "Train: Epoch [9], Batch [839/938], Loss: 0.41146397590637207\n",
      "Train: Epoch [9], Batch [840/938], Loss: 0.550647497177124\n",
      "Train: Epoch [9], Batch [841/938], Loss: 0.45641371607780457\n",
      "Train: Epoch [9], Batch [842/938], Loss: 0.5382769107818604\n",
      "Train: Epoch [9], Batch [843/938], Loss: 0.48608675599098206\n",
      "Train: Epoch [9], Batch [844/938], Loss: 0.46574223041534424\n",
      "Train: Epoch [9], Batch [845/938], Loss: 0.4628768563270569\n",
      "Train: Epoch [9], Batch [846/938], Loss: 0.5924705862998962\n",
      "Train: Epoch [9], Batch [847/938], Loss: 0.5815878510475159\n",
      "Train: Epoch [9], Batch [848/938], Loss: 0.5805542469024658\n",
      "Train: Epoch [9], Batch [849/938], Loss: 0.4765641689300537\n",
      "Train: Epoch [9], Batch [850/938], Loss: 0.6406075954437256\n",
      "Train: Epoch [9], Batch [851/938], Loss: 0.762621283531189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [9], Batch [852/938], Loss: 0.7316567897796631\n",
      "Train: Epoch [9], Batch [853/938], Loss: 0.41859692335128784\n",
      "Train: Epoch [9], Batch [854/938], Loss: 0.6864553689956665\n",
      "Train: Epoch [9], Batch [855/938], Loss: 0.7348034381866455\n",
      "Train: Epoch [9], Batch [856/938], Loss: 0.606847882270813\n",
      "Train: Epoch [9], Batch [857/938], Loss: 0.7646167278289795\n",
      "Train: Epoch [9], Batch [858/938], Loss: 0.4657810628414154\n",
      "Train: Epoch [9], Batch [859/938], Loss: 0.6293730139732361\n",
      "Train: Epoch [9], Batch [860/938], Loss: 0.3527106046676636\n",
      "Train: Epoch [9], Batch [861/938], Loss: 0.5991427898406982\n",
      "Train: Epoch [9], Batch [862/938], Loss: 0.48319992423057556\n",
      "Train: Epoch [9], Batch [863/938], Loss: 0.5511513948440552\n",
      "Train: Epoch [9], Batch [864/938], Loss: 0.4481010437011719\n",
      "Train: Epoch [9], Batch [865/938], Loss: 0.5838651657104492\n",
      "Train: Epoch [9], Batch [866/938], Loss: 0.5077629685401917\n",
      "Train: Epoch [9], Batch [867/938], Loss: 0.6650574207305908\n",
      "Train: Epoch [9], Batch [868/938], Loss: 0.4748920798301697\n",
      "Train: Epoch [9], Batch [869/938], Loss: 0.5507790446281433\n",
      "Train: Epoch [9], Batch [870/938], Loss: 0.46607547998428345\n",
      "Train: Epoch [9], Batch [871/938], Loss: 0.7561614513397217\n",
      "Train: Epoch [9], Batch [872/938], Loss: 0.5218092203140259\n",
      "Train: Epoch [9], Batch [873/938], Loss: 0.6125864386558533\n",
      "Train: Epoch [9], Batch [874/938], Loss: 0.5750623345375061\n",
      "Train: Epoch [9], Batch [875/938], Loss: 0.7965409159660339\n",
      "Train: Epoch [9], Batch [876/938], Loss: 0.6222789287567139\n",
      "Train: Epoch [9], Batch [877/938], Loss: 0.6233245730400085\n",
      "Train: Epoch [9], Batch [878/938], Loss: 0.7606095671653748\n",
      "Train: Epoch [9], Batch [879/938], Loss: 0.5738785266876221\n",
      "Train: Epoch [9], Batch [880/938], Loss: 0.5797846913337708\n",
      "Train: Epoch [9], Batch [881/938], Loss: 0.7190853357315063\n",
      "Train: Epoch [9], Batch [882/938], Loss: 0.5622102618217468\n",
      "Train: Epoch [9], Batch [883/938], Loss: 0.4903961420059204\n",
      "Train: Epoch [9], Batch [884/938], Loss: 0.7551316618919373\n",
      "Train: Epoch [9], Batch [885/938], Loss: 0.5597035884857178\n",
      "Train: Epoch [9], Batch [886/938], Loss: 0.3457496762275696\n",
      "Train: Epoch [9], Batch [887/938], Loss: 0.6983212232589722\n",
      "Train: Epoch [9], Batch [888/938], Loss: 0.4050121307373047\n",
      "Train: Epoch [9], Batch [889/938], Loss: 0.5642118453979492\n",
      "Train: Epoch [9], Batch [890/938], Loss: 0.4336070716381073\n",
      "Train: Epoch [9], Batch [891/938], Loss: 0.47567322850227356\n",
      "Train: Epoch [9], Batch [892/938], Loss: 0.38289856910705566\n",
      "Train: Epoch [9], Batch [893/938], Loss: 0.3489530682563782\n",
      "Train: Epoch [9], Batch [894/938], Loss: 0.7007008790969849\n",
      "Train: Epoch [9], Batch [895/938], Loss: 0.6500816345214844\n",
      "Train: Epoch [9], Batch [896/938], Loss: 0.5784294605255127\n",
      "Train: Epoch [9], Batch [897/938], Loss: 0.5713706016540527\n",
      "Train: Epoch [9], Batch [898/938], Loss: 0.4752015471458435\n",
      "Train: Epoch [9], Batch [899/938], Loss: 0.5352879762649536\n",
      "Train: Epoch [9], Batch [900/938], Loss: 0.5103516578674316\n",
      "Train: Epoch [9], Batch [901/938], Loss: 0.562362790107727\n",
      "Train: Epoch [9], Batch [902/938], Loss: 0.4517883360385895\n",
      "Train: Epoch [9], Batch [903/938], Loss: 0.6451215744018555\n",
      "Train: Epoch [9], Batch [904/938], Loss: 0.48789143562316895\n",
      "Train: Epoch [9], Batch [905/938], Loss: 0.703035831451416\n",
      "Train: Epoch [9], Batch [906/938], Loss: 0.7179665565490723\n",
      "Train: Epoch [9], Batch [907/938], Loss: 0.6440976858139038\n",
      "Train: Epoch [9], Batch [908/938], Loss: 0.6162545084953308\n",
      "Train: Epoch [9], Batch [909/938], Loss: 0.6686389446258545\n",
      "Train: Epoch [9], Batch [910/938], Loss: 0.7214487791061401\n",
      "Train: Epoch [9], Batch [911/938], Loss: 0.4167240262031555\n",
      "Train: Epoch [9], Batch [912/938], Loss: 0.6073026061058044\n",
      "Train: Epoch [9], Batch [913/938], Loss: 0.676307201385498\n",
      "Train: Epoch [9], Batch [914/938], Loss: 0.5860276222229004\n",
      "Train: Epoch [9], Batch [915/938], Loss: 0.5781859159469604\n",
      "Train: Epoch [9], Batch [916/938], Loss: 0.3743956387042999\n",
      "Train: Epoch [9], Batch [917/938], Loss: 0.6657259464263916\n",
      "Train: Epoch [9], Batch [918/938], Loss: 0.5210642218589783\n",
      "Train: Epoch [9], Batch [919/938], Loss: 0.3500937819480896\n",
      "Train: Epoch [9], Batch [920/938], Loss: 0.4738312363624573\n",
      "Train: Epoch [9], Batch [921/938], Loss: 0.6224234700202942\n",
      "Train: Epoch [9], Batch [922/938], Loss: 0.6918409466743469\n",
      "Train: Epoch [9], Batch [923/938], Loss: 0.3163788914680481\n",
      "Train: Epoch [9], Batch [924/938], Loss: 0.754980742931366\n",
      "Train: Epoch [9], Batch [925/938], Loss: 0.6229133009910583\n",
      "Train: Epoch [9], Batch [926/938], Loss: 0.4991815388202667\n",
      "Train: Epoch [9], Batch [927/938], Loss: 0.4578917920589447\n",
      "Train: Epoch [9], Batch [928/938], Loss: 0.7422858476638794\n",
      "Train: Epoch [9], Batch [929/938], Loss: 0.6204695105552673\n",
      "Train: Epoch [9], Batch [930/938], Loss: 0.5995234251022339\n",
      "Train: Epoch [9], Batch [931/938], Loss: 0.38076287508010864\n",
      "Train: Epoch [9], Batch [932/938], Loss: 0.39044883847236633\n",
      "Train: Epoch [9], Batch [933/938], Loss: 0.9809603691101074\n",
      "Train: Epoch [9], Batch [934/938], Loss: 0.6633316874504089\n",
      "Train: Epoch [9], Batch [935/938], Loss: 0.6328294277191162\n",
      "Train: Epoch [9], Batch [936/938], Loss: 0.5072943568229675\n",
      "Train: Epoch [9], Batch [937/938], Loss: 0.6867086887359619\n",
      "Train: Epoch [9], Batch [938/938], Loss: 0.42699679732322693\n",
      "Accuracy of train set: 0.8047833333333333\n",
      "Validation: Epoch [9], Batch [1/938], Loss: 0.5399587750434875\n",
      "Validation: Epoch [9], Batch [2/938], Loss: 0.5503001809120178\n",
      "Validation: Epoch [9], Batch [3/938], Loss: 0.4087878465652466\n",
      "Validation: Epoch [9], Batch [4/938], Loss: 0.585774302482605\n",
      "Validation: Epoch [9], Batch [5/938], Loss: 0.49671533703804016\n",
      "Validation: Epoch [9], Batch [6/938], Loss: 0.7307925820350647\n",
      "Validation: Epoch [9], Batch [7/938], Loss: 0.40214598178863525\n",
      "Validation: Epoch [9], Batch [8/938], Loss: 0.5314409732818604\n",
      "Validation: Epoch [9], Batch [9/938], Loss: 0.5477874279022217\n",
      "Validation: Epoch [9], Batch [10/938], Loss: 0.4481715261936188\n",
      "Validation: Epoch [9], Batch [11/938], Loss: 0.5588877201080322\n",
      "Validation: Epoch [9], Batch [12/938], Loss: 0.6170217990875244\n",
      "Validation: Epoch [9], Batch [13/938], Loss: 0.506603479385376\n",
      "Validation: Epoch [9], Batch [14/938], Loss: 0.5777690410614014\n",
      "Validation: Epoch [9], Batch [15/938], Loss: 0.29006868600845337\n",
      "Validation: Epoch [9], Batch [16/938], Loss: 0.47200655937194824\n",
      "Validation: Epoch [9], Batch [17/938], Loss: 0.7200859189033508\n",
      "Validation: Epoch [9], Batch [18/938], Loss: 0.4574275314807892\n",
      "Validation: Epoch [9], Batch [19/938], Loss: 0.6175312995910645\n",
      "Validation: Epoch [9], Batch [20/938], Loss: 0.49438005685806274\n",
      "Validation: Epoch [9], Batch [21/938], Loss: 0.5768262147903442\n",
      "Validation: Epoch [9], Batch [22/938], Loss: 0.7879619598388672\n",
      "Validation: Epoch [9], Batch [23/938], Loss: 0.45453551411628723\n",
      "Validation: Epoch [9], Batch [24/938], Loss: 0.687102735042572\n",
      "Validation: Epoch [9], Batch [25/938], Loss: 0.5025890469551086\n",
      "Validation: Epoch [9], Batch [26/938], Loss: 0.4819803833961487\n",
      "Validation: Epoch [9], Batch [27/938], Loss: 0.5745698809623718\n",
      "Validation: Epoch [9], Batch [28/938], Loss: 0.5352650284767151\n",
      "Validation: Epoch [9], Batch [29/938], Loss: 0.4075780212879181\n",
      "Validation: Epoch [9], Batch [30/938], Loss: 0.5188143253326416\n",
      "Validation: Epoch [9], Batch [31/938], Loss: 0.45367130637168884\n",
      "Validation: Epoch [9], Batch [32/938], Loss: 0.5061314702033997\n",
      "Validation: Epoch [9], Batch [33/938], Loss: 0.5337526798248291\n",
      "Validation: Epoch [9], Batch [34/938], Loss: 0.4165879487991333\n",
      "Validation: Epoch [9], Batch [35/938], Loss: 0.5591236352920532\n",
      "Validation: Epoch [9], Batch [36/938], Loss: 0.4622192978858948\n",
      "Validation: Epoch [9], Batch [37/938], Loss: 0.44283661246299744\n",
      "Validation: Epoch [9], Batch [38/938], Loss: 0.6286828517913818\n",
      "Validation: Epoch [9], Batch [39/938], Loss: 0.4870995283126831\n",
      "Validation: Epoch [9], Batch [40/938], Loss: 0.5216763019561768\n",
      "Validation: Epoch [9], Batch [41/938], Loss: 0.5021920800209045\n",
      "Validation: Epoch [9], Batch [42/938], Loss: 0.4908263683319092\n",
      "Validation: Epoch [9], Batch [43/938], Loss: 0.5844215154647827\n",
      "Validation: Epoch [9], Batch [44/938], Loss: 0.694439709186554\n",
      "Validation: Epoch [9], Batch [45/938], Loss: 0.7476751208305359\n",
      "Validation: Epoch [9], Batch [46/938], Loss: 0.4929739832878113\n",
      "Validation: Epoch [9], Batch [47/938], Loss: 0.4932374060153961\n",
      "Validation: Epoch [9], Batch [48/938], Loss: 0.4767894446849823\n",
      "Validation: Epoch [9], Batch [49/938], Loss: 0.4513195753097534\n",
      "Validation: Epoch [9], Batch [50/938], Loss: 0.44403475522994995\n",
      "Validation: Epoch [9], Batch [51/938], Loss: 0.6535265445709229\n",
      "Validation: Epoch [9], Batch [52/938], Loss: 0.45538341999053955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [53/938], Loss: 0.5841339826583862\n",
      "Validation: Epoch [9], Batch [54/938], Loss: 0.604939341545105\n",
      "Validation: Epoch [9], Batch [55/938], Loss: 0.5442920923233032\n",
      "Validation: Epoch [9], Batch [56/938], Loss: 0.6337541341781616\n",
      "Validation: Epoch [9], Batch [57/938], Loss: 0.9342132210731506\n",
      "Validation: Epoch [9], Batch [58/938], Loss: 0.4329397976398468\n",
      "Validation: Epoch [9], Batch [59/938], Loss: 0.47887933254241943\n",
      "Validation: Epoch [9], Batch [60/938], Loss: 0.4859892725944519\n",
      "Validation: Epoch [9], Batch [61/938], Loss: 0.5850294828414917\n",
      "Validation: Epoch [9], Batch [62/938], Loss: 0.509239673614502\n",
      "Validation: Epoch [9], Batch [63/938], Loss: 0.5108973979949951\n",
      "Validation: Epoch [9], Batch [64/938], Loss: 0.450908899307251\n",
      "Validation: Epoch [9], Batch [65/938], Loss: 0.5724884271621704\n",
      "Validation: Epoch [9], Batch [66/938], Loss: 0.6223208904266357\n",
      "Validation: Epoch [9], Batch [67/938], Loss: 0.8031080961227417\n",
      "Validation: Epoch [9], Batch [68/938], Loss: 0.48531246185302734\n",
      "Validation: Epoch [9], Batch [69/938], Loss: 0.5539990663528442\n",
      "Validation: Epoch [9], Batch [70/938], Loss: 0.521734356880188\n",
      "Validation: Epoch [9], Batch [71/938], Loss: 0.6812378764152527\n",
      "Validation: Epoch [9], Batch [72/938], Loss: 0.4977494776248932\n",
      "Validation: Epoch [9], Batch [73/938], Loss: 0.5459339022636414\n",
      "Validation: Epoch [9], Batch [74/938], Loss: 0.7691338062286377\n",
      "Validation: Epoch [9], Batch [75/938], Loss: 0.7917582392692566\n",
      "Validation: Epoch [9], Batch [76/938], Loss: 0.6703929901123047\n",
      "Validation: Epoch [9], Batch [77/938], Loss: 0.5771622061729431\n",
      "Validation: Epoch [9], Batch [78/938], Loss: 0.607894778251648\n",
      "Validation: Epoch [9], Batch [79/938], Loss: 0.44980213046073914\n",
      "Validation: Epoch [9], Batch [80/938], Loss: 0.4251689910888672\n",
      "Validation: Epoch [9], Batch [81/938], Loss: 0.5641047954559326\n",
      "Validation: Epoch [9], Batch [82/938], Loss: 0.5820709466934204\n",
      "Validation: Epoch [9], Batch [83/938], Loss: 0.5205168724060059\n",
      "Validation: Epoch [9], Batch [84/938], Loss: 0.5279432535171509\n",
      "Validation: Epoch [9], Batch [85/938], Loss: 0.42620301246643066\n",
      "Validation: Epoch [9], Batch [86/938], Loss: 0.4740152955055237\n",
      "Validation: Epoch [9], Batch [87/938], Loss: 0.5850887298583984\n",
      "Validation: Epoch [9], Batch [88/938], Loss: 0.4022795557975769\n",
      "Validation: Epoch [9], Batch [89/938], Loss: 0.4368188977241516\n",
      "Validation: Epoch [9], Batch [90/938], Loss: 0.5232487320899963\n",
      "Validation: Epoch [9], Batch [91/938], Loss: 0.5294145345687866\n",
      "Validation: Epoch [9], Batch [92/938], Loss: 0.49146977066993713\n",
      "Validation: Epoch [9], Batch [93/938], Loss: 0.5130326151847839\n",
      "Validation: Epoch [9], Batch [94/938], Loss: 0.7146325707435608\n",
      "Validation: Epoch [9], Batch [95/938], Loss: 0.5822367668151855\n",
      "Validation: Epoch [9], Batch [96/938], Loss: 0.29738670587539673\n",
      "Validation: Epoch [9], Batch [97/938], Loss: 0.4691261649131775\n",
      "Validation: Epoch [9], Batch [98/938], Loss: 0.4226599931716919\n",
      "Validation: Epoch [9], Batch [99/938], Loss: 0.46697092056274414\n",
      "Validation: Epoch [9], Batch [100/938], Loss: 0.6367706060409546\n",
      "Validation: Epoch [9], Batch [101/938], Loss: 0.4753762483596802\n",
      "Validation: Epoch [9], Batch [102/938], Loss: 0.617542028427124\n",
      "Validation: Epoch [9], Batch [103/938], Loss: 0.5852330923080444\n",
      "Validation: Epoch [9], Batch [104/938], Loss: 0.6090435981750488\n",
      "Validation: Epoch [9], Batch [105/938], Loss: 0.6094402074813843\n",
      "Validation: Epoch [9], Batch [106/938], Loss: 0.5287127494812012\n",
      "Validation: Epoch [9], Batch [107/938], Loss: 0.3970052897930145\n",
      "Validation: Epoch [9], Batch [108/938], Loss: 0.45525598526000977\n",
      "Validation: Epoch [9], Batch [109/938], Loss: 0.45803403854370117\n",
      "Validation: Epoch [9], Batch [110/938], Loss: 0.49107009172439575\n",
      "Validation: Epoch [9], Batch [111/938], Loss: 0.5639073848724365\n",
      "Validation: Epoch [9], Batch [112/938], Loss: 0.5665985345840454\n",
      "Validation: Epoch [9], Batch [113/938], Loss: 0.7425832748413086\n",
      "Validation: Epoch [9], Batch [114/938], Loss: 0.6959420442581177\n",
      "Validation: Epoch [9], Batch [115/938], Loss: 0.5993860960006714\n",
      "Validation: Epoch [9], Batch [116/938], Loss: 0.4872748553752899\n",
      "Validation: Epoch [9], Batch [117/938], Loss: 0.4814595580101013\n",
      "Validation: Epoch [9], Batch [118/938], Loss: 0.4922066330909729\n",
      "Validation: Epoch [9], Batch [119/938], Loss: 0.5941690802574158\n",
      "Validation: Epoch [9], Batch [120/938], Loss: 0.5878033638000488\n",
      "Validation: Epoch [9], Batch [121/938], Loss: 0.4233587384223938\n",
      "Validation: Epoch [9], Batch [122/938], Loss: 0.5702716112136841\n",
      "Validation: Epoch [9], Batch [123/938], Loss: 0.4129915237426758\n",
      "Validation: Epoch [9], Batch [124/938], Loss: 0.6007552146911621\n",
      "Validation: Epoch [9], Batch [125/938], Loss: 0.6148589253425598\n",
      "Validation: Epoch [9], Batch [126/938], Loss: 0.3718796670436859\n",
      "Validation: Epoch [9], Batch [127/938], Loss: 0.3531857132911682\n",
      "Validation: Epoch [9], Batch [128/938], Loss: 0.6487799882888794\n",
      "Validation: Epoch [9], Batch [129/938], Loss: 0.48518821597099304\n",
      "Validation: Epoch [9], Batch [130/938], Loss: 0.5921615362167358\n",
      "Validation: Epoch [9], Batch [131/938], Loss: 0.4556342661380768\n",
      "Validation: Epoch [9], Batch [132/938], Loss: 0.5065740942955017\n",
      "Validation: Epoch [9], Batch [133/938], Loss: 0.5419914722442627\n",
      "Validation: Epoch [9], Batch [134/938], Loss: 0.5146576166152954\n",
      "Validation: Epoch [9], Batch [135/938], Loss: 0.4060574173927307\n",
      "Validation: Epoch [9], Batch [136/938], Loss: 0.4751706123352051\n",
      "Validation: Epoch [9], Batch [137/938], Loss: 0.47552916407585144\n",
      "Validation: Epoch [9], Batch [138/938], Loss: 0.5509276390075684\n",
      "Validation: Epoch [9], Batch [139/938], Loss: 0.40335965156555176\n",
      "Validation: Epoch [9], Batch [140/938], Loss: 0.5284255743026733\n",
      "Validation: Epoch [9], Batch [141/938], Loss: 0.6379983425140381\n",
      "Validation: Epoch [9], Batch [142/938], Loss: 0.6507882475852966\n",
      "Validation: Epoch [9], Batch [143/938], Loss: 0.6104311943054199\n",
      "Validation: Epoch [9], Batch [144/938], Loss: 0.48512065410614014\n",
      "Validation: Epoch [9], Batch [145/938], Loss: 0.6661782264709473\n",
      "Validation: Epoch [9], Batch [146/938], Loss: 0.6008164286613464\n",
      "Validation: Epoch [9], Batch [147/938], Loss: 0.4947323501110077\n",
      "Validation: Epoch [9], Batch [148/938], Loss: 0.5493159890174866\n",
      "Validation: Epoch [9], Batch [149/938], Loss: 0.5595800876617432\n",
      "Validation: Epoch [9], Batch [150/938], Loss: 0.7300801277160645\n",
      "Validation: Epoch [9], Batch [151/938], Loss: 0.6607280969619751\n",
      "Validation: Epoch [9], Batch [152/938], Loss: 0.6471776962280273\n",
      "Validation: Epoch [9], Batch [153/938], Loss: 0.43880221247673035\n",
      "Validation: Epoch [9], Batch [154/938], Loss: 0.5555698871612549\n",
      "Validation: Epoch [9], Batch [155/938], Loss: 0.42742300033569336\n",
      "Validation: Epoch [9], Batch [156/938], Loss: 0.64450603723526\n",
      "Validation: Epoch [9], Batch [157/938], Loss: 0.3987278342247009\n",
      "Validation: Epoch [9], Batch [158/938], Loss: 0.6048579216003418\n",
      "Validation: Epoch [9], Batch [159/938], Loss: 0.5337079167366028\n",
      "Validation: Epoch [9], Batch [160/938], Loss: 0.5110341310501099\n",
      "Validation: Epoch [9], Batch [161/938], Loss: 0.6079996824264526\n",
      "Validation: Epoch [9], Batch [162/938], Loss: 0.5091829895973206\n",
      "Validation: Epoch [9], Batch [163/938], Loss: 0.38839733600616455\n",
      "Validation: Epoch [9], Batch [164/938], Loss: 0.5825064182281494\n",
      "Validation: Epoch [9], Batch [165/938], Loss: 0.5431323647499084\n",
      "Validation: Epoch [9], Batch [166/938], Loss: 0.4734042286872864\n",
      "Validation: Epoch [9], Batch [167/938], Loss: 0.5373038053512573\n",
      "Validation: Epoch [9], Batch [168/938], Loss: 0.5074197053909302\n",
      "Validation: Epoch [9], Batch [169/938], Loss: 0.5047205686569214\n",
      "Validation: Epoch [9], Batch [170/938], Loss: 0.5456863641738892\n",
      "Validation: Epoch [9], Batch [171/938], Loss: 0.4141542315483093\n",
      "Validation: Epoch [9], Batch [172/938], Loss: 0.6809472441673279\n",
      "Validation: Epoch [9], Batch [173/938], Loss: 0.5582863092422485\n",
      "Validation: Epoch [9], Batch [174/938], Loss: 0.42438212037086487\n",
      "Validation: Epoch [9], Batch [175/938], Loss: 0.6717636585235596\n",
      "Validation: Epoch [9], Batch [176/938], Loss: 0.4988536238670349\n",
      "Validation: Epoch [9], Batch [177/938], Loss: 0.7307305335998535\n",
      "Validation: Epoch [9], Batch [178/938], Loss: 0.4928619861602783\n",
      "Validation: Epoch [9], Batch [179/938], Loss: 0.43289026618003845\n",
      "Validation: Epoch [9], Batch [180/938], Loss: 0.6667637825012207\n",
      "Validation: Epoch [9], Batch [181/938], Loss: 0.4554779529571533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [182/938], Loss: 0.4718613624572754\n",
      "Validation: Epoch [9], Batch [183/938], Loss: 0.5137733221054077\n",
      "Validation: Epoch [9], Batch [184/938], Loss: 0.37770015001296997\n",
      "Validation: Epoch [9], Batch [185/938], Loss: 0.5102574229240417\n",
      "Validation: Epoch [9], Batch [186/938], Loss: 0.5332624912261963\n",
      "Validation: Epoch [9], Batch [187/938], Loss: 0.47214287519454956\n",
      "Validation: Epoch [9], Batch [188/938], Loss: 0.6250885128974915\n",
      "Validation: Epoch [9], Batch [189/938], Loss: 0.5158841609954834\n",
      "Validation: Epoch [9], Batch [190/938], Loss: 0.5647503137588501\n",
      "Validation: Epoch [9], Batch [191/938], Loss: 0.5825536847114563\n",
      "Validation: Epoch [9], Batch [192/938], Loss: 0.5102448463439941\n",
      "Validation: Epoch [9], Batch [193/938], Loss: 0.6867974400520325\n",
      "Validation: Epoch [9], Batch [194/938], Loss: 0.48156166076660156\n",
      "Validation: Epoch [9], Batch [195/938], Loss: 0.4576283097267151\n",
      "Validation: Epoch [9], Batch [196/938], Loss: 0.6113063097000122\n",
      "Validation: Epoch [9], Batch [197/938], Loss: 0.47735458612442017\n",
      "Validation: Epoch [9], Batch [198/938], Loss: 0.5663553476333618\n",
      "Validation: Epoch [9], Batch [199/938], Loss: 0.49954280257225037\n",
      "Validation: Epoch [9], Batch [200/938], Loss: 0.5027342438697815\n",
      "Validation: Epoch [9], Batch [201/938], Loss: 0.7014371752738953\n",
      "Validation: Epoch [9], Batch [202/938], Loss: 0.6229196786880493\n",
      "Validation: Epoch [9], Batch [203/938], Loss: 0.4417470693588257\n",
      "Validation: Epoch [9], Batch [204/938], Loss: 0.7532933950424194\n",
      "Validation: Epoch [9], Batch [205/938], Loss: 0.5649160146713257\n",
      "Validation: Epoch [9], Batch [206/938], Loss: 0.8306856751441956\n",
      "Validation: Epoch [9], Batch [207/938], Loss: 0.5402967929840088\n",
      "Validation: Epoch [9], Batch [208/938], Loss: 0.4415116012096405\n",
      "Validation: Epoch [9], Batch [209/938], Loss: 0.47466787695884705\n",
      "Validation: Epoch [9], Batch [210/938], Loss: 0.7318306565284729\n",
      "Validation: Epoch [9], Batch [211/938], Loss: 0.6999589204788208\n",
      "Validation: Epoch [9], Batch [212/938], Loss: 0.5655875205993652\n",
      "Validation: Epoch [9], Batch [213/938], Loss: 0.7175053358078003\n",
      "Validation: Epoch [9], Batch [214/938], Loss: 0.6588113307952881\n",
      "Validation: Epoch [9], Batch [215/938], Loss: 0.4623144567012787\n",
      "Validation: Epoch [9], Batch [216/938], Loss: 0.6446211934089661\n",
      "Validation: Epoch [9], Batch [217/938], Loss: 0.48594391345977783\n",
      "Validation: Epoch [9], Batch [218/938], Loss: 0.5066708326339722\n",
      "Validation: Epoch [9], Batch [219/938], Loss: 0.3420986533164978\n",
      "Validation: Epoch [9], Batch [220/938], Loss: 0.916982889175415\n",
      "Validation: Epoch [9], Batch [221/938], Loss: 0.5462818741798401\n",
      "Validation: Epoch [9], Batch [222/938], Loss: 0.4890275001525879\n",
      "Validation: Epoch [9], Batch [223/938], Loss: 0.5691647529602051\n",
      "Validation: Epoch [9], Batch [224/938], Loss: 0.8225831985473633\n",
      "Validation: Epoch [9], Batch [225/938], Loss: 0.6590133905410767\n",
      "Validation: Epoch [9], Batch [226/938], Loss: 0.5679950714111328\n",
      "Validation: Epoch [9], Batch [227/938], Loss: 0.740375280380249\n",
      "Validation: Epoch [9], Batch [228/938], Loss: 0.6335572600364685\n",
      "Validation: Epoch [9], Batch [229/938], Loss: 0.42226433753967285\n",
      "Validation: Epoch [9], Batch [230/938], Loss: 0.6679241061210632\n",
      "Validation: Epoch [9], Batch [231/938], Loss: 0.48799824714660645\n",
      "Validation: Epoch [9], Batch [232/938], Loss: 0.5345064997673035\n",
      "Validation: Epoch [9], Batch [233/938], Loss: 0.4908778965473175\n",
      "Validation: Epoch [9], Batch [234/938], Loss: 0.5647573471069336\n",
      "Validation: Epoch [9], Batch [235/938], Loss: 0.709300696849823\n",
      "Validation: Epoch [9], Batch [236/938], Loss: 0.7252337336540222\n",
      "Validation: Epoch [9], Batch [237/938], Loss: 0.3951793313026428\n",
      "Validation: Epoch [9], Batch [238/938], Loss: 0.5887753963470459\n",
      "Validation: Epoch [9], Batch [239/938], Loss: 0.5642441511154175\n",
      "Validation: Epoch [9], Batch [240/938], Loss: 0.5145926475524902\n",
      "Validation: Epoch [9], Batch [241/938], Loss: 0.6124271154403687\n",
      "Validation: Epoch [9], Batch [242/938], Loss: 0.6554868817329407\n",
      "Validation: Epoch [9], Batch [243/938], Loss: 0.7312822341918945\n",
      "Validation: Epoch [9], Batch [244/938], Loss: 0.565081000328064\n",
      "Validation: Epoch [9], Batch [245/938], Loss: 0.483837366104126\n",
      "Validation: Epoch [9], Batch [246/938], Loss: 0.5386279821395874\n",
      "Validation: Epoch [9], Batch [247/938], Loss: 0.5080154538154602\n",
      "Validation: Epoch [9], Batch [248/938], Loss: 0.40482252836227417\n",
      "Validation: Epoch [9], Batch [249/938], Loss: 0.39482825994491577\n",
      "Validation: Epoch [9], Batch [250/938], Loss: 0.6410338878631592\n",
      "Validation: Epoch [9], Batch [251/938], Loss: 0.5051475763320923\n",
      "Validation: Epoch [9], Batch [252/938], Loss: 0.5773525238037109\n",
      "Validation: Epoch [9], Batch [253/938], Loss: 0.5095572471618652\n",
      "Validation: Epoch [9], Batch [254/938], Loss: 0.5729163885116577\n",
      "Validation: Epoch [9], Batch [255/938], Loss: 0.6267833709716797\n",
      "Validation: Epoch [9], Batch [256/938], Loss: 0.5065394639968872\n",
      "Validation: Epoch [9], Batch [257/938], Loss: 0.5636985301971436\n",
      "Validation: Epoch [9], Batch [258/938], Loss: 0.49312806129455566\n",
      "Validation: Epoch [9], Batch [259/938], Loss: 0.3490173816680908\n",
      "Validation: Epoch [9], Batch [260/938], Loss: 0.4211942255496979\n",
      "Validation: Epoch [9], Batch [261/938], Loss: 0.508145272731781\n",
      "Validation: Epoch [9], Batch [262/938], Loss: 0.46748772263526917\n",
      "Validation: Epoch [9], Batch [263/938], Loss: 0.5604045391082764\n",
      "Validation: Epoch [9], Batch [264/938], Loss: 0.5965980291366577\n",
      "Validation: Epoch [9], Batch [265/938], Loss: 0.5542712211608887\n",
      "Validation: Epoch [9], Batch [266/938], Loss: 0.5868395566940308\n",
      "Validation: Epoch [9], Batch [267/938], Loss: 0.4987936019897461\n",
      "Validation: Epoch [9], Batch [268/938], Loss: 0.5577319860458374\n",
      "Validation: Epoch [9], Batch [269/938], Loss: 0.5415452122688293\n",
      "Validation: Epoch [9], Batch [270/938], Loss: 0.5514951348304749\n",
      "Validation: Epoch [9], Batch [271/938], Loss: 0.5866488814353943\n",
      "Validation: Epoch [9], Batch [272/938], Loss: 0.5044194459915161\n",
      "Validation: Epoch [9], Batch [273/938], Loss: 0.42838186025619507\n",
      "Validation: Epoch [9], Batch [274/938], Loss: 0.41268882155418396\n",
      "Validation: Epoch [9], Batch [275/938], Loss: 0.617675244808197\n",
      "Validation: Epoch [9], Batch [276/938], Loss: 0.5226900577545166\n",
      "Validation: Epoch [9], Batch [277/938], Loss: 0.4537337124347687\n",
      "Validation: Epoch [9], Batch [278/938], Loss: 0.65775465965271\n",
      "Validation: Epoch [9], Batch [279/938], Loss: 0.6212927103042603\n",
      "Validation: Epoch [9], Batch [280/938], Loss: 0.43721306324005127\n",
      "Validation: Epoch [9], Batch [281/938], Loss: 0.6493358612060547\n",
      "Validation: Epoch [9], Batch [282/938], Loss: 0.7401760220527649\n",
      "Validation: Epoch [9], Batch [283/938], Loss: 0.5164954662322998\n",
      "Validation: Epoch [9], Batch [284/938], Loss: 0.5103589296340942\n",
      "Validation: Epoch [9], Batch [285/938], Loss: 0.3361123204231262\n",
      "Validation: Epoch [9], Batch [286/938], Loss: 0.38165271282196045\n",
      "Validation: Epoch [9], Batch [287/938], Loss: 0.6129002571105957\n",
      "Validation: Epoch [9], Batch [288/938], Loss: 0.6448068618774414\n",
      "Validation: Epoch [9], Batch [289/938], Loss: 0.5247876644134521\n",
      "Validation: Epoch [9], Batch [290/938], Loss: 0.5433127880096436\n",
      "Validation: Epoch [9], Batch [291/938], Loss: 0.5540021061897278\n",
      "Validation: Epoch [9], Batch [292/938], Loss: 0.5964295864105225\n",
      "Validation: Epoch [9], Batch [293/938], Loss: 0.5603225231170654\n",
      "Validation: Epoch [9], Batch [294/938], Loss: 0.3530634939670563\n",
      "Validation: Epoch [9], Batch [295/938], Loss: 0.507574200630188\n",
      "Validation: Epoch [9], Batch [296/938], Loss: 0.6376168727874756\n",
      "Validation: Epoch [9], Batch [297/938], Loss: 0.5569108128547668\n",
      "Validation: Epoch [9], Batch [298/938], Loss: 0.40884700417518616\n",
      "Validation: Epoch [9], Batch [299/938], Loss: 0.6653469800949097\n",
      "Validation: Epoch [9], Batch [300/938], Loss: 0.5350117087364197\n",
      "Validation: Epoch [9], Batch [301/938], Loss: 0.5232463479042053\n",
      "Validation: Epoch [9], Batch [302/938], Loss: 0.5171494483947754\n",
      "Validation: Epoch [9], Batch [303/938], Loss: 0.5736427307128906\n",
      "Validation: Epoch [9], Batch [304/938], Loss: 0.6405889391899109\n",
      "Validation: Epoch [9], Batch [305/938], Loss: 0.48533573746681213\n",
      "Validation: Epoch [9], Batch [306/938], Loss: 0.42180466651916504\n",
      "Validation: Epoch [9], Batch [307/938], Loss: 0.5734173059463501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [308/938], Loss: 0.4816325008869171\n",
      "Validation: Epoch [9], Batch [309/938], Loss: 0.5004932284355164\n",
      "Validation: Epoch [9], Batch [310/938], Loss: 0.5406221151351929\n",
      "Validation: Epoch [9], Batch [311/938], Loss: 0.6998090147972107\n",
      "Validation: Epoch [9], Batch [312/938], Loss: 0.575803279876709\n",
      "Validation: Epoch [9], Batch [313/938], Loss: 0.44012022018432617\n",
      "Validation: Epoch [9], Batch [314/938], Loss: 0.6206066608428955\n",
      "Validation: Epoch [9], Batch [315/938], Loss: 0.6550986766815186\n",
      "Validation: Epoch [9], Batch [316/938], Loss: 0.5151047706604004\n",
      "Validation: Epoch [9], Batch [317/938], Loss: 0.47492000460624695\n",
      "Validation: Epoch [9], Batch [318/938], Loss: 0.6655465960502625\n",
      "Validation: Epoch [9], Batch [319/938], Loss: 0.6044244170188904\n",
      "Validation: Epoch [9], Batch [320/938], Loss: 0.47464612126350403\n",
      "Validation: Epoch [9], Batch [321/938], Loss: 0.44470152258872986\n",
      "Validation: Epoch [9], Batch [322/938], Loss: 0.5011036396026611\n",
      "Validation: Epoch [9], Batch [323/938], Loss: 0.61295086145401\n",
      "Validation: Epoch [9], Batch [324/938], Loss: 0.6523642539978027\n",
      "Validation: Epoch [9], Batch [325/938], Loss: 0.4944326877593994\n",
      "Validation: Epoch [9], Batch [326/938], Loss: 0.5345155000686646\n",
      "Validation: Epoch [9], Batch [327/938], Loss: 0.5244524478912354\n",
      "Validation: Epoch [9], Batch [328/938], Loss: 0.43750572204589844\n",
      "Validation: Epoch [9], Batch [329/938], Loss: 0.5172744393348694\n",
      "Validation: Epoch [9], Batch [330/938], Loss: 0.5885983109474182\n",
      "Validation: Epoch [9], Batch [331/938], Loss: 0.7511042356491089\n",
      "Validation: Epoch [9], Batch [332/938], Loss: 0.585873007774353\n",
      "Validation: Epoch [9], Batch [333/938], Loss: 0.720562219619751\n",
      "Validation: Epoch [9], Batch [334/938], Loss: 0.5181375741958618\n",
      "Validation: Epoch [9], Batch [335/938], Loss: 0.42530471086502075\n",
      "Validation: Epoch [9], Batch [336/938], Loss: 0.541866660118103\n",
      "Validation: Epoch [9], Batch [337/938], Loss: 0.6235310435295105\n",
      "Validation: Epoch [9], Batch [338/938], Loss: 0.6438425779342651\n",
      "Validation: Epoch [9], Batch [339/938], Loss: 0.41343802213668823\n",
      "Validation: Epoch [9], Batch [340/938], Loss: 0.5103592276573181\n",
      "Validation: Epoch [9], Batch [341/938], Loss: 0.6016143560409546\n",
      "Validation: Epoch [9], Batch [342/938], Loss: 0.8956186771392822\n",
      "Validation: Epoch [9], Batch [343/938], Loss: 0.5057828426361084\n",
      "Validation: Epoch [9], Batch [344/938], Loss: 0.5159100294113159\n",
      "Validation: Epoch [9], Batch [345/938], Loss: 0.7256036996841431\n",
      "Validation: Epoch [9], Batch [346/938], Loss: 0.6325210332870483\n",
      "Validation: Epoch [9], Batch [347/938], Loss: 0.7471731901168823\n",
      "Validation: Epoch [9], Batch [348/938], Loss: 0.6219775676727295\n",
      "Validation: Epoch [9], Batch [349/938], Loss: 0.56648850440979\n",
      "Validation: Epoch [9], Batch [350/938], Loss: 0.47490715980529785\n",
      "Validation: Epoch [9], Batch [351/938], Loss: 0.5384489297866821\n",
      "Validation: Epoch [9], Batch [352/938], Loss: 0.5933370590209961\n",
      "Validation: Epoch [9], Batch [353/938], Loss: 0.5893784165382385\n",
      "Validation: Epoch [9], Batch [354/938], Loss: 0.44031432271003723\n",
      "Validation: Epoch [9], Batch [355/938], Loss: 0.6278443336486816\n",
      "Validation: Epoch [9], Batch [356/938], Loss: 0.46102994680404663\n",
      "Validation: Epoch [9], Batch [357/938], Loss: 0.5765829086303711\n",
      "Validation: Epoch [9], Batch [358/938], Loss: 0.4611872434616089\n",
      "Validation: Epoch [9], Batch [359/938], Loss: 0.6347121000289917\n",
      "Validation: Epoch [9], Batch [360/938], Loss: 0.48721450567245483\n",
      "Validation: Epoch [9], Batch [361/938], Loss: 0.500575602054596\n",
      "Validation: Epoch [9], Batch [362/938], Loss: 0.5162749290466309\n",
      "Validation: Epoch [9], Batch [363/938], Loss: 0.474079966545105\n",
      "Validation: Epoch [9], Batch [364/938], Loss: 0.515842080116272\n",
      "Validation: Epoch [9], Batch [365/938], Loss: 0.6300564408302307\n",
      "Validation: Epoch [9], Batch [366/938], Loss: 0.5402171611785889\n",
      "Validation: Epoch [9], Batch [367/938], Loss: 0.38248854875564575\n",
      "Validation: Epoch [9], Batch [368/938], Loss: 0.715535044670105\n",
      "Validation: Epoch [9], Batch [369/938], Loss: 0.6122812032699585\n",
      "Validation: Epoch [9], Batch [370/938], Loss: 0.4954156279563904\n",
      "Validation: Epoch [9], Batch [371/938], Loss: 0.5135337710380554\n",
      "Validation: Epoch [9], Batch [372/938], Loss: 0.5101683139801025\n",
      "Validation: Epoch [9], Batch [373/938], Loss: 0.6754416823387146\n",
      "Validation: Epoch [9], Batch [374/938], Loss: 0.5778740644454956\n",
      "Validation: Epoch [9], Batch [375/938], Loss: 0.8582495450973511\n",
      "Validation: Epoch [9], Batch [376/938], Loss: 0.5677319765090942\n",
      "Validation: Epoch [9], Batch [377/938], Loss: 0.8156353235244751\n",
      "Validation: Epoch [9], Batch [378/938], Loss: 0.6787452697753906\n",
      "Validation: Epoch [9], Batch [379/938], Loss: 0.5991361141204834\n",
      "Validation: Epoch [9], Batch [380/938], Loss: 0.8245404958724976\n",
      "Validation: Epoch [9], Batch [381/938], Loss: 0.6166185140609741\n",
      "Validation: Epoch [9], Batch [382/938], Loss: 0.5110554695129395\n",
      "Validation: Epoch [9], Batch [383/938], Loss: 0.5317954421043396\n",
      "Validation: Epoch [9], Batch [384/938], Loss: 0.7137107849121094\n",
      "Validation: Epoch [9], Batch [385/938], Loss: 0.6205969452857971\n",
      "Validation: Epoch [9], Batch [386/938], Loss: 0.5764633417129517\n",
      "Validation: Epoch [9], Batch [387/938], Loss: 0.7712883949279785\n",
      "Validation: Epoch [9], Batch [388/938], Loss: 0.5074230432510376\n",
      "Validation: Epoch [9], Batch [389/938], Loss: 0.5326148271560669\n",
      "Validation: Epoch [9], Batch [390/938], Loss: 0.5571423172950745\n",
      "Validation: Epoch [9], Batch [391/938], Loss: 0.3537023067474365\n",
      "Validation: Epoch [9], Batch [392/938], Loss: 0.7640253305435181\n",
      "Validation: Epoch [9], Batch [393/938], Loss: 0.45295342803001404\n",
      "Validation: Epoch [9], Batch [394/938], Loss: 0.4351382553577423\n",
      "Validation: Epoch [9], Batch [395/938], Loss: 0.48242175579071045\n",
      "Validation: Epoch [9], Batch [396/938], Loss: 0.4690505266189575\n",
      "Validation: Epoch [9], Batch [397/938], Loss: 0.7565541863441467\n",
      "Validation: Epoch [9], Batch [398/938], Loss: 0.802798867225647\n",
      "Validation: Epoch [9], Batch [399/938], Loss: 0.6585379838943481\n",
      "Validation: Epoch [9], Batch [400/938], Loss: 0.647050678730011\n",
      "Validation: Epoch [9], Batch [401/938], Loss: 0.6780112385749817\n",
      "Validation: Epoch [9], Batch [402/938], Loss: 0.7639569640159607\n",
      "Validation: Epoch [9], Batch [403/938], Loss: 0.5799211263656616\n",
      "Validation: Epoch [9], Batch [404/938], Loss: 0.699693500995636\n",
      "Validation: Epoch [9], Batch [405/938], Loss: 0.44153469800949097\n",
      "Validation: Epoch [9], Batch [406/938], Loss: 0.5495463013648987\n",
      "Validation: Epoch [9], Batch [407/938], Loss: 0.491994172334671\n",
      "Validation: Epoch [9], Batch [408/938], Loss: 0.3963022530078888\n",
      "Validation: Epoch [9], Batch [409/938], Loss: 0.445838987827301\n",
      "Validation: Epoch [9], Batch [410/938], Loss: 0.6012061834335327\n",
      "Validation: Epoch [9], Batch [411/938], Loss: 0.7134398221969604\n",
      "Validation: Epoch [9], Batch [412/938], Loss: 0.5206801891326904\n",
      "Validation: Epoch [9], Batch [413/938], Loss: 0.3730788826942444\n",
      "Validation: Epoch [9], Batch [414/938], Loss: 0.46462181210517883\n",
      "Validation: Epoch [9], Batch [415/938], Loss: 0.4962694048881531\n",
      "Validation: Epoch [9], Batch [416/938], Loss: 0.605946958065033\n",
      "Validation: Epoch [9], Batch [417/938], Loss: 0.5878360867500305\n",
      "Validation: Epoch [9], Batch [418/938], Loss: 0.49252623319625854\n",
      "Validation: Epoch [9], Batch [419/938], Loss: 0.4838710427284241\n",
      "Validation: Epoch [9], Batch [420/938], Loss: 0.5990519523620605\n",
      "Validation: Epoch [9], Batch [421/938], Loss: 0.4662294089794159\n",
      "Validation: Epoch [9], Batch [422/938], Loss: 0.47564083337783813\n",
      "Validation: Epoch [9], Batch [423/938], Loss: 0.6465113162994385\n",
      "Validation: Epoch [9], Batch [424/938], Loss: 0.5726035237312317\n",
      "Validation: Epoch [9], Batch [425/938], Loss: 0.5622700452804565\n",
      "Validation: Epoch [9], Batch [426/938], Loss: 0.7471153140068054\n",
      "Validation: Epoch [9], Batch [427/938], Loss: 0.5764566659927368\n",
      "Validation: Epoch [9], Batch [428/938], Loss: 0.5537522435188293\n",
      "Validation: Epoch [9], Batch [429/938], Loss: 0.7617654204368591\n",
      "Validation: Epoch [9], Batch [430/938], Loss: 0.5274182558059692\n",
      "Validation: Epoch [9], Batch [431/938], Loss: 0.4256066679954529\n",
      "Validation: Epoch [9], Batch [432/938], Loss: 0.48738276958465576\n",
      "Validation: Epoch [9], Batch [433/938], Loss: 0.4686807096004486\n",
      "Validation: Epoch [9], Batch [434/938], Loss: 0.5993760824203491\n",
      "Validation: Epoch [9], Batch [435/938], Loss: 0.7062838077545166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [436/938], Loss: 0.6266053318977356\n",
      "Validation: Epoch [9], Batch [437/938], Loss: 0.6140835285186768\n",
      "Validation: Epoch [9], Batch [438/938], Loss: 0.5380751490592957\n",
      "Validation: Epoch [9], Batch [439/938], Loss: 0.6174129247665405\n",
      "Validation: Epoch [9], Batch [440/938], Loss: 0.7801832556724548\n",
      "Validation: Epoch [9], Batch [441/938], Loss: 0.49812161922454834\n",
      "Validation: Epoch [9], Batch [442/938], Loss: 0.5272157788276672\n",
      "Validation: Epoch [9], Batch [443/938], Loss: 0.5694641470909119\n",
      "Validation: Epoch [9], Batch [444/938], Loss: 0.5460313558578491\n",
      "Validation: Epoch [9], Batch [445/938], Loss: 0.377400279045105\n",
      "Validation: Epoch [9], Batch [446/938], Loss: 0.51337730884552\n",
      "Validation: Epoch [9], Batch [447/938], Loss: 0.3266090154647827\n",
      "Validation: Epoch [9], Batch [448/938], Loss: 0.5429167747497559\n",
      "Validation: Epoch [9], Batch [449/938], Loss: 0.4946393370628357\n",
      "Validation: Epoch [9], Batch [450/938], Loss: 0.4352788031101227\n",
      "Validation: Epoch [9], Batch [451/938], Loss: 0.5999590158462524\n",
      "Validation: Epoch [9], Batch [452/938], Loss: 0.6318526268005371\n",
      "Validation: Epoch [9], Batch [453/938], Loss: 0.6977019906044006\n",
      "Validation: Epoch [9], Batch [454/938], Loss: 0.42686694860458374\n",
      "Validation: Epoch [9], Batch [455/938], Loss: 0.5685330629348755\n",
      "Validation: Epoch [9], Batch [456/938], Loss: 0.6015100479125977\n",
      "Validation: Epoch [9], Batch [457/938], Loss: 0.4504943788051605\n",
      "Validation: Epoch [9], Batch [458/938], Loss: 0.6890292763710022\n",
      "Validation: Epoch [9], Batch [459/938], Loss: 0.582658588886261\n",
      "Validation: Epoch [9], Batch [460/938], Loss: 0.5287283658981323\n",
      "Validation: Epoch [9], Batch [461/938], Loss: 0.487501859664917\n",
      "Validation: Epoch [9], Batch [462/938], Loss: 0.5211507081985474\n",
      "Validation: Epoch [9], Batch [463/938], Loss: 0.4448714852333069\n",
      "Validation: Epoch [9], Batch [464/938], Loss: 0.516940712928772\n",
      "Validation: Epoch [9], Batch [465/938], Loss: 0.4396485388278961\n",
      "Validation: Epoch [9], Batch [466/938], Loss: 0.4382418394088745\n",
      "Validation: Epoch [9], Batch [467/938], Loss: 0.6811314225196838\n",
      "Validation: Epoch [9], Batch [468/938], Loss: 0.47407642006874084\n",
      "Validation: Epoch [9], Batch [469/938], Loss: 0.515135645866394\n",
      "Validation: Epoch [9], Batch [470/938], Loss: 0.46105700731277466\n",
      "Validation: Epoch [9], Batch [471/938], Loss: 0.6793692111968994\n",
      "Validation: Epoch [9], Batch [472/938], Loss: 0.524749219417572\n",
      "Validation: Epoch [9], Batch [473/938], Loss: 0.4016534686088562\n",
      "Validation: Epoch [9], Batch [474/938], Loss: 0.39811915159225464\n",
      "Validation: Epoch [9], Batch [475/938], Loss: 0.7426908016204834\n",
      "Validation: Epoch [9], Batch [476/938], Loss: 0.6154168844223022\n",
      "Validation: Epoch [9], Batch [477/938], Loss: 0.5665584206581116\n",
      "Validation: Epoch [9], Batch [478/938], Loss: 0.6946321725845337\n",
      "Validation: Epoch [9], Batch [479/938], Loss: 0.5988811254501343\n",
      "Validation: Epoch [9], Batch [480/938], Loss: 0.6906112432479858\n",
      "Validation: Epoch [9], Batch [481/938], Loss: 0.4144314229488373\n",
      "Validation: Epoch [9], Batch [482/938], Loss: 0.4053145945072174\n",
      "Validation: Epoch [9], Batch [483/938], Loss: 0.48584678769111633\n",
      "Validation: Epoch [9], Batch [484/938], Loss: 0.542662501335144\n",
      "Validation: Epoch [9], Batch [485/938], Loss: 0.3850700855255127\n",
      "Validation: Epoch [9], Batch [486/938], Loss: 0.43482905626296997\n",
      "Validation: Epoch [9], Batch [487/938], Loss: 0.41767093539237976\n",
      "Validation: Epoch [9], Batch [488/938], Loss: 0.5388966798782349\n",
      "Validation: Epoch [9], Batch [489/938], Loss: 0.34478700160980225\n",
      "Validation: Epoch [9], Batch [490/938], Loss: 0.45719531178474426\n",
      "Validation: Epoch [9], Batch [491/938], Loss: 0.5695324540138245\n",
      "Validation: Epoch [9], Batch [492/938], Loss: 0.48796743154525757\n",
      "Validation: Epoch [9], Batch [493/938], Loss: 0.5008634924888611\n",
      "Validation: Epoch [9], Batch [494/938], Loss: 0.4685291647911072\n",
      "Validation: Epoch [9], Batch [495/938], Loss: 0.3745843172073364\n",
      "Validation: Epoch [9], Batch [496/938], Loss: 0.5352561473846436\n",
      "Validation: Epoch [9], Batch [497/938], Loss: 0.49533024430274963\n",
      "Validation: Epoch [9], Batch [498/938], Loss: 0.5706534385681152\n",
      "Validation: Epoch [9], Batch [499/938], Loss: 0.6329265832901001\n",
      "Validation: Epoch [9], Batch [500/938], Loss: 0.6583144068717957\n",
      "Validation: Epoch [9], Batch [501/938], Loss: 0.6097049117088318\n",
      "Validation: Epoch [9], Batch [502/938], Loss: 0.46538686752319336\n",
      "Validation: Epoch [9], Batch [503/938], Loss: 0.7094059586524963\n",
      "Validation: Epoch [9], Batch [504/938], Loss: 0.4108867049217224\n",
      "Validation: Epoch [9], Batch [505/938], Loss: 0.557471513748169\n",
      "Validation: Epoch [9], Batch [506/938], Loss: 0.5350086688995361\n",
      "Validation: Epoch [9], Batch [507/938], Loss: 0.4225955605506897\n",
      "Validation: Epoch [9], Batch [508/938], Loss: 0.4861987233161926\n",
      "Validation: Epoch [9], Batch [509/938], Loss: 0.5589253902435303\n",
      "Validation: Epoch [9], Batch [510/938], Loss: 0.7151350975036621\n",
      "Validation: Epoch [9], Batch [511/938], Loss: 0.6467495560646057\n",
      "Validation: Epoch [9], Batch [512/938], Loss: 0.5927368402481079\n",
      "Validation: Epoch [9], Batch [513/938], Loss: 0.5077440738677979\n",
      "Validation: Epoch [9], Batch [514/938], Loss: 0.4724558889865875\n",
      "Validation: Epoch [9], Batch [515/938], Loss: 0.578363299369812\n",
      "Validation: Epoch [9], Batch [516/938], Loss: 0.8177763223648071\n",
      "Validation: Epoch [9], Batch [517/938], Loss: 0.7766389846801758\n",
      "Validation: Epoch [9], Batch [518/938], Loss: 0.6654092073440552\n",
      "Validation: Epoch [9], Batch [519/938], Loss: 0.4744691848754883\n",
      "Validation: Epoch [9], Batch [520/938], Loss: 0.3697502315044403\n",
      "Validation: Epoch [9], Batch [521/938], Loss: 0.38545748591423035\n",
      "Validation: Epoch [9], Batch [522/938], Loss: 0.6037560701370239\n",
      "Validation: Epoch [9], Batch [523/938], Loss: 0.6364461779594421\n",
      "Validation: Epoch [9], Batch [524/938], Loss: 0.5794469118118286\n",
      "Validation: Epoch [9], Batch [525/938], Loss: 0.4106749892234802\n",
      "Validation: Epoch [9], Batch [526/938], Loss: 0.4839811325073242\n",
      "Validation: Epoch [9], Batch [527/938], Loss: 0.5240885019302368\n",
      "Validation: Epoch [9], Batch [528/938], Loss: 0.5094351768493652\n",
      "Validation: Epoch [9], Batch [529/938], Loss: 0.4152301847934723\n",
      "Validation: Epoch [9], Batch [530/938], Loss: 0.7150614261627197\n",
      "Validation: Epoch [9], Batch [531/938], Loss: 0.7123434543609619\n",
      "Validation: Epoch [9], Batch [532/938], Loss: 0.521071195602417\n",
      "Validation: Epoch [9], Batch [533/938], Loss: 0.5773176550865173\n",
      "Validation: Epoch [9], Batch [534/938], Loss: 0.4696524143218994\n",
      "Validation: Epoch [9], Batch [535/938], Loss: 0.4509263038635254\n",
      "Validation: Epoch [9], Batch [536/938], Loss: 0.483637273311615\n",
      "Validation: Epoch [9], Batch [537/938], Loss: 0.5326693058013916\n",
      "Validation: Epoch [9], Batch [538/938], Loss: 0.5645076036453247\n",
      "Validation: Epoch [9], Batch [539/938], Loss: 0.466045081615448\n",
      "Validation: Epoch [9], Batch [540/938], Loss: 0.7337602376937866\n",
      "Validation: Epoch [9], Batch [541/938], Loss: 0.468353807926178\n",
      "Validation: Epoch [9], Batch [542/938], Loss: 0.6604938507080078\n",
      "Validation: Epoch [9], Batch [543/938], Loss: 0.6178030967712402\n",
      "Validation: Epoch [9], Batch [544/938], Loss: 0.49546918272972107\n",
      "Validation: Epoch [9], Batch [545/938], Loss: 0.518947958946228\n",
      "Validation: Epoch [9], Batch [546/938], Loss: 0.5815681219100952\n",
      "Validation: Epoch [9], Batch [547/938], Loss: 0.5540329217910767\n",
      "Validation: Epoch [9], Batch [548/938], Loss: 0.44122549891471863\n",
      "Validation: Epoch [9], Batch [549/938], Loss: 0.5308655500411987\n",
      "Validation: Epoch [9], Batch [550/938], Loss: 0.47626638412475586\n",
      "Validation: Epoch [9], Batch [551/938], Loss: 0.5885026454925537\n",
      "Validation: Epoch [9], Batch [552/938], Loss: 0.5912425518035889\n",
      "Validation: Epoch [9], Batch [553/938], Loss: 0.466778039932251\n",
      "Validation: Epoch [9], Batch [554/938], Loss: 0.42527610063552856\n",
      "Validation: Epoch [9], Batch [555/938], Loss: 0.5366570949554443\n",
      "Validation: Epoch [9], Batch [556/938], Loss: 0.5000338554382324\n",
      "Validation: Epoch [9], Batch [557/938], Loss: 0.5389412641525269\n",
      "Validation: Epoch [9], Batch [558/938], Loss: 0.5464628338813782\n",
      "Validation: Epoch [9], Batch [559/938], Loss: 0.4622703194618225\n",
      "Validation: Epoch [9], Batch [560/938], Loss: 0.6114859580993652\n",
      "Validation: Epoch [9], Batch [561/938], Loss: 0.7524846792221069\n",
      "Validation: Epoch [9], Batch [562/938], Loss: 0.5605157017707825\n",
      "Validation: Epoch [9], Batch [563/938], Loss: 0.6872047185897827\n",
      "Validation: Epoch [9], Batch [564/938], Loss: 0.6941310167312622\n",
      "Validation: Epoch [9], Batch [565/938], Loss: 0.5682054758071899\n",
      "Validation: Epoch [9], Batch [566/938], Loss: 0.4964981973171234\n",
      "Validation: Epoch [9], Batch [567/938], Loss: 0.5556668043136597\n",
      "Validation: Epoch [9], Batch [568/938], Loss: 0.7014484405517578\n",
      "Validation: Epoch [9], Batch [569/938], Loss: 0.3331323266029358\n",
      "Validation: Epoch [9], Batch [570/938], Loss: 0.35896480083465576\n",
      "Validation: Epoch [9], Batch [571/938], Loss: 0.58741295337677\n",
      "Validation: Epoch [9], Batch [572/938], Loss: 0.33086061477661133\n",
      "Validation: Epoch [9], Batch [573/938], Loss: 0.47678786516189575\n",
      "Validation: Epoch [9], Batch [574/938], Loss: 0.43912971019744873\n",
      "Validation: Epoch [9], Batch [575/938], Loss: 0.4092213213443756\n",
      "Validation: Epoch [9], Batch [576/938], Loss: 0.5478218197822571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [577/938], Loss: 0.7449429035186768\n",
      "Validation: Epoch [9], Batch [578/938], Loss: 0.4410400688648224\n",
      "Validation: Epoch [9], Batch [579/938], Loss: 0.7602095603942871\n",
      "Validation: Epoch [9], Batch [580/938], Loss: 0.5228334069252014\n",
      "Validation: Epoch [9], Batch [581/938], Loss: 0.44348978996276855\n",
      "Validation: Epoch [9], Batch [582/938], Loss: 0.5411418080329895\n",
      "Validation: Epoch [9], Batch [583/938], Loss: 0.4980252981185913\n",
      "Validation: Epoch [9], Batch [584/938], Loss: 0.44546252489089966\n",
      "Validation: Epoch [9], Batch [585/938], Loss: 0.5216672420501709\n",
      "Validation: Epoch [9], Batch [586/938], Loss: 0.45685839653015137\n",
      "Validation: Epoch [9], Batch [587/938], Loss: 0.5596473217010498\n",
      "Validation: Epoch [9], Batch [588/938], Loss: 0.7514228820800781\n",
      "Validation: Epoch [9], Batch [589/938], Loss: 0.5664162635803223\n",
      "Validation: Epoch [9], Batch [590/938], Loss: 0.38980793952941895\n",
      "Validation: Epoch [9], Batch [591/938], Loss: 0.5038636922836304\n",
      "Validation: Epoch [9], Batch [592/938], Loss: 0.4622057378292084\n",
      "Validation: Epoch [9], Batch [593/938], Loss: 0.5660234689712524\n",
      "Validation: Epoch [9], Batch [594/938], Loss: 0.7873721122741699\n",
      "Validation: Epoch [9], Batch [595/938], Loss: 0.7067378759384155\n",
      "Validation: Epoch [9], Batch [596/938], Loss: 0.5316126346588135\n",
      "Validation: Epoch [9], Batch [597/938], Loss: 0.4482244551181793\n",
      "Validation: Epoch [9], Batch [598/938], Loss: 0.4796203374862671\n",
      "Validation: Epoch [9], Batch [599/938], Loss: 0.6075918674468994\n",
      "Validation: Epoch [9], Batch [600/938], Loss: 0.45731592178344727\n",
      "Validation: Epoch [9], Batch [601/938], Loss: 0.4112077057361603\n",
      "Validation: Epoch [9], Batch [602/938], Loss: 0.4894183576107025\n",
      "Validation: Epoch [9], Batch [603/938], Loss: 0.5827513933181763\n",
      "Validation: Epoch [9], Batch [604/938], Loss: 0.6425631046295166\n",
      "Validation: Epoch [9], Batch [605/938], Loss: 0.524752140045166\n",
      "Validation: Epoch [9], Batch [606/938], Loss: 0.5156453847885132\n",
      "Validation: Epoch [9], Batch [607/938], Loss: 0.5392863154411316\n",
      "Validation: Epoch [9], Batch [608/938], Loss: 0.4830596446990967\n",
      "Validation: Epoch [9], Batch [609/938], Loss: 0.5927814841270447\n",
      "Validation: Epoch [9], Batch [610/938], Loss: 0.3748309910297394\n",
      "Validation: Epoch [9], Batch [611/938], Loss: 0.3983096182346344\n",
      "Validation: Epoch [9], Batch [612/938], Loss: 0.4067106246948242\n",
      "Validation: Epoch [9], Batch [613/938], Loss: 0.5282750725746155\n",
      "Validation: Epoch [9], Batch [614/938], Loss: 0.7141006588935852\n",
      "Validation: Epoch [9], Batch [615/938], Loss: 0.4539865255355835\n",
      "Validation: Epoch [9], Batch [616/938], Loss: 0.6401153802871704\n",
      "Validation: Epoch [9], Batch [617/938], Loss: 0.7219932079315186\n",
      "Validation: Epoch [9], Batch [618/938], Loss: 0.557639479637146\n",
      "Validation: Epoch [9], Batch [619/938], Loss: 0.678752601146698\n",
      "Validation: Epoch [9], Batch [620/938], Loss: 0.43039068579673767\n",
      "Validation: Epoch [9], Batch [621/938], Loss: 0.648095965385437\n",
      "Validation: Epoch [9], Batch [622/938], Loss: 0.5189434289932251\n",
      "Validation: Epoch [9], Batch [623/938], Loss: 0.48711931705474854\n",
      "Validation: Epoch [9], Batch [624/938], Loss: 0.49150216579437256\n",
      "Validation: Epoch [9], Batch [625/938], Loss: 0.6343021988868713\n",
      "Validation: Epoch [9], Batch [626/938], Loss: 0.737994909286499\n",
      "Validation: Epoch [9], Batch [627/938], Loss: 0.4462473392486572\n",
      "Validation: Epoch [9], Batch [628/938], Loss: 0.5335166454315186\n",
      "Validation: Epoch [9], Batch [629/938], Loss: 0.5869404077529907\n",
      "Validation: Epoch [9], Batch [630/938], Loss: 0.5609759092330933\n",
      "Validation: Epoch [9], Batch [631/938], Loss: 0.8193042278289795\n",
      "Validation: Epoch [9], Batch [632/938], Loss: 0.5184005498886108\n",
      "Validation: Epoch [9], Batch [633/938], Loss: 0.6346929669380188\n",
      "Validation: Epoch [9], Batch [634/938], Loss: 0.567455530166626\n",
      "Validation: Epoch [9], Batch [635/938], Loss: 0.4453290104866028\n",
      "Validation: Epoch [9], Batch [636/938], Loss: 0.8379132151603699\n",
      "Validation: Epoch [9], Batch [637/938], Loss: 0.4833132028579712\n",
      "Validation: Epoch [9], Batch [638/938], Loss: 0.5619383454322815\n",
      "Validation: Epoch [9], Batch [639/938], Loss: 0.576941728591919\n",
      "Validation: Epoch [9], Batch [640/938], Loss: 0.5432215929031372\n",
      "Validation: Epoch [9], Batch [641/938], Loss: 0.4756453037261963\n",
      "Validation: Epoch [9], Batch [642/938], Loss: 0.4887832999229431\n",
      "Validation: Epoch [9], Batch [643/938], Loss: 0.46755218505859375\n",
      "Validation: Epoch [9], Batch [644/938], Loss: 0.4376339316368103\n",
      "Validation: Epoch [9], Batch [645/938], Loss: 0.5501487255096436\n",
      "Validation: Epoch [9], Batch [646/938], Loss: 0.35335367918014526\n",
      "Validation: Epoch [9], Batch [647/938], Loss: 0.6420174241065979\n",
      "Validation: Epoch [9], Batch [648/938], Loss: 0.563018798828125\n",
      "Validation: Epoch [9], Batch [649/938], Loss: 0.5251612663269043\n",
      "Validation: Epoch [9], Batch [650/938], Loss: 0.43084782361984253\n",
      "Validation: Epoch [9], Batch [651/938], Loss: 0.46971726417541504\n",
      "Validation: Epoch [9], Batch [652/938], Loss: 0.6446242332458496\n",
      "Validation: Epoch [9], Batch [653/938], Loss: 0.5326867699623108\n",
      "Validation: Epoch [9], Batch [654/938], Loss: 0.36969321966171265\n",
      "Validation: Epoch [9], Batch [655/938], Loss: 0.42227810621261597\n",
      "Validation: Epoch [9], Batch [656/938], Loss: 0.6371172070503235\n",
      "Validation: Epoch [9], Batch [657/938], Loss: 0.5934774875640869\n",
      "Validation: Epoch [9], Batch [658/938], Loss: 0.5120598077774048\n",
      "Validation: Epoch [9], Batch [659/938], Loss: 0.6451860666275024\n",
      "Validation: Epoch [9], Batch [660/938], Loss: 0.5549355745315552\n",
      "Validation: Epoch [9], Batch [661/938], Loss: 0.32547926902770996\n",
      "Validation: Epoch [9], Batch [662/938], Loss: 0.6927794218063354\n",
      "Validation: Epoch [9], Batch [663/938], Loss: 0.5981030464172363\n",
      "Validation: Epoch [9], Batch [664/938], Loss: 0.47987183928489685\n",
      "Validation: Epoch [9], Batch [665/938], Loss: 0.4435133934020996\n",
      "Validation: Epoch [9], Batch [666/938], Loss: 0.5226491093635559\n",
      "Validation: Epoch [9], Batch [667/938], Loss: 0.6022540926933289\n",
      "Validation: Epoch [9], Batch [668/938], Loss: 0.4915311336517334\n",
      "Validation: Epoch [9], Batch [669/938], Loss: 0.44321027398109436\n",
      "Validation: Epoch [9], Batch [670/938], Loss: 0.5658316016197205\n",
      "Validation: Epoch [9], Batch [671/938], Loss: 0.6381219625473022\n",
      "Validation: Epoch [9], Batch [672/938], Loss: 0.4790867269039154\n",
      "Validation: Epoch [9], Batch [673/938], Loss: 0.4873977601528168\n",
      "Validation: Epoch [9], Batch [674/938], Loss: 0.5337918996810913\n",
      "Validation: Epoch [9], Batch [675/938], Loss: 0.41911259293556213\n",
      "Validation: Epoch [9], Batch [676/938], Loss: 0.2934783697128296\n",
      "Validation: Epoch [9], Batch [677/938], Loss: 0.32366105914115906\n",
      "Validation: Epoch [9], Batch [678/938], Loss: 0.6457897424697876\n",
      "Validation: Epoch [9], Batch [679/938], Loss: 0.6150897741317749\n",
      "Validation: Epoch [9], Batch [680/938], Loss: 0.4644813537597656\n",
      "Validation: Epoch [9], Batch [681/938], Loss: 0.5398312211036682\n",
      "Validation: Epoch [9], Batch [682/938], Loss: 0.4066944420337677\n",
      "Validation: Epoch [9], Batch [683/938], Loss: 0.4744052290916443\n",
      "Validation: Epoch [9], Batch [684/938], Loss: 0.5157217979431152\n",
      "Validation: Epoch [9], Batch [685/938], Loss: 0.7163670659065247\n",
      "Validation: Epoch [9], Batch [686/938], Loss: 0.7889509797096252\n",
      "Validation: Epoch [9], Batch [687/938], Loss: 0.49446529150009155\n",
      "Validation: Epoch [9], Batch [688/938], Loss: 0.5967932939529419\n",
      "Validation: Epoch [9], Batch [689/938], Loss: 0.7748856544494629\n",
      "Validation: Epoch [9], Batch [690/938], Loss: 0.45280134677886963\n",
      "Validation: Epoch [9], Batch [691/938], Loss: 0.5036785006523132\n",
      "Validation: Epoch [9], Batch [692/938], Loss: 0.43288862705230713\n",
      "Validation: Epoch [9], Batch [693/938], Loss: 0.4444209933280945\n",
      "Validation: Epoch [9], Batch [694/938], Loss: 0.4922410249710083\n",
      "Validation: Epoch [9], Batch [695/938], Loss: 0.47757500410079956\n",
      "Validation: Epoch [9], Batch [696/938], Loss: 0.6691761016845703\n",
      "Validation: Epoch [9], Batch [697/938], Loss: 0.4809170961380005\n",
      "Validation: Epoch [9], Batch [698/938], Loss: 0.40284040570259094\n",
      "Validation: Epoch [9], Batch [699/938], Loss: 0.538763165473938\n",
      "Validation: Epoch [9], Batch [700/938], Loss: 0.5420550107955933\n",
      "Validation: Epoch [9], Batch [701/938], Loss: 0.7240263819694519\n",
      "Validation: Epoch [9], Batch [702/938], Loss: 0.4901863932609558\n",
      "Validation: Epoch [9], Batch [703/938], Loss: 0.42228102684020996\n",
      "Validation: Epoch [9], Batch [704/938], Loss: 0.510745644569397\n",
      "Validation: Epoch [9], Batch [705/938], Loss: 0.5421037673950195\n",
      "Validation: Epoch [9], Batch [706/938], Loss: 0.6545263528823853\n",
      "Validation: Epoch [9], Batch [707/938], Loss: 0.5240684747695923\n",
      "Validation: Epoch [9], Batch [708/938], Loss: 0.6668904423713684\n",
      "Validation: Epoch [9], Batch [709/938], Loss: 0.593953013420105\n",
      "Validation: Epoch [9], Batch [710/938], Loss: 0.5882227420806885\n",
      "Validation: Epoch [9], Batch [711/938], Loss: 0.42842602729797363\n",
      "Validation: Epoch [9], Batch [712/938], Loss: 0.4865592122077942\n",
      "Validation: Epoch [9], Batch [713/938], Loss: 0.5828850865364075\n",
      "Validation: Epoch [9], Batch [714/938], Loss: 0.4850815534591675\n",
      "Validation: Epoch [9], Batch [715/938], Loss: 0.5113763809204102\n",
      "Validation: Epoch [9], Batch [716/938], Loss: 0.42163005471229553\n",
      "Validation: Epoch [9], Batch [717/938], Loss: 0.5426097512245178\n",
      "Validation: Epoch [9], Batch [718/938], Loss: 0.3943248391151428\n",
      "Validation: Epoch [9], Batch [719/938], Loss: 0.5771461725234985\n",
      "Validation: Epoch [9], Batch [720/938], Loss: 0.5730305910110474\n",
      "Validation: Epoch [9], Batch [721/938], Loss: 0.5906410813331604\n",
      "Validation: Epoch [9], Batch [722/938], Loss: 0.488281786441803\n",
      "Validation: Epoch [9], Batch [723/938], Loss: 0.7412022352218628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [724/938], Loss: 0.3863813877105713\n",
      "Validation: Epoch [9], Batch [725/938], Loss: 0.5790817737579346\n",
      "Validation: Epoch [9], Batch [726/938], Loss: 0.5978571772575378\n",
      "Validation: Epoch [9], Batch [727/938], Loss: 0.6465973854064941\n",
      "Validation: Epoch [9], Batch [728/938], Loss: 0.47850197553634644\n",
      "Validation: Epoch [9], Batch [729/938], Loss: 0.5431378483772278\n",
      "Validation: Epoch [9], Batch [730/938], Loss: 0.5001382231712341\n",
      "Validation: Epoch [9], Batch [731/938], Loss: 0.4505152702331543\n",
      "Validation: Epoch [9], Batch [732/938], Loss: 0.5562254190444946\n",
      "Validation: Epoch [9], Batch [733/938], Loss: 0.5690523386001587\n",
      "Validation: Epoch [9], Batch [734/938], Loss: 0.6527978181838989\n",
      "Validation: Epoch [9], Batch [735/938], Loss: 0.5142015218734741\n",
      "Validation: Epoch [9], Batch [736/938], Loss: 0.49742376804351807\n",
      "Validation: Epoch [9], Batch [737/938], Loss: 0.5167697668075562\n",
      "Validation: Epoch [9], Batch [738/938], Loss: 0.674402117729187\n",
      "Validation: Epoch [9], Batch [739/938], Loss: 0.6639167070388794\n",
      "Validation: Epoch [9], Batch [740/938], Loss: 0.48166191577911377\n",
      "Validation: Epoch [9], Batch [741/938], Loss: 0.6158764362335205\n",
      "Validation: Epoch [9], Batch [742/938], Loss: 0.6548728942871094\n",
      "Validation: Epoch [9], Batch [743/938], Loss: 0.5653420090675354\n",
      "Validation: Epoch [9], Batch [744/938], Loss: 0.4012136161327362\n",
      "Validation: Epoch [9], Batch [745/938], Loss: 0.45298123359680176\n",
      "Validation: Epoch [9], Batch [746/938], Loss: 0.3719162344932556\n",
      "Validation: Epoch [9], Batch [747/938], Loss: 0.6759645938873291\n",
      "Validation: Epoch [9], Batch [748/938], Loss: 0.42927879095077515\n",
      "Validation: Epoch [9], Batch [749/938], Loss: 0.4630308747291565\n",
      "Validation: Epoch [9], Batch [750/938], Loss: 0.5991131067276001\n",
      "Validation: Epoch [9], Batch [751/938], Loss: 0.7463606595993042\n",
      "Validation: Epoch [9], Batch [752/938], Loss: 0.5138221383094788\n",
      "Validation: Epoch [9], Batch [753/938], Loss: 0.4834754467010498\n",
      "Validation: Epoch [9], Batch [754/938], Loss: 0.7410687804222107\n",
      "Validation: Epoch [9], Batch [755/938], Loss: 0.3826631009578705\n",
      "Validation: Epoch [9], Batch [756/938], Loss: 0.5812616944313049\n",
      "Validation: Epoch [9], Batch [757/938], Loss: 0.5771932601928711\n",
      "Validation: Epoch [9], Batch [758/938], Loss: 0.4092472195625305\n",
      "Validation: Epoch [9], Batch [759/938], Loss: 0.3658559322357178\n",
      "Validation: Epoch [9], Batch [760/938], Loss: 0.48037874698638916\n",
      "Validation: Epoch [9], Batch [761/938], Loss: 0.47498977184295654\n",
      "Validation: Epoch [9], Batch [762/938], Loss: 0.6198917627334595\n",
      "Validation: Epoch [9], Batch [763/938], Loss: 0.6636437177658081\n",
      "Validation: Epoch [9], Batch [764/938], Loss: 0.5803705453872681\n",
      "Validation: Epoch [9], Batch [765/938], Loss: 0.639701247215271\n",
      "Validation: Epoch [9], Batch [766/938], Loss: 0.6241183876991272\n",
      "Validation: Epoch [9], Batch [767/938], Loss: 0.544526994228363\n",
      "Validation: Epoch [9], Batch [768/938], Loss: 0.49228280782699585\n",
      "Validation: Epoch [9], Batch [769/938], Loss: 0.4389849305152893\n",
      "Validation: Epoch [9], Batch [770/938], Loss: 0.5335767269134521\n",
      "Validation: Epoch [9], Batch [771/938], Loss: 0.7457339763641357\n",
      "Validation: Epoch [9], Batch [772/938], Loss: 0.46975213289260864\n",
      "Validation: Epoch [9], Batch [773/938], Loss: 0.611731231212616\n",
      "Validation: Epoch [9], Batch [774/938], Loss: 0.5129844546318054\n",
      "Validation: Epoch [9], Batch [775/938], Loss: 0.6144177317619324\n",
      "Validation: Epoch [9], Batch [776/938], Loss: 0.5791196823120117\n",
      "Validation: Epoch [9], Batch [777/938], Loss: 0.5909138917922974\n",
      "Validation: Epoch [9], Batch [778/938], Loss: 0.48445165157318115\n",
      "Validation: Epoch [9], Batch [779/938], Loss: 0.46888938546180725\n",
      "Validation: Epoch [9], Batch [780/938], Loss: 0.479979932308197\n",
      "Validation: Epoch [9], Batch [781/938], Loss: 0.5371984243392944\n",
      "Validation: Epoch [9], Batch [782/938], Loss: 0.528006911277771\n",
      "Validation: Epoch [9], Batch [783/938], Loss: 0.5878275632858276\n",
      "Validation: Epoch [9], Batch [784/938], Loss: 0.5302327871322632\n",
      "Validation: Epoch [9], Batch [785/938], Loss: 0.6244557499885559\n",
      "Validation: Epoch [9], Batch [786/938], Loss: 0.49621444940567017\n",
      "Validation: Epoch [9], Batch [787/938], Loss: 0.36741358041763306\n",
      "Validation: Epoch [9], Batch [788/938], Loss: 0.36612042784690857\n",
      "Validation: Epoch [9], Batch [789/938], Loss: 0.5967784523963928\n",
      "Validation: Epoch [9], Batch [790/938], Loss: 0.5028506517410278\n",
      "Validation: Epoch [9], Batch [791/938], Loss: 0.6156290769577026\n",
      "Validation: Epoch [9], Batch [792/938], Loss: 0.47670185565948486\n",
      "Validation: Epoch [9], Batch [793/938], Loss: 0.5364670753479004\n",
      "Validation: Epoch [9], Batch [794/938], Loss: 0.35077011585235596\n",
      "Validation: Epoch [9], Batch [795/938], Loss: 0.7588915228843689\n",
      "Validation: Epoch [9], Batch [796/938], Loss: 0.43985384702682495\n",
      "Validation: Epoch [9], Batch [797/938], Loss: 0.6606984734535217\n",
      "Validation: Epoch [9], Batch [798/938], Loss: 0.8615451455116272\n",
      "Validation: Epoch [9], Batch [799/938], Loss: 0.6854411959648132\n",
      "Validation: Epoch [9], Batch [800/938], Loss: 0.6034184098243713\n",
      "Validation: Epoch [9], Batch [801/938], Loss: 0.93999183177948\n",
      "Validation: Epoch [9], Batch [802/938], Loss: 0.6465342044830322\n",
      "Validation: Epoch [9], Batch [803/938], Loss: 0.5186824798583984\n",
      "Validation: Epoch [9], Batch [804/938], Loss: 0.3842770457267761\n",
      "Validation: Epoch [9], Batch [805/938], Loss: 0.4198454022407532\n",
      "Validation: Epoch [9], Batch [806/938], Loss: 0.43674513697624207\n",
      "Validation: Epoch [9], Batch [807/938], Loss: 0.47168034315109253\n",
      "Validation: Epoch [9], Batch [808/938], Loss: 0.5319327712059021\n",
      "Validation: Epoch [9], Batch [809/938], Loss: 0.4387393593788147\n",
      "Validation: Epoch [9], Batch [810/938], Loss: 0.6072779297828674\n",
      "Validation: Epoch [9], Batch [811/938], Loss: 0.5908249616622925\n",
      "Validation: Epoch [9], Batch [812/938], Loss: 0.5595834255218506\n",
      "Validation: Epoch [9], Batch [813/938], Loss: 0.7891336679458618\n",
      "Validation: Epoch [9], Batch [814/938], Loss: 0.48425620794296265\n",
      "Validation: Epoch [9], Batch [815/938], Loss: 0.4878459572792053\n",
      "Validation: Epoch [9], Batch [816/938], Loss: 0.6291197538375854\n",
      "Validation: Epoch [9], Batch [817/938], Loss: 0.5204168558120728\n",
      "Validation: Epoch [9], Batch [818/938], Loss: 0.6632708311080933\n",
      "Validation: Epoch [9], Batch [819/938], Loss: 0.5796500444412231\n",
      "Validation: Epoch [9], Batch [820/938], Loss: 0.6921176314353943\n",
      "Validation: Epoch [9], Batch [821/938], Loss: 0.44994568824768066\n",
      "Validation: Epoch [9], Batch [822/938], Loss: 0.5157762765884399\n",
      "Validation: Epoch [9], Batch [823/938], Loss: 0.4888719916343689\n",
      "Validation: Epoch [9], Batch [824/938], Loss: 0.4271036982536316\n",
      "Validation: Epoch [9], Batch [825/938], Loss: 0.5128062963485718\n",
      "Validation: Epoch [9], Batch [826/938], Loss: 0.38984793424606323\n",
      "Validation: Epoch [9], Batch [827/938], Loss: 0.5623520612716675\n",
      "Validation: Epoch [9], Batch [828/938], Loss: 0.5602775812149048\n",
      "Validation: Epoch [9], Batch [829/938], Loss: 0.6813402771949768\n",
      "Validation: Epoch [9], Batch [830/938], Loss: 0.4908488988876343\n",
      "Validation: Epoch [9], Batch [831/938], Loss: 0.4835571050643921\n",
      "Validation: Epoch [9], Batch [832/938], Loss: 0.7049853801727295\n",
      "Validation: Epoch [9], Batch [833/938], Loss: 0.8347318768501282\n",
      "Validation: Epoch [9], Batch [834/938], Loss: 0.4316650629043579\n",
      "Validation: Epoch [9], Batch [835/938], Loss: 0.34767958521842957\n",
      "Validation: Epoch [9], Batch [836/938], Loss: 0.5737441778182983\n",
      "Validation: Epoch [9], Batch [837/938], Loss: 0.41784510016441345\n",
      "Validation: Epoch [9], Batch [838/938], Loss: 0.7946491241455078\n",
      "Validation: Epoch [9], Batch [839/938], Loss: 0.49518418312072754\n",
      "Validation: Epoch [9], Batch [840/938], Loss: 0.38864558935165405\n",
      "Validation: Epoch [9], Batch [841/938], Loss: 0.4283442199230194\n",
      "Validation: Epoch [9], Batch [842/938], Loss: 0.7279011607170105\n",
      "Validation: Epoch [9], Batch [843/938], Loss: 0.4734686315059662\n",
      "Validation: Epoch [9], Batch [844/938], Loss: 0.43661752343177795\n",
      "Validation: Epoch [9], Batch [845/938], Loss: 0.3081929683685303\n",
      "Validation: Epoch [9], Batch [846/938], Loss: 0.4271698594093323\n",
      "Validation: Epoch [9], Batch [847/938], Loss: 0.5681584477424622\n",
      "Validation: Epoch [9], Batch [848/938], Loss: 0.45376941561698914\n",
      "Validation: Epoch [9], Batch [849/938], Loss: 0.508435070514679\n",
      "Validation: Epoch [9], Batch [850/938], Loss: 0.6055045127868652\n",
      "Validation: Epoch [9], Batch [851/938], Loss: 0.39641472697257996\n",
      "Validation: Epoch [9], Batch [852/938], Loss: 0.46978652477264404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [9], Batch [853/938], Loss: 0.604781985282898\n",
      "Validation: Epoch [9], Batch [854/938], Loss: 0.7054949998855591\n",
      "Validation: Epoch [9], Batch [855/938], Loss: 0.5206229090690613\n",
      "Validation: Epoch [9], Batch [856/938], Loss: 0.6263419389724731\n",
      "Validation: Epoch [9], Batch [857/938], Loss: 0.4426448941230774\n",
      "Validation: Epoch [9], Batch [858/938], Loss: 0.5377719402313232\n",
      "Validation: Epoch [9], Batch [859/938], Loss: 0.47843196988105774\n",
      "Validation: Epoch [9], Batch [860/938], Loss: 0.49766847491264343\n",
      "Validation: Epoch [9], Batch [861/938], Loss: 0.40457427501678467\n",
      "Validation: Epoch [9], Batch [862/938], Loss: 0.5177935361862183\n",
      "Validation: Epoch [9], Batch [863/938], Loss: 0.7994158864021301\n",
      "Validation: Epoch [9], Batch [864/938], Loss: 0.6333388090133667\n",
      "Validation: Epoch [9], Batch [865/938], Loss: 0.7598874568939209\n",
      "Validation: Epoch [9], Batch [866/938], Loss: 0.668494701385498\n",
      "Validation: Epoch [9], Batch [867/938], Loss: 0.5059943199157715\n",
      "Validation: Epoch [9], Batch [868/938], Loss: 0.4289937913417816\n",
      "Validation: Epoch [9], Batch [869/938], Loss: 0.45188918709754944\n",
      "Validation: Epoch [9], Batch [870/938], Loss: 0.7743178606033325\n",
      "Validation: Epoch [9], Batch [871/938], Loss: 0.4823627769947052\n",
      "Validation: Epoch [9], Batch [872/938], Loss: 0.7263778448104858\n",
      "Validation: Epoch [9], Batch [873/938], Loss: 0.6202924847602844\n",
      "Validation: Epoch [9], Batch [874/938], Loss: 0.46047183871269226\n",
      "Validation: Epoch [9], Batch [875/938], Loss: 0.4904190003871918\n",
      "Validation: Epoch [9], Batch [876/938], Loss: 0.5220903158187866\n",
      "Validation: Epoch [9], Batch [877/938], Loss: 0.49652916193008423\n",
      "Validation: Epoch [9], Batch [878/938], Loss: 0.727983832359314\n",
      "Validation: Epoch [9], Batch [879/938], Loss: 0.4089635908603668\n",
      "Validation: Epoch [9], Batch [880/938], Loss: 0.6972619295120239\n",
      "Validation: Epoch [9], Batch [881/938], Loss: 0.7864311933517456\n",
      "Validation: Epoch [9], Batch [882/938], Loss: 0.7689846158027649\n",
      "Validation: Epoch [9], Batch [883/938], Loss: 0.5765396356582642\n",
      "Validation: Epoch [9], Batch [884/938], Loss: 0.4669815003871918\n",
      "Validation: Epoch [9], Batch [885/938], Loss: 0.6319805979728699\n",
      "Validation: Epoch [9], Batch [886/938], Loss: 0.4441471993923187\n",
      "Validation: Epoch [9], Batch [887/938], Loss: 0.3242337107658386\n",
      "Validation: Epoch [9], Batch [888/938], Loss: 0.3488385081291199\n",
      "Validation: Epoch [9], Batch [889/938], Loss: 0.4822930097579956\n",
      "Validation: Epoch [9], Batch [890/938], Loss: 0.755243718624115\n",
      "Validation: Epoch [9], Batch [891/938], Loss: 0.7476602792739868\n",
      "Validation: Epoch [9], Batch [892/938], Loss: 0.41543710231781006\n",
      "Validation: Epoch [9], Batch [893/938], Loss: 0.5858453512191772\n",
      "Validation: Epoch [9], Batch [894/938], Loss: 0.47177940607070923\n",
      "Validation: Epoch [9], Batch [895/938], Loss: 0.7944603562355042\n",
      "Validation: Epoch [9], Batch [896/938], Loss: 0.47855210304260254\n",
      "Validation: Epoch [9], Batch [897/938], Loss: 0.45818543434143066\n",
      "Validation: Epoch [9], Batch [898/938], Loss: 0.6265268325805664\n",
      "Validation: Epoch [9], Batch [899/938], Loss: 0.6413904428482056\n",
      "Validation: Epoch [9], Batch [900/938], Loss: 0.4313163161277771\n",
      "Validation: Epoch [9], Batch [901/938], Loss: 0.6630661487579346\n",
      "Validation: Epoch [9], Batch [902/938], Loss: 0.6321123838424683\n",
      "Validation: Epoch [9], Batch [903/938], Loss: 0.5694210529327393\n",
      "Validation: Epoch [9], Batch [904/938], Loss: 0.7436887621879578\n",
      "Validation: Epoch [9], Batch [905/938], Loss: 0.5704387426376343\n",
      "Validation: Epoch [9], Batch [906/938], Loss: 0.35086584091186523\n",
      "Validation: Epoch [9], Batch [907/938], Loss: 0.49318963289260864\n",
      "Validation: Epoch [9], Batch [908/938], Loss: 0.4962572753429413\n",
      "Validation: Epoch [9], Batch [909/938], Loss: 0.6297539472579956\n",
      "Validation: Epoch [9], Batch [910/938], Loss: 0.49031856656074524\n",
      "Validation: Epoch [9], Batch [911/938], Loss: 0.45144057273864746\n",
      "Validation: Epoch [9], Batch [912/938], Loss: 0.3856455087661743\n",
      "Validation: Epoch [9], Batch [913/938], Loss: 0.5950654745101929\n",
      "Validation: Epoch [9], Batch [914/938], Loss: 0.6256593465805054\n",
      "Validation: Epoch [9], Batch [915/938], Loss: 0.5028870105743408\n",
      "Validation: Epoch [9], Batch [916/938], Loss: 0.5861464142799377\n",
      "Validation: Epoch [9], Batch [917/938], Loss: 0.5710091590881348\n",
      "Validation: Epoch [9], Batch [918/938], Loss: 0.5114392042160034\n",
      "Validation: Epoch [9], Batch [919/938], Loss: 0.5108543634414673\n",
      "Validation: Epoch [9], Batch [920/938], Loss: 0.6439331769943237\n",
      "Validation: Epoch [9], Batch [921/938], Loss: 0.553145170211792\n",
      "Validation: Epoch [9], Batch [922/938], Loss: 0.6791313290596008\n",
      "Validation: Epoch [9], Batch [923/938], Loss: 0.510556697845459\n",
      "Validation: Epoch [9], Batch [924/938], Loss: 0.4187576472759247\n",
      "Validation: Epoch [9], Batch [925/938], Loss: 0.6105088591575623\n",
      "Validation: Epoch [9], Batch [926/938], Loss: 0.5656347274780273\n",
      "Validation: Epoch [9], Batch [927/938], Loss: 0.41136690974235535\n",
      "Validation: Epoch [9], Batch [928/938], Loss: 0.6310276985168457\n",
      "Validation: Epoch [9], Batch [929/938], Loss: 0.7139696478843689\n",
      "Validation: Epoch [9], Batch [930/938], Loss: 0.43883633613586426\n",
      "Validation: Epoch [9], Batch [931/938], Loss: 0.3308287262916565\n",
      "Validation: Epoch [9], Batch [932/938], Loss: 0.6499703526496887\n",
      "Validation: Epoch [9], Batch [933/938], Loss: 0.6462448239326477\n",
      "Validation: Epoch [9], Batch [934/938], Loss: 0.5706665515899658\n",
      "Validation: Epoch [9], Batch [935/938], Loss: 0.4917297959327698\n",
      "Validation: Epoch [9], Batch [936/938], Loss: 0.449717253446579\n",
      "Validation: Epoch [9], Batch [937/938], Loss: 0.5808219313621521\n",
      "Validation: Epoch [9], Batch [938/938], Loss: 0.6196001172065735\n",
      "Accuracy of test set: 0.80925\n",
      "Train: Epoch [10], Batch [1/938], Loss: 0.6692333221435547\n",
      "Train: Epoch [10], Batch [2/938], Loss: 0.6659983396530151\n",
      "Train: Epoch [10], Batch [3/938], Loss: 0.4709126055240631\n",
      "Train: Epoch [10], Batch [4/938], Loss: 0.5451298952102661\n",
      "Train: Epoch [10], Batch [5/938], Loss: 0.5903525948524475\n",
      "Train: Epoch [10], Batch [6/938], Loss: 0.6372522115707397\n",
      "Train: Epoch [10], Batch [7/938], Loss: 0.5614510178565979\n",
      "Train: Epoch [10], Batch [8/938], Loss: 0.5248835682868958\n",
      "Train: Epoch [10], Batch [9/938], Loss: 0.516968846321106\n",
      "Train: Epoch [10], Batch [10/938], Loss: 0.538272500038147\n",
      "Train: Epoch [10], Batch [11/938], Loss: 0.5482884049415588\n",
      "Train: Epoch [10], Batch [12/938], Loss: 0.5583595633506775\n",
      "Train: Epoch [10], Batch [13/938], Loss: 0.5635039806365967\n",
      "Train: Epoch [10], Batch [14/938], Loss: 0.4836002588272095\n",
      "Train: Epoch [10], Batch [15/938], Loss: 0.504630446434021\n",
      "Train: Epoch [10], Batch [16/938], Loss: 0.40216895937919617\n",
      "Train: Epoch [10], Batch [17/938], Loss: 0.6442059874534607\n",
      "Train: Epoch [10], Batch [18/938], Loss: 0.5266966819763184\n",
      "Train: Epoch [10], Batch [19/938], Loss: 0.5127606987953186\n",
      "Train: Epoch [10], Batch [20/938], Loss: 0.592825710773468\n",
      "Train: Epoch [10], Batch [21/938], Loss: 0.36882030963897705\n",
      "Train: Epoch [10], Batch [22/938], Loss: 0.6240805387496948\n",
      "Train: Epoch [10], Batch [23/938], Loss: 0.28269243240356445\n",
      "Train: Epoch [10], Batch [24/938], Loss: 0.6518470048904419\n",
      "Train: Epoch [10], Batch [25/938], Loss: 0.5464004874229431\n",
      "Train: Epoch [10], Batch [26/938], Loss: 0.4490472674369812\n",
      "Train: Epoch [10], Batch [27/938], Loss: 0.39675042033195496\n",
      "Train: Epoch [10], Batch [28/938], Loss: 0.490984171628952\n",
      "Train: Epoch [10], Batch [29/938], Loss: 0.6516363620758057\n",
      "Train: Epoch [10], Batch [30/938], Loss: 0.5242031812667847\n",
      "Train: Epoch [10], Batch [31/938], Loss: 0.7484151124954224\n",
      "Train: Epoch [10], Batch [32/938], Loss: 0.44814372062683105\n",
      "Train: Epoch [10], Batch [33/938], Loss: 0.5780279636383057\n",
      "Train: Epoch [10], Batch [34/938], Loss: 0.7799752950668335\n",
      "Train: Epoch [10], Batch [35/938], Loss: 0.596245527267456\n",
      "Train: Epoch [10], Batch [36/938], Loss: 0.713534951210022\n",
      "Train: Epoch [10], Batch [37/938], Loss: 0.5415683388710022\n",
      "Train: Epoch [10], Batch [38/938], Loss: 0.48256754875183105\n",
      "Train: Epoch [10], Batch [39/938], Loss: 0.7765790224075317\n",
      "Train: Epoch [10], Batch [40/938], Loss: 0.5781651139259338\n",
      "Train: Epoch [10], Batch [41/938], Loss: 0.5579049587249756\n",
      "Train: Epoch [10], Batch [42/938], Loss: 0.6255438327789307\n",
      "Train: Epoch [10], Batch [43/938], Loss: 0.5693819522857666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [44/938], Loss: 0.48083972930908203\n",
      "Train: Epoch [10], Batch [45/938], Loss: 0.43175721168518066\n",
      "Train: Epoch [10], Batch [46/938], Loss: 0.5354700088500977\n",
      "Train: Epoch [10], Batch [47/938], Loss: 0.44834333658218384\n",
      "Train: Epoch [10], Batch [48/938], Loss: 0.6365498304367065\n",
      "Train: Epoch [10], Batch [49/938], Loss: 0.5688287019729614\n",
      "Train: Epoch [10], Batch [50/938], Loss: 0.48227596282958984\n",
      "Train: Epoch [10], Batch [51/938], Loss: 0.6713929176330566\n",
      "Train: Epoch [10], Batch [52/938], Loss: 0.5567519664764404\n",
      "Train: Epoch [10], Batch [53/938], Loss: 0.5342981219291687\n",
      "Train: Epoch [10], Batch [54/938], Loss: 0.6395050287246704\n",
      "Train: Epoch [10], Batch [55/938], Loss: 0.5515997409820557\n",
      "Train: Epoch [10], Batch [56/938], Loss: 0.6255956888198853\n",
      "Train: Epoch [10], Batch [57/938], Loss: 0.6663966178894043\n",
      "Train: Epoch [10], Batch [58/938], Loss: 0.31870314478874207\n",
      "Train: Epoch [10], Batch [59/938], Loss: 0.5460734367370605\n",
      "Train: Epoch [10], Batch [60/938], Loss: 0.41771748661994934\n",
      "Train: Epoch [10], Batch [61/938], Loss: 0.5961055755615234\n",
      "Train: Epoch [10], Batch [62/938], Loss: 0.48144251108169556\n",
      "Train: Epoch [10], Batch [63/938], Loss: 0.8134414553642273\n",
      "Train: Epoch [10], Batch [64/938], Loss: 0.6230270862579346\n",
      "Train: Epoch [10], Batch [65/938], Loss: 0.5840413570404053\n",
      "Train: Epoch [10], Batch [66/938], Loss: 0.567607581615448\n",
      "Train: Epoch [10], Batch [67/938], Loss: 0.43211615085601807\n",
      "Train: Epoch [10], Batch [68/938], Loss: 0.8523463010787964\n",
      "Train: Epoch [10], Batch [69/938], Loss: 0.6020234227180481\n",
      "Train: Epoch [10], Batch [70/938], Loss: 0.5984948873519897\n",
      "Train: Epoch [10], Batch [71/938], Loss: 0.46510541439056396\n",
      "Train: Epoch [10], Batch [72/938], Loss: 0.3791843354701996\n",
      "Train: Epoch [10], Batch [73/938], Loss: 0.449258953332901\n",
      "Train: Epoch [10], Batch [74/938], Loss: 0.44862329959869385\n",
      "Train: Epoch [10], Batch [75/938], Loss: 0.6773163676261902\n",
      "Train: Epoch [10], Batch [76/938], Loss: 0.5961910486221313\n",
      "Train: Epoch [10], Batch [77/938], Loss: 0.502962589263916\n",
      "Train: Epoch [10], Batch [78/938], Loss: 0.5085089802742004\n",
      "Train: Epoch [10], Batch [79/938], Loss: 0.5749934911727905\n",
      "Train: Epoch [10], Batch [80/938], Loss: 0.6599950790405273\n",
      "Train: Epoch [10], Batch [81/938], Loss: 0.5773272514343262\n",
      "Train: Epoch [10], Batch [82/938], Loss: 0.32984602451324463\n",
      "Train: Epoch [10], Batch [83/938], Loss: 0.5936704874038696\n",
      "Train: Epoch [10], Batch [84/938], Loss: 0.7627885937690735\n",
      "Train: Epoch [10], Batch [85/938], Loss: 0.6424382925033569\n",
      "Train: Epoch [10], Batch [86/938], Loss: 0.5576483607292175\n",
      "Train: Epoch [10], Batch [87/938], Loss: 0.5280142426490784\n",
      "Train: Epoch [10], Batch [88/938], Loss: 0.6521638631820679\n",
      "Train: Epoch [10], Batch [89/938], Loss: 0.6842812895774841\n",
      "Train: Epoch [10], Batch [90/938], Loss: 0.43548059463500977\n",
      "Train: Epoch [10], Batch [91/938], Loss: 0.4495585560798645\n",
      "Train: Epoch [10], Batch [92/938], Loss: 0.5436211228370667\n",
      "Train: Epoch [10], Batch [93/938], Loss: 0.4546704888343811\n",
      "Train: Epoch [10], Batch [94/938], Loss: 0.6036437749862671\n",
      "Train: Epoch [10], Batch [95/938], Loss: 0.44240713119506836\n",
      "Train: Epoch [10], Batch [96/938], Loss: 0.3894484043121338\n",
      "Train: Epoch [10], Batch [97/938], Loss: 0.5437406301498413\n",
      "Train: Epoch [10], Batch [98/938], Loss: 0.5938965082168579\n",
      "Train: Epoch [10], Batch [99/938], Loss: 0.5676091313362122\n",
      "Train: Epoch [10], Batch [100/938], Loss: 0.6993265151977539\n",
      "Train: Epoch [10], Batch [101/938], Loss: 0.5129435062408447\n",
      "Train: Epoch [10], Batch [102/938], Loss: 0.5148501396179199\n",
      "Train: Epoch [10], Batch [103/938], Loss: 0.5156105756759644\n",
      "Train: Epoch [10], Batch [104/938], Loss: 0.4968697726726532\n",
      "Train: Epoch [10], Batch [105/938], Loss: 0.4691310524940491\n",
      "Train: Epoch [10], Batch [106/938], Loss: 0.4677037298679352\n",
      "Train: Epoch [10], Batch [107/938], Loss: 0.6394368410110474\n",
      "Train: Epoch [10], Batch [108/938], Loss: 0.7343443632125854\n",
      "Train: Epoch [10], Batch [109/938], Loss: 0.43936246633529663\n",
      "Train: Epoch [10], Batch [110/938], Loss: 0.42671501636505127\n",
      "Train: Epoch [10], Batch [111/938], Loss: 0.3361643850803375\n",
      "Train: Epoch [10], Batch [112/938], Loss: 0.49450284242630005\n",
      "Train: Epoch [10], Batch [113/938], Loss: 0.4340258538722992\n",
      "Train: Epoch [10], Batch [114/938], Loss: 0.511920690536499\n",
      "Train: Epoch [10], Batch [115/938], Loss: 0.505408525466919\n",
      "Train: Epoch [10], Batch [116/938], Loss: 0.688753604888916\n",
      "Train: Epoch [10], Batch [117/938], Loss: 0.4125673472881317\n",
      "Train: Epoch [10], Batch [118/938], Loss: 0.4410346448421478\n",
      "Train: Epoch [10], Batch [119/938], Loss: 0.6338453888893127\n",
      "Train: Epoch [10], Batch [120/938], Loss: 0.5825440883636475\n",
      "Train: Epoch [10], Batch [121/938], Loss: 0.5167350769042969\n",
      "Train: Epoch [10], Batch [122/938], Loss: 0.5966770648956299\n",
      "Train: Epoch [10], Batch [123/938], Loss: 0.6468532085418701\n",
      "Train: Epoch [10], Batch [124/938], Loss: 0.400765061378479\n",
      "Train: Epoch [10], Batch [125/938], Loss: 0.508240818977356\n",
      "Train: Epoch [10], Batch [126/938], Loss: 0.4679148197174072\n",
      "Train: Epoch [10], Batch [127/938], Loss: 0.570622444152832\n",
      "Train: Epoch [10], Batch [128/938], Loss: 0.43099743127822876\n",
      "Train: Epoch [10], Batch [129/938], Loss: 0.574333667755127\n",
      "Train: Epoch [10], Batch [130/938], Loss: 0.7322003841400146\n",
      "Train: Epoch [10], Batch [131/938], Loss: 0.6958785057067871\n",
      "Train: Epoch [10], Batch [132/938], Loss: 0.5535299777984619\n",
      "Train: Epoch [10], Batch [133/938], Loss: 0.5031285285949707\n",
      "Train: Epoch [10], Batch [134/938], Loss: 0.7261476516723633\n",
      "Train: Epoch [10], Batch [135/938], Loss: 0.574762225151062\n",
      "Train: Epoch [10], Batch [136/938], Loss: 0.7655459642410278\n",
      "Train: Epoch [10], Batch [137/938], Loss: 0.6726806163787842\n",
      "Train: Epoch [10], Batch [138/938], Loss: 0.6880477070808411\n",
      "Train: Epoch [10], Batch [139/938], Loss: 0.5649492740631104\n",
      "Train: Epoch [10], Batch [140/938], Loss: 0.5911186933517456\n",
      "Train: Epoch [10], Batch [141/938], Loss: 0.4810677468776703\n",
      "Train: Epoch [10], Batch [142/938], Loss: 0.40610483288764954\n",
      "Train: Epoch [10], Batch [143/938], Loss: 0.5045273900032043\n",
      "Train: Epoch [10], Batch [144/938], Loss: 0.4889530837535858\n",
      "Train: Epoch [10], Batch [145/938], Loss: 0.5024613738059998\n",
      "Train: Epoch [10], Batch [146/938], Loss: 0.6017494201660156\n",
      "Train: Epoch [10], Batch [147/938], Loss: 0.6618174314498901\n",
      "Train: Epoch [10], Batch [148/938], Loss: 0.6514253616333008\n",
      "Train: Epoch [10], Batch [149/938], Loss: 0.5123189687728882\n",
      "Train: Epoch [10], Batch [150/938], Loss: 0.5109676122665405\n",
      "Train: Epoch [10], Batch [151/938], Loss: 0.6180432438850403\n",
      "Train: Epoch [10], Batch [152/938], Loss: 0.6437914371490479\n",
      "Train: Epoch [10], Batch [153/938], Loss: 0.4401395916938782\n",
      "Train: Epoch [10], Batch [154/938], Loss: 0.43452686071395874\n",
      "Train: Epoch [10], Batch [155/938], Loss: 0.7087358832359314\n",
      "Train: Epoch [10], Batch [156/938], Loss: 0.6803251504898071\n",
      "Train: Epoch [10], Batch [157/938], Loss: 0.5752056837081909\n",
      "Train: Epoch [10], Batch [158/938], Loss: 0.4932432174682617\n",
      "Train: Epoch [10], Batch [159/938], Loss: 0.5963778495788574\n",
      "Train: Epoch [10], Batch [160/938], Loss: 0.5229754447937012\n",
      "Train: Epoch [10], Batch [161/938], Loss: 0.4255809485912323\n",
      "Train: Epoch [10], Batch [162/938], Loss: 0.5039087533950806\n",
      "Train: Epoch [10], Batch [163/938], Loss: 0.5556420087814331\n",
      "Train: Epoch [10], Batch [164/938], Loss: 0.7878000736236572\n",
      "Train: Epoch [10], Batch [165/938], Loss: 0.48176056146621704\n",
      "Train: Epoch [10], Batch [166/938], Loss: 0.5718214511871338\n",
      "Train: Epoch [10], Batch [167/938], Loss: 0.35567736625671387\n",
      "Train: Epoch [10], Batch [168/938], Loss: 0.5854219198226929\n",
      "Train: Epoch [10], Batch [169/938], Loss: 0.6776891350746155\n",
      "Train: Epoch [10], Batch [170/938], Loss: 0.5029230117797852\n",
      "Train: Epoch [10], Batch [171/938], Loss: 0.3319130837917328\n",
      "Train: Epoch [10], Batch [172/938], Loss: 0.5839877128601074\n",
      "Train: Epoch [10], Batch [173/938], Loss: 0.4410051703453064\n",
      "Train: Epoch [10], Batch [174/938], Loss: 0.5992604494094849\n",
      "Train: Epoch [10], Batch [175/938], Loss: 0.3226216435432434\n",
      "Train: Epoch [10], Batch [176/938], Loss: 0.5030286908149719\n",
      "Train: Epoch [10], Batch [177/938], Loss: 0.3193804919719696\n",
      "Train: Epoch [10], Batch [178/938], Loss: 0.44819602370262146\n",
      "Train: Epoch [10], Batch [179/938], Loss: 0.7089747786521912\n",
      "Train: Epoch [10], Batch [180/938], Loss: 0.46568432450294495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [181/938], Loss: 0.6696773767471313\n",
      "Train: Epoch [10], Batch [182/938], Loss: 0.48704811930656433\n",
      "Train: Epoch [10], Batch [183/938], Loss: 0.7135018110275269\n",
      "Train: Epoch [10], Batch [184/938], Loss: 0.48338785767555237\n",
      "Train: Epoch [10], Batch [185/938], Loss: 0.5556620955467224\n",
      "Train: Epoch [10], Batch [186/938], Loss: 0.43583792448043823\n",
      "Train: Epoch [10], Batch [187/938], Loss: 0.6099429726600647\n",
      "Train: Epoch [10], Batch [188/938], Loss: 0.5839706659317017\n",
      "Train: Epoch [10], Batch [189/938], Loss: 0.5637138485908508\n",
      "Train: Epoch [10], Batch [190/938], Loss: 0.4586015045642853\n",
      "Train: Epoch [10], Batch [191/938], Loss: 0.3894287347793579\n",
      "Train: Epoch [10], Batch [192/938], Loss: 0.6509424448013306\n",
      "Train: Epoch [10], Batch [193/938], Loss: 0.6212736368179321\n",
      "Train: Epoch [10], Batch [194/938], Loss: 0.4638385772705078\n",
      "Train: Epoch [10], Batch [195/938], Loss: 0.6770967245101929\n",
      "Train: Epoch [10], Batch [196/938], Loss: 0.5268185138702393\n",
      "Train: Epoch [10], Batch [197/938], Loss: 0.3964310884475708\n",
      "Train: Epoch [10], Batch [198/938], Loss: 0.47384777665138245\n",
      "Train: Epoch [10], Batch [199/938], Loss: 0.6925427317619324\n",
      "Train: Epoch [10], Batch [200/938], Loss: 0.6619529724121094\n",
      "Train: Epoch [10], Batch [201/938], Loss: 0.59710693359375\n",
      "Train: Epoch [10], Batch [202/938], Loss: 0.41534698009490967\n",
      "Train: Epoch [10], Batch [203/938], Loss: 0.58549964427948\n",
      "Train: Epoch [10], Batch [204/938], Loss: 0.5665684342384338\n",
      "Train: Epoch [10], Batch [205/938], Loss: 0.5416199564933777\n",
      "Train: Epoch [10], Batch [206/938], Loss: 0.4376595616340637\n",
      "Train: Epoch [10], Batch [207/938], Loss: 0.45879608392715454\n",
      "Train: Epoch [10], Batch [208/938], Loss: 0.6067432165145874\n",
      "Train: Epoch [10], Batch [209/938], Loss: 0.6072246432304382\n",
      "Train: Epoch [10], Batch [210/938], Loss: 0.5546121597290039\n",
      "Train: Epoch [10], Batch [211/938], Loss: 0.46931612491607666\n",
      "Train: Epoch [10], Batch [212/938], Loss: 0.5968812704086304\n",
      "Train: Epoch [10], Batch [213/938], Loss: 0.7107247114181519\n",
      "Train: Epoch [10], Batch [214/938], Loss: 0.6530617475509644\n",
      "Train: Epoch [10], Batch [215/938], Loss: 0.7228595018386841\n",
      "Train: Epoch [10], Batch [216/938], Loss: 0.6637996435165405\n",
      "Train: Epoch [10], Batch [217/938], Loss: 0.3792210519313812\n",
      "Train: Epoch [10], Batch [218/938], Loss: 0.46735474467277527\n",
      "Train: Epoch [10], Batch [219/938], Loss: 0.6242297887802124\n",
      "Train: Epoch [10], Batch [220/938], Loss: 0.5340592861175537\n",
      "Train: Epoch [10], Batch [221/938], Loss: 0.538051187992096\n",
      "Train: Epoch [10], Batch [222/938], Loss: 0.6133064031600952\n",
      "Train: Epoch [10], Batch [223/938], Loss: 0.51814866065979\n",
      "Train: Epoch [10], Batch [224/938], Loss: 0.36651304364204407\n",
      "Train: Epoch [10], Batch [225/938], Loss: 0.5364064574241638\n",
      "Train: Epoch [10], Batch [226/938], Loss: 0.4850412607192993\n",
      "Train: Epoch [10], Batch [227/938], Loss: 0.5740652084350586\n",
      "Train: Epoch [10], Batch [228/938], Loss: 0.45430031418800354\n",
      "Train: Epoch [10], Batch [229/938], Loss: 0.530356764793396\n",
      "Train: Epoch [10], Batch [230/938], Loss: 0.31490376591682434\n",
      "Train: Epoch [10], Batch [231/938], Loss: 0.5904956459999084\n",
      "Train: Epoch [10], Batch [232/938], Loss: 0.42756980657577515\n",
      "Train: Epoch [10], Batch [233/938], Loss: 0.5408632755279541\n",
      "Train: Epoch [10], Batch [234/938], Loss: 0.5782047510147095\n",
      "Train: Epoch [10], Batch [235/938], Loss: 0.48398521542549133\n",
      "Train: Epoch [10], Batch [236/938], Loss: 0.49679988622665405\n",
      "Train: Epoch [10], Batch [237/938], Loss: 0.4854150712490082\n",
      "Train: Epoch [10], Batch [238/938], Loss: 0.449215292930603\n",
      "Train: Epoch [10], Batch [239/938], Loss: 0.585181474685669\n",
      "Train: Epoch [10], Batch [240/938], Loss: 0.7101994752883911\n",
      "Train: Epoch [10], Batch [241/938], Loss: 0.4717494249343872\n",
      "Train: Epoch [10], Batch [242/938], Loss: 0.5210906267166138\n",
      "Train: Epoch [10], Batch [243/938], Loss: 0.511415958404541\n",
      "Train: Epoch [10], Batch [244/938], Loss: 0.4486061632633209\n",
      "Train: Epoch [10], Batch [245/938], Loss: 0.7284743785858154\n",
      "Train: Epoch [10], Batch [246/938], Loss: 0.7049926519393921\n",
      "Train: Epoch [10], Batch [247/938], Loss: 0.5338818430900574\n",
      "Train: Epoch [10], Batch [248/938], Loss: 0.4179530441761017\n",
      "Train: Epoch [10], Batch [249/938], Loss: 0.5762293338775635\n",
      "Train: Epoch [10], Batch [250/938], Loss: 0.4206344485282898\n",
      "Train: Epoch [10], Batch [251/938], Loss: 0.4743129014968872\n",
      "Train: Epoch [10], Batch [252/938], Loss: 0.5668468475341797\n",
      "Train: Epoch [10], Batch [253/938], Loss: 0.35065242648124695\n",
      "Train: Epoch [10], Batch [254/938], Loss: 0.47919410467147827\n",
      "Train: Epoch [10], Batch [255/938], Loss: 0.8613842129707336\n",
      "Train: Epoch [10], Batch [256/938], Loss: 0.44852912425994873\n",
      "Train: Epoch [10], Batch [257/938], Loss: 0.4367634057998657\n",
      "Train: Epoch [10], Batch [258/938], Loss: 0.43573471903800964\n",
      "Train: Epoch [10], Batch [259/938], Loss: 0.5117930769920349\n",
      "Train: Epoch [10], Batch [260/938], Loss: 0.5260598659515381\n",
      "Train: Epoch [10], Batch [261/938], Loss: 0.5125142335891724\n",
      "Train: Epoch [10], Batch [262/938], Loss: 0.6858252286911011\n",
      "Train: Epoch [10], Batch [263/938], Loss: 0.4651823937892914\n",
      "Train: Epoch [10], Batch [264/938], Loss: 0.5636212825775146\n",
      "Train: Epoch [10], Batch [265/938], Loss: 0.4611106216907501\n",
      "Train: Epoch [10], Batch [266/938], Loss: 0.5852283835411072\n",
      "Train: Epoch [10], Batch [267/938], Loss: 0.43576523661613464\n",
      "Train: Epoch [10], Batch [268/938], Loss: 0.5973445773124695\n",
      "Train: Epoch [10], Batch [269/938], Loss: 0.6022470593452454\n",
      "Train: Epoch [10], Batch [270/938], Loss: 0.542320728302002\n",
      "Train: Epoch [10], Batch [271/938], Loss: 0.3677145838737488\n",
      "Train: Epoch [10], Batch [272/938], Loss: 0.48039770126342773\n",
      "Train: Epoch [10], Batch [273/938], Loss: 0.5685349702835083\n",
      "Train: Epoch [10], Batch [274/938], Loss: 0.6938901543617249\n",
      "Train: Epoch [10], Batch [275/938], Loss: 0.6234105825424194\n",
      "Train: Epoch [10], Batch [276/938], Loss: 0.6765770316123962\n",
      "Train: Epoch [10], Batch [277/938], Loss: 0.549602746963501\n",
      "Train: Epoch [10], Batch [278/938], Loss: 0.6433106660842896\n",
      "Train: Epoch [10], Batch [279/938], Loss: 0.6005150079727173\n",
      "Train: Epoch [10], Batch [280/938], Loss: 0.4486258327960968\n",
      "Train: Epoch [10], Batch [281/938], Loss: 0.5237883925437927\n",
      "Train: Epoch [10], Batch [282/938], Loss: 0.3979502022266388\n",
      "Train: Epoch [10], Batch [283/938], Loss: 0.416398286819458\n",
      "Train: Epoch [10], Batch [284/938], Loss: 0.6053029298782349\n",
      "Train: Epoch [10], Batch [285/938], Loss: 0.7194410562515259\n",
      "Train: Epoch [10], Batch [286/938], Loss: 0.3494492769241333\n",
      "Train: Epoch [10], Batch [287/938], Loss: 0.5013730525970459\n",
      "Train: Epoch [10], Batch [288/938], Loss: 0.44966578483581543\n",
      "Train: Epoch [10], Batch [289/938], Loss: 0.7043932676315308\n",
      "Train: Epoch [10], Batch [290/938], Loss: 0.5934852361679077\n",
      "Train: Epoch [10], Batch [291/938], Loss: 0.5968432426452637\n",
      "Train: Epoch [10], Batch [292/938], Loss: 0.6770709753036499\n",
      "Train: Epoch [10], Batch [293/938], Loss: 0.3932669460773468\n",
      "Train: Epoch [10], Batch [294/938], Loss: 0.598723828792572\n",
      "Train: Epoch [10], Batch [295/938], Loss: 0.5098670721054077\n",
      "Train: Epoch [10], Batch [296/938], Loss: 0.6093255281448364\n",
      "Train: Epoch [10], Batch [297/938], Loss: 0.5672972202301025\n",
      "Train: Epoch [10], Batch [298/938], Loss: 0.5452362298965454\n",
      "Train: Epoch [10], Batch [299/938], Loss: 0.37166911363601685\n",
      "Train: Epoch [10], Batch [300/938], Loss: 0.42827197909355164\n",
      "Train: Epoch [10], Batch [301/938], Loss: 0.3831130266189575\n",
      "Train: Epoch [10], Batch [302/938], Loss: 0.6824603080749512\n",
      "Train: Epoch [10], Batch [303/938], Loss: 0.5576059818267822\n",
      "Train: Epoch [10], Batch [304/938], Loss: 0.49536651372909546\n",
      "Train: Epoch [10], Batch [305/938], Loss: 0.6068233251571655\n",
      "Train: Epoch [10], Batch [306/938], Loss: 0.5195419788360596\n",
      "Train: Epoch [10], Batch [307/938], Loss: 0.7141138315200806\n",
      "Train: Epoch [10], Batch [308/938], Loss: 0.6964221000671387\n",
      "Train: Epoch [10], Batch [309/938], Loss: 0.6447925567626953\n",
      "Train: Epoch [10], Batch [310/938], Loss: 0.4626452922821045\n",
      "Train: Epoch [10], Batch [311/938], Loss: 0.45468783378601074\n",
      "Train: Epoch [10], Batch [312/938], Loss: 0.44512444734573364\n",
      "Train: Epoch [10], Batch [313/938], Loss: 0.5298212766647339\n",
      "Train: Epoch [10], Batch [314/938], Loss: 0.53066086769104\n",
      "Train: Epoch [10], Batch [315/938], Loss: 0.44946402311325073\n",
      "Train: Epoch [10], Batch [316/938], Loss: 0.7201923727989197\n",
      "Train: Epoch [10], Batch [317/938], Loss: 0.574186384677887\n",
      "Train: Epoch [10], Batch [318/938], Loss: 0.5715855360031128\n",
      "Train: Epoch [10], Batch [319/938], Loss: 0.6634029746055603\n",
      "Train: Epoch [10], Batch [320/938], Loss: 0.6080514788627625\n",
      "Train: Epoch [10], Batch [321/938], Loss: 0.28221067786216736\n",
      "Train: Epoch [10], Batch [322/938], Loss: 0.4365047216415405\n",
      "Train: Epoch [10], Batch [323/938], Loss: 0.4687391221523285\n",
      "Train: Epoch [10], Batch [324/938], Loss: 0.3615945875644684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [325/938], Loss: 0.6028328537940979\n",
      "Train: Epoch [10], Batch [326/938], Loss: 0.45314040780067444\n",
      "Train: Epoch [10], Batch [327/938], Loss: 0.4624575674533844\n",
      "Train: Epoch [10], Batch [328/938], Loss: 0.6135263442993164\n",
      "Train: Epoch [10], Batch [329/938], Loss: 0.5180519819259644\n",
      "Train: Epoch [10], Batch [330/938], Loss: 0.4734763503074646\n",
      "Train: Epoch [10], Batch [331/938], Loss: 0.7442421913146973\n",
      "Train: Epoch [10], Batch [332/938], Loss: 0.5087536573410034\n",
      "Train: Epoch [10], Batch [333/938], Loss: 0.7943019866943359\n",
      "Train: Epoch [10], Batch [334/938], Loss: 0.553939163684845\n",
      "Train: Epoch [10], Batch [335/938], Loss: 0.536026656627655\n",
      "Train: Epoch [10], Batch [336/938], Loss: 0.5477899312973022\n",
      "Train: Epoch [10], Batch [337/938], Loss: 0.363426148891449\n",
      "Train: Epoch [10], Batch [338/938], Loss: 0.7207237482070923\n",
      "Train: Epoch [10], Batch [339/938], Loss: 0.30424392223358154\n",
      "Train: Epoch [10], Batch [340/938], Loss: 0.7204265594482422\n",
      "Train: Epoch [10], Batch [341/938], Loss: 0.45099014043807983\n",
      "Train: Epoch [10], Batch [342/938], Loss: 0.35190874338150024\n",
      "Train: Epoch [10], Batch [343/938], Loss: 0.41302382946014404\n",
      "Train: Epoch [10], Batch [344/938], Loss: 0.6969998478889465\n",
      "Train: Epoch [10], Batch [345/938], Loss: 0.46562206745147705\n",
      "Train: Epoch [10], Batch [346/938], Loss: 0.3726332187652588\n",
      "Train: Epoch [10], Batch [347/938], Loss: 0.5093857049942017\n",
      "Train: Epoch [10], Batch [348/938], Loss: 0.5790801644325256\n",
      "Train: Epoch [10], Batch [349/938], Loss: 0.7679749131202698\n",
      "Train: Epoch [10], Batch [350/938], Loss: 0.4137573838233948\n",
      "Train: Epoch [10], Batch [351/938], Loss: 0.38603517413139343\n",
      "Train: Epoch [10], Batch [352/938], Loss: 0.39821857213974\n",
      "Train: Epoch [10], Batch [353/938], Loss: 0.4364084005355835\n",
      "Train: Epoch [10], Batch [354/938], Loss: 0.48408353328704834\n",
      "Train: Epoch [10], Batch [355/938], Loss: 0.3390827178955078\n",
      "Train: Epoch [10], Batch [356/938], Loss: 0.526215672492981\n",
      "Train: Epoch [10], Batch [357/938], Loss: 0.3600020408630371\n",
      "Train: Epoch [10], Batch [358/938], Loss: 0.5871134400367737\n",
      "Train: Epoch [10], Batch [359/938], Loss: 0.38126158714294434\n",
      "Train: Epoch [10], Batch [360/938], Loss: 0.5002676844596863\n",
      "Train: Epoch [10], Batch [361/938], Loss: 0.5207831859588623\n",
      "Train: Epoch [10], Batch [362/938], Loss: 0.45412129163742065\n",
      "Train: Epoch [10], Batch [363/938], Loss: 0.5202537775039673\n",
      "Train: Epoch [10], Batch [364/938], Loss: 0.5880497694015503\n",
      "Train: Epoch [10], Batch [365/938], Loss: 0.5743402242660522\n",
      "Train: Epoch [10], Batch [366/938], Loss: 0.7253342866897583\n",
      "Train: Epoch [10], Batch [367/938], Loss: 0.47716253995895386\n",
      "Train: Epoch [10], Batch [368/938], Loss: 0.44286322593688965\n",
      "Train: Epoch [10], Batch [369/938], Loss: 0.49315398931503296\n",
      "Train: Epoch [10], Batch [370/938], Loss: 0.5435436964035034\n",
      "Train: Epoch [10], Batch [371/938], Loss: 0.683910608291626\n",
      "Train: Epoch [10], Batch [372/938], Loss: 0.4788942039012909\n",
      "Train: Epoch [10], Batch [373/938], Loss: 0.5477436780929565\n",
      "Train: Epoch [10], Batch [374/938], Loss: 0.603649377822876\n",
      "Train: Epoch [10], Batch [375/938], Loss: 0.3866204023361206\n",
      "Train: Epoch [10], Batch [376/938], Loss: 0.7132371664047241\n",
      "Train: Epoch [10], Batch [377/938], Loss: 0.5689964294433594\n",
      "Train: Epoch [10], Batch [378/938], Loss: 0.4131217896938324\n",
      "Train: Epoch [10], Batch [379/938], Loss: 0.6919779181480408\n",
      "Train: Epoch [10], Batch [380/938], Loss: 0.47059166431427\n",
      "Train: Epoch [10], Batch [381/938], Loss: 0.7111497521400452\n",
      "Train: Epoch [10], Batch [382/938], Loss: 0.5855350494384766\n",
      "Train: Epoch [10], Batch [383/938], Loss: 0.4679120182991028\n",
      "Train: Epoch [10], Batch [384/938], Loss: 0.48705804347991943\n",
      "Train: Epoch [10], Batch [385/938], Loss: 0.5594995617866516\n",
      "Train: Epoch [10], Batch [386/938], Loss: 0.5154175758361816\n",
      "Train: Epoch [10], Batch [387/938], Loss: 0.5144752264022827\n",
      "Train: Epoch [10], Batch [388/938], Loss: 0.6131877899169922\n",
      "Train: Epoch [10], Batch [389/938], Loss: 0.5882800817489624\n",
      "Train: Epoch [10], Batch [390/938], Loss: 0.5612937211990356\n",
      "Train: Epoch [10], Batch [391/938], Loss: 0.7301371097564697\n",
      "Train: Epoch [10], Batch [392/938], Loss: 0.6942501664161682\n",
      "Train: Epoch [10], Batch [393/938], Loss: 0.37502336502075195\n",
      "Train: Epoch [10], Batch [394/938], Loss: 0.6574802398681641\n",
      "Train: Epoch [10], Batch [395/938], Loss: 0.4165783226490021\n",
      "Train: Epoch [10], Batch [396/938], Loss: 0.35409918427467346\n",
      "Train: Epoch [10], Batch [397/938], Loss: 0.7466285824775696\n",
      "Train: Epoch [10], Batch [398/938], Loss: 0.4902496635913849\n",
      "Train: Epoch [10], Batch [399/938], Loss: 0.6095584630966187\n",
      "Train: Epoch [10], Batch [400/938], Loss: 0.6033987998962402\n",
      "Train: Epoch [10], Batch [401/938], Loss: 0.40331387519836426\n",
      "Train: Epoch [10], Batch [402/938], Loss: 0.5970298051834106\n",
      "Train: Epoch [10], Batch [403/938], Loss: 0.5700441598892212\n",
      "Train: Epoch [10], Batch [404/938], Loss: 0.664151668548584\n",
      "Train: Epoch [10], Batch [405/938], Loss: 0.529207170009613\n",
      "Train: Epoch [10], Batch [406/938], Loss: 0.5003095865249634\n",
      "Train: Epoch [10], Batch [407/938], Loss: 0.6464768648147583\n",
      "Train: Epoch [10], Batch [408/938], Loss: 0.4167787730693817\n",
      "Train: Epoch [10], Batch [409/938], Loss: 0.5234755277633667\n",
      "Train: Epoch [10], Batch [410/938], Loss: 0.5525069236755371\n",
      "Train: Epoch [10], Batch [411/938], Loss: 0.5633106231689453\n",
      "Train: Epoch [10], Batch [412/938], Loss: 0.33533477783203125\n",
      "Train: Epoch [10], Batch [413/938], Loss: 0.4546237885951996\n",
      "Train: Epoch [10], Batch [414/938], Loss: 0.4873925745487213\n",
      "Train: Epoch [10], Batch [415/938], Loss: 0.6519845724105835\n",
      "Train: Epoch [10], Batch [416/938], Loss: 0.7121913433074951\n",
      "Train: Epoch [10], Batch [417/938], Loss: 0.4959412217140198\n",
      "Train: Epoch [10], Batch [418/938], Loss: 0.5416618585586548\n",
      "Train: Epoch [10], Batch [419/938], Loss: 0.5307995080947876\n",
      "Train: Epoch [10], Batch [420/938], Loss: 0.5715560913085938\n",
      "Train: Epoch [10], Batch [421/938], Loss: 0.6547574996948242\n",
      "Train: Epoch [10], Batch [422/938], Loss: 0.6301627159118652\n",
      "Train: Epoch [10], Batch [423/938], Loss: 0.4901479184627533\n",
      "Train: Epoch [10], Batch [424/938], Loss: 0.6796294450759888\n",
      "Train: Epoch [10], Batch [425/938], Loss: 0.5710890293121338\n",
      "Train: Epoch [10], Batch [426/938], Loss: 0.45102715492248535\n",
      "Train: Epoch [10], Batch [427/938], Loss: 0.6918140649795532\n",
      "Train: Epoch [10], Batch [428/938], Loss: 0.5464414358139038\n",
      "Train: Epoch [10], Batch [429/938], Loss: 0.4669932425022125\n",
      "Train: Epoch [10], Batch [430/938], Loss: 0.6132091283798218\n",
      "Train: Epoch [10], Batch [431/938], Loss: 0.4172740876674652\n",
      "Train: Epoch [10], Batch [432/938], Loss: 0.808781623840332\n",
      "Train: Epoch [10], Batch [433/938], Loss: 0.7462430596351624\n",
      "Train: Epoch [10], Batch [434/938], Loss: 0.49300098419189453\n",
      "Train: Epoch [10], Batch [435/938], Loss: 0.445923388004303\n",
      "Train: Epoch [10], Batch [436/938], Loss: 0.49893176555633545\n",
      "Train: Epoch [10], Batch [437/938], Loss: 0.500577449798584\n",
      "Train: Epoch [10], Batch [438/938], Loss: 0.6141082644462585\n",
      "Train: Epoch [10], Batch [439/938], Loss: 0.5173577070236206\n",
      "Train: Epoch [10], Batch [440/938], Loss: 0.4199436604976654\n",
      "Train: Epoch [10], Batch [441/938], Loss: 0.5373551249504089\n",
      "Train: Epoch [10], Batch [442/938], Loss: 0.5476918816566467\n",
      "Train: Epoch [10], Batch [443/938], Loss: 0.44095706939697266\n",
      "Train: Epoch [10], Batch [444/938], Loss: 0.4645508825778961\n",
      "Train: Epoch [10], Batch [445/938], Loss: 0.46786928176879883\n",
      "Train: Epoch [10], Batch [446/938], Loss: 0.6427723169326782\n",
      "Train: Epoch [10], Batch [447/938], Loss: 0.5349151492118835\n",
      "Train: Epoch [10], Batch [448/938], Loss: 0.36470624804496765\n",
      "Train: Epoch [10], Batch [449/938], Loss: 0.5570626854896545\n",
      "Train: Epoch [10], Batch [450/938], Loss: 0.40224704146385193\n",
      "Train: Epoch [10], Batch [451/938], Loss: 0.4092438817024231\n",
      "Train: Epoch [10], Batch [452/938], Loss: 0.3806038498878479\n",
      "Train: Epoch [10], Batch [453/938], Loss: 0.39155131578445435\n",
      "Train: Epoch [10], Batch [454/938], Loss: 0.4799245595932007\n",
      "Train: Epoch [10], Batch [455/938], Loss: 0.5963398814201355\n",
      "Train: Epoch [10], Batch [456/938], Loss: 0.4612996280193329\n",
      "Train: Epoch [10], Batch [457/938], Loss: 0.7646366357803345\n",
      "Train: Epoch [10], Batch [458/938], Loss: 0.7186120748519897\n",
      "Train: Epoch [10], Batch [459/938], Loss: 0.5134479999542236\n",
      "Train: Epoch [10], Batch [460/938], Loss: 0.23386645317077637\n",
      "Train: Epoch [10], Batch [461/938], Loss: 0.6663801670074463\n",
      "Train: Epoch [10], Batch [462/938], Loss: 0.4685356020927429\n",
      "Train: Epoch [10], Batch [463/938], Loss: 0.5357486009597778\n",
      "Train: Epoch [10], Batch [464/938], Loss: 0.47264736890792847\n",
      "Train: Epoch [10], Batch [465/938], Loss: 0.5681602358818054\n",
      "Train: Epoch [10], Batch [466/938], Loss: 0.536421000957489\n",
      "Train: Epoch [10], Batch [467/938], Loss: 0.5176931023597717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [468/938], Loss: 0.657894492149353\n",
      "Train: Epoch [10], Batch [469/938], Loss: 0.49489614367485046\n",
      "Train: Epoch [10], Batch [470/938], Loss: 0.5668034553527832\n",
      "Train: Epoch [10], Batch [471/938], Loss: 0.5114800930023193\n",
      "Train: Epoch [10], Batch [472/938], Loss: 0.5231019258499146\n",
      "Train: Epoch [10], Batch [473/938], Loss: 0.5338351726531982\n",
      "Train: Epoch [10], Batch [474/938], Loss: 0.44308075308799744\n",
      "Train: Epoch [10], Batch [475/938], Loss: 0.5230922698974609\n",
      "Train: Epoch [10], Batch [476/938], Loss: 0.5929350852966309\n",
      "Train: Epoch [10], Batch [477/938], Loss: 0.5509343147277832\n",
      "Train: Epoch [10], Batch [478/938], Loss: 0.5809969305992126\n",
      "Train: Epoch [10], Batch [479/938], Loss: 0.5246290564537048\n",
      "Train: Epoch [10], Batch [480/938], Loss: 0.5226711630821228\n",
      "Train: Epoch [10], Batch [481/938], Loss: 0.3933336138725281\n",
      "Train: Epoch [10], Batch [482/938], Loss: 0.5549915432929993\n",
      "Train: Epoch [10], Batch [483/938], Loss: 0.5152269005775452\n",
      "Train: Epoch [10], Batch [484/938], Loss: 0.5606938600540161\n",
      "Train: Epoch [10], Batch [485/938], Loss: 0.4381958842277527\n",
      "Train: Epoch [10], Batch [486/938], Loss: 0.4823840856552124\n",
      "Train: Epoch [10], Batch [487/938], Loss: 0.6854822635650635\n",
      "Train: Epoch [10], Batch [488/938], Loss: 0.41193339228630066\n",
      "Train: Epoch [10], Batch [489/938], Loss: 0.44817107915878296\n",
      "Train: Epoch [10], Batch [490/938], Loss: 0.44285714626312256\n",
      "Train: Epoch [10], Batch [491/938], Loss: 0.5403753519058228\n",
      "Train: Epoch [10], Batch [492/938], Loss: 0.46125882863998413\n",
      "Train: Epoch [10], Batch [493/938], Loss: 0.5070838928222656\n",
      "Train: Epoch [10], Batch [494/938], Loss: 0.5625213384628296\n",
      "Train: Epoch [10], Batch [495/938], Loss: 0.3864477574825287\n",
      "Train: Epoch [10], Batch [496/938], Loss: 0.39690956473350525\n",
      "Train: Epoch [10], Batch [497/938], Loss: 0.37007543444633484\n",
      "Train: Epoch [10], Batch [498/938], Loss: 0.8382304906845093\n",
      "Train: Epoch [10], Batch [499/938], Loss: 0.3913869857788086\n",
      "Train: Epoch [10], Batch [500/938], Loss: 0.556929886341095\n",
      "Train: Epoch [10], Batch [501/938], Loss: 0.4983723759651184\n",
      "Train: Epoch [10], Batch [502/938], Loss: 0.6292505860328674\n",
      "Train: Epoch [10], Batch [503/938], Loss: 0.8350136280059814\n",
      "Train: Epoch [10], Batch [504/938], Loss: 0.5039970874786377\n",
      "Train: Epoch [10], Batch [505/938], Loss: 0.48787060379981995\n",
      "Train: Epoch [10], Batch [506/938], Loss: 0.4305843412876129\n",
      "Train: Epoch [10], Batch [507/938], Loss: 0.5685297250747681\n",
      "Train: Epoch [10], Batch [508/938], Loss: 0.5215029716491699\n",
      "Train: Epoch [10], Batch [509/938], Loss: 0.43342339992523193\n",
      "Train: Epoch [10], Batch [510/938], Loss: 0.5714343786239624\n",
      "Train: Epoch [10], Batch [511/938], Loss: 0.6143869161605835\n",
      "Train: Epoch [10], Batch [512/938], Loss: 0.46777528524398804\n",
      "Train: Epoch [10], Batch [513/938], Loss: 0.8746621012687683\n",
      "Train: Epoch [10], Batch [514/938], Loss: 0.5839428901672363\n",
      "Train: Epoch [10], Batch [515/938], Loss: 0.48719292879104614\n",
      "Train: Epoch [10], Batch [516/938], Loss: 0.8178845643997192\n",
      "Train: Epoch [10], Batch [517/938], Loss: 0.5610255002975464\n",
      "Train: Epoch [10], Batch [518/938], Loss: 0.5738640427589417\n",
      "Train: Epoch [10], Batch [519/938], Loss: 0.5390127897262573\n",
      "Train: Epoch [10], Batch [520/938], Loss: 0.4487683176994324\n",
      "Train: Epoch [10], Batch [521/938], Loss: 0.690160870552063\n",
      "Train: Epoch [10], Batch [522/938], Loss: 0.3822341561317444\n",
      "Train: Epoch [10], Batch [523/938], Loss: 0.5480207800865173\n",
      "Train: Epoch [10], Batch [524/938], Loss: 0.5781148672103882\n",
      "Train: Epoch [10], Batch [525/938], Loss: 0.40470072627067566\n",
      "Train: Epoch [10], Batch [526/938], Loss: 0.4242735505104065\n",
      "Train: Epoch [10], Batch [527/938], Loss: 0.6543797850608826\n",
      "Train: Epoch [10], Batch [528/938], Loss: 0.5430091619491577\n",
      "Train: Epoch [10], Batch [529/938], Loss: 0.4858582615852356\n",
      "Train: Epoch [10], Batch [530/938], Loss: 0.6027740836143494\n",
      "Train: Epoch [10], Batch [531/938], Loss: 0.6051977872848511\n",
      "Train: Epoch [10], Batch [532/938], Loss: 0.4511610269546509\n",
      "Train: Epoch [10], Batch [533/938], Loss: 0.5119914412498474\n",
      "Train: Epoch [10], Batch [534/938], Loss: 0.53147292137146\n",
      "Train: Epoch [10], Batch [535/938], Loss: 0.45680534839630127\n",
      "Train: Epoch [10], Batch [536/938], Loss: 0.4286958575248718\n",
      "Train: Epoch [10], Batch [537/938], Loss: 0.5156618356704712\n",
      "Train: Epoch [10], Batch [538/938], Loss: 0.4506414532661438\n",
      "Train: Epoch [10], Batch [539/938], Loss: 0.6836428642272949\n",
      "Train: Epoch [10], Batch [540/938], Loss: 0.6914410591125488\n",
      "Train: Epoch [10], Batch [541/938], Loss: 0.5036917924880981\n",
      "Train: Epoch [10], Batch [542/938], Loss: 0.41994935274124146\n",
      "Train: Epoch [10], Batch [543/938], Loss: 0.38252004981040955\n",
      "Train: Epoch [10], Batch [544/938], Loss: 0.4743267297744751\n",
      "Train: Epoch [10], Batch [545/938], Loss: 0.4012581706047058\n",
      "Train: Epoch [10], Batch [546/938], Loss: 0.629769504070282\n",
      "Train: Epoch [10], Batch [547/938], Loss: 0.54036945104599\n",
      "Train: Epoch [10], Batch [548/938], Loss: 0.49438872933387756\n",
      "Train: Epoch [10], Batch [549/938], Loss: 0.5422987937927246\n",
      "Train: Epoch [10], Batch [550/938], Loss: 0.44878262281417847\n",
      "Train: Epoch [10], Batch [551/938], Loss: 0.5235605835914612\n",
      "Train: Epoch [10], Batch [552/938], Loss: 0.5476177930831909\n",
      "Train: Epoch [10], Batch [553/938], Loss: 0.48054951429367065\n",
      "Train: Epoch [10], Batch [554/938], Loss: 0.3208310008049011\n",
      "Train: Epoch [10], Batch [555/938], Loss: 0.5839673280715942\n",
      "Train: Epoch [10], Batch [556/938], Loss: 0.4723520278930664\n",
      "Train: Epoch [10], Batch [557/938], Loss: 0.3858369290828705\n",
      "Train: Epoch [10], Batch [558/938], Loss: 0.3274535536766052\n",
      "Train: Epoch [10], Batch [559/938], Loss: 0.6246432065963745\n",
      "Train: Epoch [10], Batch [560/938], Loss: 0.43648141622543335\n",
      "Train: Epoch [10], Batch [561/938], Loss: 0.5623294115066528\n",
      "Train: Epoch [10], Batch [562/938], Loss: 0.4677523672580719\n",
      "Train: Epoch [10], Batch [563/938], Loss: 0.43195706605911255\n",
      "Train: Epoch [10], Batch [564/938], Loss: 0.5489219427108765\n",
      "Train: Epoch [10], Batch [565/938], Loss: 0.4470870792865753\n",
      "Train: Epoch [10], Batch [566/938], Loss: 0.5686079263687134\n",
      "Train: Epoch [10], Batch [567/938], Loss: 0.5436121821403503\n",
      "Train: Epoch [10], Batch [568/938], Loss: 0.6029864549636841\n",
      "Train: Epoch [10], Batch [569/938], Loss: 0.6385986804962158\n",
      "Train: Epoch [10], Batch [570/938], Loss: 0.5981882810592651\n",
      "Train: Epoch [10], Batch [571/938], Loss: 0.5413649678230286\n",
      "Train: Epoch [10], Batch [572/938], Loss: 0.4538819491863251\n",
      "Train: Epoch [10], Batch [573/938], Loss: 0.45307230949401855\n",
      "Train: Epoch [10], Batch [574/938], Loss: 0.434271901845932\n",
      "Train: Epoch [10], Batch [575/938], Loss: 0.44641774892807007\n",
      "Train: Epoch [10], Batch [576/938], Loss: 0.4189779460430145\n",
      "Train: Epoch [10], Batch [577/938], Loss: 0.4627683162689209\n",
      "Train: Epoch [10], Batch [578/938], Loss: 0.4521091878414154\n",
      "Train: Epoch [10], Batch [579/938], Loss: 0.5624035000801086\n",
      "Train: Epoch [10], Batch [580/938], Loss: 0.5024150609970093\n",
      "Train: Epoch [10], Batch [581/938], Loss: 0.5337091684341431\n",
      "Train: Epoch [10], Batch [582/938], Loss: 0.48375579714775085\n",
      "Train: Epoch [10], Batch [583/938], Loss: 0.5642964839935303\n",
      "Train: Epoch [10], Batch [584/938], Loss: 0.540196418762207\n",
      "Train: Epoch [10], Batch [585/938], Loss: 0.45913437008857727\n",
      "Train: Epoch [10], Batch [586/938], Loss: 0.6486548185348511\n",
      "Train: Epoch [10], Batch [587/938], Loss: 0.5071593523025513\n",
      "Train: Epoch [10], Batch [588/938], Loss: 0.5586303472518921\n",
      "Train: Epoch [10], Batch [589/938], Loss: 0.4312002658843994\n",
      "Train: Epoch [10], Batch [590/938], Loss: 0.7815093994140625\n",
      "Train: Epoch [10], Batch [591/938], Loss: 0.5629813075065613\n",
      "Train: Epoch [10], Batch [592/938], Loss: 0.47786030173301697\n",
      "Train: Epoch [10], Batch [593/938], Loss: 0.5973557829856873\n",
      "Train: Epoch [10], Batch [594/938], Loss: 0.5646708011627197\n",
      "Train: Epoch [10], Batch [595/938], Loss: 0.6634395122528076\n",
      "Train: Epoch [10], Batch [596/938], Loss: 0.5249224305152893\n",
      "Train: Epoch [10], Batch [597/938], Loss: 0.5911688804626465\n",
      "Train: Epoch [10], Batch [598/938], Loss: 0.4837944507598877\n",
      "Train: Epoch [10], Batch [599/938], Loss: 0.5443808436393738\n",
      "Train: Epoch [10], Batch [600/938], Loss: 0.45974984765052795\n",
      "Train: Epoch [10], Batch [601/938], Loss: 0.4610050916671753\n",
      "Train: Epoch [10], Batch [602/938], Loss: 0.5351362228393555\n",
      "Train: Epoch [10], Batch [603/938], Loss: 0.4058058261871338\n",
      "Train: Epoch [10], Batch [604/938], Loss: 0.5521712303161621\n",
      "Train: Epoch [10], Batch [605/938], Loss: 0.6743031740188599\n",
      "Train: Epoch [10], Batch [606/938], Loss: 0.5297598838806152\n",
      "Train: Epoch [10], Batch [607/938], Loss: 0.6435149908065796\n",
      "Train: Epoch [10], Batch [608/938], Loss: 0.7318742275238037\n",
      "Train: Epoch [10], Batch [609/938], Loss: 0.6202191114425659\n",
      "Train: Epoch [10], Batch [610/938], Loss: 0.5238208770751953\n",
      "Train: Epoch [10], Batch [611/938], Loss: 0.42096003890037537\n",
      "Train: Epoch [10], Batch [612/938], Loss: 0.36528366804122925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [613/938], Loss: 0.623630166053772\n",
      "Train: Epoch [10], Batch [614/938], Loss: 0.420510858297348\n",
      "Train: Epoch [10], Batch [615/938], Loss: 0.4111793637275696\n",
      "Train: Epoch [10], Batch [616/938], Loss: 0.550505518913269\n",
      "Train: Epoch [10], Batch [617/938], Loss: 0.48548123240470886\n",
      "Train: Epoch [10], Batch [618/938], Loss: 0.4899492859840393\n",
      "Train: Epoch [10], Batch [619/938], Loss: 0.4944530129432678\n",
      "Train: Epoch [10], Batch [620/938], Loss: 0.6467716097831726\n",
      "Train: Epoch [10], Batch [621/938], Loss: 0.7526392936706543\n",
      "Train: Epoch [10], Batch [622/938], Loss: 0.5999035835266113\n",
      "Train: Epoch [10], Batch [623/938], Loss: 0.47460466623306274\n",
      "Train: Epoch [10], Batch [624/938], Loss: 0.4933810532093048\n",
      "Train: Epoch [10], Batch [625/938], Loss: 0.5166430473327637\n",
      "Train: Epoch [10], Batch [626/938], Loss: 0.4175128638744354\n",
      "Train: Epoch [10], Batch [627/938], Loss: 0.3846369683742523\n",
      "Train: Epoch [10], Batch [628/938], Loss: 0.42735162377357483\n",
      "Train: Epoch [10], Batch [629/938], Loss: 0.5234934687614441\n",
      "Train: Epoch [10], Batch [630/938], Loss: 0.4995284974575043\n",
      "Train: Epoch [10], Batch [631/938], Loss: 0.47835594415664673\n",
      "Train: Epoch [10], Batch [632/938], Loss: 0.5455315113067627\n",
      "Train: Epoch [10], Batch [633/938], Loss: 0.6980211734771729\n",
      "Train: Epoch [10], Batch [634/938], Loss: 0.6763779520988464\n",
      "Train: Epoch [10], Batch [635/938], Loss: 0.40386056900024414\n",
      "Train: Epoch [10], Batch [636/938], Loss: 0.5454328060150146\n",
      "Train: Epoch [10], Batch [637/938], Loss: 0.43378978967666626\n",
      "Train: Epoch [10], Batch [638/938], Loss: 0.6012885570526123\n",
      "Train: Epoch [10], Batch [639/938], Loss: 0.474426805973053\n",
      "Train: Epoch [10], Batch [640/938], Loss: 0.4785316288471222\n",
      "Train: Epoch [10], Batch [641/938], Loss: 0.5226837992668152\n",
      "Train: Epoch [10], Batch [642/938], Loss: 0.5866639018058777\n",
      "Train: Epoch [10], Batch [643/938], Loss: 0.6521193981170654\n",
      "Train: Epoch [10], Batch [644/938], Loss: 0.5684016942977905\n",
      "Train: Epoch [10], Batch [645/938], Loss: 0.4654574394226074\n",
      "Train: Epoch [10], Batch [646/938], Loss: 0.7235870957374573\n",
      "Train: Epoch [10], Batch [647/938], Loss: 0.5097814798355103\n",
      "Train: Epoch [10], Batch [648/938], Loss: 0.4312184453010559\n",
      "Train: Epoch [10], Batch [649/938], Loss: 0.3727573752403259\n",
      "Train: Epoch [10], Batch [650/938], Loss: 0.35450461506843567\n",
      "Train: Epoch [10], Batch [651/938], Loss: 0.5422022342681885\n",
      "Train: Epoch [10], Batch [652/938], Loss: 0.5281777381896973\n",
      "Train: Epoch [10], Batch [653/938], Loss: 0.4528329372406006\n",
      "Train: Epoch [10], Batch [654/938], Loss: 0.6062895059585571\n",
      "Train: Epoch [10], Batch [655/938], Loss: 0.5094481706619263\n",
      "Train: Epoch [10], Batch [656/938], Loss: 0.50249183177948\n",
      "Train: Epoch [10], Batch [657/938], Loss: 0.4858558177947998\n",
      "Train: Epoch [10], Batch [658/938], Loss: 0.7484862804412842\n",
      "Train: Epoch [10], Batch [659/938], Loss: 0.41120070219039917\n",
      "Train: Epoch [10], Batch [660/938], Loss: 0.777707576751709\n",
      "Train: Epoch [10], Batch [661/938], Loss: 0.5550110340118408\n",
      "Train: Epoch [10], Batch [662/938], Loss: 0.47248345613479614\n",
      "Train: Epoch [10], Batch [663/938], Loss: 0.6560767889022827\n",
      "Train: Epoch [10], Batch [664/938], Loss: 0.6620924472808838\n",
      "Train: Epoch [10], Batch [665/938], Loss: 0.2904953360557556\n",
      "Train: Epoch [10], Batch [666/938], Loss: 0.40902280807495117\n",
      "Train: Epoch [10], Batch [667/938], Loss: 0.6941552758216858\n",
      "Train: Epoch [10], Batch [668/938], Loss: 0.4367220997810364\n",
      "Train: Epoch [10], Batch [669/938], Loss: 0.34343934059143066\n",
      "Train: Epoch [10], Batch [670/938], Loss: 0.6046484708786011\n",
      "Train: Epoch [10], Batch [671/938], Loss: 0.4767037630081177\n",
      "Train: Epoch [10], Batch [672/938], Loss: 0.6717509031295776\n",
      "Train: Epoch [10], Batch [673/938], Loss: 0.48415622115135193\n",
      "Train: Epoch [10], Batch [674/938], Loss: 0.7430011034011841\n",
      "Train: Epoch [10], Batch [675/938], Loss: 0.5444526076316833\n",
      "Train: Epoch [10], Batch [676/938], Loss: 0.5103139877319336\n",
      "Train: Epoch [10], Batch [677/938], Loss: 0.3148348033428192\n",
      "Train: Epoch [10], Batch [678/938], Loss: 0.7909024953842163\n",
      "Train: Epoch [10], Batch [679/938], Loss: 0.40260082483291626\n",
      "Train: Epoch [10], Batch [680/938], Loss: 0.42522478103637695\n",
      "Train: Epoch [10], Batch [681/938], Loss: 0.6829336881637573\n",
      "Train: Epoch [10], Batch [682/938], Loss: 0.5933253765106201\n",
      "Train: Epoch [10], Batch [683/938], Loss: 0.46921277046203613\n",
      "Train: Epoch [10], Batch [684/938], Loss: 0.5738437175750732\n",
      "Train: Epoch [10], Batch [685/938], Loss: 0.4815302789211273\n",
      "Train: Epoch [10], Batch [686/938], Loss: 0.6459940671920776\n",
      "Train: Epoch [10], Batch [687/938], Loss: 0.4433392584323883\n",
      "Train: Epoch [10], Batch [688/938], Loss: 0.5433057546615601\n",
      "Train: Epoch [10], Batch [689/938], Loss: 0.3835087716579437\n",
      "Train: Epoch [10], Batch [690/938], Loss: 0.5582931041717529\n",
      "Train: Epoch [10], Batch [691/938], Loss: 0.46918681263923645\n",
      "Train: Epoch [10], Batch [692/938], Loss: 0.529265820980072\n",
      "Train: Epoch [10], Batch [693/938], Loss: 0.6381893157958984\n",
      "Train: Epoch [10], Batch [694/938], Loss: 0.4262559115886688\n",
      "Train: Epoch [10], Batch [695/938], Loss: 0.46226003766059875\n",
      "Train: Epoch [10], Batch [696/938], Loss: 0.5057351589202881\n",
      "Train: Epoch [10], Batch [697/938], Loss: 0.4421842694282532\n",
      "Train: Epoch [10], Batch [698/938], Loss: 0.6053347587585449\n",
      "Train: Epoch [10], Batch [699/938], Loss: 0.560096025466919\n",
      "Train: Epoch [10], Batch [700/938], Loss: 0.47741860151290894\n",
      "Train: Epoch [10], Batch [701/938], Loss: 0.49849697947502136\n",
      "Train: Epoch [10], Batch [702/938], Loss: 0.6180163621902466\n",
      "Train: Epoch [10], Batch [703/938], Loss: 0.6002618074417114\n",
      "Train: Epoch [10], Batch [704/938], Loss: 0.6225173473358154\n",
      "Train: Epoch [10], Batch [705/938], Loss: 0.5569301247596741\n",
      "Train: Epoch [10], Batch [706/938], Loss: 0.6820850968360901\n",
      "Train: Epoch [10], Batch [707/938], Loss: 0.49141639471054077\n",
      "Train: Epoch [10], Batch [708/938], Loss: 0.37433159351348877\n",
      "Train: Epoch [10], Batch [709/938], Loss: 0.5391985774040222\n",
      "Train: Epoch [10], Batch [710/938], Loss: 0.5701815485954285\n",
      "Train: Epoch [10], Batch [711/938], Loss: 0.4630221724510193\n",
      "Train: Epoch [10], Batch [712/938], Loss: 0.6863377094268799\n",
      "Train: Epoch [10], Batch [713/938], Loss: 0.4591705799102783\n",
      "Train: Epoch [10], Batch [714/938], Loss: 0.45445120334625244\n",
      "Train: Epoch [10], Batch [715/938], Loss: 0.38149207830429077\n",
      "Train: Epoch [10], Batch [716/938], Loss: 0.6473413109779358\n",
      "Train: Epoch [10], Batch [717/938], Loss: 0.6295695304870605\n",
      "Train: Epoch [10], Batch [718/938], Loss: 0.6031060218811035\n",
      "Train: Epoch [10], Batch [719/938], Loss: 0.47782468795776367\n",
      "Train: Epoch [10], Batch [720/938], Loss: 0.4128510057926178\n",
      "Train: Epoch [10], Batch [721/938], Loss: 0.6731396913528442\n",
      "Train: Epoch [10], Batch [722/938], Loss: 0.4543057382106781\n",
      "Train: Epoch [10], Batch [723/938], Loss: 0.6383370161056519\n",
      "Train: Epoch [10], Batch [724/938], Loss: 0.5287834405899048\n",
      "Train: Epoch [10], Batch [725/938], Loss: 0.7951532006263733\n",
      "Train: Epoch [10], Batch [726/938], Loss: 0.4839557409286499\n",
      "Train: Epoch [10], Batch [727/938], Loss: 0.33903950452804565\n",
      "Train: Epoch [10], Batch [728/938], Loss: 0.5837376117706299\n",
      "Train: Epoch [10], Batch [729/938], Loss: 0.5498871207237244\n",
      "Train: Epoch [10], Batch [730/938], Loss: 0.4082786440849304\n",
      "Train: Epoch [10], Batch [731/938], Loss: 0.6152036786079407\n",
      "Train: Epoch [10], Batch [732/938], Loss: 0.5004050731658936\n",
      "Train: Epoch [10], Batch [733/938], Loss: 0.37052810192108154\n",
      "Train: Epoch [10], Batch [734/938], Loss: 0.5819311141967773\n",
      "Train: Epoch [10], Batch [735/938], Loss: 0.4301716089248657\n",
      "Train: Epoch [10], Batch [736/938], Loss: 0.6242216229438782\n",
      "Train: Epoch [10], Batch [737/938], Loss: 0.7506445646286011\n",
      "Train: Epoch [10], Batch [738/938], Loss: 0.4727579355239868\n",
      "Train: Epoch [10], Batch [739/938], Loss: 0.6709564924240112\n",
      "Train: Epoch [10], Batch [740/938], Loss: 0.6229494214057922\n",
      "Train: Epoch [10], Batch [741/938], Loss: 0.5131601691246033\n",
      "Train: Epoch [10], Batch [742/938], Loss: 0.42432594299316406\n",
      "Train: Epoch [10], Batch [743/938], Loss: 0.4746866226196289\n",
      "Train: Epoch [10], Batch [744/938], Loss: 0.5734374523162842\n",
      "Train: Epoch [10], Batch [745/938], Loss: 0.4872691035270691\n",
      "Train: Epoch [10], Batch [746/938], Loss: 0.7726786136627197\n",
      "Train: Epoch [10], Batch [747/938], Loss: 0.3437308669090271\n",
      "Train: Epoch [10], Batch [748/938], Loss: 0.4509466290473938\n",
      "Train: Epoch [10], Batch [749/938], Loss: 0.9956033229827881\n",
      "Train: Epoch [10], Batch [750/938], Loss: 0.8010692596435547\n",
      "Train: Epoch [10], Batch [751/938], Loss: 0.600280225276947\n",
      "Train: Epoch [10], Batch [752/938], Loss: 0.4503183364868164\n",
      "Train: Epoch [10], Batch [753/938], Loss: 0.5152252316474915\n",
      "Train: Epoch [10], Batch [754/938], Loss: 0.6532699465751648\n",
      "Train: Epoch [10], Batch [755/938], Loss: 0.47932401299476624\n",
      "Train: Epoch [10], Batch [756/938], Loss: 0.45845550298690796\n",
      "Train: Epoch [10], Batch [757/938], Loss: 0.44551539421081543\n",
      "Train: Epoch [10], Batch [758/938], Loss: 0.3338322937488556\n",
      "Train: Epoch [10], Batch [759/938], Loss: 0.629088282585144\n",
      "Train: Epoch [10], Batch [760/938], Loss: 0.6839865446090698\n",
      "Train: Epoch [10], Batch [761/938], Loss: 0.5744651556015015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [762/938], Loss: 0.33241763710975647\n",
      "Train: Epoch [10], Batch [763/938], Loss: 0.4429299235343933\n",
      "Train: Epoch [10], Batch [764/938], Loss: 0.5625038146972656\n",
      "Train: Epoch [10], Batch [765/938], Loss: 0.4755759835243225\n",
      "Train: Epoch [10], Batch [766/938], Loss: 0.8433521389961243\n",
      "Train: Epoch [10], Batch [767/938], Loss: 0.46727120876312256\n",
      "Train: Epoch [10], Batch [768/938], Loss: 0.4658244848251343\n",
      "Train: Epoch [10], Batch [769/938], Loss: 0.5729547739028931\n",
      "Train: Epoch [10], Batch [770/938], Loss: 0.4854409694671631\n",
      "Train: Epoch [10], Batch [771/938], Loss: 0.39046400785446167\n",
      "Train: Epoch [10], Batch [772/938], Loss: 0.6165236234664917\n",
      "Train: Epoch [10], Batch [773/938], Loss: 0.6343944072723389\n",
      "Train: Epoch [10], Batch [774/938], Loss: 0.6312254667282104\n",
      "Train: Epoch [10], Batch [775/938], Loss: 0.5922961235046387\n",
      "Train: Epoch [10], Batch [776/938], Loss: 0.5836828947067261\n",
      "Train: Epoch [10], Batch [777/938], Loss: 0.452544629573822\n",
      "Train: Epoch [10], Batch [778/938], Loss: 0.8431694507598877\n",
      "Train: Epoch [10], Batch [779/938], Loss: 0.6954970359802246\n",
      "Train: Epoch [10], Batch [780/938], Loss: 0.3442453145980835\n",
      "Train: Epoch [10], Batch [781/938], Loss: 0.48471394181251526\n",
      "Train: Epoch [10], Batch [782/938], Loss: 0.4442691206932068\n",
      "Train: Epoch [10], Batch [783/938], Loss: 0.5396736860275269\n",
      "Train: Epoch [10], Batch [784/938], Loss: 0.36245349049568176\n",
      "Train: Epoch [10], Batch [785/938], Loss: 0.7872953414916992\n",
      "Train: Epoch [10], Batch [786/938], Loss: 0.5420029163360596\n",
      "Train: Epoch [10], Batch [787/938], Loss: 0.820263147354126\n",
      "Train: Epoch [10], Batch [788/938], Loss: 0.9487974047660828\n",
      "Train: Epoch [10], Batch [789/938], Loss: 0.6203683614730835\n",
      "Train: Epoch [10], Batch [790/938], Loss: 0.6381275653839111\n",
      "Train: Epoch [10], Batch [791/938], Loss: 0.3733103275299072\n",
      "Train: Epoch [10], Batch [792/938], Loss: 0.5939797163009644\n",
      "Train: Epoch [10], Batch [793/938], Loss: 0.5144063830375671\n",
      "Train: Epoch [10], Batch [794/938], Loss: 0.5566002130508423\n",
      "Train: Epoch [10], Batch [795/938], Loss: 0.32913297414779663\n",
      "Train: Epoch [10], Batch [796/938], Loss: 0.5243349671363831\n",
      "Train: Epoch [10], Batch [797/938], Loss: 0.6150772571563721\n",
      "Train: Epoch [10], Batch [798/938], Loss: 0.6172914505004883\n",
      "Train: Epoch [10], Batch [799/938], Loss: 0.644618034362793\n",
      "Train: Epoch [10], Batch [800/938], Loss: 0.41637685894966125\n",
      "Train: Epoch [10], Batch [801/938], Loss: 0.48509031534194946\n",
      "Train: Epoch [10], Batch [802/938], Loss: 0.5407512187957764\n",
      "Train: Epoch [10], Batch [803/938], Loss: 0.7730076313018799\n",
      "Train: Epoch [10], Batch [804/938], Loss: 0.3848087191581726\n",
      "Train: Epoch [10], Batch [805/938], Loss: 0.4904669225215912\n",
      "Train: Epoch [10], Batch [806/938], Loss: 0.5940252542495728\n",
      "Train: Epoch [10], Batch [807/938], Loss: 0.5508453845977783\n",
      "Train: Epoch [10], Batch [808/938], Loss: 0.7103383541107178\n",
      "Train: Epoch [10], Batch [809/938], Loss: 0.7352120876312256\n",
      "Train: Epoch [10], Batch [810/938], Loss: 0.5154238939285278\n",
      "Train: Epoch [10], Batch [811/938], Loss: 0.5026630163192749\n",
      "Train: Epoch [10], Batch [812/938], Loss: 0.569533109664917\n",
      "Train: Epoch [10], Batch [813/938], Loss: 0.6390307545661926\n",
      "Train: Epoch [10], Batch [814/938], Loss: 0.49537280201911926\n",
      "Train: Epoch [10], Batch [815/938], Loss: 0.4780573546886444\n",
      "Train: Epoch [10], Batch [816/938], Loss: 0.6355621814727783\n",
      "Train: Epoch [10], Batch [817/938], Loss: 0.6224815845489502\n",
      "Train: Epoch [10], Batch [818/938], Loss: 0.4199526906013489\n",
      "Train: Epoch [10], Batch [819/938], Loss: 0.5256398916244507\n",
      "Train: Epoch [10], Batch [820/938], Loss: 0.4581160247325897\n",
      "Train: Epoch [10], Batch [821/938], Loss: 0.6541599631309509\n",
      "Train: Epoch [10], Batch [822/938], Loss: 0.485378623008728\n",
      "Train: Epoch [10], Batch [823/938], Loss: 0.5256813168525696\n",
      "Train: Epoch [10], Batch [824/938], Loss: 0.37423092126846313\n",
      "Train: Epoch [10], Batch [825/938], Loss: 0.4724566638469696\n",
      "Train: Epoch [10], Batch [826/938], Loss: 0.40200209617614746\n",
      "Train: Epoch [10], Batch [827/938], Loss: 0.49879181385040283\n",
      "Train: Epoch [10], Batch [828/938], Loss: 0.8205721378326416\n",
      "Train: Epoch [10], Batch [829/938], Loss: 0.5799010992050171\n",
      "Train: Epoch [10], Batch [830/938], Loss: 0.2212817370891571\n",
      "Train: Epoch [10], Batch [831/938], Loss: 0.6922531127929688\n",
      "Train: Epoch [10], Batch [832/938], Loss: 0.564659833908081\n",
      "Train: Epoch [10], Batch [833/938], Loss: 0.4900023341178894\n",
      "Train: Epoch [10], Batch [834/938], Loss: 0.5283474922180176\n",
      "Train: Epoch [10], Batch [835/938], Loss: 0.5311697721481323\n",
      "Train: Epoch [10], Batch [836/938], Loss: 0.4400578737258911\n",
      "Train: Epoch [10], Batch [837/938], Loss: 0.5590460300445557\n",
      "Train: Epoch [10], Batch [838/938], Loss: 0.556278645992279\n",
      "Train: Epoch [10], Batch [839/938], Loss: 0.48160937428474426\n",
      "Train: Epoch [10], Batch [840/938], Loss: 0.4787258803844452\n",
      "Train: Epoch [10], Batch [841/938], Loss: 0.6221986413002014\n",
      "Train: Epoch [10], Batch [842/938], Loss: 0.5457375049591064\n",
      "Train: Epoch [10], Batch [843/938], Loss: 0.37152817845344543\n",
      "Train: Epoch [10], Batch [844/938], Loss: 0.5687028765678406\n",
      "Train: Epoch [10], Batch [845/938], Loss: 0.6082212924957275\n",
      "Train: Epoch [10], Batch [846/938], Loss: 0.728994607925415\n",
      "Train: Epoch [10], Batch [847/938], Loss: 0.7102412581443787\n",
      "Train: Epoch [10], Batch [848/938], Loss: 0.426292359828949\n",
      "Train: Epoch [10], Batch [849/938], Loss: 0.5028363466262817\n",
      "Train: Epoch [10], Batch [850/938], Loss: 0.5773646831512451\n",
      "Train: Epoch [10], Batch [851/938], Loss: 0.6867920160293579\n",
      "Train: Epoch [10], Batch [852/938], Loss: 0.36125117540359497\n",
      "Train: Epoch [10], Batch [853/938], Loss: 0.5771471261978149\n",
      "Train: Epoch [10], Batch [854/938], Loss: 0.39720141887664795\n",
      "Train: Epoch [10], Batch [855/938], Loss: 0.5648592710494995\n",
      "Train: Epoch [10], Batch [856/938], Loss: 0.694115936756134\n",
      "Train: Epoch [10], Batch [857/938], Loss: 0.5745853781700134\n",
      "Train: Epoch [10], Batch [858/938], Loss: 0.43256381154060364\n",
      "Train: Epoch [10], Batch [859/938], Loss: 0.5027867555618286\n",
      "Train: Epoch [10], Batch [860/938], Loss: 0.3470890522003174\n",
      "Train: Epoch [10], Batch [861/938], Loss: 0.44176048040390015\n",
      "Train: Epoch [10], Batch [862/938], Loss: 0.3564234972000122\n",
      "Train: Epoch [10], Batch [863/938], Loss: 0.4725961685180664\n",
      "Train: Epoch [10], Batch [864/938], Loss: 0.5717583894729614\n",
      "Train: Epoch [10], Batch [865/938], Loss: 0.726955771446228\n",
      "Train: Epoch [10], Batch [866/938], Loss: 0.6451542973518372\n",
      "Train: Epoch [10], Batch [867/938], Loss: 0.44109106063842773\n",
      "Train: Epoch [10], Batch [868/938], Loss: 0.2926749885082245\n",
      "Train: Epoch [10], Batch [869/938], Loss: 0.4722594618797302\n",
      "Train: Epoch [10], Batch [870/938], Loss: 0.6930502653121948\n",
      "Train: Epoch [10], Batch [871/938], Loss: 0.7629466652870178\n",
      "Train: Epoch [10], Batch [872/938], Loss: 0.48418283462524414\n",
      "Train: Epoch [10], Batch [873/938], Loss: 0.4508274793624878\n",
      "Train: Epoch [10], Batch [874/938], Loss: 0.5497758388519287\n",
      "Train: Epoch [10], Batch [875/938], Loss: 0.36180567741394043\n",
      "Train: Epoch [10], Batch [876/938], Loss: 0.44368433952331543\n",
      "Train: Epoch [10], Batch [877/938], Loss: 0.6289832592010498\n",
      "Train: Epoch [10], Batch [878/938], Loss: 0.5011129379272461\n",
      "Train: Epoch [10], Batch [879/938], Loss: 0.4281110167503357\n",
      "Train: Epoch [10], Batch [880/938], Loss: 0.4501568078994751\n",
      "Train: Epoch [10], Batch [881/938], Loss: 0.6237269639968872\n",
      "Train: Epoch [10], Batch [882/938], Loss: 0.8219336271286011\n",
      "Train: Epoch [10], Batch [883/938], Loss: 0.47347888350486755\n",
      "Train: Epoch [10], Batch [884/938], Loss: 0.5733181238174438\n",
      "Train: Epoch [10], Batch [885/938], Loss: 0.6539281606674194\n",
      "Train: Epoch [10], Batch [886/938], Loss: 0.5166847705841064\n",
      "Train: Epoch [10], Batch [887/938], Loss: 0.6090618371963501\n",
      "Train: Epoch [10], Batch [888/938], Loss: 0.4071575999259949\n",
      "Train: Epoch [10], Batch [889/938], Loss: 0.5077786445617676\n",
      "Train: Epoch [10], Batch [890/938], Loss: 0.44210371375083923\n",
      "Train: Epoch [10], Batch [891/938], Loss: 0.714286208152771\n",
      "Train: Epoch [10], Batch [892/938], Loss: 0.4356713593006134\n",
      "Train: Epoch [10], Batch [893/938], Loss: 0.4089045822620392\n",
      "Train: Epoch [10], Batch [894/938], Loss: 0.5302836894989014\n",
      "Train: Epoch [10], Batch [895/938], Loss: 0.3861309885978699\n",
      "Train: Epoch [10], Batch [896/938], Loss: 0.42561793327331543\n",
      "Train: Epoch [10], Batch [897/938], Loss: 0.31047409772872925\n",
      "Train: Epoch [10], Batch [898/938], Loss: 0.6663440465927124\n",
      "Train: Epoch [10], Batch [899/938], Loss: 0.4531669020652771\n",
      "Train: Epoch [10], Batch [900/938], Loss: 0.4343509376049042\n",
      "Train: Epoch [10], Batch [901/938], Loss: 0.7099869251251221\n",
      "Train: Epoch [10], Batch [902/938], Loss: 0.507035493850708\n",
      "Train: Epoch [10], Batch [903/938], Loss: 0.6714016199111938\n",
      "Train: Epoch [10], Batch [904/938], Loss: 0.3091897666454315\n",
      "Train: Epoch [10], Batch [905/938], Loss: 0.4728991389274597\n",
      "Train: Epoch [10], Batch [906/938], Loss: 0.3579603433609009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [10], Batch [907/938], Loss: 0.5997965335845947\n",
      "Train: Epoch [10], Batch [908/938], Loss: 0.4879777133464813\n",
      "Train: Epoch [10], Batch [909/938], Loss: 0.6107550263404846\n",
      "Train: Epoch [10], Batch [910/938], Loss: 0.590002715587616\n",
      "Train: Epoch [10], Batch [911/938], Loss: 0.6671442985534668\n",
      "Train: Epoch [10], Batch [912/938], Loss: 0.39607274532318115\n",
      "Train: Epoch [10], Batch [913/938], Loss: 0.528731107711792\n",
      "Train: Epoch [10], Batch [914/938], Loss: 0.4540231227874756\n",
      "Train: Epoch [10], Batch [915/938], Loss: 0.4825795590877533\n",
      "Train: Epoch [10], Batch [916/938], Loss: 0.49288398027420044\n",
      "Train: Epoch [10], Batch [917/938], Loss: 0.4828212857246399\n",
      "Train: Epoch [10], Batch [918/938], Loss: 0.4901857078075409\n",
      "Train: Epoch [10], Batch [919/938], Loss: 0.4354088604450226\n",
      "Train: Epoch [10], Batch [920/938], Loss: 0.4370615482330322\n",
      "Train: Epoch [10], Batch [921/938], Loss: 0.6850463151931763\n",
      "Train: Epoch [10], Batch [922/938], Loss: 0.468338280916214\n",
      "Train: Epoch [10], Batch [923/938], Loss: 0.4383851885795593\n",
      "Train: Epoch [10], Batch [924/938], Loss: 0.5139552354812622\n",
      "Train: Epoch [10], Batch [925/938], Loss: 0.5830116271972656\n",
      "Train: Epoch [10], Batch [926/938], Loss: 0.549837052822113\n",
      "Train: Epoch [10], Batch [927/938], Loss: 0.5437051653862\n",
      "Train: Epoch [10], Batch [928/938], Loss: 0.3958097994327545\n",
      "Train: Epoch [10], Batch [929/938], Loss: 0.46300429105758667\n",
      "Train: Epoch [10], Batch [930/938], Loss: 0.5039739608764648\n",
      "Train: Epoch [10], Batch [931/938], Loss: 0.4398936331272125\n",
      "Train: Epoch [10], Batch [932/938], Loss: 0.4658241868019104\n",
      "Train: Epoch [10], Batch [933/938], Loss: 0.6353703141212463\n",
      "Train: Epoch [10], Batch [934/938], Loss: 0.5271153450012207\n",
      "Train: Epoch [10], Batch [935/938], Loss: 0.686654269695282\n",
      "Train: Epoch [10], Batch [936/938], Loss: 0.5375882983207703\n",
      "Train: Epoch [10], Batch [937/938], Loss: 0.5995352268218994\n",
      "Train: Epoch [10], Batch [938/938], Loss: 0.5721102952957153\n",
      "Accuracy of train set: 0.81385\n",
      "Validation: Epoch [10], Batch [1/938], Loss: 0.4464169144630432\n",
      "Validation: Epoch [10], Batch [2/938], Loss: 0.33519887924194336\n",
      "Validation: Epoch [10], Batch [3/938], Loss: 0.4312935173511505\n",
      "Validation: Epoch [10], Batch [4/938], Loss: 0.5872644186019897\n",
      "Validation: Epoch [10], Batch [5/938], Loss: 0.7653740644454956\n",
      "Validation: Epoch [10], Batch [6/938], Loss: 0.5112106800079346\n",
      "Validation: Epoch [10], Batch [7/938], Loss: 0.42359814047813416\n",
      "Validation: Epoch [10], Batch [8/938], Loss: 0.4352928400039673\n",
      "Validation: Epoch [10], Batch [9/938], Loss: 0.48954111337661743\n",
      "Validation: Epoch [10], Batch [10/938], Loss: 0.5670865774154663\n",
      "Validation: Epoch [10], Batch [11/938], Loss: 0.6318970322608948\n",
      "Validation: Epoch [10], Batch [12/938], Loss: 0.32700616121292114\n",
      "Validation: Epoch [10], Batch [13/938], Loss: 0.5116918087005615\n",
      "Validation: Epoch [10], Batch [14/938], Loss: 0.7080553770065308\n",
      "Validation: Epoch [10], Batch [15/938], Loss: 0.6078004837036133\n",
      "Validation: Epoch [10], Batch [16/938], Loss: 0.5767032504081726\n",
      "Validation: Epoch [10], Batch [17/938], Loss: 0.5397777557373047\n",
      "Validation: Epoch [10], Batch [18/938], Loss: 0.412386417388916\n",
      "Validation: Epoch [10], Batch [19/938], Loss: 0.740311861038208\n",
      "Validation: Epoch [10], Batch [20/938], Loss: 0.4641212821006775\n",
      "Validation: Epoch [10], Batch [21/938], Loss: 0.6575839519500732\n",
      "Validation: Epoch [10], Batch [22/938], Loss: 0.6264346837997437\n",
      "Validation: Epoch [10], Batch [23/938], Loss: 0.5541834831237793\n",
      "Validation: Epoch [10], Batch [24/938], Loss: 0.6150074005126953\n",
      "Validation: Epoch [10], Batch [25/938], Loss: 0.4862609803676605\n",
      "Validation: Epoch [10], Batch [26/938], Loss: 0.5460993647575378\n",
      "Validation: Epoch [10], Batch [27/938], Loss: 0.7594556212425232\n",
      "Validation: Epoch [10], Batch [28/938], Loss: 0.43671950697898865\n",
      "Validation: Epoch [10], Batch [29/938], Loss: 0.4848673343658447\n",
      "Validation: Epoch [10], Batch [30/938], Loss: 0.5700302124023438\n",
      "Validation: Epoch [10], Batch [31/938], Loss: 0.4552912414073944\n",
      "Validation: Epoch [10], Batch [32/938], Loss: 0.6130867004394531\n",
      "Validation: Epoch [10], Batch [33/938], Loss: 0.5080474615097046\n",
      "Validation: Epoch [10], Batch [34/938], Loss: 0.5149524807929993\n",
      "Validation: Epoch [10], Batch [35/938], Loss: 0.7091143727302551\n",
      "Validation: Epoch [10], Batch [36/938], Loss: 0.5576163530349731\n",
      "Validation: Epoch [10], Batch [37/938], Loss: 0.5602355003356934\n",
      "Validation: Epoch [10], Batch [38/938], Loss: 0.47588488459587097\n",
      "Validation: Epoch [10], Batch [39/938], Loss: 0.37991583347320557\n",
      "Validation: Epoch [10], Batch [40/938], Loss: 0.41576308012008667\n",
      "Validation: Epoch [10], Batch [41/938], Loss: 0.6524966955184937\n",
      "Validation: Epoch [10], Batch [42/938], Loss: 0.6595792174339294\n",
      "Validation: Epoch [10], Batch [43/938], Loss: 0.4449862539768219\n",
      "Validation: Epoch [10], Batch [44/938], Loss: 0.6383218169212341\n",
      "Validation: Epoch [10], Batch [45/938], Loss: 0.6482933759689331\n",
      "Validation: Epoch [10], Batch [46/938], Loss: 0.49831244349479675\n",
      "Validation: Epoch [10], Batch [47/938], Loss: 0.6079562902450562\n",
      "Validation: Epoch [10], Batch [48/938], Loss: 0.4885607361793518\n",
      "Validation: Epoch [10], Batch [49/938], Loss: 0.424908846616745\n",
      "Validation: Epoch [10], Batch [50/938], Loss: 0.5719815492630005\n",
      "Validation: Epoch [10], Batch [51/938], Loss: 0.4436960518360138\n",
      "Validation: Epoch [10], Batch [52/938], Loss: 0.3492172360420227\n",
      "Validation: Epoch [10], Batch [53/938], Loss: 0.8708187341690063\n",
      "Validation: Epoch [10], Batch [54/938], Loss: 0.4254912734031677\n",
      "Validation: Epoch [10], Batch [55/938], Loss: 0.6166360378265381\n",
      "Validation: Epoch [10], Batch [56/938], Loss: 0.4174063205718994\n",
      "Validation: Epoch [10], Batch [57/938], Loss: 0.6581909656524658\n",
      "Validation: Epoch [10], Batch [58/938], Loss: 0.43747973442077637\n",
      "Validation: Epoch [10], Batch [59/938], Loss: 0.514857292175293\n",
      "Validation: Epoch [10], Batch [60/938], Loss: 0.6452571749687195\n",
      "Validation: Epoch [10], Batch [61/938], Loss: 0.5348657369613647\n",
      "Validation: Epoch [10], Batch [62/938], Loss: 0.440172016620636\n",
      "Validation: Epoch [10], Batch [63/938], Loss: 0.4885450601577759\n",
      "Validation: Epoch [10], Batch [64/938], Loss: 0.49454063177108765\n",
      "Validation: Epoch [10], Batch [65/938], Loss: 0.8082766532897949\n",
      "Validation: Epoch [10], Batch [66/938], Loss: 0.5852080583572388\n",
      "Validation: Epoch [10], Batch [67/938], Loss: 0.6262787580490112\n",
      "Validation: Epoch [10], Batch [68/938], Loss: 0.4898892045021057\n",
      "Validation: Epoch [10], Batch [69/938], Loss: 0.6087441444396973\n",
      "Validation: Epoch [10], Batch [70/938], Loss: 0.47392454743385315\n",
      "Validation: Epoch [10], Batch [71/938], Loss: 0.4813154339790344\n",
      "Validation: Epoch [10], Batch [72/938], Loss: 0.5722631216049194\n",
      "Validation: Epoch [10], Batch [73/938], Loss: 0.518748939037323\n",
      "Validation: Epoch [10], Batch [74/938], Loss: 0.5160232186317444\n",
      "Validation: Epoch [10], Batch [75/938], Loss: 0.4888690114021301\n",
      "Validation: Epoch [10], Batch [76/938], Loss: 0.8340427279472351\n",
      "Validation: Epoch [10], Batch [77/938], Loss: 0.47927942872047424\n",
      "Validation: Epoch [10], Batch [78/938], Loss: 0.584439218044281\n",
      "Validation: Epoch [10], Batch [79/938], Loss: 0.5506105422973633\n",
      "Validation: Epoch [10], Batch [80/938], Loss: 0.6286220550537109\n",
      "Validation: Epoch [10], Batch [81/938], Loss: 0.6374973654747009\n",
      "Validation: Epoch [10], Batch [82/938], Loss: 0.3640196919441223\n",
      "Validation: Epoch [10], Batch [83/938], Loss: 0.4724245071411133\n",
      "Validation: Epoch [10], Batch [84/938], Loss: 0.5481423139572144\n",
      "Validation: Epoch [10], Batch [85/938], Loss: 0.5828630328178406\n",
      "Validation: Epoch [10], Batch [86/938], Loss: 0.5632331967353821\n",
      "Validation: Epoch [10], Batch [87/938], Loss: 0.4687902629375458\n",
      "Validation: Epoch [10], Batch [88/938], Loss: 0.4229641854763031\n",
      "Validation: Epoch [10], Batch [89/938], Loss: 0.6400144100189209\n",
      "Validation: Epoch [10], Batch [90/938], Loss: 0.5980721712112427\n",
      "Validation: Epoch [10], Batch [91/938], Loss: 0.5068434476852417\n",
      "Validation: Epoch [10], Batch [92/938], Loss: 0.4225156903266907\n",
      "Validation: Epoch [10], Batch [93/938], Loss: 0.4165042042732239\n",
      "Validation: Epoch [10], Batch [94/938], Loss: 0.5582470893859863\n",
      "Validation: Epoch [10], Batch [95/938], Loss: 0.6252792477607727\n",
      "Validation: Epoch [10], Batch [96/938], Loss: 0.3988233208656311\n",
      "Validation: Epoch [10], Batch [97/938], Loss: 0.508657693862915\n",
      "Validation: Epoch [10], Batch [98/938], Loss: 0.6893213391304016\n",
      "Validation: Epoch [10], Batch [99/938], Loss: 0.7662656307220459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [100/938], Loss: 0.32536429166793823\n",
      "Validation: Epoch [10], Batch [101/938], Loss: 0.5016156435012817\n",
      "Validation: Epoch [10], Batch [102/938], Loss: 0.694411039352417\n",
      "Validation: Epoch [10], Batch [103/938], Loss: 0.7481713891029358\n",
      "Validation: Epoch [10], Batch [104/938], Loss: 0.44990694522857666\n",
      "Validation: Epoch [10], Batch [105/938], Loss: 0.6031764149665833\n",
      "Validation: Epoch [10], Batch [106/938], Loss: 0.48634806275367737\n",
      "Validation: Epoch [10], Batch [107/938], Loss: 0.5646578073501587\n",
      "Validation: Epoch [10], Batch [108/938], Loss: 0.47519737482070923\n",
      "Validation: Epoch [10], Batch [109/938], Loss: 0.4106753468513489\n",
      "Validation: Epoch [10], Batch [110/938], Loss: 0.5186367034912109\n",
      "Validation: Epoch [10], Batch [111/938], Loss: 0.36182284355163574\n",
      "Validation: Epoch [10], Batch [112/938], Loss: 0.48322492837905884\n",
      "Validation: Epoch [10], Batch [113/938], Loss: 0.4989146292209625\n",
      "Validation: Epoch [10], Batch [114/938], Loss: 0.6361953020095825\n",
      "Validation: Epoch [10], Batch [115/938], Loss: 0.5751205682754517\n",
      "Validation: Epoch [10], Batch [116/938], Loss: 0.5153235197067261\n",
      "Validation: Epoch [10], Batch [117/938], Loss: 0.4614007771015167\n",
      "Validation: Epoch [10], Batch [118/938], Loss: 0.49584218859672546\n",
      "Validation: Epoch [10], Batch [119/938], Loss: 0.5837281942367554\n",
      "Validation: Epoch [10], Batch [120/938], Loss: 0.5300515294075012\n",
      "Validation: Epoch [10], Batch [121/938], Loss: 0.3274863362312317\n",
      "Validation: Epoch [10], Batch [122/938], Loss: 0.46706244349479675\n",
      "Validation: Epoch [10], Batch [123/938], Loss: 0.7337855100631714\n",
      "Validation: Epoch [10], Batch [124/938], Loss: 0.6712668538093567\n",
      "Validation: Epoch [10], Batch [125/938], Loss: 0.47835832834243774\n",
      "Validation: Epoch [10], Batch [126/938], Loss: 0.42129725217819214\n",
      "Validation: Epoch [10], Batch [127/938], Loss: 0.4381251931190491\n",
      "Validation: Epoch [10], Batch [128/938], Loss: 0.5555047392845154\n",
      "Validation: Epoch [10], Batch [129/938], Loss: 0.5064115524291992\n",
      "Validation: Epoch [10], Batch [130/938], Loss: 0.5846787095069885\n",
      "Validation: Epoch [10], Batch [131/938], Loss: 0.64248126745224\n",
      "Validation: Epoch [10], Batch [132/938], Loss: 0.6220225095748901\n",
      "Validation: Epoch [10], Batch [133/938], Loss: 0.600087583065033\n",
      "Validation: Epoch [10], Batch [134/938], Loss: 0.6532431840896606\n",
      "Validation: Epoch [10], Batch [135/938], Loss: 0.6419212222099304\n",
      "Validation: Epoch [10], Batch [136/938], Loss: 0.5815474987030029\n",
      "Validation: Epoch [10], Batch [137/938], Loss: 0.6122821569442749\n",
      "Validation: Epoch [10], Batch [138/938], Loss: 0.34240561723709106\n",
      "Validation: Epoch [10], Batch [139/938], Loss: 0.45400065183639526\n",
      "Validation: Epoch [10], Batch [140/938], Loss: 0.5304083824157715\n",
      "Validation: Epoch [10], Batch [141/938], Loss: 0.4683215022087097\n",
      "Validation: Epoch [10], Batch [142/938], Loss: 0.6311132907867432\n",
      "Validation: Epoch [10], Batch [143/938], Loss: 0.45480507612228394\n",
      "Validation: Epoch [10], Batch [144/938], Loss: 0.6588821411132812\n",
      "Validation: Epoch [10], Batch [145/938], Loss: 0.4699266254901886\n",
      "Validation: Epoch [10], Batch [146/938], Loss: 0.3703102767467499\n",
      "Validation: Epoch [10], Batch [147/938], Loss: 0.6780932545661926\n",
      "Validation: Epoch [10], Batch [148/938], Loss: 0.6878007054328918\n",
      "Validation: Epoch [10], Batch [149/938], Loss: 0.446868360042572\n",
      "Validation: Epoch [10], Batch [150/938], Loss: 0.4823572039604187\n",
      "Validation: Epoch [10], Batch [151/938], Loss: 0.4032136797904968\n",
      "Validation: Epoch [10], Batch [152/938], Loss: 0.48092490434646606\n",
      "Validation: Epoch [10], Batch [153/938], Loss: 0.5331020355224609\n",
      "Validation: Epoch [10], Batch [154/938], Loss: 0.37077492475509644\n",
      "Validation: Epoch [10], Batch [155/938], Loss: 0.5909318923950195\n",
      "Validation: Epoch [10], Batch [156/938], Loss: 0.5112031102180481\n",
      "Validation: Epoch [10], Batch [157/938], Loss: 0.49165529012680054\n",
      "Validation: Epoch [10], Batch [158/938], Loss: 0.5672776103019714\n",
      "Validation: Epoch [10], Batch [159/938], Loss: 0.5350725650787354\n",
      "Validation: Epoch [10], Batch [160/938], Loss: 0.5983134508132935\n",
      "Validation: Epoch [10], Batch [161/938], Loss: 0.6296648383140564\n",
      "Validation: Epoch [10], Batch [162/938], Loss: 0.5101114511489868\n",
      "Validation: Epoch [10], Batch [163/938], Loss: 0.633443295955658\n",
      "Validation: Epoch [10], Batch [164/938], Loss: 0.6049315929412842\n",
      "Validation: Epoch [10], Batch [165/938], Loss: 0.5092941522598267\n",
      "Validation: Epoch [10], Batch [166/938], Loss: 0.5709406137466431\n",
      "Validation: Epoch [10], Batch [167/938], Loss: 0.6511161923408508\n",
      "Validation: Epoch [10], Batch [168/938], Loss: 0.6472511291503906\n",
      "Validation: Epoch [10], Batch [169/938], Loss: 0.3838825225830078\n",
      "Validation: Epoch [10], Batch [170/938], Loss: 0.5672255158424377\n",
      "Validation: Epoch [10], Batch [171/938], Loss: 0.4067508280277252\n",
      "Validation: Epoch [10], Batch [172/938], Loss: 0.4790468215942383\n",
      "Validation: Epoch [10], Batch [173/938], Loss: 0.4979192316532135\n",
      "Validation: Epoch [10], Batch [174/938], Loss: 0.5101209878921509\n",
      "Validation: Epoch [10], Batch [175/938], Loss: 0.44169753789901733\n",
      "Validation: Epoch [10], Batch [176/938], Loss: 0.6381509900093079\n",
      "Validation: Epoch [10], Batch [177/938], Loss: 0.5536845922470093\n",
      "Validation: Epoch [10], Batch [178/938], Loss: 0.39832112193107605\n",
      "Validation: Epoch [10], Batch [179/938], Loss: 0.8502282500267029\n",
      "Validation: Epoch [10], Batch [180/938], Loss: 0.6066892147064209\n",
      "Validation: Epoch [10], Batch [181/938], Loss: 0.448188841342926\n",
      "Validation: Epoch [10], Batch [182/938], Loss: 0.46533477306365967\n",
      "Validation: Epoch [10], Batch [183/938], Loss: 0.4629135727882385\n",
      "Validation: Epoch [10], Batch [184/938], Loss: 0.39631035923957825\n",
      "Validation: Epoch [10], Batch [185/938], Loss: 0.6692395210266113\n",
      "Validation: Epoch [10], Batch [186/938], Loss: 0.4798043668270111\n",
      "Validation: Epoch [10], Batch [187/938], Loss: 0.5518418550491333\n",
      "Validation: Epoch [10], Batch [188/938], Loss: 0.6805927753448486\n",
      "Validation: Epoch [10], Batch [189/938], Loss: 0.4921434819698334\n",
      "Validation: Epoch [10], Batch [190/938], Loss: 0.4558075964450836\n",
      "Validation: Epoch [10], Batch [191/938], Loss: 0.7100707292556763\n",
      "Validation: Epoch [10], Batch [192/938], Loss: 0.36586064100265503\n",
      "Validation: Epoch [10], Batch [193/938], Loss: 0.4510083794593811\n",
      "Validation: Epoch [10], Batch [194/938], Loss: 0.621839702129364\n",
      "Validation: Epoch [10], Batch [195/938], Loss: 0.32583075761795044\n",
      "Validation: Epoch [10], Batch [196/938], Loss: 0.5136901140213013\n",
      "Validation: Epoch [10], Batch [197/938], Loss: 0.5837233066558838\n",
      "Validation: Epoch [10], Batch [198/938], Loss: 0.45940306782722473\n",
      "Validation: Epoch [10], Batch [199/938], Loss: 0.47980955243110657\n",
      "Validation: Epoch [10], Batch [200/938], Loss: 0.5381728410720825\n",
      "Validation: Epoch [10], Batch [201/938], Loss: 0.44299012422561646\n",
      "Validation: Epoch [10], Batch [202/938], Loss: 0.5050457715988159\n",
      "Validation: Epoch [10], Batch [203/938], Loss: 0.58328777551651\n",
      "Validation: Epoch [10], Batch [204/938], Loss: 0.4366295635700226\n",
      "Validation: Epoch [10], Batch [205/938], Loss: 0.524465799331665\n",
      "Validation: Epoch [10], Batch [206/938], Loss: 0.44878193736076355\n",
      "Validation: Epoch [10], Batch [207/938], Loss: 0.5393714308738708\n",
      "Validation: Epoch [10], Batch [208/938], Loss: 0.5298425555229187\n",
      "Validation: Epoch [10], Batch [209/938], Loss: 0.3665086627006531\n",
      "Validation: Epoch [10], Batch [210/938], Loss: 0.6847327947616577\n",
      "Validation: Epoch [10], Batch [211/938], Loss: 0.49519550800323486\n",
      "Validation: Epoch [10], Batch [212/938], Loss: 0.45999252796173096\n",
      "Validation: Epoch [10], Batch [213/938], Loss: 0.6138170957565308\n",
      "Validation: Epoch [10], Batch [214/938], Loss: 0.73274827003479\n",
      "Validation: Epoch [10], Batch [215/938], Loss: 0.5898460745811462\n",
      "Validation: Epoch [10], Batch [216/938], Loss: 0.7256428003311157\n",
      "Validation: Epoch [10], Batch [217/938], Loss: 0.5416548848152161\n",
      "Validation: Epoch [10], Batch [218/938], Loss: 0.42964497208595276\n",
      "Validation: Epoch [10], Batch [219/938], Loss: 0.5396652221679688\n",
      "Validation: Epoch [10], Batch [220/938], Loss: 0.3486708998680115\n",
      "Validation: Epoch [10], Batch [221/938], Loss: 0.45401182770729065\n",
      "Validation: Epoch [10], Batch [222/938], Loss: 0.5511711835861206\n",
      "Validation: Epoch [10], Batch [223/938], Loss: 0.5359150171279907\n",
      "Validation: Epoch [10], Batch [224/938], Loss: 0.6053751111030579\n",
      "Validation: Epoch [10], Batch [225/938], Loss: 0.43532294034957886\n",
      "Validation: Epoch [10], Batch [226/938], Loss: 0.460578978061676\n",
      "Validation: Epoch [10], Batch [227/938], Loss: 0.6491937637329102\n",
      "Validation: Epoch [10], Batch [228/938], Loss: 0.4462762475013733\n",
      "Validation: Epoch [10], Batch [229/938], Loss: 0.4542119801044464\n",
      "Validation: Epoch [10], Batch [230/938], Loss: 0.37527334690093994\n",
      "Validation: Epoch [10], Batch [231/938], Loss: 0.4234905242919922\n",
      "Validation: Epoch [10], Batch [232/938], Loss: 0.39390066266059875\n",
      "Validation: Epoch [10], Batch [233/938], Loss: 0.36917370557785034\n",
      "Validation: Epoch [10], Batch [234/938], Loss: 0.516176700592041\n",
      "Validation: Epoch [10], Batch [235/938], Loss: 0.5175567269325256\n",
      "Validation: Epoch [10], Batch [236/938], Loss: 0.5446162223815918\n",
      "Validation: Epoch [10], Batch [237/938], Loss: 0.500710666179657\n",
      "Validation: Epoch [10], Batch [238/938], Loss: 0.5061355233192444\n",
      "Validation: Epoch [10], Batch [239/938], Loss: 0.5148046016693115\n",
      "Validation: Epoch [10], Batch [240/938], Loss: 0.5306639671325684\n",
      "Validation: Epoch [10], Batch [241/938], Loss: 0.4958800971508026\n",
      "Validation: Epoch [10], Batch [242/938], Loss: 0.48369258642196655\n",
      "Validation: Epoch [10], Batch [243/938], Loss: 0.6345187425613403\n",
      "Validation: Epoch [10], Batch [244/938], Loss: 0.433917760848999\n",
      "Validation: Epoch [10], Batch [245/938], Loss: 0.48022258281707764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [246/938], Loss: 0.6255132555961609\n",
      "Validation: Epoch [10], Batch [247/938], Loss: 0.5897698402404785\n",
      "Validation: Epoch [10], Batch [248/938], Loss: 0.46933916211128235\n",
      "Validation: Epoch [10], Batch [249/938], Loss: 0.4370667040348053\n",
      "Validation: Epoch [10], Batch [250/938], Loss: 0.37577128410339355\n",
      "Validation: Epoch [10], Batch [251/938], Loss: 0.6211143136024475\n",
      "Validation: Epoch [10], Batch [252/938], Loss: 0.5479755401611328\n",
      "Validation: Epoch [10], Batch [253/938], Loss: 0.5226128697395325\n",
      "Validation: Epoch [10], Batch [254/938], Loss: 0.5712011456489563\n",
      "Validation: Epoch [10], Batch [255/938], Loss: 0.6196234822273254\n",
      "Validation: Epoch [10], Batch [256/938], Loss: 0.5940341949462891\n",
      "Validation: Epoch [10], Batch [257/938], Loss: 0.5349016189575195\n",
      "Validation: Epoch [10], Batch [258/938], Loss: 0.49284034967422485\n",
      "Validation: Epoch [10], Batch [259/938], Loss: 0.5101532936096191\n",
      "Validation: Epoch [10], Batch [260/938], Loss: 0.488288938999176\n",
      "Validation: Epoch [10], Batch [261/938], Loss: 0.8404825329780579\n",
      "Validation: Epoch [10], Batch [262/938], Loss: 0.8347203731536865\n",
      "Validation: Epoch [10], Batch [263/938], Loss: 0.4126385748386383\n",
      "Validation: Epoch [10], Batch [264/938], Loss: 0.6498399376869202\n",
      "Validation: Epoch [10], Batch [265/938], Loss: 0.6142065525054932\n",
      "Validation: Epoch [10], Batch [266/938], Loss: 0.2815359830856323\n",
      "Validation: Epoch [10], Batch [267/938], Loss: 0.38936635851860046\n",
      "Validation: Epoch [10], Batch [268/938], Loss: 0.6028567552566528\n",
      "Validation: Epoch [10], Batch [269/938], Loss: 0.9307941198348999\n",
      "Validation: Epoch [10], Batch [270/938], Loss: 0.6563897132873535\n",
      "Validation: Epoch [10], Batch [271/938], Loss: 0.43685436248779297\n",
      "Validation: Epoch [10], Batch [272/938], Loss: 0.343728244304657\n",
      "Validation: Epoch [10], Batch [273/938], Loss: 0.49979498982429504\n",
      "Validation: Epoch [10], Batch [274/938], Loss: 0.48649606108665466\n",
      "Validation: Epoch [10], Batch [275/938], Loss: 0.5611202716827393\n",
      "Validation: Epoch [10], Batch [276/938], Loss: 0.8737198114395142\n",
      "Validation: Epoch [10], Batch [277/938], Loss: 0.49754980206489563\n",
      "Validation: Epoch [10], Batch [278/938], Loss: 0.3215855062007904\n",
      "Validation: Epoch [10], Batch [279/938], Loss: 0.5186724662780762\n",
      "Validation: Epoch [10], Batch [280/938], Loss: 0.4156389534473419\n",
      "Validation: Epoch [10], Batch [281/938], Loss: 0.5583240985870361\n",
      "Validation: Epoch [10], Batch [282/938], Loss: 0.470287024974823\n",
      "Validation: Epoch [10], Batch [283/938], Loss: 0.4354102313518524\n",
      "Validation: Epoch [10], Batch [284/938], Loss: 0.4928973913192749\n",
      "Validation: Epoch [10], Batch [285/938], Loss: 0.7021843791007996\n",
      "Validation: Epoch [10], Batch [286/938], Loss: 0.432009220123291\n",
      "Validation: Epoch [10], Batch [287/938], Loss: 0.37699389457702637\n",
      "Validation: Epoch [10], Batch [288/938], Loss: 0.5042456388473511\n",
      "Validation: Epoch [10], Batch [289/938], Loss: 0.4844999611377716\n",
      "Validation: Epoch [10], Batch [290/938], Loss: 0.4904822111129761\n",
      "Validation: Epoch [10], Batch [291/938], Loss: 0.596610426902771\n",
      "Validation: Epoch [10], Batch [292/938], Loss: 0.616008460521698\n",
      "Validation: Epoch [10], Batch [293/938], Loss: 0.4987947344779968\n",
      "Validation: Epoch [10], Batch [294/938], Loss: 0.5468111038208008\n",
      "Validation: Epoch [10], Batch [295/938], Loss: 0.5963342189788818\n",
      "Validation: Epoch [10], Batch [296/938], Loss: 0.6062692999839783\n",
      "Validation: Epoch [10], Batch [297/938], Loss: 0.4817260205745697\n",
      "Validation: Epoch [10], Batch [298/938], Loss: 0.6468051671981812\n",
      "Validation: Epoch [10], Batch [299/938], Loss: 0.4605664312839508\n",
      "Validation: Epoch [10], Batch [300/938], Loss: 0.5762136578559875\n",
      "Validation: Epoch [10], Batch [301/938], Loss: 0.45932185649871826\n",
      "Validation: Epoch [10], Batch [302/938], Loss: 0.7090569138526917\n",
      "Validation: Epoch [10], Batch [303/938], Loss: 0.44879332184791565\n",
      "Validation: Epoch [10], Batch [304/938], Loss: 0.45064684748649597\n",
      "Validation: Epoch [10], Batch [305/938], Loss: 0.4937065839767456\n",
      "Validation: Epoch [10], Batch [306/938], Loss: 0.3916710615158081\n",
      "Validation: Epoch [10], Batch [307/938], Loss: 0.45865026116371155\n",
      "Validation: Epoch [10], Batch [308/938], Loss: 0.3862578272819519\n",
      "Validation: Epoch [10], Batch [309/938], Loss: 0.3449975848197937\n",
      "Validation: Epoch [10], Batch [310/938], Loss: 0.5780596137046814\n",
      "Validation: Epoch [10], Batch [311/938], Loss: 0.7450963258743286\n",
      "Validation: Epoch [10], Batch [312/938], Loss: 0.4529459476470947\n",
      "Validation: Epoch [10], Batch [313/938], Loss: 0.6111418008804321\n",
      "Validation: Epoch [10], Batch [314/938], Loss: 0.5500500798225403\n",
      "Validation: Epoch [10], Batch [315/938], Loss: 0.4874054789543152\n",
      "Validation: Epoch [10], Batch [316/938], Loss: 0.7172134518623352\n",
      "Validation: Epoch [10], Batch [317/938], Loss: 0.5463920831680298\n",
      "Validation: Epoch [10], Batch [318/938], Loss: 0.4287795126438141\n",
      "Validation: Epoch [10], Batch [319/938], Loss: 0.40557435154914856\n",
      "Validation: Epoch [10], Batch [320/938], Loss: 0.437988817691803\n",
      "Validation: Epoch [10], Batch [321/938], Loss: 0.5469937324523926\n",
      "Validation: Epoch [10], Batch [322/938], Loss: 0.5304743051528931\n",
      "Validation: Epoch [10], Batch [323/938], Loss: 0.41152939200401306\n",
      "Validation: Epoch [10], Batch [324/938], Loss: 0.5838342308998108\n",
      "Validation: Epoch [10], Batch [325/938], Loss: 0.6477828621864319\n",
      "Validation: Epoch [10], Batch [326/938], Loss: 0.6080601215362549\n",
      "Validation: Epoch [10], Batch [327/938], Loss: 0.5537362694740295\n",
      "Validation: Epoch [10], Batch [328/938], Loss: 0.624639093875885\n",
      "Validation: Epoch [10], Batch [329/938], Loss: 0.43738552927970886\n",
      "Validation: Epoch [10], Batch [330/938], Loss: 0.5880097150802612\n",
      "Validation: Epoch [10], Batch [331/938], Loss: 0.4885886013507843\n",
      "Validation: Epoch [10], Batch [332/938], Loss: 0.4068785607814789\n",
      "Validation: Epoch [10], Batch [333/938], Loss: 0.48861441016197205\n",
      "Validation: Epoch [10], Batch [334/938], Loss: 0.43734341859817505\n",
      "Validation: Epoch [10], Batch [335/938], Loss: 0.5473906397819519\n",
      "Validation: Epoch [10], Batch [336/938], Loss: 0.8318884372711182\n",
      "Validation: Epoch [10], Batch [337/938], Loss: 0.43954721093177795\n",
      "Validation: Epoch [10], Batch [338/938], Loss: 0.4745914936065674\n",
      "Validation: Epoch [10], Batch [339/938], Loss: 0.4156700372695923\n",
      "Validation: Epoch [10], Batch [340/938], Loss: 0.4381166100502014\n",
      "Validation: Epoch [10], Batch [341/938], Loss: 0.38864293694496155\n",
      "Validation: Epoch [10], Batch [342/938], Loss: 0.3150412440299988\n",
      "Validation: Epoch [10], Batch [343/938], Loss: 0.5138107538223267\n",
      "Validation: Epoch [10], Batch [344/938], Loss: 0.6506403088569641\n",
      "Validation: Epoch [10], Batch [345/938], Loss: 0.5293982028961182\n",
      "Validation: Epoch [10], Batch [346/938], Loss: 0.5536901354789734\n",
      "Validation: Epoch [10], Batch [347/938], Loss: 0.5257341861724854\n",
      "Validation: Epoch [10], Batch [348/938], Loss: 0.49356406927108765\n",
      "Validation: Epoch [10], Batch [349/938], Loss: 0.7964444160461426\n",
      "Validation: Epoch [10], Batch [350/938], Loss: 0.7114565968513489\n",
      "Validation: Epoch [10], Batch [351/938], Loss: 0.4612768888473511\n",
      "Validation: Epoch [10], Batch [352/938], Loss: 0.48661065101623535\n",
      "Validation: Epoch [10], Batch [353/938], Loss: 0.5086641311645508\n",
      "Validation: Epoch [10], Batch [354/938], Loss: 0.731521725654602\n",
      "Validation: Epoch [10], Batch [355/938], Loss: 0.5141832232475281\n",
      "Validation: Epoch [10], Batch [356/938], Loss: 0.4627531170845032\n",
      "Validation: Epoch [10], Batch [357/938], Loss: 0.44189515709877014\n",
      "Validation: Epoch [10], Batch [358/938], Loss: 0.42516976594924927\n",
      "Validation: Epoch [10], Batch [359/938], Loss: 0.6075066924095154\n",
      "Validation: Epoch [10], Batch [360/938], Loss: 0.7465232014656067\n",
      "Validation: Epoch [10], Batch [361/938], Loss: 0.5636268258094788\n",
      "Validation: Epoch [10], Batch [362/938], Loss: 0.4385845363140106\n",
      "Validation: Epoch [10], Batch [363/938], Loss: 0.4698528051376343\n",
      "Validation: Epoch [10], Batch [364/938], Loss: 0.29482030868530273\n",
      "Validation: Epoch [10], Batch [365/938], Loss: 0.5233969688415527\n",
      "Validation: Epoch [10], Batch [366/938], Loss: 0.46206629276275635\n",
      "Validation: Epoch [10], Batch [367/938], Loss: 0.4669824242591858\n",
      "Validation: Epoch [10], Batch [368/938], Loss: 0.49321386218070984\n",
      "Validation: Epoch [10], Batch [369/938], Loss: 0.33324486017227173\n",
      "Validation: Epoch [10], Batch [370/938], Loss: 0.46561941504478455\n",
      "Validation: Epoch [10], Batch [371/938], Loss: 0.6088395118713379\n",
      "Validation: Epoch [10], Batch [372/938], Loss: 0.498809814453125\n",
      "Validation: Epoch [10], Batch [373/938], Loss: 0.4640725553035736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [374/938], Loss: 0.6476149559020996\n",
      "Validation: Epoch [10], Batch [375/938], Loss: 0.3899931013584137\n",
      "Validation: Epoch [10], Batch [376/938], Loss: 0.37127023935317993\n",
      "Validation: Epoch [10], Batch [377/938], Loss: 0.5615456104278564\n",
      "Validation: Epoch [10], Batch [378/938], Loss: 0.5183071494102478\n",
      "Validation: Epoch [10], Batch [379/938], Loss: 0.7387469410896301\n",
      "Validation: Epoch [10], Batch [380/938], Loss: 0.5628789663314819\n",
      "Validation: Epoch [10], Batch [381/938], Loss: 0.5206254720687866\n",
      "Validation: Epoch [10], Batch [382/938], Loss: 0.6151397824287415\n",
      "Validation: Epoch [10], Batch [383/938], Loss: 0.4737911522388458\n",
      "Validation: Epoch [10], Batch [384/938], Loss: 0.5136180520057678\n",
      "Validation: Epoch [10], Batch [385/938], Loss: 0.3404485583305359\n",
      "Validation: Epoch [10], Batch [386/938], Loss: 0.4792757034301758\n",
      "Validation: Epoch [10], Batch [387/938], Loss: 0.5916426181793213\n",
      "Validation: Epoch [10], Batch [388/938], Loss: 0.4477252960205078\n",
      "Validation: Epoch [10], Batch [389/938], Loss: 0.4951387643814087\n",
      "Validation: Epoch [10], Batch [390/938], Loss: 0.5318132638931274\n",
      "Validation: Epoch [10], Batch [391/938], Loss: 0.7405563592910767\n",
      "Validation: Epoch [10], Batch [392/938], Loss: 0.43836843967437744\n",
      "Validation: Epoch [10], Batch [393/938], Loss: 0.5899481773376465\n",
      "Validation: Epoch [10], Batch [394/938], Loss: 0.4629198908805847\n",
      "Validation: Epoch [10], Batch [395/938], Loss: 0.7824051976203918\n",
      "Validation: Epoch [10], Batch [396/938], Loss: 0.6618213653564453\n",
      "Validation: Epoch [10], Batch [397/938], Loss: 0.4681225121021271\n",
      "Validation: Epoch [10], Batch [398/938], Loss: 0.5799809694290161\n",
      "Validation: Epoch [10], Batch [399/938], Loss: 0.3802238404750824\n",
      "Validation: Epoch [10], Batch [400/938], Loss: 0.5655381679534912\n",
      "Validation: Epoch [10], Batch [401/938], Loss: 0.5808043479919434\n",
      "Validation: Epoch [10], Batch [402/938], Loss: 0.7009674310684204\n",
      "Validation: Epoch [10], Batch [403/938], Loss: 0.6363707184791565\n",
      "Validation: Epoch [10], Batch [404/938], Loss: 0.4387429654598236\n",
      "Validation: Epoch [10], Batch [405/938], Loss: 0.6241577863693237\n",
      "Validation: Epoch [10], Batch [406/938], Loss: 0.8760701417922974\n",
      "Validation: Epoch [10], Batch [407/938], Loss: 0.43322062492370605\n",
      "Validation: Epoch [10], Batch [408/938], Loss: 0.6702788472175598\n",
      "Validation: Epoch [10], Batch [409/938], Loss: 0.48124730587005615\n",
      "Validation: Epoch [10], Batch [410/938], Loss: 0.508456289768219\n",
      "Validation: Epoch [10], Batch [411/938], Loss: 0.4522010087966919\n",
      "Validation: Epoch [10], Batch [412/938], Loss: 0.6156709790229797\n",
      "Validation: Epoch [10], Batch [413/938], Loss: 0.48740309476852417\n",
      "Validation: Epoch [10], Batch [414/938], Loss: 0.4415930211544037\n",
      "Validation: Epoch [10], Batch [415/938], Loss: 0.5656607151031494\n",
      "Validation: Epoch [10], Batch [416/938], Loss: 0.4415982961654663\n",
      "Validation: Epoch [10], Batch [417/938], Loss: 0.5334911346435547\n",
      "Validation: Epoch [10], Batch [418/938], Loss: 0.6219304800033569\n",
      "Validation: Epoch [10], Batch [419/938], Loss: 0.6117006540298462\n",
      "Validation: Epoch [10], Batch [420/938], Loss: 0.5694831609725952\n",
      "Validation: Epoch [10], Batch [421/938], Loss: 0.40671294927597046\n",
      "Validation: Epoch [10], Batch [422/938], Loss: 0.388396680355072\n",
      "Validation: Epoch [10], Batch [423/938], Loss: 0.42631715536117554\n",
      "Validation: Epoch [10], Batch [424/938], Loss: 0.6128252744674683\n",
      "Validation: Epoch [10], Batch [425/938], Loss: 0.642030656337738\n",
      "Validation: Epoch [10], Batch [426/938], Loss: 0.3867250084877014\n",
      "Validation: Epoch [10], Batch [427/938], Loss: 0.3881158232688904\n",
      "Validation: Epoch [10], Batch [428/938], Loss: 0.6600261926651001\n",
      "Validation: Epoch [10], Batch [429/938], Loss: 0.7902784943580627\n",
      "Validation: Epoch [10], Batch [430/938], Loss: 0.5235299468040466\n",
      "Validation: Epoch [10], Batch [431/938], Loss: 0.46342751383781433\n",
      "Validation: Epoch [10], Batch [432/938], Loss: 0.57976233959198\n",
      "Validation: Epoch [10], Batch [433/938], Loss: 0.6974296569824219\n",
      "Validation: Epoch [10], Batch [434/938], Loss: 0.4654984474182129\n",
      "Validation: Epoch [10], Batch [435/938], Loss: 0.46041736006736755\n",
      "Validation: Epoch [10], Batch [436/938], Loss: 0.5591968297958374\n",
      "Validation: Epoch [10], Batch [437/938], Loss: 0.389169842004776\n",
      "Validation: Epoch [10], Batch [438/938], Loss: 0.5355398058891296\n",
      "Validation: Epoch [10], Batch [439/938], Loss: 0.6852992177009583\n",
      "Validation: Epoch [10], Batch [440/938], Loss: 0.4035457968711853\n",
      "Validation: Epoch [10], Batch [441/938], Loss: 0.322208046913147\n",
      "Validation: Epoch [10], Batch [442/938], Loss: 0.5497673749923706\n",
      "Validation: Epoch [10], Batch [443/938], Loss: 0.7136889696121216\n",
      "Validation: Epoch [10], Batch [444/938], Loss: 0.4407286047935486\n",
      "Validation: Epoch [10], Batch [445/938], Loss: 0.5159194469451904\n",
      "Validation: Epoch [10], Batch [446/938], Loss: 0.5555349588394165\n",
      "Validation: Epoch [10], Batch [447/938], Loss: 0.5404691696166992\n",
      "Validation: Epoch [10], Batch [448/938], Loss: 0.4290226697921753\n",
      "Validation: Epoch [10], Batch [449/938], Loss: 0.43021896481513977\n",
      "Validation: Epoch [10], Batch [450/938], Loss: 0.5079445242881775\n",
      "Validation: Epoch [10], Batch [451/938], Loss: 0.6305423378944397\n",
      "Validation: Epoch [10], Batch [452/938], Loss: 0.6250945925712585\n",
      "Validation: Epoch [10], Batch [453/938], Loss: 0.4235939085483551\n",
      "Validation: Epoch [10], Batch [454/938], Loss: 0.48099809885025024\n",
      "Validation: Epoch [10], Batch [455/938], Loss: 0.5120940804481506\n",
      "Validation: Epoch [10], Batch [456/938], Loss: 0.4857015609741211\n",
      "Validation: Epoch [10], Batch [457/938], Loss: 0.6051062345504761\n",
      "Validation: Epoch [10], Batch [458/938], Loss: 0.5256489515304565\n",
      "Validation: Epoch [10], Batch [459/938], Loss: 0.31678688526153564\n",
      "Validation: Epoch [10], Batch [460/938], Loss: 0.48848092555999756\n",
      "Validation: Epoch [10], Batch [461/938], Loss: 0.535078763961792\n",
      "Validation: Epoch [10], Batch [462/938], Loss: 0.42577439546585083\n",
      "Validation: Epoch [10], Batch [463/938], Loss: 0.5979574918746948\n",
      "Validation: Epoch [10], Batch [464/938], Loss: 0.4863506555557251\n",
      "Validation: Epoch [10], Batch [465/938], Loss: 0.9160463809967041\n",
      "Validation: Epoch [10], Batch [466/938], Loss: 0.4510679244995117\n",
      "Validation: Epoch [10], Batch [467/938], Loss: 0.37216389179229736\n",
      "Validation: Epoch [10], Batch [468/938], Loss: 0.5957461595535278\n",
      "Validation: Epoch [10], Batch [469/938], Loss: 0.4808320999145508\n",
      "Validation: Epoch [10], Batch [470/938], Loss: 0.37915876507759094\n",
      "Validation: Epoch [10], Batch [471/938], Loss: 0.4007298946380615\n",
      "Validation: Epoch [10], Batch [472/938], Loss: 0.6141910552978516\n",
      "Validation: Epoch [10], Batch [473/938], Loss: 0.4582343101501465\n",
      "Validation: Epoch [10], Batch [474/938], Loss: 0.5919536352157593\n",
      "Validation: Epoch [10], Batch [475/938], Loss: 0.5452151298522949\n",
      "Validation: Epoch [10], Batch [476/938], Loss: 0.40742993354797363\n",
      "Validation: Epoch [10], Batch [477/938], Loss: 0.5101411938667297\n",
      "Validation: Epoch [10], Batch [478/938], Loss: 0.4961453676223755\n",
      "Validation: Epoch [10], Batch [479/938], Loss: 0.43177127838134766\n",
      "Validation: Epoch [10], Batch [480/938], Loss: 0.5852161049842834\n",
      "Validation: Epoch [10], Batch [481/938], Loss: 0.45840227603912354\n",
      "Validation: Epoch [10], Batch [482/938], Loss: 0.39835983514785767\n",
      "Validation: Epoch [10], Batch [483/938], Loss: 0.4546615481376648\n",
      "Validation: Epoch [10], Batch [484/938], Loss: 0.6656656265258789\n",
      "Validation: Epoch [10], Batch [485/938], Loss: 0.537569522857666\n",
      "Validation: Epoch [10], Batch [486/938], Loss: 0.3906157612800598\n",
      "Validation: Epoch [10], Batch [487/938], Loss: 0.5604429244995117\n",
      "Validation: Epoch [10], Batch [488/938], Loss: 0.4368116855621338\n",
      "Validation: Epoch [10], Batch [489/938], Loss: 0.4125431776046753\n",
      "Validation: Epoch [10], Batch [490/938], Loss: 0.4539330005645752\n",
      "Validation: Epoch [10], Batch [491/938], Loss: 0.4429292678833008\n",
      "Validation: Epoch [10], Batch [492/938], Loss: 0.6224539279937744\n",
      "Validation: Epoch [10], Batch [493/938], Loss: 0.5342788696289062\n",
      "Validation: Epoch [10], Batch [494/938], Loss: 0.6232793927192688\n",
      "Validation: Epoch [10], Batch [495/938], Loss: 0.7581502795219421\n",
      "Validation: Epoch [10], Batch [496/938], Loss: 0.5497698783874512\n",
      "Validation: Epoch [10], Batch [497/938], Loss: 0.4986500144004822\n",
      "Validation: Epoch [10], Batch [498/938], Loss: 0.7414480447769165\n",
      "Validation: Epoch [10], Batch [499/938], Loss: 0.6223383545875549\n",
      "Validation: Epoch [10], Batch [500/938], Loss: 0.5140169858932495\n",
      "Validation: Epoch [10], Batch [501/938], Loss: 0.43252694606781006\n",
      "Validation: Epoch [10], Batch [502/938], Loss: 0.5285722017288208\n",
      "Validation: Epoch [10], Batch [503/938], Loss: 0.6255919933319092\n",
      "Validation: Epoch [10], Batch [504/938], Loss: 0.5079411268234253\n",
      "Validation: Epoch [10], Batch [505/938], Loss: 0.4946402311325073\n",
      "Validation: Epoch [10], Batch [506/938], Loss: 0.39161550998687744\n",
      "Validation: Epoch [10], Batch [507/938], Loss: 0.5114399194717407\n",
      "Validation: Epoch [10], Batch [508/938], Loss: 0.6247984170913696\n",
      "Validation: Epoch [10], Batch [509/938], Loss: 0.5323333740234375\n",
      "Validation: Epoch [10], Batch [510/938], Loss: 0.47091245651245117\n",
      "Validation: Epoch [10], Batch [511/938], Loss: 0.500320315361023\n",
      "Validation: Epoch [10], Batch [512/938], Loss: 0.433236300945282\n",
      "Validation: Epoch [10], Batch [513/938], Loss: 0.8792721033096313\n",
      "Validation: Epoch [10], Batch [514/938], Loss: 0.5372587442398071\n",
      "Validation: Epoch [10], Batch [515/938], Loss: 0.5676184892654419\n",
      "Validation: Epoch [10], Batch [516/938], Loss: 0.5626856088638306\n",
      "Validation: Epoch [10], Batch [517/938], Loss: 0.4540405869483948\n",
      "Validation: Epoch [10], Batch [518/938], Loss: 0.4588531255722046\n",
      "Validation: Epoch [10], Batch [519/938], Loss: 0.4698273837566376\n",
      "Validation: Epoch [10], Batch [520/938], Loss: 0.5588365197181702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [521/938], Loss: 0.5158973932266235\n",
      "Validation: Epoch [10], Batch [522/938], Loss: 0.512911319732666\n",
      "Validation: Epoch [10], Batch [523/938], Loss: 0.48376238346099854\n",
      "Validation: Epoch [10], Batch [524/938], Loss: 0.38682761788368225\n",
      "Validation: Epoch [10], Batch [525/938], Loss: 0.8011013269424438\n",
      "Validation: Epoch [10], Batch [526/938], Loss: 0.5341784954071045\n",
      "Validation: Epoch [10], Batch [527/938], Loss: 0.4005914330482483\n",
      "Validation: Epoch [10], Batch [528/938], Loss: 0.4830341339111328\n",
      "Validation: Epoch [10], Batch [529/938], Loss: 0.5391747951507568\n",
      "Validation: Epoch [10], Batch [530/938], Loss: 0.661250114440918\n",
      "Validation: Epoch [10], Batch [531/938], Loss: 0.5793796181678772\n",
      "Validation: Epoch [10], Batch [532/938], Loss: 0.6803375482559204\n",
      "Validation: Epoch [10], Batch [533/938], Loss: 0.5070832967758179\n",
      "Validation: Epoch [10], Batch [534/938], Loss: 0.5236279964447021\n",
      "Validation: Epoch [10], Batch [535/938], Loss: 0.4520648717880249\n",
      "Validation: Epoch [10], Batch [536/938], Loss: 0.844568133354187\n",
      "Validation: Epoch [10], Batch [537/938], Loss: 0.4993063509464264\n",
      "Validation: Epoch [10], Batch [538/938], Loss: 0.5979996919631958\n",
      "Validation: Epoch [10], Batch [539/938], Loss: 0.5075744390487671\n",
      "Validation: Epoch [10], Batch [540/938], Loss: 0.6546484231948853\n",
      "Validation: Epoch [10], Batch [541/938], Loss: 0.6245009899139404\n",
      "Validation: Epoch [10], Batch [542/938], Loss: 0.5986950993537903\n",
      "Validation: Epoch [10], Batch [543/938], Loss: 0.5387487411499023\n",
      "Validation: Epoch [10], Batch [544/938], Loss: 0.4009510278701782\n",
      "Validation: Epoch [10], Batch [545/938], Loss: 0.5682817101478577\n",
      "Validation: Epoch [10], Batch [546/938], Loss: 0.7156449556350708\n",
      "Validation: Epoch [10], Batch [547/938], Loss: 0.6143686771392822\n",
      "Validation: Epoch [10], Batch [548/938], Loss: 0.5502017140388489\n",
      "Validation: Epoch [10], Batch [549/938], Loss: 0.6474847197532654\n",
      "Validation: Epoch [10], Batch [550/938], Loss: 0.5357858538627625\n",
      "Validation: Epoch [10], Batch [551/938], Loss: 0.6189369559288025\n",
      "Validation: Epoch [10], Batch [552/938], Loss: 0.40149134397506714\n",
      "Validation: Epoch [10], Batch [553/938], Loss: 0.4760763347148895\n",
      "Validation: Epoch [10], Batch [554/938], Loss: 0.5677763223648071\n",
      "Validation: Epoch [10], Batch [555/938], Loss: 0.4945946931838989\n",
      "Validation: Epoch [10], Batch [556/938], Loss: 0.46001046895980835\n",
      "Validation: Epoch [10], Batch [557/938], Loss: 0.5053806304931641\n",
      "Validation: Epoch [10], Batch [558/938], Loss: 0.4386140704154968\n",
      "Validation: Epoch [10], Batch [559/938], Loss: 0.5614011883735657\n",
      "Validation: Epoch [10], Batch [560/938], Loss: 0.43201157450675964\n",
      "Validation: Epoch [10], Batch [561/938], Loss: 0.48456668853759766\n",
      "Validation: Epoch [10], Batch [562/938], Loss: 0.5075849890708923\n",
      "Validation: Epoch [10], Batch [563/938], Loss: 0.6352053880691528\n",
      "Validation: Epoch [10], Batch [564/938], Loss: 0.5062025785446167\n",
      "Validation: Epoch [10], Batch [565/938], Loss: 0.4679011106491089\n",
      "Validation: Epoch [10], Batch [566/938], Loss: 0.49846771359443665\n",
      "Validation: Epoch [10], Batch [567/938], Loss: 0.43558114767074585\n",
      "Validation: Epoch [10], Batch [568/938], Loss: 0.4688046872615814\n",
      "Validation: Epoch [10], Batch [569/938], Loss: 0.5455139875411987\n",
      "Validation: Epoch [10], Batch [570/938], Loss: 0.5640461444854736\n",
      "Validation: Epoch [10], Batch [571/938], Loss: 0.6426836252212524\n",
      "Validation: Epoch [10], Batch [572/938], Loss: 0.5582991242408752\n",
      "Validation: Epoch [10], Batch [573/938], Loss: 0.4663224220275879\n",
      "Validation: Epoch [10], Batch [574/938], Loss: 0.5571063160896301\n",
      "Validation: Epoch [10], Batch [575/938], Loss: 0.47484228014945984\n",
      "Validation: Epoch [10], Batch [576/938], Loss: 0.4653875231742859\n",
      "Validation: Epoch [10], Batch [577/938], Loss: 0.5868998765945435\n",
      "Validation: Epoch [10], Batch [578/938], Loss: 0.5383776426315308\n",
      "Validation: Epoch [10], Batch [579/938], Loss: 0.4433490037918091\n",
      "Validation: Epoch [10], Batch [580/938], Loss: 0.4533903896808624\n",
      "Validation: Epoch [10], Batch [581/938], Loss: 0.3882133960723877\n",
      "Validation: Epoch [10], Batch [582/938], Loss: 0.35291600227355957\n",
      "Validation: Epoch [10], Batch [583/938], Loss: 0.3909434378147125\n",
      "Validation: Epoch [10], Batch [584/938], Loss: 0.6252744197845459\n",
      "Validation: Epoch [10], Batch [585/938], Loss: 0.5052086710929871\n",
      "Validation: Epoch [10], Batch [586/938], Loss: 0.44003719091415405\n",
      "Validation: Epoch [10], Batch [587/938], Loss: 0.5469100475311279\n",
      "Validation: Epoch [10], Batch [588/938], Loss: 0.49488869309425354\n",
      "Validation: Epoch [10], Batch [589/938], Loss: 0.5494072437286377\n",
      "Validation: Epoch [10], Batch [590/938], Loss: 0.5346462726593018\n",
      "Validation: Epoch [10], Batch [591/938], Loss: 0.4807080030441284\n",
      "Validation: Epoch [10], Batch [592/938], Loss: 0.5760266780853271\n",
      "Validation: Epoch [10], Batch [593/938], Loss: 0.5727265477180481\n",
      "Validation: Epoch [10], Batch [594/938], Loss: 0.5261867642402649\n",
      "Validation: Epoch [10], Batch [595/938], Loss: 0.4252500534057617\n",
      "Validation: Epoch [10], Batch [596/938], Loss: 0.3499716520309448\n",
      "Validation: Epoch [10], Batch [597/938], Loss: 0.29660764336586\n",
      "Validation: Epoch [10], Batch [598/938], Loss: 0.5384538173675537\n",
      "Validation: Epoch [10], Batch [599/938], Loss: 0.5459545254707336\n",
      "Validation: Epoch [10], Batch [600/938], Loss: 0.5173033475875854\n",
      "Validation: Epoch [10], Batch [601/938], Loss: 0.4949869215488434\n",
      "Validation: Epoch [10], Batch [602/938], Loss: 0.5568495988845825\n",
      "Validation: Epoch [10], Batch [603/938], Loss: 0.5031028985977173\n",
      "Validation: Epoch [10], Batch [604/938], Loss: 0.6722536087036133\n",
      "Validation: Epoch [10], Batch [605/938], Loss: 0.3225466310977936\n",
      "Validation: Epoch [10], Batch [606/938], Loss: 0.4658094644546509\n",
      "Validation: Epoch [10], Batch [607/938], Loss: 0.3500187397003174\n",
      "Validation: Epoch [10], Batch [608/938], Loss: 0.45914512872695923\n",
      "Validation: Epoch [10], Batch [609/938], Loss: 0.36775797605514526\n",
      "Validation: Epoch [10], Batch [610/938], Loss: 0.5236157178878784\n",
      "Validation: Epoch [10], Batch [611/938], Loss: 0.40937674045562744\n",
      "Validation: Epoch [10], Batch [612/938], Loss: 0.6989701986312866\n",
      "Validation: Epoch [10], Batch [613/938], Loss: 0.3842686414718628\n",
      "Validation: Epoch [10], Batch [614/938], Loss: 0.638312578201294\n",
      "Validation: Epoch [10], Batch [615/938], Loss: 0.4529360234737396\n",
      "Validation: Epoch [10], Batch [616/938], Loss: 0.3133741021156311\n",
      "Validation: Epoch [10], Batch [617/938], Loss: 0.6132104992866516\n",
      "Validation: Epoch [10], Batch [618/938], Loss: 0.413690984249115\n",
      "Validation: Epoch [10], Batch [619/938], Loss: 0.4922368824481964\n",
      "Validation: Epoch [10], Batch [620/938], Loss: 0.5834069848060608\n",
      "Validation: Epoch [10], Batch [621/938], Loss: 0.4258407652378082\n",
      "Validation: Epoch [10], Batch [622/938], Loss: 0.521568775177002\n",
      "Validation: Epoch [10], Batch [623/938], Loss: 0.5800133943557739\n",
      "Validation: Epoch [10], Batch [624/938], Loss: 0.5112787485122681\n",
      "Validation: Epoch [10], Batch [625/938], Loss: 0.5148782730102539\n",
      "Validation: Epoch [10], Batch [626/938], Loss: 0.5797641277313232\n",
      "Validation: Epoch [10], Batch [627/938], Loss: 0.5692335963249207\n",
      "Validation: Epoch [10], Batch [628/938], Loss: 0.5861360430717468\n",
      "Validation: Epoch [10], Batch [629/938], Loss: 0.3961012661457062\n",
      "Validation: Epoch [10], Batch [630/938], Loss: 0.5114206075668335\n",
      "Validation: Epoch [10], Batch [631/938], Loss: 0.5241041779518127\n",
      "Validation: Epoch [10], Batch [632/938], Loss: 0.4833662211894989\n",
      "Validation: Epoch [10], Batch [633/938], Loss: 0.4641495943069458\n",
      "Validation: Epoch [10], Batch [634/938], Loss: 0.4721943140029907\n",
      "Validation: Epoch [10], Batch [635/938], Loss: 0.43927350640296936\n",
      "Validation: Epoch [10], Batch [636/938], Loss: 0.5136524438858032\n",
      "Validation: Epoch [10], Batch [637/938], Loss: 0.575484037399292\n",
      "Validation: Epoch [10], Batch [638/938], Loss: 0.5231257081031799\n",
      "Validation: Epoch [10], Batch [639/938], Loss: 0.46015340089797974\n",
      "Validation: Epoch [10], Batch [640/938], Loss: 0.6945716142654419\n",
      "Validation: Epoch [10], Batch [641/938], Loss: 0.44834837317466736\n",
      "Validation: Epoch [10], Batch [642/938], Loss: 0.5557223558425903\n",
      "Validation: Epoch [10], Batch [643/938], Loss: 0.6047185659408569\n",
      "Validation: Epoch [10], Batch [644/938], Loss: 0.44420701265335083\n",
      "Validation: Epoch [10], Batch [645/938], Loss: 0.6011913418769836\n",
      "Validation: Epoch [10], Batch [646/938], Loss: 0.4042428731918335\n",
      "Validation: Epoch [10], Batch [647/938], Loss: 0.37942206859588623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [648/938], Loss: 0.6181104183197021\n",
      "Validation: Epoch [10], Batch [649/938], Loss: 0.49934738874435425\n",
      "Validation: Epoch [10], Batch [650/938], Loss: 0.374788761138916\n",
      "Validation: Epoch [10], Batch [651/938], Loss: 0.6425105333328247\n",
      "Validation: Epoch [10], Batch [652/938], Loss: 0.3923899233341217\n",
      "Validation: Epoch [10], Batch [653/938], Loss: 0.28899073600769043\n",
      "Validation: Epoch [10], Batch [654/938], Loss: 0.6236616373062134\n",
      "Validation: Epoch [10], Batch [655/938], Loss: 0.4762112498283386\n",
      "Validation: Epoch [10], Batch [656/938], Loss: 0.4225589334964752\n",
      "Validation: Epoch [10], Batch [657/938], Loss: 0.46034902334213257\n",
      "Validation: Epoch [10], Batch [658/938], Loss: 0.5039825439453125\n",
      "Validation: Epoch [10], Batch [659/938], Loss: 0.5172520279884338\n",
      "Validation: Epoch [10], Batch [660/938], Loss: 0.5296391248703003\n",
      "Validation: Epoch [10], Batch [661/938], Loss: 0.5639863014221191\n",
      "Validation: Epoch [10], Batch [662/938], Loss: 0.6548556089401245\n",
      "Validation: Epoch [10], Batch [663/938], Loss: 0.6454101800918579\n",
      "Validation: Epoch [10], Batch [664/938], Loss: 0.4311521649360657\n",
      "Validation: Epoch [10], Batch [665/938], Loss: 0.36425602436065674\n",
      "Validation: Epoch [10], Batch [666/938], Loss: 0.5928828716278076\n",
      "Validation: Epoch [10], Batch [667/938], Loss: 0.538083553314209\n",
      "Validation: Epoch [10], Batch [668/938], Loss: 0.5962072610855103\n",
      "Validation: Epoch [10], Batch [669/938], Loss: 0.4282546043395996\n",
      "Validation: Epoch [10], Batch [670/938], Loss: 0.48614004254341125\n",
      "Validation: Epoch [10], Batch [671/938], Loss: 0.7196611166000366\n",
      "Validation: Epoch [10], Batch [672/938], Loss: 0.8034415245056152\n",
      "Validation: Epoch [10], Batch [673/938], Loss: 0.4681432545185089\n",
      "Validation: Epoch [10], Batch [674/938], Loss: 0.5919381976127625\n",
      "Validation: Epoch [10], Batch [675/938], Loss: 0.3215672969818115\n",
      "Validation: Epoch [10], Batch [676/938], Loss: 0.5216418504714966\n",
      "Validation: Epoch [10], Batch [677/938], Loss: 0.5313159227371216\n",
      "Validation: Epoch [10], Batch [678/938], Loss: 0.44603431224823\n",
      "Validation: Epoch [10], Batch [679/938], Loss: 0.4719984233379364\n",
      "Validation: Epoch [10], Batch [680/938], Loss: 0.5964711904525757\n",
      "Validation: Epoch [10], Batch [681/938], Loss: 0.6340147256851196\n",
      "Validation: Epoch [10], Batch [682/938], Loss: 0.5413069128990173\n",
      "Validation: Epoch [10], Batch [683/938], Loss: 0.7801217436790466\n",
      "Validation: Epoch [10], Batch [684/938], Loss: 0.5410045385360718\n",
      "Validation: Epoch [10], Batch [685/938], Loss: 0.5353490114212036\n",
      "Validation: Epoch [10], Batch [686/938], Loss: 0.4524053931236267\n",
      "Validation: Epoch [10], Batch [687/938], Loss: 0.47369444370269775\n",
      "Validation: Epoch [10], Batch [688/938], Loss: 0.5241824388504028\n",
      "Validation: Epoch [10], Batch [689/938], Loss: 0.5381227731704712\n",
      "Validation: Epoch [10], Batch [690/938], Loss: 0.4942527711391449\n",
      "Validation: Epoch [10], Batch [691/938], Loss: 0.5270510315895081\n",
      "Validation: Epoch [10], Batch [692/938], Loss: 0.5742586255073547\n",
      "Validation: Epoch [10], Batch [693/938], Loss: 0.4183545708656311\n",
      "Validation: Epoch [10], Batch [694/938], Loss: 0.717987060546875\n",
      "Validation: Epoch [10], Batch [695/938], Loss: 0.4697214365005493\n",
      "Validation: Epoch [10], Batch [696/938], Loss: 0.5492536425590515\n",
      "Validation: Epoch [10], Batch [697/938], Loss: 0.5355207920074463\n",
      "Validation: Epoch [10], Batch [698/938], Loss: 0.4227610230445862\n",
      "Validation: Epoch [10], Batch [699/938], Loss: 0.4265325665473938\n",
      "Validation: Epoch [10], Batch [700/938], Loss: 0.6899979114532471\n",
      "Validation: Epoch [10], Batch [701/938], Loss: 0.563351571559906\n",
      "Validation: Epoch [10], Batch [702/938], Loss: 0.5516536235809326\n",
      "Validation: Epoch [10], Batch [703/938], Loss: 0.31450849771499634\n",
      "Validation: Epoch [10], Batch [704/938], Loss: 0.39330530166625977\n",
      "Validation: Epoch [10], Batch [705/938], Loss: 0.6054196953773499\n",
      "Validation: Epoch [10], Batch [706/938], Loss: 0.674081563949585\n",
      "Validation: Epoch [10], Batch [707/938], Loss: 0.5123649835586548\n",
      "Validation: Epoch [10], Batch [708/938], Loss: 0.5863311290740967\n",
      "Validation: Epoch [10], Batch [709/938], Loss: 0.40946877002716064\n",
      "Validation: Epoch [10], Batch [710/938], Loss: 0.31352663040161133\n",
      "Validation: Epoch [10], Batch [711/938], Loss: 0.597602367401123\n",
      "Validation: Epoch [10], Batch [712/938], Loss: 0.528126060962677\n",
      "Validation: Epoch [10], Batch [713/938], Loss: 0.3412805199623108\n",
      "Validation: Epoch [10], Batch [714/938], Loss: 0.3277769088745117\n",
      "Validation: Epoch [10], Batch [715/938], Loss: 0.5184797048568726\n",
      "Validation: Epoch [10], Batch [716/938], Loss: 0.5998077392578125\n",
      "Validation: Epoch [10], Batch [717/938], Loss: 0.4783635139465332\n",
      "Validation: Epoch [10], Batch [718/938], Loss: 0.6977323889732361\n",
      "Validation: Epoch [10], Batch [719/938], Loss: 0.529133141040802\n",
      "Validation: Epoch [10], Batch [720/938], Loss: 0.5792720913887024\n",
      "Validation: Epoch [10], Batch [721/938], Loss: 0.6516320109367371\n",
      "Validation: Epoch [10], Batch [722/938], Loss: 0.6666412353515625\n",
      "Validation: Epoch [10], Batch [723/938], Loss: 0.578500509262085\n",
      "Validation: Epoch [10], Batch [724/938], Loss: 0.49076929688453674\n",
      "Validation: Epoch [10], Batch [725/938], Loss: 0.5681371688842773\n",
      "Validation: Epoch [10], Batch [726/938], Loss: 0.3979482054710388\n",
      "Validation: Epoch [10], Batch [727/938], Loss: 0.6273135542869568\n",
      "Validation: Epoch [10], Batch [728/938], Loss: 0.36531877517700195\n",
      "Validation: Epoch [10], Batch [729/938], Loss: 0.46494054794311523\n",
      "Validation: Epoch [10], Batch [730/938], Loss: 0.5189475417137146\n",
      "Validation: Epoch [10], Batch [731/938], Loss: 0.8271989226341248\n",
      "Validation: Epoch [10], Batch [732/938], Loss: 0.6130480766296387\n",
      "Validation: Epoch [10], Batch [733/938], Loss: 0.3448978364467621\n",
      "Validation: Epoch [10], Batch [734/938], Loss: 0.40593811869621277\n",
      "Validation: Epoch [10], Batch [735/938], Loss: 0.5325210094451904\n",
      "Validation: Epoch [10], Batch [736/938], Loss: 0.5419048070907593\n",
      "Validation: Epoch [10], Batch [737/938], Loss: 0.44513076543807983\n",
      "Validation: Epoch [10], Batch [738/938], Loss: 0.4819185137748718\n",
      "Validation: Epoch [10], Batch [739/938], Loss: 0.5581571459770203\n",
      "Validation: Epoch [10], Batch [740/938], Loss: 0.5565891265869141\n",
      "Validation: Epoch [10], Batch [741/938], Loss: 0.5843632221221924\n",
      "Validation: Epoch [10], Batch [742/938], Loss: 0.6338504552841187\n",
      "Validation: Epoch [10], Batch [743/938], Loss: 0.429537296295166\n",
      "Validation: Epoch [10], Batch [744/938], Loss: 0.44822555780410767\n",
      "Validation: Epoch [10], Batch [745/938], Loss: 0.5219009518623352\n",
      "Validation: Epoch [10], Batch [746/938], Loss: 0.985052227973938\n",
      "Validation: Epoch [10], Batch [747/938], Loss: 0.3238239288330078\n",
      "Validation: Epoch [10], Batch [748/938], Loss: 0.702771008014679\n",
      "Validation: Epoch [10], Batch [749/938], Loss: 0.46992766857147217\n",
      "Validation: Epoch [10], Batch [750/938], Loss: 0.46753352880477905\n",
      "Validation: Epoch [10], Batch [751/938], Loss: 0.631454586982727\n",
      "Validation: Epoch [10], Batch [752/938], Loss: 0.6256929039955139\n",
      "Validation: Epoch [10], Batch [753/938], Loss: 0.3521026074886322\n",
      "Validation: Epoch [10], Batch [754/938], Loss: 0.41767215728759766\n",
      "Validation: Epoch [10], Batch [755/938], Loss: 0.3580491542816162\n",
      "Validation: Epoch [10], Batch [756/938], Loss: 0.5991301536560059\n",
      "Validation: Epoch [10], Batch [757/938], Loss: 0.5116121768951416\n",
      "Validation: Epoch [10], Batch [758/938], Loss: 0.517704963684082\n",
      "Validation: Epoch [10], Batch [759/938], Loss: 0.45670008659362793\n",
      "Validation: Epoch [10], Batch [760/938], Loss: 0.4473463296890259\n",
      "Validation: Epoch [10], Batch [761/938], Loss: 0.5683766007423401\n",
      "Validation: Epoch [10], Batch [762/938], Loss: 0.46874985098838806\n",
      "Validation: Epoch [10], Batch [763/938], Loss: 0.5614190101623535\n",
      "Validation: Epoch [10], Batch [764/938], Loss: 0.3132815957069397\n",
      "Validation: Epoch [10], Batch [765/938], Loss: 0.5994638800621033\n",
      "Validation: Epoch [10], Batch [766/938], Loss: 0.4986658990383148\n",
      "Validation: Epoch [10], Batch [767/938], Loss: 0.4613299071788788\n",
      "Validation: Epoch [10], Batch [768/938], Loss: 0.5860124826431274\n",
      "Validation: Epoch [10], Batch [769/938], Loss: 0.474201500415802\n",
      "Validation: Epoch [10], Batch [770/938], Loss: 0.3922232985496521\n",
      "Validation: Epoch [10], Batch [771/938], Loss: 0.6361463665962219\n",
      "Validation: Epoch [10], Batch [772/938], Loss: 0.734134316444397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [773/938], Loss: 0.4169573187828064\n",
      "Validation: Epoch [10], Batch [774/938], Loss: 0.33927926421165466\n",
      "Validation: Epoch [10], Batch [775/938], Loss: 0.36546483635902405\n",
      "Validation: Epoch [10], Batch [776/938], Loss: 0.47865766286849976\n",
      "Validation: Epoch [10], Batch [777/938], Loss: 0.5359534025192261\n",
      "Validation: Epoch [10], Batch [778/938], Loss: 0.6183736324310303\n",
      "Validation: Epoch [10], Batch [779/938], Loss: 0.3907226622104645\n",
      "Validation: Epoch [10], Batch [780/938], Loss: 0.6896255016326904\n",
      "Validation: Epoch [10], Batch [781/938], Loss: 0.4904569387435913\n",
      "Validation: Epoch [10], Batch [782/938], Loss: 0.5943723917007446\n",
      "Validation: Epoch [10], Batch [783/938], Loss: 0.45290452241897583\n",
      "Validation: Epoch [10], Batch [784/938], Loss: 0.5262888669967651\n",
      "Validation: Epoch [10], Batch [785/938], Loss: 0.4202996790409088\n",
      "Validation: Epoch [10], Batch [786/938], Loss: 0.5565569400787354\n",
      "Validation: Epoch [10], Batch [787/938], Loss: 0.4224094748497009\n",
      "Validation: Epoch [10], Batch [788/938], Loss: 0.363461434841156\n",
      "Validation: Epoch [10], Batch [789/938], Loss: 0.3349190354347229\n",
      "Validation: Epoch [10], Batch [790/938], Loss: 0.6071112155914307\n",
      "Validation: Epoch [10], Batch [791/938], Loss: 0.6532150506973267\n",
      "Validation: Epoch [10], Batch [792/938], Loss: 0.4824708104133606\n",
      "Validation: Epoch [10], Batch [793/938], Loss: 0.5358784198760986\n",
      "Validation: Epoch [10], Batch [794/938], Loss: 0.6637653112411499\n",
      "Validation: Epoch [10], Batch [795/938], Loss: 0.5895562171936035\n",
      "Validation: Epoch [10], Batch [796/938], Loss: 0.5928660035133362\n",
      "Validation: Epoch [10], Batch [797/938], Loss: 0.5217934846878052\n",
      "Validation: Epoch [10], Batch [798/938], Loss: 0.6862442493438721\n",
      "Validation: Epoch [10], Batch [799/938], Loss: 0.3407260477542877\n",
      "Validation: Epoch [10], Batch [800/938], Loss: 0.6993609666824341\n",
      "Validation: Epoch [10], Batch [801/938], Loss: 0.7002211809158325\n",
      "Validation: Epoch [10], Batch [802/938], Loss: 0.4832705855369568\n",
      "Validation: Epoch [10], Batch [803/938], Loss: 0.595586359500885\n",
      "Validation: Epoch [10], Batch [804/938], Loss: 0.5689727067947388\n",
      "Validation: Epoch [10], Batch [805/938], Loss: 0.38314980268478394\n",
      "Validation: Epoch [10], Batch [806/938], Loss: 0.4967256784439087\n",
      "Validation: Epoch [10], Batch [807/938], Loss: 0.45014306902885437\n",
      "Validation: Epoch [10], Batch [808/938], Loss: 0.40214869379997253\n",
      "Validation: Epoch [10], Batch [809/938], Loss: 0.4203646779060364\n",
      "Validation: Epoch [10], Batch [810/938], Loss: 0.41838106513023376\n",
      "Validation: Epoch [10], Batch [811/938], Loss: 0.3405362665653229\n",
      "Validation: Epoch [10], Batch [812/938], Loss: 0.5407537221908569\n",
      "Validation: Epoch [10], Batch [813/938], Loss: 0.5054711699485779\n",
      "Validation: Epoch [10], Batch [814/938], Loss: 0.4460722804069519\n",
      "Validation: Epoch [10], Batch [815/938], Loss: 0.39248785376548767\n",
      "Validation: Epoch [10], Batch [816/938], Loss: 0.5349087715148926\n",
      "Validation: Epoch [10], Batch [817/938], Loss: 0.32348906993865967\n",
      "Validation: Epoch [10], Batch [818/938], Loss: 0.5450267791748047\n",
      "Validation: Epoch [10], Batch [819/938], Loss: 0.38009077310562134\n",
      "Validation: Epoch [10], Batch [820/938], Loss: 0.4650408625602722\n",
      "Validation: Epoch [10], Batch [821/938], Loss: 0.8260095119476318\n",
      "Validation: Epoch [10], Batch [822/938], Loss: 0.4362735152244568\n",
      "Validation: Epoch [10], Batch [823/938], Loss: 0.40583765506744385\n",
      "Validation: Epoch [10], Batch [824/938], Loss: 0.4763062596321106\n",
      "Validation: Epoch [10], Batch [825/938], Loss: 0.4620901942253113\n",
      "Validation: Epoch [10], Batch [826/938], Loss: 0.8687108755111694\n",
      "Validation: Epoch [10], Batch [827/938], Loss: 0.69025057554245\n",
      "Validation: Epoch [10], Batch [828/938], Loss: 0.483734130859375\n",
      "Validation: Epoch [10], Batch [829/938], Loss: 0.5287179350852966\n",
      "Validation: Epoch [10], Batch [830/938], Loss: 0.5091959238052368\n",
      "Validation: Epoch [10], Batch [831/938], Loss: 0.43334001302719116\n",
      "Validation: Epoch [10], Batch [832/938], Loss: 0.3322034478187561\n",
      "Validation: Epoch [10], Batch [833/938], Loss: 0.5207685232162476\n",
      "Validation: Epoch [10], Batch [834/938], Loss: 0.6194992065429688\n",
      "Validation: Epoch [10], Batch [835/938], Loss: 0.5150107145309448\n",
      "Validation: Epoch [10], Batch [836/938], Loss: 0.6610814929008484\n",
      "Validation: Epoch [10], Batch [837/938], Loss: 0.2807036340236664\n",
      "Validation: Epoch [10], Batch [838/938], Loss: 0.5476096272468567\n",
      "Validation: Epoch [10], Batch [839/938], Loss: 0.33921384811401367\n",
      "Validation: Epoch [10], Batch [840/938], Loss: 0.7633916139602661\n",
      "Validation: Epoch [10], Batch [841/938], Loss: 0.4545140862464905\n",
      "Validation: Epoch [10], Batch [842/938], Loss: 0.44979771971702576\n",
      "Validation: Epoch [10], Batch [843/938], Loss: 0.45871102809906006\n",
      "Validation: Epoch [10], Batch [844/938], Loss: 0.4150310158729553\n",
      "Validation: Epoch [10], Batch [845/938], Loss: 0.44801831245422363\n",
      "Validation: Epoch [10], Batch [846/938], Loss: 0.5057070255279541\n",
      "Validation: Epoch [10], Batch [847/938], Loss: 0.6081390976905823\n",
      "Validation: Epoch [10], Batch [848/938], Loss: 0.4360937476158142\n",
      "Validation: Epoch [10], Batch [849/938], Loss: 0.810469388961792\n",
      "Validation: Epoch [10], Batch [850/938], Loss: 0.4348911643028259\n",
      "Validation: Epoch [10], Batch [851/938], Loss: 0.5472604036331177\n",
      "Validation: Epoch [10], Batch [852/938], Loss: 0.560803234577179\n",
      "Validation: Epoch [10], Batch [853/938], Loss: 0.42156121134757996\n",
      "Validation: Epoch [10], Batch [854/938], Loss: 0.5404500961303711\n",
      "Validation: Epoch [10], Batch [855/938], Loss: 0.45457494258880615\n",
      "Validation: Epoch [10], Batch [856/938], Loss: 0.39189445972442627\n",
      "Validation: Epoch [10], Batch [857/938], Loss: 0.3581828474998474\n",
      "Validation: Epoch [10], Batch [858/938], Loss: 0.4804147779941559\n",
      "Validation: Epoch [10], Batch [859/938], Loss: 0.26143506169319153\n",
      "Validation: Epoch [10], Batch [860/938], Loss: 0.6169031858444214\n",
      "Validation: Epoch [10], Batch [861/938], Loss: 0.4766291379928589\n",
      "Validation: Epoch [10], Batch [862/938], Loss: 0.4355987310409546\n",
      "Validation: Epoch [10], Batch [863/938], Loss: 0.5318747162818909\n",
      "Validation: Epoch [10], Batch [864/938], Loss: 0.4701859951019287\n",
      "Validation: Epoch [10], Batch [865/938], Loss: 0.45176082849502563\n",
      "Validation: Epoch [10], Batch [866/938], Loss: 0.4543081521987915\n",
      "Validation: Epoch [10], Batch [867/938], Loss: 0.44883251190185547\n",
      "Validation: Epoch [10], Batch [868/938], Loss: 0.6422477960586548\n",
      "Validation: Epoch [10], Batch [869/938], Loss: 0.43047693371772766\n",
      "Validation: Epoch [10], Batch [870/938], Loss: 0.28368642926216125\n",
      "Validation: Epoch [10], Batch [871/938], Loss: 0.7120223045349121\n",
      "Validation: Epoch [10], Batch [872/938], Loss: 0.5597546100616455\n",
      "Validation: Epoch [10], Batch [873/938], Loss: 0.46591469645500183\n",
      "Validation: Epoch [10], Batch [874/938], Loss: 0.5308171510696411\n",
      "Validation: Epoch [10], Batch [875/938], Loss: 0.3932971954345703\n",
      "Validation: Epoch [10], Batch [876/938], Loss: 0.42059391736984253\n",
      "Validation: Epoch [10], Batch [877/938], Loss: 0.4427175521850586\n",
      "Validation: Epoch [10], Batch [878/938], Loss: 0.5116352438926697\n",
      "Validation: Epoch [10], Batch [879/938], Loss: 0.564793586730957\n",
      "Validation: Epoch [10], Batch [880/938], Loss: 0.7636709213256836\n",
      "Validation: Epoch [10], Batch [881/938], Loss: 0.9115239381790161\n",
      "Validation: Epoch [10], Batch [882/938], Loss: 0.7034311294555664\n",
      "Validation: Epoch [10], Batch [883/938], Loss: 0.5110338926315308\n",
      "Validation: Epoch [10], Batch [884/938], Loss: 0.6543515920639038\n",
      "Validation: Epoch [10], Batch [885/938], Loss: 0.3835476338863373\n",
      "Validation: Epoch [10], Batch [886/938], Loss: 0.3856470584869385\n",
      "Validation: Epoch [10], Batch [887/938], Loss: 0.5174592733383179\n",
      "Validation: Epoch [10], Batch [888/938], Loss: 0.5144467353820801\n",
      "Validation: Epoch [10], Batch [889/938], Loss: 0.6141809225082397\n",
      "Validation: Epoch [10], Batch [890/938], Loss: 0.6458779573440552\n",
      "Validation: Epoch [10], Batch [891/938], Loss: 0.47665131092071533\n",
      "Validation: Epoch [10], Batch [892/938], Loss: 0.5191259384155273\n",
      "Validation: Epoch [10], Batch [893/938], Loss: 0.47409000992774963\n",
      "Validation: Epoch [10], Batch [894/938], Loss: 0.520561695098877\n",
      "Validation: Epoch [10], Batch [895/938], Loss: 0.5846373438835144\n",
      "Validation: Epoch [10], Batch [896/938], Loss: 0.6165930032730103\n",
      "Validation: Epoch [10], Batch [897/938], Loss: 0.4513372480869293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [10], Batch [898/938], Loss: 0.5216268301010132\n",
      "Validation: Epoch [10], Batch [899/938], Loss: 0.6684080362319946\n",
      "Validation: Epoch [10], Batch [900/938], Loss: 0.5283398628234863\n",
      "Validation: Epoch [10], Batch [901/938], Loss: 0.45920059084892273\n",
      "Validation: Epoch [10], Batch [902/938], Loss: 0.5205784440040588\n",
      "Validation: Epoch [10], Batch [903/938], Loss: 0.5167438387870789\n",
      "Validation: Epoch [10], Batch [904/938], Loss: 0.4508190155029297\n",
      "Validation: Epoch [10], Batch [905/938], Loss: 0.5325543880462646\n",
      "Validation: Epoch [10], Batch [906/938], Loss: 0.41578757762908936\n",
      "Validation: Epoch [10], Batch [907/938], Loss: 0.47546228766441345\n",
      "Validation: Epoch [10], Batch [908/938], Loss: 0.49266961216926575\n",
      "Validation: Epoch [10], Batch [909/938], Loss: 0.4156481623649597\n",
      "Validation: Epoch [10], Batch [910/938], Loss: 0.38781917095184326\n",
      "Validation: Epoch [10], Batch [911/938], Loss: 0.27240923047065735\n",
      "Validation: Epoch [10], Batch [912/938], Loss: 0.7125131487846375\n",
      "Validation: Epoch [10], Batch [913/938], Loss: 0.5756518840789795\n",
      "Validation: Epoch [10], Batch [914/938], Loss: 0.4815440773963928\n",
      "Validation: Epoch [10], Batch [915/938], Loss: 0.5681370496749878\n",
      "Validation: Epoch [10], Batch [916/938], Loss: 0.3647705316543579\n",
      "Validation: Epoch [10], Batch [917/938], Loss: 0.6285349130630493\n",
      "Validation: Epoch [10], Batch [918/938], Loss: 0.5573404431343079\n",
      "Validation: Epoch [10], Batch [919/938], Loss: 0.48269587755203247\n",
      "Validation: Epoch [10], Batch [920/938], Loss: 0.6904693841934204\n",
      "Validation: Epoch [10], Batch [921/938], Loss: 0.4191482663154602\n",
      "Validation: Epoch [10], Batch [922/938], Loss: 0.5199350118637085\n",
      "Validation: Epoch [10], Batch [923/938], Loss: 0.5621838569641113\n",
      "Validation: Epoch [10], Batch [924/938], Loss: 0.36825817823410034\n",
      "Validation: Epoch [10], Batch [925/938], Loss: 0.5902239084243774\n",
      "Validation: Epoch [10], Batch [926/938], Loss: 0.4136028289794922\n",
      "Validation: Epoch [10], Batch [927/938], Loss: 0.627677321434021\n",
      "Validation: Epoch [10], Batch [928/938], Loss: 0.23232170939445496\n",
      "Validation: Epoch [10], Batch [929/938], Loss: 0.38707178831100464\n",
      "Validation: Epoch [10], Batch [930/938], Loss: 0.5464460253715515\n",
      "Validation: Epoch [10], Batch [931/938], Loss: 0.38712286949157715\n",
      "Validation: Epoch [10], Batch [932/938], Loss: 0.4808361530303955\n",
      "Validation: Epoch [10], Batch [933/938], Loss: 0.3850198984146118\n",
      "Validation: Epoch [10], Batch [934/938], Loss: 0.4062998294830322\n",
      "Validation: Epoch [10], Batch [935/938], Loss: 0.5448104739189148\n",
      "Validation: Epoch [10], Batch [936/938], Loss: 0.5192161798477173\n",
      "Validation: Epoch [10], Batch [937/938], Loss: 0.617851734161377\n",
      "Validation: Epoch [10], Batch [938/938], Loss: 0.47311776876449585\n",
      "Accuracy of test set: 0.8218166666666666\n",
      "Train: Epoch [11], Batch [1/938], Loss: 0.439548134803772\n",
      "Train: Epoch [11], Batch [2/938], Loss: 0.6098770499229431\n",
      "Train: Epoch [11], Batch [3/938], Loss: 0.35231900215148926\n",
      "Train: Epoch [11], Batch [4/938], Loss: 0.6283588409423828\n",
      "Train: Epoch [11], Batch [5/938], Loss: 0.41738638281822205\n",
      "Train: Epoch [11], Batch [6/938], Loss: 0.3380267322063446\n",
      "Train: Epoch [11], Batch [7/938], Loss: 1.0529670715332031\n",
      "Train: Epoch [11], Batch [8/938], Loss: 0.7098939418792725\n",
      "Train: Epoch [11], Batch [9/938], Loss: 0.3588438034057617\n",
      "Train: Epoch [11], Batch [10/938], Loss: 0.7261950969696045\n",
      "Train: Epoch [11], Batch [11/938], Loss: 0.5400396585464478\n",
      "Train: Epoch [11], Batch [12/938], Loss: 0.38632702827453613\n",
      "Train: Epoch [11], Batch [13/938], Loss: 0.4889887571334839\n",
      "Train: Epoch [11], Batch [14/938], Loss: 0.6309629678726196\n",
      "Train: Epoch [11], Batch [15/938], Loss: 0.5039545297622681\n",
      "Train: Epoch [11], Batch [16/938], Loss: 0.5428593158721924\n",
      "Train: Epoch [11], Batch [17/938], Loss: 0.45714154839515686\n",
      "Train: Epoch [11], Batch [18/938], Loss: 0.6294721364974976\n",
      "Train: Epoch [11], Batch [19/938], Loss: 0.5142149925231934\n",
      "Train: Epoch [11], Batch [20/938], Loss: 0.4537343680858612\n",
      "Train: Epoch [11], Batch [21/938], Loss: 0.4753704071044922\n",
      "Train: Epoch [11], Batch [22/938], Loss: 0.4896387457847595\n",
      "Train: Epoch [11], Batch [23/938], Loss: 0.4139661192893982\n",
      "Train: Epoch [11], Batch [24/938], Loss: 0.4175795912742615\n",
      "Train: Epoch [11], Batch [25/938], Loss: 0.5522047281265259\n",
      "Train: Epoch [11], Batch [26/938], Loss: 0.5582382082939148\n",
      "Train: Epoch [11], Batch [27/938], Loss: 0.5249757766723633\n",
      "Train: Epoch [11], Batch [28/938], Loss: 0.43002617359161377\n",
      "Train: Epoch [11], Batch [29/938], Loss: 0.5348593592643738\n",
      "Train: Epoch [11], Batch [30/938], Loss: 0.5237489938735962\n",
      "Train: Epoch [11], Batch [31/938], Loss: 0.5574284791946411\n",
      "Train: Epoch [11], Batch [32/938], Loss: 0.6103096008300781\n",
      "Train: Epoch [11], Batch [33/938], Loss: 0.4357314109802246\n",
      "Train: Epoch [11], Batch [34/938], Loss: 0.4554443359375\n",
      "Train: Epoch [11], Batch [35/938], Loss: 0.537735104560852\n",
      "Train: Epoch [11], Batch [36/938], Loss: 0.5006632804870605\n",
      "Train: Epoch [11], Batch [37/938], Loss: 0.46324867010116577\n",
      "Train: Epoch [11], Batch [38/938], Loss: 0.3835671544075012\n",
      "Train: Epoch [11], Batch [39/938], Loss: 0.5403304696083069\n",
      "Train: Epoch [11], Batch [40/938], Loss: 0.49658331274986267\n",
      "Train: Epoch [11], Batch [41/938], Loss: 0.6084780693054199\n",
      "Train: Epoch [11], Batch [42/938], Loss: 0.5683331489562988\n",
      "Train: Epoch [11], Batch [43/938], Loss: 0.4573580026626587\n",
      "Train: Epoch [11], Batch [44/938], Loss: 0.5424636602401733\n",
      "Train: Epoch [11], Batch [45/938], Loss: 0.34544000029563904\n",
      "Train: Epoch [11], Batch [46/938], Loss: 0.2649824023246765\n",
      "Train: Epoch [11], Batch [47/938], Loss: 0.4673151969909668\n",
      "Train: Epoch [11], Batch [48/938], Loss: 0.5866494178771973\n",
      "Train: Epoch [11], Batch [49/938], Loss: 0.5762829780578613\n",
      "Train: Epoch [11], Batch [50/938], Loss: 0.6322115659713745\n",
      "Train: Epoch [11], Batch [51/938], Loss: 0.46818578243255615\n",
      "Train: Epoch [11], Batch [52/938], Loss: 0.4919033348560333\n",
      "Train: Epoch [11], Batch [53/938], Loss: 0.4527505934238434\n",
      "Train: Epoch [11], Batch [54/938], Loss: 0.4570786654949188\n",
      "Train: Epoch [11], Batch [55/938], Loss: 0.6615428924560547\n",
      "Train: Epoch [11], Batch [56/938], Loss: 0.9066683053970337\n",
      "Train: Epoch [11], Batch [57/938], Loss: 0.5573257207870483\n",
      "Train: Epoch [11], Batch [58/938], Loss: 0.6306387186050415\n",
      "Train: Epoch [11], Batch [59/938], Loss: 0.5799737572669983\n",
      "Train: Epoch [11], Batch [60/938], Loss: 0.5406449437141418\n",
      "Train: Epoch [11], Batch [61/938], Loss: 0.6782865524291992\n",
      "Train: Epoch [11], Batch [62/938], Loss: 0.6010109186172485\n",
      "Train: Epoch [11], Batch [63/938], Loss: 0.38862788677215576\n",
      "Train: Epoch [11], Batch [64/938], Loss: 0.5453663468360901\n",
      "Train: Epoch [11], Batch [65/938], Loss: 0.606520414352417\n",
      "Train: Epoch [11], Batch [66/938], Loss: 0.47602832317352295\n",
      "Train: Epoch [11], Batch [67/938], Loss: 0.4073297679424286\n",
      "Train: Epoch [11], Batch [68/938], Loss: 0.6785845756530762\n",
      "Train: Epoch [11], Batch [69/938], Loss: 0.4816742539405823\n",
      "Train: Epoch [11], Batch [70/938], Loss: 0.7173553705215454\n",
      "Train: Epoch [11], Batch [71/938], Loss: 0.5545704960823059\n",
      "Train: Epoch [11], Batch [72/938], Loss: 0.437086820602417\n",
      "Train: Epoch [11], Batch [73/938], Loss: 0.6069701910018921\n",
      "Train: Epoch [11], Batch [74/938], Loss: 0.7644489407539368\n",
      "Train: Epoch [11], Batch [75/938], Loss: 0.5995736122131348\n",
      "Train: Epoch [11], Batch [76/938], Loss: 0.6929185390472412\n",
      "Train: Epoch [11], Batch [77/938], Loss: 0.41981279850006104\n",
      "Train: Epoch [11], Batch [78/938], Loss: 0.5517300367355347\n",
      "Train: Epoch [11], Batch [79/938], Loss: 0.5013317465782166\n",
      "Train: Epoch [11], Batch [80/938], Loss: 0.4333685040473938\n",
      "Train: Epoch [11], Batch [81/938], Loss: 0.45095252990722656\n",
      "Train: Epoch [11], Batch [82/938], Loss: 0.47960782051086426\n",
      "Train: Epoch [11], Batch [83/938], Loss: 0.4098629951477051\n",
      "Train: Epoch [11], Batch [84/938], Loss: 0.3766629695892334\n",
      "Train: Epoch [11], Batch [85/938], Loss: 0.4016672670841217\n",
      "Train: Epoch [11], Batch [86/938], Loss: 0.5208850502967834\n",
      "Train: Epoch [11], Batch [87/938], Loss: 0.5338735580444336\n",
      "Train: Epoch [11], Batch [88/938], Loss: 0.49106043577194214\n",
      "Train: Epoch [11], Batch [89/938], Loss: 0.5603234171867371\n",
      "Train: Epoch [11], Batch [90/938], Loss: 0.6579556465148926\n",
      "Train: Epoch [11], Batch [91/938], Loss: 0.6613474488258362\n",
      "Train: Epoch [11], Batch [92/938], Loss: 0.6277660131454468\n",
      "Train: Epoch [11], Batch [93/938], Loss: 0.5127938985824585\n",
      "Train: Epoch [11], Batch [94/938], Loss: 0.6089725494384766\n",
      "Train: Epoch [11], Batch [95/938], Loss: 0.7979810237884521\n",
      "Train: Epoch [11], Batch [96/938], Loss: 0.5282347798347473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [11], Batch [97/938], Loss: 0.4062420725822449\n",
      "Train: Epoch [11], Batch [98/938], Loss: 0.44892677664756775\n",
      "Train: Epoch [11], Batch [99/938], Loss: 0.4841679036617279\n",
      "Train: Epoch [11], Batch [100/938], Loss: 0.431591272354126\n",
      "Train: Epoch [11], Batch [101/938], Loss: 0.6200376749038696\n",
      "Train: Epoch [11], Batch [102/938], Loss: 0.5302143096923828\n",
      "Train: Epoch [11], Batch [103/938], Loss: 0.4843607544898987\n",
      "Train: Epoch [11], Batch [104/938], Loss: 0.5527352094650269\n",
      "Train: Epoch [11], Batch [105/938], Loss: 0.44817519187927246\n",
      "Train: Epoch [11], Batch [106/938], Loss: 0.48665350675582886\n",
      "Train: Epoch [11], Batch [107/938], Loss: 0.3572026789188385\n",
      "Train: Epoch [11], Batch [108/938], Loss: 0.4331931471824646\n",
      "Train: Epoch [11], Batch [109/938], Loss: 0.48469221591949463\n",
      "Train: Epoch [11], Batch [110/938], Loss: 0.4227536916732788\n",
      "Train: Epoch [11], Batch [111/938], Loss: 0.6026204824447632\n",
      "Train: Epoch [11], Batch [112/938], Loss: 0.611126720905304\n",
      "Train: Epoch [11], Batch [113/938], Loss: 0.5379047393798828\n",
      "Train: Epoch [11], Batch [114/938], Loss: 0.6015908718109131\n",
      "Train: Epoch [11], Batch [115/938], Loss: 0.5023303031921387\n",
      "Train: Epoch [11], Batch [116/938], Loss: 0.6752293109893799\n",
      "Train: Epoch [11], Batch [117/938], Loss: 0.5297964811325073\n",
      "Train: Epoch [11], Batch [118/938], Loss: 0.625760555267334\n",
      "Train: Epoch [11], Batch [119/938], Loss: 0.7024792432785034\n",
      "Train: Epoch [11], Batch [120/938], Loss: 0.5282575488090515\n",
      "Train: Epoch [11], Batch [121/938], Loss: 0.5329402685165405\n",
      "Train: Epoch [11], Batch [122/938], Loss: 0.549845278263092\n",
      "Train: Epoch [11], Batch [123/938], Loss: 0.4019198417663574\n",
      "Train: Epoch [11], Batch [124/938], Loss: 0.46654006838798523\n",
      "Train: Epoch [11], Batch [125/938], Loss: 0.4456438422203064\n",
      "Train: Epoch [11], Batch [126/938], Loss: 0.6232413053512573\n",
      "Train: Epoch [11], Batch [127/938], Loss: 0.6150803565979004\n",
      "Train: Epoch [11], Batch [128/938], Loss: 0.3765694797039032\n",
      "Train: Epoch [11], Batch [129/938], Loss: 0.5744936466217041\n",
      "Train: Epoch [11], Batch [130/938], Loss: 0.5262113213539124\n",
      "Train: Epoch [11], Batch [131/938], Loss: 0.47084516286849976\n",
      "Train: Epoch [11], Batch [132/938], Loss: 0.7464459538459778\n",
      "Train: Epoch [11], Batch [133/938], Loss: 0.44592487812042236\n",
      "Train: Epoch [11], Batch [134/938], Loss: 0.44710320234298706\n",
      "Train: Epoch [11], Batch [135/938], Loss: 0.49618276953697205\n",
      "Train: Epoch [11], Batch [136/938], Loss: 0.5537539124488831\n",
      "Train: Epoch [11], Batch [137/938], Loss: 0.41281789541244507\n",
      "Train: Epoch [11], Batch [138/938], Loss: 0.33585500717163086\n",
      "Train: Epoch [11], Batch [139/938], Loss: 0.495331346988678\n",
      "Train: Epoch [11], Batch [140/938], Loss: 0.6295652389526367\n",
      "Train: Epoch [11], Batch [141/938], Loss: 0.5482650995254517\n",
      "Train: Epoch [11], Batch [142/938], Loss: 0.5899804830551147\n",
      "Train: Epoch [11], Batch [143/938], Loss: 0.7888991236686707\n",
      "Train: Epoch [11], Batch [144/938], Loss: 0.36533433198928833\n",
      "Train: Epoch [11], Batch [145/938], Loss: 0.6138033270835876\n",
      "Train: Epoch [11], Batch [146/938], Loss: 0.5892549157142639\n",
      "Train: Epoch [11], Batch [147/938], Loss: 0.4639609456062317\n",
      "Train: Epoch [11], Batch [148/938], Loss: 0.47954243421554565\n",
      "Train: Epoch [11], Batch [149/938], Loss: 0.3935490548610687\n",
      "Train: Epoch [11], Batch [150/938], Loss: 0.5759605169296265\n",
      "Train: Epoch [11], Batch [151/938], Loss: 0.3692605495452881\n",
      "Train: Epoch [11], Batch [152/938], Loss: 0.486757755279541\n",
      "Train: Epoch [11], Batch [153/938], Loss: 0.5178140997886658\n",
      "Train: Epoch [11], Batch [154/938], Loss: 0.580337643623352\n",
      "Train: Epoch [11], Batch [155/938], Loss: 0.529281735420227\n",
      "Train: Epoch [11], Batch [156/938], Loss: 0.5804860591888428\n",
      "Train: Epoch [11], Batch [157/938], Loss: 0.47204822301864624\n",
      "Train: Epoch [11], Batch [158/938], Loss: 0.48040202260017395\n",
      "Train: Epoch [11], Batch [159/938], Loss: 0.4993670880794525\n",
      "Train: Epoch [11], Batch [160/938], Loss: 0.9818658232688904\n",
      "Train: Epoch [11], Batch [161/938], Loss: 0.4974345862865448\n",
      "Train: Epoch [11], Batch [162/938], Loss: 0.47207099199295044\n",
      "Train: Epoch [11], Batch [163/938], Loss: 0.6212129592895508\n",
      "Train: Epoch [11], Batch [164/938], Loss: 0.40087735652923584\n",
      "Train: Epoch [11], Batch [165/938], Loss: 0.5701127052307129\n",
      "Train: Epoch [11], Batch [166/938], Loss: 0.4646151065826416\n",
      "Train: Epoch [11], Batch [167/938], Loss: 0.5214558839797974\n",
      "Train: Epoch [11], Batch [168/938], Loss: 0.3116419315338135\n",
      "Train: Epoch [11], Batch [169/938], Loss: 0.5196865797042847\n",
      "Train: Epoch [11], Batch [170/938], Loss: 0.7279707789421082\n",
      "Train: Epoch [11], Batch [171/938], Loss: 0.7614990472793579\n",
      "Train: Epoch [11], Batch [172/938], Loss: 0.42936164140701294\n",
      "Train: Epoch [11], Batch [173/938], Loss: 0.650888204574585\n",
      "Train: Epoch [11], Batch [174/938], Loss: 0.5652494430541992\n",
      "Train: Epoch [11], Batch [175/938], Loss: 0.41742417216300964\n",
      "Train: Epoch [11], Batch [176/938], Loss: 0.45314592123031616\n",
      "Train: Epoch [11], Batch [177/938], Loss: 0.6050302982330322\n",
      "Train: Epoch [11], Batch [178/938], Loss: 0.5016343593597412\n",
      "Train: Epoch [11], Batch [179/938], Loss: 0.5715808272361755\n",
      "Train: Epoch [11], Batch [180/938], Loss: 0.7571832537651062\n",
      "Train: Epoch [11], Batch [181/938], Loss: 0.6296033263206482\n",
      "Train: Epoch [11], Batch [182/938], Loss: 0.5717216730117798\n",
      "Train: Epoch [11], Batch [183/938], Loss: 0.4383154511451721\n",
      "Train: Epoch [11], Batch [184/938], Loss: 0.4324410855770111\n",
      "Train: Epoch [11], Batch [185/938], Loss: 0.5092135667800903\n",
      "Train: Epoch [11], Batch [186/938], Loss: 0.4662790894508362\n",
      "Train: Epoch [11], Batch [187/938], Loss: 0.6068809032440186\n",
      "Train: Epoch [11], Batch [188/938], Loss: 0.665093183517456\n",
      "Train: Epoch [11], Batch [189/938], Loss: 0.4503163695335388\n",
      "Train: Epoch [11], Batch [190/938], Loss: 0.5374765992164612\n",
      "Train: Epoch [11], Batch [191/938], Loss: 0.4729166626930237\n",
      "Train: Epoch [11], Batch [192/938], Loss: 0.6585556268692017\n",
      "Train: Epoch [11], Batch [193/938], Loss: 0.47719335556030273\n",
      "Train: Epoch [11], Batch [194/938], Loss: 0.6257697343826294\n",
      "Train: Epoch [11], Batch [195/938], Loss: 0.40242287516593933\n",
      "Train: Epoch [11], Batch [196/938], Loss: 0.38720181584358215\n",
      "Train: Epoch [11], Batch [197/938], Loss: 0.4216216504573822\n",
      "Train: Epoch [11], Batch [198/938], Loss: 0.3385874032974243\n",
      "Train: Epoch [11], Batch [199/938], Loss: 0.7464297413825989\n",
      "Train: Epoch [11], Batch [200/938], Loss: 0.560867190361023\n",
      "Train: Epoch [11], Batch [201/938], Loss: 0.42226535081863403\n",
      "Train: Epoch [11], Batch [202/938], Loss: 0.5310522317886353\n",
      "Train: Epoch [11], Batch [203/938], Loss: 0.5027176737785339\n",
      "Train: Epoch [11], Batch [204/938], Loss: 0.4021534025669098\n",
      "Train: Epoch [11], Batch [205/938], Loss: 0.5434145927429199\n",
      "Train: Epoch [11], Batch [206/938], Loss: 0.44997984170913696\n",
      "Train: Epoch [11], Batch [207/938], Loss: 0.41512423753738403\n",
      "Train: Epoch [11], Batch [208/938], Loss: 0.6243127584457397\n",
      "Train: Epoch [11], Batch [209/938], Loss: 0.547206461429596\n",
      "Train: Epoch [11], Batch [210/938], Loss: 0.6330968141555786\n",
      "Train: Epoch [11], Batch [211/938], Loss: 0.5328537225723267\n",
      "Train: Epoch [11], Batch [212/938], Loss: 0.5847622752189636\n",
      "Train: Epoch [11], Batch [213/938], Loss: 0.598471999168396\n",
      "Train: Epoch [11], Batch [214/938], Loss: 0.5272053480148315\n",
      "Train: Epoch [11], Batch [215/938], Loss: 0.5676677227020264\n",
      "Train: Epoch [11], Batch [216/938], Loss: 0.5008442401885986\n",
      "Train: Epoch [11], Batch [217/938], Loss: 0.6237605810165405\n",
      "Train: Epoch [11], Batch [218/938], Loss: 0.5485424399375916\n",
      "Train: Epoch [11], Batch [219/938], Loss: 0.525305986404419\n",
      "Train: Epoch [11], Batch [220/938], Loss: 0.5372790694236755\n",
      "Train: Epoch [11], Batch [221/938], Loss: 0.6418135762214661\n",
      "Train: Epoch [11], Batch [222/938], Loss: 0.4490436017513275\n",
      "Train: Epoch [11], Batch [223/938], Loss: 0.603718638420105\n",
      "Train: Epoch [11], Batch [224/938], Loss: 0.7201883792877197\n",
      "Train: Epoch [11], Batch [225/938], Loss: 0.4120458960533142\n",
      "Train: Epoch [11], Batch [226/938], Loss: 0.6213394403457642\n",
      "Train: Epoch [11], Batch [227/938], Loss: 0.6426736116409302\n",
      "Train: Epoch [11], Batch [228/938], Loss: 0.39522624015808105\n",
      "Train: Epoch [11], Batch [229/938], Loss: 0.4252152442932129\n",
      "Train: Epoch [11], Batch [230/938], Loss: 0.5471590161323547\n",
      "Train: Epoch [11], Batch [231/938], Loss: 0.42731529474258423\n",
      "Train: Epoch [11], Batch [232/938], Loss: 0.422416090965271\n",
      "Train: Epoch [11], Batch [233/938], Loss: 0.5598623752593994\n",
      "Train: Epoch [11], Batch [234/938], Loss: 0.5681265592575073\n",
      "Train: Epoch [11], Batch [235/938], Loss: 0.45608294010162354\n",
      "Train: Epoch [11], Batch [236/938], Loss: 0.5055527687072754\n",
      "Train: Epoch [11], Batch [237/938], Loss: 0.7425408959388733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [11], Batch [238/938], Loss: 0.47600501775741577\n",
      "Train: Epoch [11], Batch [239/938], Loss: 0.51383376121521\n",
      "Train: Epoch [11], Batch [240/938], Loss: 0.6330015659332275\n",
      "Train: Epoch [11], Batch [241/938], Loss: 0.38090193271636963\n",
      "Train: Epoch [11], Batch [242/938], Loss: 0.4141700267791748\n",
      "Train: Epoch [11], Batch [243/938], Loss: 0.46458014845848083\n",
      "Train: Epoch [11], Batch [244/938], Loss: 0.5297368168830872\n",
      "Train: Epoch [11], Batch [245/938], Loss: 0.5637354850769043\n",
      "Train: Epoch [11], Batch [246/938], Loss: 0.33222252130508423\n",
      "Train: Epoch [11], Batch [247/938], Loss: 0.4801650643348694\n",
      "Train: Epoch [11], Batch [248/938], Loss: 0.554544985294342\n",
      "Train: Epoch [11], Batch [249/938], Loss: 0.6673355102539062\n",
      "Train: Epoch [11], Batch [250/938], Loss: 0.32439810037612915\n",
      "Train: Epoch [11], Batch [251/938], Loss: 0.5581294298171997\n",
      "Train: Epoch [11], Batch [252/938], Loss: 0.48832178115844727\n",
      "Train: Epoch [11], Batch [253/938], Loss: 0.46020424365997314\n",
      "Train: Epoch [11], Batch [254/938], Loss: 0.44086652994155884\n",
      "Train: Epoch [11], Batch [255/938], Loss: 0.4962562322616577\n",
      "Train: Epoch [11], Batch [256/938], Loss: 0.45709824562072754\n",
      "Train: Epoch [11], Batch [257/938], Loss: 0.531875491142273\n",
      "Train: Epoch [11], Batch [258/938], Loss: 0.599656343460083\n",
      "Train: Epoch [11], Batch [259/938], Loss: 0.48476460576057434\n",
      "Train: Epoch [11], Batch [260/938], Loss: 0.5887296199798584\n",
      "Train: Epoch [11], Batch [261/938], Loss: 0.4955825209617615\n",
      "Train: Epoch [11], Batch [262/938], Loss: 0.6395049095153809\n",
      "Train: Epoch [11], Batch [263/938], Loss: 0.6658074259757996\n",
      "Train: Epoch [11], Batch [264/938], Loss: 0.43959617614746094\n",
      "Train: Epoch [11], Batch [265/938], Loss: 0.6016671657562256\n",
      "Train: Epoch [11], Batch [266/938], Loss: 0.5010764002799988\n",
      "Train: Epoch [11], Batch [267/938], Loss: 0.48501020669937134\n",
      "Train: Epoch [11], Batch [268/938], Loss: 0.5325092077255249\n",
      "Train: Epoch [11], Batch [269/938], Loss: 0.7811068296432495\n",
      "Train: Epoch [11], Batch [270/938], Loss: 0.657081127166748\n",
      "Train: Epoch [11], Batch [271/938], Loss: 0.46598103642463684\n",
      "Train: Epoch [11], Batch [272/938], Loss: 0.373974084854126\n",
      "Train: Epoch [11], Batch [273/938], Loss: 0.4250258207321167\n",
      "Train: Epoch [11], Batch [274/938], Loss: 0.3810971975326538\n",
      "Train: Epoch [11], Batch [275/938], Loss: 0.4679827392101288\n",
      "Train: Epoch [11], Batch [276/938], Loss: 0.4805942177772522\n",
      "Train: Epoch [11], Batch [277/938], Loss: 0.7062770128250122\n",
      "Train: Epoch [11], Batch [278/938], Loss: 0.6067278385162354\n",
      "Train: Epoch [11], Batch [279/938], Loss: 0.4456217885017395\n",
      "Train: Epoch [11], Batch [280/938], Loss: 0.624019980430603\n",
      "Train: Epoch [11], Batch [281/938], Loss: 0.4061909019947052\n",
      "Train: Epoch [11], Batch [282/938], Loss: 0.8770217895507812\n",
      "Train: Epoch [11], Batch [283/938], Loss: 0.45602333545684814\n",
      "Train: Epoch [11], Batch [284/938], Loss: 0.654109001159668\n",
      "Train: Epoch [11], Batch [285/938], Loss: 0.5017658472061157\n",
      "Train: Epoch [11], Batch [286/938], Loss: 0.5847817659378052\n",
      "Train: Epoch [11], Batch [287/938], Loss: 0.4510863125324249\n",
      "Train: Epoch [11], Batch [288/938], Loss: 0.6130836606025696\n",
      "Train: Epoch [11], Batch [289/938], Loss: 0.6185381412506104\n",
      "Train: Epoch [11], Batch [290/938], Loss: 0.5278513431549072\n",
      "Train: Epoch [11], Batch [291/938], Loss: 0.28413769602775574\n",
      "Train: Epoch [11], Batch [292/938], Loss: 0.6094837188720703\n",
      "Train: Epoch [11], Batch [293/938], Loss: 0.707328736782074\n",
      "Train: Epoch [11], Batch [294/938], Loss: 0.3793131113052368\n",
      "Train: Epoch [11], Batch [295/938], Loss: 0.5251845717430115\n",
      "Train: Epoch [11], Batch [296/938], Loss: 0.5481653213500977\n",
      "Train: Epoch [11], Batch [297/938], Loss: 0.6604920029640198\n",
      "Train: Epoch [11], Batch [298/938], Loss: 0.47293156385421753\n",
      "Train: Epoch [11], Batch [299/938], Loss: 0.56488436460495\n",
      "Train: Epoch [11], Batch [300/938], Loss: 0.4601433575153351\n",
      "Train: Epoch [11], Batch [301/938], Loss: 0.5027930736541748\n",
      "Train: Epoch [11], Batch [302/938], Loss: 0.6377815008163452\n",
      "Train: Epoch [11], Batch [303/938], Loss: 0.5694627165794373\n",
      "Train: Epoch [11], Batch [304/938], Loss: 0.52829909324646\n",
      "Train: Epoch [11], Batch [305/938], Loss: 0.5330800414085388\n",
      "Train: Epoch [11], Batch [306/938], Loss: 0.5964241623878479\n",
      "Train: Epoch [11], Batch [307/938], Loss: 0.5693100094795227\n",
      "Train: Epoch [11], Batch [308/938], Loss: 0.39188599586486816\n",
      "Train: Epoch [11], Batch [309/938], Loss: 0.49690988659858704\n",
      "Train: Epoch [11], Batch [310/938], Loss: 0.5745164155960083\n",
      "Train: Epoch [11], Batch [311/938], Loss: 0.53490149974823\n",
      "Train: Epoch [11], Batch [312/938], Loss: 0.5730512142181396\n",
      "Train: Epoch [11], Batch [313/938], Loss: 0.46616727113723755\n",
      "Train: Epoch [11], Batch [314/938], Loss: 0.5068311095237732\n",
      "Train: Epoch [11], Batch [315/938], Loss: 0.6987529397010803\n",
      "Train: Epoch [11], Batch [316/938], Loss: 0.27940833568573\n",
      "Train: Epoch [11], Batch [317/938], Loss: 0.4414213299751282\n",
      "Train: Epoch [11], Batch [318/938], Loss: 0.4280571937561035\n",
      "Train: Epoch [11], Batch [319/938], Loss: 0.5615887641906738\n",
      "Train: Epoch [11], Batch [320/938], Loss: 0.7712194323539734\n",
      "Train: Epoch [11], Batch [321/938], Loss: 0.6058955192565918\n",
      "Train: Epoch [11], Batch [322/938], Loss: 0.5369744896888733\n",
      "Train: Epoch [11], Batch [323/938], Loss: 0.5072028636932373\n",
      "Train: Epoch [11], Batch [324/938], Loss: 0.3690371513366699\n",
      "Train: Epoch [11], Batch [325/938], Loss: 0.352457195520401\n",
      "Train: Epoch [11], Batch [326/938], Loss: 0.5667080879211426\n",
      "Train: Epoch [11], Batch [327/938], Loss: 0.43851181864738464\n",
      "Train: Epoch [11], Batch [328/938], Loss: 0.4609706401824951\n",
      "Train: Epoch [11], Batch [329/938], Loss: 0.5037195086479187\n",
      "Train: Epoch [11], Batch [330/938], Loss: 0.38939595222473145\n",
      "Train: Epoch [11], Batch [331/938], Loss: 0.4204780161380768\n",
      "Train: Epoch [11], Batch [332/938], Loss: 0.5050876140594482\n",
      "Train: Epoch [11], Batch [333/938], Loss: 0.4475628733634949\n",
      "Train: Epoch [11], Batch [334/938], Loss: 0.6145635843276978\n",
      "Train: Epoch [11], Batch [335/938], Loss: 0.5623266696929932\n",
      "Train: Epoch [11], Batch [336/938], Loss: 0.5261877775192261\n",
      "Train: Epoch [11], Batch [337/938], Loss: 0.4693715572357178\n",
      "Train: Epoch [11], Batch [338/938], Loss: 0.37279748916625977\n",
      "Train: Epoch [11], Batch [339/938], Loss: 0.47352999448776245\n",
      "Train: Epoch [11], Batch [340/938], Loss: 0.5402404069900513\n",
      "Train: Epoch [11], Batch [341/938], Loss: 0.7310231924057007\n",
      "Train: Epoch [11], Batch [342/938], Loss: 0.41860073804855347\n",
      "Train: Epoch [11], Batch [343/938], Loss: 0.6289864778518677\n",
      "Train: Epoch [11], Batch [344/938], Loss: 0.5133175849914551\n",
      "Train: Epoch [11], Batch [345/938], Loss: 0.4953027367591858\n",
      "Train: Epoch [11], Batch [346/938], Loss: 0.4587283432483673\n",
      "Train: Epoch [11], Batch [347/938], Loss: 0.4350171685218811\n",
      "Train: Epoch [11], Batch [348/938], Loss: 0.5299034118652344\n",
      "Train: Epoch [11], Batch [349/938], Loss: 0.7712644338607788\n",
      "Train: Epoch [11], Batch [350/938], Loss: 0.5207734107971191\n",
      "Train: Epoch [11], Batch [351/938], Loss: 0.6438366174697876\n",
      "Train: Epoch [11], Batch [352/938], Loss: 0.4640575647354126\n",
      "Train: Epoch [11], Batch [353/938], Loss: 0.7614661455154419\n",
      "Train: Epoch [11], Batch [354/938], Loss: 0.7890225648880005\n",
      "Train: Epoch [11], Batch [355/938], Loss: 0.5618131160736084\n",
      "Train: Epoch [11], Batch [356/938], Loss: 0.47909778356552124\n",
      "Train: Epoch [11], Batch [357/938], Loss: 0.6072064638137817\n",
      "Train: Epoch [11], Batch [358/938], Loss: 0.4716950058937073\n",
      "Train: Epoch [11], Batch [359/938], Loss: 0.5818215608596802\n",
      "Train: Epoch [11], Batch [360/938], Loss: 0.6368144154548645\n",
      "Train: Epoch [11], Batch [361/938], Loss: 0.5400228500366211\n",
      "Train: Epoch [11], Batch [362/938], Loss: 0.5200293064117432\n",
      "Train: Epoch [11], Batch [363/938], Loss: 0.38891837000846863\n",
      "Train: Epoch [11], Batch [364/938], Loss: 0.6492588520050049\n",
      "Train: Epoch [11], Batch [365/938], Loss: 0.4772107005119324\n",
      "Train: Epoch [11], Batch [366/938], Loss: 0.6852349042892456\n",
      "Train: Epoch [11], Batch [367/938], Loss: 0.6051874160766602\n",
      "Train: Epoch [11], Batch [368/938], Loss: 0.5424333810806274\n",
      "Train: Epoch [11], Batch [369/938], Loss: 0.6860712766647339\n",
      "Train: Epoch [11], Batch [370/938], Loss: 0.7010225653648376\n",
      "Train: Epoch [11], Batch [371/938], Loss: 0.32657065987586975\n",
      "Train: Epoch [11], Batch [372/938], Loss: 0.3961251974105835\n",
      "Train: Epoch [11], Batch [373/938], Loss: 0.38183820247650146\n",
      "Train: Epoch [11], Batch [374/938], Loss: 0.5303241014480591\n",
      "Train: Epoch [11], Batch [375/938], Loss: 0.5135861039161682\n",
      "Train: Epoch [11], Batch [376/938], Loss: 0.6760317087173462\n",
      "Train: Epoch [11], Batch [377/938], Loss: 0.6238646507263184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [11], Batch [378/938], Loss: 0.6297812461853027\n",
      "Train: Epoch [11], Batch [379/938], Loss: 0.31546103954315186\n",
      "Train: Epoch [11], Batch [380/938], Loss: 0.418496310710907\n",
      "Train: Epoch [11], Batch [381/938], Loss: 0.6527458429336548\n",
      "Train: Epoch [11], Batch [382/938], Loss: 0.4743539094924927\n",
      "Train: Epoch [11], Batch [383/938], Loss: 0.473341703414917\n",
      "Train: Epoch [11], Batch [384/938], Loss: 0.6072005033493042\n",
      "Train: Epoch [11], Batch [385/938], Loss: 0.4735011160373688\n",
      "Train: Epoch [11], Batch [386/938], Loss: 0.5090621709823608\n",
      "Train: Epoch [11], Batch [387/938], Loss: 0.3971162438392639\n",
      "Train: Epoch [11], Batch [388/938], Loss: 0.36629629135131836\n",
      "Train: Epoch [11], Batch [389/938], Loss: 0.5594139099121094\n",
      "Train: Epoch [11], Batch [390/938], Loss: 0.5704516172409058\n",
      "Train: Epoch [11], Batch [391/938], Loss: 0.6538822650909424\n",
      "Train: Epoch [11], Batch [392/938], Loss: 0.5906324982643127\n",
      "Train: Epoch [11], Batch [393/938], Loss: 0.4977356493473053\n",
      "Train: Epoch [11], Batch [394/938], Loss: 0.7113723754882812\n",
      "Train: Epoch [11], Batch [395/938], Loss: 0.6595402956008911\n",
      "Train: Epoch [11], Batch [396/938], Loss: 0.45089757442474365\n",
      "Train: Epoch [11], Batch [397/938], Loss: 0.4385432004928589\n",
      "Train: Epoch [11], Batch [398/938], Loss: 0.33677560091018677\n",
      "Train: Epoch [11], Batch [399/938], Loss: 0.6431866884231567\n",
      "Train: Epoch [11], Batch [400/938], Loss: 0.6833325028419495\n",
      "Train: Epoch [11], Batch [401/938], Loss: 0.41952115297317505\n",
      "Train: Epoch [11], Batch [402/938], Loss: 0.5092433094978333\n",
      "Train: Epoch [11], Batch [403/938], Loss: 0.33185747265815735\n",
      "Train: Epoch [11], Batch [404/938], Loss: 0.5731860995292664\n",
      "Train: Epoch [11], Batch [405/938], Loss: 0.6126431226730347\n",
      "Train: Epoch [11], Batch [406/938], Loss: 0.6166409254074097\n",
      "Train: Epoch [11], Batch [407/938], Loss: 0.5894976854324341\n",
      "Train: Epoch [11], Batch [408/938], Loss: 0.42742273211479187\n",
      "Train: Epoch [11], Batch [409/938], Loss: 0.45357000827789307\n",
      "Train: Epoch [11], Batch [410/938], Loss: 0.4762013256549835\n",
      "Train: Epoch [11], Batch [411/938], Loss: 0.46456030011177063\n",
      "Train: Epoch [11], Batch [412/938], Loss: 0.7796709537506104\n",
      "Train: Epoch [11], Batch [413/938], Loss: 0.6192634701728821\n",
      "Train: Epoch [11], Batch [414/938], Loss: 0.37864720821380615\n",
      "Train: Epoch [11], Batch [415/938], Loss: 0.5727269649505615\n",
      "Train: Epoch [11], Batch [416/938], Loss: 0.3678781986236572\n",
      "Train: Epoch [11], Batch [417/938], Loss: 0.3374432325363159\n",
      "Train: Epoch [11], Batch [418/938], Loss: 0.5566778182983398\n",
      "Train: Epoch [11], Batch [419/938], Loss: 0.5928670167922974\n",
      "Train: Epoch [11], Batch [420/938], Loss: 0.4912700951099396\n",
      "Train: Epoch [11], Batch [421/938], Loss: 0.6733739972114563\n",
      "Train: Epoch [11], Batch [422/938], Loss: 0.497383713722229\n",
      "Train: Epoch [11], Batch [423/938], Loss: 0.47022825479507446\n",
      "Train: Epoch [11], Batch [424/938], Loss: 0.46149057149887085\n",
      "Train: Epoch [11], Batch [425/938], Loss: 0.46689435839653015\n",
      "Train: Epoch [11], Batch [426/938], Loss: 0.5725055932998657\n",
      "Train: Epoch [11], Batch [427/938], Loss: 0.564887285232544\n",
      "Train: Epoch [11], Batch [428/938], Loss: 0.32888638973236084\n",
      "Train: Epoch [11], Batch [429/938], Loss: 0.7944542765617371\n",
      "Train: Epoch [11], Batch [430/938], Loss: 0.608404278755188\n",
      "Train: Epoch [11], Batch [431/938], Loss: 0.4287513494491577\n",
      "Train: Epoch [11], Batch [432/938], Loss: 0.48488491773605347\n",
      "Train: Epoch [11], Batch [433/938], Loss: 0.44082334637641907\n",
      "Train: Epoch [11], Batch [434/938], Loss: 0.496986448764801\n",
      "Train: Epoch [11], Batch [435/938], Loss: 0.44637149572372437\n",
      "Train: Epoch [11], Batch [436/938], Loss: 0.663005530834198\n",
      "Train: Epoch [11], Batch [437/938], Loss: 0.394060343503952\n",
      "Train: Epoch [11], Batch [438/938], Loss: 0.283893883228302\n",
      "Train: Epoch [11], Batch [439/938], Loss: 0.3126903772354126\n",
      "Train: Epoch [11], Batch [440/938], Loss: 0.4336409866809845\n",
      "Train: Epoch [11], Batch [441/938], Loss: 0.43667325377464294\n",
      "Train: Epoch [11], Batch [442/938], Loss: 0.3827613592147827\n",
      "Train: Epoch [11], Batch [443/938], Loss: 0.6771769523620605\n",
      "Train: Epoch [11], Batch [444/938], Loss: 0.5202922821044922\n",
      "Train: Epoch [11], Batch [445/938], Loss: 0.41733071208000183\n",
      "Train: Epoch [11], Batch [446/938], Loss: 0.38867849111557007\n",
      "Train: Epoch [11], Batch [447/938], Loss: 0.610225260257721\n",
      "Train: Epoch [11], Batch [448/938], Loss: 0.49349358677864075\n",
      "Train: Epoch [11], Batch [449/938], Loss: 0.6462107300758362\n",
      "Train: Epoch [11], Batch [450/938], Loss: 0.3697139322757721\n",
      "Train: Epoch [11], Batch [451/938], Loss: 0.688610315322876\n",
      "Train: Epoch [11], Batch [452/938], Loss: 0.4164770543575287\n",
      "Train: Epoch [11], Batch [453/938], Loss: 0.6032940745353699\n",
      "Train: Epoch [11], Batch [454/938], Loss: 0.4102707803249359\n",
      "Train: Epoch [11], Batch [455/938], Loss: 0.5090588331222534\n",
      "Train: Epoch [11], Batch [456/938], Loss: 0.46712005138397217\n",
      "Train: Epoch [11], Batch [457/938], Loss: 0.5122519731521606\n",
      "Train: Epoch [11], Batch [458/938], Loss: 0.5576210021972656\n",
      "Train: Epoch [11], Batch [459/938], Loss: 0.5720891356468201\n",
      "Train: Epoch [11], Batch [460/938], Loss: 0.37144702672958374\n",
      "Train: Epoch [11], Batch [461/938], Loss: 0.5632362961769104\n",
      "Train: Epoch [11], Batch [462/938], Loss: 0.6077497005462646\n",
      "Train: Epoch [11], Batch [463/938], Loss: 0.7870740294456482\n",
      "Train: Epoch [11], Batch [464/938], Loss: 0.6063528060913086\n",
      "Train: Epoch [11], Batch [465/938], Loss: 0.8045943975448608\n",
      "Train: Epoch [11], Batch [466/938], Loss: 0.4984176754951477\n",
      "Train: Epoch [11], Batch [467/938], Loss: 0.4255342483520508\n",
      "Train: Epoch [11], Batch [468/938], Loss: 0.5180111527442932\n",
      "Train: Epoch [11], Batch [469/938], Loss: 0.560584545135498\n",
      "Train: Epoch [11], Batch [470/938], Loss: 0.6753171682357788\n",
      "Train: Epoch [11], Batch [471/938], Loss: 0.5140595436096191\n",
      "Train: Epoch [11], Batch [472/938], Loss: 0.4790971875190735\n",
      "Train: Epoch [11], Batch [473/938], Loss: 0.39202016592025757\n",
      "Train: Epoch [11], Batch [474/938], Loss: 0.40320366621017456\n",
      "Train: Epoch [11], Batch [475/938], Loss: 0.47708117961883545\n",
      "Train: Epoch [11], Batch [476/938], Loss: 0.6619538068771362\n",
      "Train: Epoch [11], Batch [477/938], Loss: 0.5069173574447632\n",
      "Train: Epoch [11], Batch [478/938], Loss: 0.3404196500778198\n",
      "Train: Epoch [11], Batch [479/938], Loss: 0.5067847967147827\n",
      "Train: Epoch [11], Batch [480/938], Loss: 0.4331163763999939\n",
      "Train: Epoch [11], Batch [481/938], Loss: 0.44691187143325806\n",
      "Train: Epoch [11], Batch [482/938], Loss: 0.44257572293281555\n",
      "Train: Epoch [11], Batch [483/938], Loss: 0.548812985420227\n",
      "Train: Epoch [11], Batch [484/938], Loss: 0.37651193141937256\n",
      "Train: Epoch [11], Batch [485/938], Loss: 0.500750720500946\n",
      "Train: Epoch [11], Batch [486/938], Loss: 0.6559906601905823\n",
      "Train: Epoch [11], Batch [487/938], Loss: 0.47237759828567505\n",
      "Train: Epoch [11], Batch [488/938], Loss: 0.5657213926315308\n",
      "Train: Epoch [11], Batch [489/938], Loss: 0.5421741008758545\n",
      "Train: Epoch [11], Batch [490/938], Loss: 0.6190165281295776\n",
      "Train: Epoch [11], Batch [491/938], Loss: 0.46810442209243774\n",
      "Train: Epoch [11], Batch [492/938], Loss: 0.3824973702430725\n",
      "Train: Epoch [11], Batch [493/938], Loss: 0.31562840938568115\n",
      "Train: Epoch [11], Batch [494/938], Loss: 0.36348071694374084\n",
      "Train: Epoch [11], Batch [495/938], Loss: 0.5803697109222412\n",
      "Train: Epoch [11], Batch [496/938], Loss: 0.38614290952682495\n",
      "Train: Epoch [11], Batch [497/938], Loss: 0.48613524436950684\n",
      "Train: Epoch [11], Batch [498/938], Loss: 0.562243640422821\n",
      "Train: Epoch [11], Batch [499/938], Loss: 0.40895211696624756\n",
      "Train: Epoch [11], Batch [500/938], Loss: 0.5317820310592651\n",
      "Train: Epoch [11], Batch [501/938], Loss: 0.44872933626174927\n",
      "Train: Epoch [11], Batch [502/938], Loss: 0.595404863357544\n",
      "Train: Epoch [11], Batch [503/938], Loss: 0.4894724488258362\n",
      "Train: Epoch [11], Batch [504/938], Loss: 0.4067991375923157\n",
      "Train: Epoch [11], Batch [505/938], Loss: 0.5430612564086914\n",
      "Train: Epoch [11], Batch [506/938], Loss: 0.42790406942367554\n",
      "Train: Epoch [11], Batch [507/938], Loss: 0.45845186710357666\n",
      "Train: Epoch [11], Batch [508/938], Loss: 0.4765571355819702\n",
      "Train: Epoch [11], Batch [509/938], Loss: 0.5290526151657104\n",
      "Train: Epoch [11], Batch [510/938], Loss: 0.7188048958778381\n",
      "Train: Epoch [11], Batch [511/938], Loss: 0.39548200368881226\n",
      "Train: Epoch [11], Batch [512/938], Loss: 0.5392991304397583\n",
      "Train: Epoch [11], Batch [513/938], Loss: 0.6098225116729736\n",
      "Train: Epoch [11], Batch [514/938], Loss: 0.48941829800605774\n",
      "Train: Epoch [11], Batch [515/938], Loss: 0.40218743681907654\n",
      "Train: Epoch [11], Batch [516/938], Loss: 0.36507588624954224\n",
      "Train: Epoch [11], Batch [517/938], Loss: 0.6327270269393921\n",
      "Train: Epoch [11], Batch [518/938], Loss: 0.4797861874103546\n",
      "Train: Epoch [11], Batch [519/938], Loss: 0.766512393951416\n",
      "Train: Epoch [11], Batch [520/938], Loss: 0.4833654761314392\n",
      "Train: Epoch [11], Batch [521/938], Loss: 0.41578248143196106\n",
      "Train: Epoch [11], Batch [522/938], Loss: 0.5914696455001831\n",
      "Train: Epoch [11], Batch [523/938], Loss: 0.42113834619522095\n",
      "Train: Epoch [11], Batch [524/938], Loss: 0.44601693749427795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [11], Batch [525/938], Loss: 0.5140159130096436\n",
      "Train: Epoch [11], Batch [526/938], Loss: 0.5866751670837402\n",
      "Train: Epoch [11], Batch [527/938], Loss: 0.3234081268310547\n",
      "Train: Epoch [11], Batch [528/938], Loss: 0.5917419791221619\n",
      "Train: Epoch [11], Batch [529/938], Loss: 0.6846760511398315\n",
      "Train: Epoch [11], Batch [530/938], Loss: 0.7218207120895386\n",
      "Train: Epoch [11], Batch [531/938], Loss: 0.4019716680049896\n",
      "Train: Epoch [11], Batch [532/938], Loss: 0.4133487045764923\n",
      "Train: Epoch [11], Batch [533/938], Loss: 0.36424750089645386\n",
      "Train: Epoch [11], Batch [534/938], Loss: 0.5961319804191589\n",
      "Train: Epoch [11], Batch [535/938], Loss: 0.288164883852005\n",
      "Train: Epoch [11], Batch [536/938], Loss: 0.6347841024398804\n",
      "Train: Epoch [11], Batch [537/938], Loss: 0.5358186960220337\n",
      "Train: Epoch [11], Batch [538/938], Loss: 0.4542911946773529\n",
      "Train: Epoch [11], Batch [539/938], Loss: 0.5773529410362244\n",
      "Train: Epoch [11], Batch [540/938], Loss: 0.6175787448883057\n",
      "Train: Epoch [11], Batch [541/938], Loss: 0.5127317905426025\n",
      "Train: Epoch [11], Batch [542/938], Loss: 0.6630446910858154\n",
      "Train: Epoch [11], Batch [543/938], Loss: 0.4747293293476105\n",
      "Train: Epoch [11], Batch [544/938], Loss: 0.5113048553466797\n",
      "Train: Epoch [11], Batch [545/938], Loss: 0.38475653529167175\n",
      "Train: Epoch [11], Batch [546/938], Loss: 0.6352235078811646\n",
      "Train: Epoch [11], Batch [547/938], Loss: 0.4687862694263458\n",
      "Train: Epoch [11], Batch [548/938], Loss: 0.5057646036148071\n",
      "Train: Epoch [11], Batch [549/938], Loss: 0.5706865787506104\n",
      "Train: Epoch [11], Batch [550/938], Loss: 0.5894646644592285\n",
      "Train: Epoch [11], Batch [551/938], Loss: 0.31416529417037964\n",
      "Train: Epoch [11], Batch [552/938], Loss: 0.6050764322280884\n",
      "Train: Epoch [11], Batch [553/938], Loss: 0.48985034227371216\n",
      "Train: Epoch [11], Batch [554/938], Loss: 0.5293761491775513\n",
      "Train: Epoch [11], Batch [555/938], Loss: 0.3986630439758301\n",
      "Train: Epoch [11], Batch [556/938], Loss: 0.3797816038131714\n",
      "Train: Epoch [11], Batch [557/938], Loss: 0.6362600326538086\n",
      "Train: Epoch [11], Batch [558/938], Loss: 0.7171604633331299\n",
      "Train: Epoch [11], Batch [559/938], Loss: 0.6312445402145386\n",
      "Train: Epoch [11], Batch [560/938], Loss: 0.4856857359409332\n",
      "Train: Epoch [11], Batch [561/938], Loss: 0.5511184334754944\n",
      "Train: Epoch [11], Batch [562/938], Loss: 0.6037672758102417\n",
      "Train: Epoch [11], Batch [563/938], Loss: 0.6883522272109985\n",
      "Train: Epoch [11], Batch [564/938], Loss: 0.5912986993789673\n",
      "Train: Epoch [11], Batch [565/938], Loss: 0.4002183675765991\n",
      "Train: Epoch [11], Batch [566/938], Loss: 0.6073347330093384\n",
      "Train: Epoch [11], Batch [567/938], Loss: 0.5650872588157654\n",
      "Train: Epoch [11], Batch [568/938], Loss: 0.4799017906188965\n",
      "Train: Epoch [11], Batch [569/938], Loss: 0.4161956310272217\n",
      "Train: Epoch [11], Batch [570/938], Loss: 0.7560615539550781\n",
      "Train: Epoch [11], Batch [571/938], Loss: 0.41966718435287476\n",
      "Train: Epoch [11], Batch [572/938], Loss: 0.49446243047714233\n",
      "Train: Epoch [11], Batch [573/938], Loss: 0.7416570782661438\n",
      "Train: Epoch [11], Batch [574/938], Loss: 0.44061478972435\n",
      "Train: Epoch [11], Batch [575/938], Loss: 0.4818992018699646\n",
      "Train: Epoch [11], Batch [576/938], Loss: 0.4553776979446411\n",
      "Train: Epoch [11], Batch [577/938], Loss: 0.5317209362983704\n",
      "Train: Epoch [11], Batch [578/938], Loss: 0.5562365055084229\n",
      "Train: Epoch [11], Batch [579/938], Loss: 0.5609843730926514\n",
      "Train: Epoch [11], Batch [580/938], Loss: 0.6133573055267334\n",
      "Train: Epoch [11], Batch [581/938], Loss: 0.522067666053772\n",
      "Train: Epoch [11], Batch [582/938], Loss: 0.6312265396118164\n",
      "Train: Epoch [11], Batch [583/938], Loss: 0.5123522877693176\n",
      "Train: Epoch [11], Batch [584/938], Loss: 0.3714824914932251\n",
      "Train: Epoch [11], Batch [585/938], Loss: 0.3553709387779236\n",
      "Train: Epoch [11], Batch [586/938], Loss: 0.5257980823516846\n",
      "Train: Epoch [11], Batch [587/938], Loss: 0.5544070601463318\n",
      "Train: Epoch [11], Batch [588/938], Loss: 0.6715976595878601\n",
      "Train: Epoch [11], Batch [589/938], Loss: 0.5166661143302917\n",
      "Train: Epoch [11], Batch [590/938], Loss: 0.3981646001338959\n",
      "Train: Epoch [11], Batch [591/938], Loss: 0.36926335096359253\n",
      "Train: Epoch [11], Batch [592/938], Loss: 0.31104928255081177\n",
      "Train: Epoch [11], Batch [593/938], Loss: 0.5435003042221069\n",
      "Train: Epoch [11], Batch [594/938], Loss: 0.6153395175933838\n",
      "Train: Epoch [11], Batch [595/938], Loss: 0.5432161092758179\n",
      "Train: Epoch [11], Batch [596/938], Loss: 0.7460150718688965\n",
      "Train: Epoch [11], Batch [597/938], Loss: 0.7852311730384827\n",
      "Train: Epoch [11], Batch [598/938], Loss: 0.6355136632919312\n",
      "Train: Epoch [11], Batch [599/938], Loss: 0.5083759427070618\n",
      "Train: Epoch [11], Batch [600/938], Loss: 0.300020694732666\n",
      "Train: Epoch [11], Batch [601/938], Loss: 0.6249234676361084\n",
      "Train: Epoch [11], Batch [602/938], Loss: 0.24166202545166016\n",
      "Train: Epoch [11], Batch [603/938], Loss: 0.8240410089492798\n",
      "Train: Epoch [11], Batch [604/938], Loss: 0.5400257110595703\n",
      "Train: Epoch [11], Batch [605/938], Loss: 0.4516110122203827\n",
      "Train: Epoch [11], Batch [606/938], Loss: 0.5602983236312866\n",
      "Train: Epoch [11], Batch [607/938], Loss: 0.48577457666397095\n",
      "Train: Epoch [11], Batch [608/938], Loss: 0.5402915477752686\n",
      "Train: Epoch [11], Batch [609/938], Loss: 0.5392473936080933\n",
      "Train: Epoch [11], Batch [610/938], Loss: 0.5105531215667725\n",
      "Train: Epoch [11], Batch [611/938], Loss: 0.4038110375404358\n",
      "Train: Epoch [11], Batch [612/938], Loss: 0.39140158891677856\n",
      "Train: Epoch [11], Batch [613/938], Loss: 0.6068211197853088\n",
      "Train: Epoch [11], Batch [614/938], Loss: 0.4962599277496338\n",
      "Train: Epoch [11], Batch [615/938], Loss: 0.6285772919654846\n",
      "Train: Epoch [11], Batch [616/938], Loss: 0.42570704221725464\n",
      "Train: Epoch [11], Batch [617/938], Loss: 0.5214507579803467\n",
      "Train: Epoch [11], Batch [618/938], Loss: 0.5354790687561035\n",
      "Train: Epoch [11], Batch [619/938], Loss: 0.46085986495018005\n",
      "Train: Epoch [11], Batch [620/938], Loss: 0.43777501583099365\n",
      "Train: Epoch [11], Batch [621/938], Loss: 0.40593037009239197\n",
      "Train: Epoch [11], Batch [622/938], Loss: 0.37359297275543213\n",
      "Train: Epoch [11], Batch [623/938], Loss: 0.6179760694503784\n",
      "Train: Epoch [11], Batch [624/938], Loss: 0.5650797486305237\n",
      "Train: Epoch [11], Batch [625/938], Loss: 0.44962477684020996\n",
      "Train: Epoch [11], Batch [626/938], Loss: 0.34844353795051575\n",
      "Train: Epoch [11], Batch [627/938], Loss: 0.7534559965133667\n",
      "Train: Epoch [11], Batch [628/938], Loss: 0.3290446400642395\n",
      "Train: Epoch [11], Batch [629/938], Loss: 0.4176035523414612\n",
      "Train: Epoch [11], Batch [630/938], Loss: 0.5654609203338623\n",
      "Train: Epoch [11], Batch [631/938], Loss: 0.6651756763458252\n",
      "Train: Epoch [11], Batch [632/938], Loss: 0.5075197219848633\n",
      "Train: Epoch [11], Batch [633/938], Loss: 0.5666247606277466\n",
      "Train: Epoch [11], Batch [634/938], Loss: 0.47834330797195435\n",
      "Train: Epoch [11], Batch [635/938], Loss: 0.54938805103302\n",
      "Train: Epoch [11], Batch [636/938], Loss: 0.5083816647529602\n",
      "Train: Epoch [11], Batch [637/938], Loss: 0.7185986638069153\n",
      "Train: Epoch [11], Batch [638/938], Loss: 0.45766758918762207\n",
      "Train: Epoch [11], Batch [639/938], Loss: 0.7717188596725464\n",
      "Train: Epoch [11], Batch [640/938], Loss: 0.5223839282989502\n",
      "Train: Epoch [11], Batch [641/938], Loss: 0.306702196598053\n",
      "Train: Epoch [11], Batch [642/938], Loss: 0.5010583400726318\n",
      "Train: Epoch [11], Batch [643/938], Loss: 0.6897945404052734\n",
      "Train: Epoch [11], Batch [644/938], Loss: 0.48067784309387207\n",
      "Train: Epoch [11], Batch [645/938], Loss: 0.3226674497127533\n",
      "Train: Epoch [11], Batch [646/938], Loss: 0.5157253742218018\n",
      "Train: Epoch [11], Batch [647/938], Loss: 0.4979075789451599\n",
      "Train: Epoch [11], Batch [648/938], Loss: 0.542790412902832\n",
      "Train: Epoch [11], Batch [649/938], Loss: 0.48889386653900146\n",
      "Train: Epoch [11], Batch [650/938], Loss: 0.5732786655426025\n",
      "Train: Epoch [11], Batch [651/938], Loss: 0.47217392921447754\n",
      "Train: Epoch [11], Batch [652/938], Loss: 0.5340834856033325\n",
      "Train: Epoch [11], Batch [653/938], Loss: 0.5344237089157104\n",
      "Train: Epoch [11], Batch [654/938], Loss: 0.5175365209579468\n",
      "Train: Epoch [11], Batch [655/938], Loss: 0.584635853767395\n",
      "Train: Epoch [11], Batch [656/938], Loss: 0.48315081000328064\n",
      "Train: Epoch [11], Batch [657/938], Loss: 0.4638742506504059\n",
      "Train: Epoch [11], Batch [658/938], Loss: 0.3977951109409332\n",
      "Train: Epoch [11], Batch [659/938], Loss: 0.5233389139175415\n",
      "Train: Epoch [11], Batch [660/938], Loss: 0.42094653844833374\n",
      "Train: Epoch [11], Batch [661/938], Loss: 0.3956611752510071\n",
      "Train: Epoch [11], Batch [662/938], Loss: 0.3681279718875885\n",
      "Train: Epoch [11], Batch [663/938], Loss: 0.4528695344924927\n",
      "Train: Epoch [11], Batch [664/938], Loss: 0.6383700966835022\n",
      "Train: Epoch [11], Batch [665/938], Loss: 0.6171262860298157\n",
      "Train: Epoch [11], Batch [666/938], Loss: 0.6560385823249817\n",
      "Train: Epoch [11], Batch [667/938], Loss: 0.5018802881240845\n",
      "Train: Epoch [11], Batch [668/938], Loss: 0.41554439067840576\n",
      "Train: Epoch [11], Batch [669/938], Loss: 0.33492302894592285\n",
      "Train: Epoch [11], Batch [670/938], Loss: 0.45835989713668823\n",
      "Train: Epoch [11], Batch [671/938], Loss: 0.436984658241272\n",
      "Train: Epoch [11], Batch [672/938], Loss: 0.692655622959137\n",
      "Train: Epoch [11], Batch [673/938], Loss: 0.4757198095321655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [11], Batch [674/938], Loss: 0.4733874797821045\n",
      "Train: Epoch [11], Batch [675/938], Loss: 0.5981141328811646\n",
      "Train: Epoch [11], Batch [676/938], Loss: 0.6844123005867004\n",
      "Train: Epoch [11], Batch [677/938], Loss: 0.5209571123123169\n",
      "Train: Epoch [11], Batch [678/938], Loss: 0.35753729939460754\n",
      "Train: Epoch [11], Batch [679/938], Loss: 0.436801016330719\n",
      "Train: Epoch [11], Batch [680/938], Loss: 0.5046647787094116\n",
      "Train: Epoch [11], Batch [681/938], Loss: 0.5911423563957214\n",
      "Train: Epoch [11], Batch [682/938], Loss: 0.5235777497291565\n",
      "Train: Epoch [11], Batch [683/938], Loss: 0.5310324430465698\n",
      "Train: Epoch [11], Batch [684/938], Loss: 0.47825872898101807\n",
      "Train: Epoch [11], Batch [685/938], Loss: 0.3910224735736847\n",
      "Train: Epoch [11], Batch [686/938], Loss: 0.5127168297767639\n",
      "Train: Epoch [11], Batch [687/938], Loss: 0.5386663675308228\n",
      "Train: Epoch [11], Batch [688/938], Loss: 0.6908372640609741\n",
      "Train: Epoch [11], Batch [689/938], Loss: 0.4739401936531067\n",
      "Train: Epoch [11], Batch [690/938], Loss: 0.7031710147857666\n",
      "Train: Epoch [11], Batch [691/938], Loss: 0.6238917112350464\n",
      "Train: Epoch [11], Batch [692/938], Loss: 0.5227797627449036\n",
      "Train: Epoch [11], Batch [693/938], Loss: 0.4100290536880493\n",
      "Train: Epoch [11], Batch [694/938], Loss: 0.44020700454711914\n",
      "Train: Epoch [11], Batch [695/938], Loss: 0.4431478977203369\n",
      "Train: Epoch [11], Batch [696/938], Loss: 0.6075066328048706\n",
      "Train: Epoch [11], Batch [697/938], Loss: 0.4010591506958008\n",
      "Train: Epoch [11], Batch [698/938], Loss: 0.6616969108581543\n",
      "Train: Epoch [11], Batch [699/938], Loss: 0.5649726986885071\n",
      "Train: Epoch [11], Batch [700/938], Loss: 0.6308640241622925\n",
      "Train: Epoch [11], Batch [701/938], Loss: 0.6532829403877258\n",
      "Train: Epoch [11], Batch [702/938], Loss: 0.8501149415969849\n",
      "Train: Epoch [11], Batch [703/938], Loss: 0.531851589679718\n",
      "Train: Epoch [11], Batch [704/938], Loss: 0.5630249381065369\n",
      "Train: Epoch [11], Batch [705/938], Loss: 0.4327630400657654\n",
      "Train: Epoch [11], Batch [706/938], Loss: 0.5908019542694092\n",
      "Train: Epoch [11], Batch [707/938], Loss: 0.46289223432540894\n",
      "Train: Epoch [11], Batch [708/938], Loss: 0.7470282912254333\n",
      "Train: Epoch [11], Batch [709/938], Loss: 0.4181644022464752\n",
      "Train: Epoch [11], Batch [710/938], Loss: 0.5055382251739502\n",
      "Train: Epoch [11], Batch [711/938], Loss: 0.4037948250770569\n",
      "Train: Epoch [11], Batch [712/938], Loss: 0.7989855408668518\n",
      "Train: Epoch [11], Batch [713/938], Loss: 0.4041348397731781\n",
      "Train: Epoch [11], Batch [714/938], Loss: 0.3796488642692566\n",
      "Train: Epoch [11], Batch [715/938], Loss: 0.3757937252521515\n",
      "Train: Epoch [11], Batch [716/938], Loss: 0.5401204228401184\n",
      "Train: Epoch [11], Batch [717/938], Loss: 0.3389850854873657\n",
      "Train: Epoch [11], Batch [718/938], Loss: 0.3981441557407379\n",
      "Train: Epoch [11], Batch [719/938], Loss: 0.6192843317985535\n",
      "Train: Epoch [11], Batch [720/938], Loss: 0.537812352180481\n",
      "Train: Epoch [11], Batch [721/938], Loss: 0.5600975751876831\n",
      "Train: Epoch [11], Batch [722/938], Loss: 0.6007013320922852\n",
      "Train: Epoch [11], Batch [723/938], Loss: 0.3515529930591583\n",
      "Train: Epoch [11], Batch [724/938], Loss: 0.36799585819244385\n",
      "Train: Epoch [11], Batch [725/938], Loss: 0.5240558385848999\n",
      "Train: Epoch [11], Batch [726/938], Loss: 0.5188868641853333\n",
      "Train: Epoch [11], Batch [727/938], Loss: 0.4833836555480957\n",
      "Train: Epoch [11], Batch [728/938], Loss: 0.7870039939880371\n",
      "Train: Epoch [11], Batch [729/938], Loss: 0.6066721677780151\n",
      "Train: Epoch [11], Batch [730/938], Loss: 0.5865252017974854\n",
      "Train: Epoch [11], Batch [731/938], Loss: 0.5234643220901489\n",
      "Train: Epoch [11], Batch [732/938], Loss: 0.41684094071388245\n",
      "Train: Epoch [11], Batch [733/938], Loss: 0.8985327482223511\n",
      "Train: Epoch [11], Batch [734/938], Loss: 0.41246041655540466\n",
      "Train: Epoch [11], Batch [735/938], Loss: 0.3423363268375397\n",
      "Train: Epoch [11], Batch [736/938], Loss: 0.3899032771587372\n",
      "Train: Epoch [11], Batch [737/938], Loss: 0.5054981708526611\n",
      "Train: Epoch [11], Batch [738/938], Loss: 0.3879868984222412\n",
      "Train: Epoch [11], Batch [739/938], Loss: 0.42636239528656006\n",
      "Train: Epoch [11], Batch [740/938], Loss: 0.3787069022655487\n",
      "Train: Epoch [11], Batch [741/938], Loss: 0.49449166655540466\n",
      "Train: Epoch [11], Batch [742/938], Loss: 0.6477214097976685\n",
      "Train: Epoch [11], Batch [743/938], Loss: 0.4748994708061218\n",
      "Train: Epoch [11], Batch [744/938], Loss: 0.29222342371940613\n",
      "Train: Epoch [11], Batch [745/938], Loss: 0.4501027762889862\n",
      "Train: Epoch [11], Batch [746/938], Loss: 0.4831434488296509\n",
      "Train: Epoch [11], Batch [747/938], Loss: 0.4665929675102234\n",
      "Train: Epoch [11], Batch [748/938], Loss: 0.6586614847183228\n",
      "Train: Epoch [11], Batch [749/938], Loss: 0.5154569149017334\n",
      "Train: Epoch [11], Batch [750/938], Loss: 0.3704814016819\n",
      "Train: Epoch [11], Batch [751/938], Loss: 0.5465436577796936\n",
      "Train: Epoch [11], Batch [752/938], Loss: 0.6760178208351135\n",
      "Train: Epoch [11], Batch [753/938], Loss: 0.4377369284629822\n",
      "Train: Epoch [11], Batch [754/938], Loss: 0.5205837488174438\n",
      "Train: Epoch [11], Batch [755/938], Loss: 0.6994127035140991\n",
      "Train: Epoch [11], Batch [756/938], Loss: 0.5594689249992371\n",
      "Train: Epoch [11], Batch [757/938], Loss: 0.28505825996398926\n",
      "Train: Epoch [11], Batch [758/938], Loss: 0.44692403078079224\n",
      "Train: Epoch [11], Batch [759/938], Loss: 0.5123600959777832\n",
      "Train: Epoch [11], Batch [760/938], Loss: 0.6370452642440796\n",
      "Train: Epoch [11], Batch [761/938], Loss: 0.49054455757141113\n",
      "Train: Epoch [11], Batch [762/938], Loss: 0.5340759754180908\n",
      "Train: Epoch [11], Batch [763/938], Loss: 0.39747366309165955\n",
      "Train: Epoch [11], Batch [764/938], Loss: 0.45703405141830444\n",
      "Train: Epoch [11], Batch [765/938], Loss: 0.4849861264228821\n",
      "Train: Epoch [11], Batch [766/938], Loss: 0.42474448680877686\n",
      "Train: Epoch [11], Batch [767/938], Loss: 0.4649430215358734\n",
      "Train: Epoch [11], Batch [768/938], Loss: 0.4136550724506378\n",
      "Train: Epoch [11], Batch [769/938], Loss: 0.40601423382759094\n",
      "Train: Epoch [11], Batch [770/938], Loss: 0.5949182510375977\n",
      "Train: Epoch [11], Batch [771/938], Loss: 0.48407015204429626\n",
      "Train: Epoch [11], Batch [772/938], Loss: 0.43093016743659973\n",
      "Train: Epoch [11], Batch [773/938], Loss: 0.45253825187683105\n",
      "Train: Epoch [11], Batch [774/938], Loss: 0.5022276639938354\n",
      "Train: Epoch [11], Batch [775/938], Loss: 0.5662973523139954\n",
      "Train: Epoch [11], Batch [776/938], Loss: 0.2342686653137207\n",
      "Train: Epoch [11], Batch [777/938], Loss: 0.7351051568984985\n",
      "Train: Epoch [11], Batch [778/938], Loss: 0.5228403210639954\n",
      "Train: Epoch [11], Batch [779/938], Loss: 0.48260629177093506\n",
      "Train: Epoch [11], Batch [780/938], Loss: 0.5229560136795044\n",
      "Train: Epoch [11], Batch [781/938], Loss: 0.38416528701782227\n",
      "Train: Epoch [11], Batch [782/938], Loss: 0.5159430503845215\n",
      "Train: Epoch [11], Batch [783/938], Loss: 0.32385972142219543\n",
      "Train: Epoch [11], Batch [784/938], Loss: 0.46477314829826355\n",
      "Train: Epoch [11], Batch [785/938], Loss: 0.5993912816047668\n",
      "Train: Epoch [11], Batch [786/938], Loss: 0.5129857063293457\n",
      "Train: Epoch [11], Batch [787/938], Loss: 0.43166038393974304\n",
      "Train: Epoch [11], Batch [788/938], Loss: 0.5660247802734375\n",
      "Train: Epoch [11], Batch [789/938], Loss: 0.33179426193237305\n",
      "Train: Epoch [11], Batch [790/938], Loss: 0.6289786696434021\n",
      "Train: Epoch [11], Batch [791/938], Loss: 0.47186338901519775\n",
      "Train: Epoch [11], Batch [792/938], Loss: 0.5853747129440308\n",
      "Train: Epoch [11], Batch [793/938], Loss: 0.4396347105503082\n",
      "Train: Epoch [11], Batch [794/938], Loss: 0.589826762676239\n",
      "Train: Epoch [11], Batch [795/938], Loss: 0.6221829652786255\n",
      "Train: Epoch [11], Batch [796/938], Loss: 0.4712159037590027\n",
      "Train: Epoch [11], Batch [797/938], Loss: 0.604908287525177\n",
      "Train: Epoch [11], Batch [798/938], Loss: 0.7242701053619385\n",
      "Train: Epoch [11], Batch [799/938], Loss: 0.39739370346069336\n",
      "Train: Epoch [11], Batch [800/938], Loss: 0.5312696695327759\n",
      "Train: Epoch [11], Batch [801/938], Loss: 0.4095795750617981\n",
      "Train: Epoch [11], Batch [802/938], Loss: 0.41956907510757446\n",
      "Train: Epoch [11], Batch [803/938], Loss: 0.7142987847328186\n",
      "Train: Epoch [11], Batch [804/938], Loss: 0.6032297611236572\n",
      "Train: Epoch [11], Batch [805/938], Loss: 0.4968562722206116\n",
      "Train: Epoch [11], Batch [806/938], Loss: 0.6834895610809326\n",
      "Train: Epoch [11], Batch [807/938], Loss: 0.5540063977241516\n",
      "Train: Epoch [11], Batch [808/938], Loss: 0.7776779532432556\n",
      "Train: Epoch [11], Batch [809/938], Loss: 0.4675501585006714\n",
      "Train: Epoch [11], Batch [810/938], Loss: 0.5915535688400269\n",
      "Train: Epoch [11], Batch [811/938], Loss: 0.5523170828819275\n",
      "Train: Epoch [11], Batch [812/938], Loss: 0.3713962733745575\n",
      "Train: Epoch [11], Batch [813/938], Loss: 0.4954814314842224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [11], Batch [814/938], Loss: 0.550501823425293\n",
      "Train: Epoch [11], Batch [815/938], Loss: 0.6043151617050171\n",
      "Train: Epoch [11], Batch [816/938], Loss: 0.39877623319625854\n",
      "Train: Epoch [11], Batch [817/938], Loss: 0.5219553112983704\n",
      "Train: Epoch [11], Batch [818/938], Loss: 0.5021218061447144\n",
      "Train: Epoch [11], Batch [819/938], Loss: 0.38039350509643555\n",
      "Train: Epoch [11], Batch [820/938], Loss: 0.5344233512878418\n",
      "Train: Epoch [11], Batch [821/938], Loss: 0.5864683985710144\n",
      "Train: Epoch [11], Batch [822/938], Loss: 0.4156530499458313\n",
      "Train: Epoch [11], Batch [823/938], Loss: 0.40006083250045776\n",
      "Train: Epoch [11], Batch [824/938], Loss: 0.5261169672012329\n",
      "Train: Epoch [11], Batch [825/938], Loss: 0.5971983075141907\n",
      "Train: Epoch [11], Batch [826/938], Loss: 0.5932101607322693\n",
      "Train: Epoch [11], Batch [827/938], Loss: 0.9109420776367188\n",
      "Train: Epoch [11], Batch [828/938], Loss: 0.5085880756378174\n",
      "Train: Epoch [11], Batch [829/938], Loss: 0.48099958896636963\n",
      "Train: Epoch [11], Batch [830/938], Loss: 0.5537734031677246\n",
      "Train: Epoch [11], Batch [831/938], Loss: 0.4136744737625122\n",
      "Train: Epoch [11], Batch [832/938], Loss: 0.4959762692451477\n",
      "Train: Epoch [11], Batch [833/938], Loss: 0.5502880215644836\n",
      "Train: Epoch [11], Batch [834/938], Loss: 0.5483523011207581\n",
      "Train: Epoch [11], Batch [835/938], Loss: 0.4005160927772522\n",
      "Train: Epoch [11], Batch [836/938], Loss: 0.5092151761054993\n",
      "Train: Epoch [11], Batch [837/938], Loss: 0.49130451679229736\n",
      "Train: Epoch [11], Batch [838/938], Loss: 0.6272832751274109\n",
      "Train: Epoch [11], Batch [839/938], Loss: 0.4275839030742645\n",
      "Train: Epoch [11], Batch [840/938], Loss: 0.6258476972579956\n",
      "Train: Epoch [11], Batch [841/938], Loss: 0.3909481167793274\n",
      "Train: Epoch [11], Batch [842/938], Loss: 0.4969816505908966\n",
      "Train: Epoch [11], Batch [843/938], Loss: 0.4654313623905182\n",
      "Train: Epoch [11], Batch [844/938], Loss: 0.4157336950302124\n",
      "Train: Epoch [11], Batch [845/938], Loss: 0.3908950388431549\n",
      "Train: Epoch [11], Batch [846/938], Loss: 0.5042257308959961\n",
      "Train: Epoch [11], Batch [847/938], Loss: 0.442291796207428\n",
      "Train: Epoch [11], Batch [848/938], Loss: 0.7736089825630188\n",
      "Train: Epoch [11], Batch [849/938], Loss: 0.6244819760322571\n",
      "Train: Epoch [11], Batch [850/938], Loss: 0.5200631022453308\n",
      "Train: Epoch [11], Batch [851/938], Loss: 0.5103254318237305\n",
      "Train: Epoch [11], Batch [852/938], Loss: 0.5292984247207642\n",
      "Train: Epoch [11], Batch [853/938], Loss: 0.37075865268707275\n",
      "Train: Epoch [11], Batch [854/938], Loss: 0.4040968120098114\n",
      "Train: Epoch [11], Batch [855/938], Loss: 0.38222408294677734\n",
      "Train: Epoch [11], Batch [856/938], Loss: 0.5438055992126465\n",
      "Train: Epoch [11], Batch [857/938], Loss: 0.5173875093460083\n",
      "Train: Epoch [11], Batch [858/938], Loss: 0.5412548780441284\n",
      "Train: Epoch [11], Batch [859/938], Loss: 0.5158809423446655\n",
      "Train: Epoch [11], Batch [860/938], Loss: 0.4357830286026001\n",
      "Train: Epoch [11], Batch [861/938], Loss: 0.3684765696525574\n",
      "Train: Epoch [11], Batch [862/938], Loss: 0.3093833923339844\n",
      "Train: Epoch [11], Batch [863/938], Loss: 0.49512916803359985\n",
      "Train: Epoch [11], Batch [864/938], Loss: 0.4222031533718109\n",
      "Train: Epoch [11], Batch [865/938], Loss: 0.5521292686462402\n",
      "Train: Epoch [11], Batch [866/938], Loss: 0.4073852300643921\n",
      "Train: Epoch [11], Batch [867/938], Loss: 0.44094204902648926\n",
      "Train: Epoch [11], Batch [868/938], Loss: 0.5762859582901001\n",
      "Train: Epoch [11], Batch [869/938], Loss: 0.4444689154624939\n",
      "Train: Epoch [11], Batch [870/938], Loss: 0.4171629846096039\n",
      "Train: Epoch [11], Batch [871/938], Loss: 0.47290635108947754\n",
      "Train: Epoch [11], Batch [872/938], Loss: 0.6426517963409424\n",
      "Train: Epoch [11], Batch [873/938], Loss: 0.5190631747245789\n",
      "Train: Epoch [11], Batch [874/938], Loss: 0.36702731251716614\n",
      "Train: Epoch [11], Batch [875/938], Loss: 0.5686209201812744\n",
      "Train: Epoch [11], Batch [876/938], Loss: 0.4961642920970917\n",
      "Train: Epoch [11], Batch [877/938], Loss: 0.3851665258407593\n",
      "Train: Epoch [11], Batch [878/938], Loss: 0.49300122261047363\n",
      "Train: Epoch [11], Batch [879/938], Loss: 0.5337181091308594\n",
      "Train: Epoch [11], Batch [880/938], Loss: 0.7880783081054688\n",
      "Train: Epoch [11], Batch [881/938], Loss: 0.5592897534370422\n",
      "Train: Epoch [11], Batch [882/938], Loss: 0.4820643961429596\n",
      "Train: Epoch [11], Batch [883/938], Loss: 0.47855401039123535\n",
      "Train: Epoch [11], Batch [884/938], Loss: 0.6484847068786621\n",
      "Train: Epoch [11], Batch [885/938], Loss: 0.8070113062858582\n",
      "Train: Epoch [11], Batch [886/938], Loss: 0.48520228266716003\n",
      "Train: Epoch [11], Batch [887/938], Loss: 0.5442670583724976\n",
      "Train: Epoch [11], Batch [888/938], Loss: 0.374744713306427\n",
      "Train: Epoch [11], Batch [889/938], Loss: 0.45000314712524414\n",
      "Train: Epoch [11], Batch [890/938], Loss: 0.3957754969596863\n",
      "Train: Epoch [11], Batch [891/938], Loss: 0.6717627644538879\n",
      "Train: Epoch [11], Batch [892/938], Loss: 0.6048580408096313\n",
      "Train: Epoch [11], Batch [893/938], Loss: 0.4404997229576111\n",
      "Train: Epoch [11], Batch [894/938], Loss: 0.579824686050415\n",
      "Train: Epoch [11], Batch [895/938], Loss: 0.41212135553359985\n",
      "Train: Epoch [11], Batch [896/938], Loss: 0.522049605846405\n",
      "Train: Epoch [11], Batch [897/938], Loss: 0.4956018328666687\n",
      "Train: Epoch [11], Batch [898/938], Loss: 0.48265892267227173\n",
      "Train: Epoch [11], Batch [899/938], Loss: 0.45562049746513367\n",
      "Train: Epoch [11], Batch [900/938], Loss: 0.48471570014953613\n",
      "Train: Epoch [11], Batch [901/938], Loss: 0.4575499892234802\n",
      "Train: Epoch [11], Batch [902/938], Loss: 0.4514370560646057\n",
      "Train: Epoch [11], Batch [903/938], Loss: 0.8006725311279297\n",
      "Train: Epoch [11], Batch [904/938], Loss: 0.5858216881752014\n",
      "Train: Epoch [11], Batch [905/938], Loss: 0.3954768776893616\n",
      "Train: Epoch [11], Batch [906/938], Loss: 0.4669191539287567\n",
      "Train: Epoch [11], Batch [907/938], Loss: 0.6165891885757446\n",
      "Train: Epoch [11], Batch [908/938], Loss: 0.5350595712661743\n",
      "Train: Epoch [11], Batch [909/938], Loss: 0.31542015075683594\n",
      "Train: Epoch [11], Batch [910/938], Loss: 0.4832810163497925\n",
      "Train: Epoch [11], Batch [911/938], Loss: 0.7873307466506958\n",
      "Train: Epoch [11], Batch [912/938], Loss: 0.9205708503723145\n",
      "Train: Epoch [11], Batch [913/938], Loss: 0.7819660902023315\n",
      "Train: Epoch [11], Batch [914/938], Loss: 0.5729789137840271\n",
      "Train: Epoch [11], Batch [915/938], Loss: 0.5890253782272339\n",
      "Train: Epoch [11], Batch [916/938], Loss: 0.4846287667751312\n",
      "Train: Epoch [11], Batch [917/938], Loss: 0.4204787015914917\n",
      "Train: Epoch [11], Batch [918/938], Loss: 0.5265175104141235\n",
      "Train: Epoch [11], Batch [919/938], Loss: 0.3488868474960327\n",
      "Train: Epoch [11], Batch [920/938], Loss: 0.4602982997894287\n",
      "Train: Epoch [11], Batch [921/938], Loss: 0.4918558597564697\n",
      "Train: Epoch [11], Batch [922/938], Loss: 0.3351520895957947\n",
      "Train: Epoch [11], Batch [923/938], Loss: 0.3529173731803894\n",
      "Train: Epoch [11], Batch [924/938], Loss: 0.3770684599876404\n",
      "Train: Epoch [11], Batch [925/938], Loss: 0.48406320810317993\n",
      "Train: Epoch [11], Batch [926/938], Loss: 0.30757859349250793\n",
      "Train: Epoch [11], Batch [927/938], Loss: 0.4014963209629059\n",
      "Train: Epoch [11], Batch [928/938], Loss: 0.5225039720535278\n",
      "Train: Epoch [11], Batch [929/938], Loss: 0.38991591334342957\n",
      "Train: Epoch [11], Batch [930/938], Loss: 0.5603630542755127\n",
      "Train: Epoch [11], Batch [931/938], Loss: 0.37970900535583496\n",
      "Train: Epoch [11], Batch [932/938], Loss: 0.44089680910110474\n",
      "Train: Epoch [11], Batch [933/938], Loss: 0.5679546594619751\n",
      "Train: Epoch [11], Batch [934/938], Loss: 0.5512049198150635\n",
      "Train: Epoch [11], Batch [935/938], Loss: 0.5869660377502441\n",
      "Train: Epoch [11], Batch [936/938], Loss: 0.6957640647888184\n",
      "Train: Epoch [11], Batch [937/938], Loss: 0.3865799903869629\n",
      "Train: Epoch [11], Batch [938/938], Loss: 0.3682570457458496\n",
      "Accuracy of train set: 0.8205666666666667\n",
      "Validation: Epoch [11], Batch [1/938], Loss: 0.6861315965652466\n",
      "Validation: Epoch [11], Batch [2/938], Loss: 0.4912680685520172\n",
      "Validation: Epoch [11], Batch [3/938], Loss: 0.3872060775756836\n",
      "Validation: Epoch [11], Batch [4/938], Loss: 0.29737353324890137\n",
      "Validation: Epoch [11], Batch [5/938], Loss: 0.4881684482097626\n",
      "Validation: Epoch [11], Batch [6/938], Loss: 0.3806055188179016\n",
      "Validation: Epoch [11], Batch [7/938], Loss: 0.5516611933708191\n",
      "Validation: Epoch [11], Batch [8/938], Loss: 0.4024333655834198\n",
      "Validation: Epoch [11], Batch [9/938], Loss: 0.4802103042602539\n",
      "Validation: Epoch [11], Batch [10/938], Loss: 0.4237792491912842\n",
      "Validation: Epoch [11], Batch [11/938], Loss: 0.5424032211303711\n",
      "Validation: Epoch [11], Batch [12/938], Loss: 0.35803690552711487\n",
      "Validation: Epoch [11], Batch [13/938], Loss: 0.34119170904159546\n",
      "Validation: Epoch [11], Batch [14/938], Loss: 0.35352206230163574\n",
      "Validation: Epoch [11], Batch [15/938], Loss: 0.7057020664215088\n",
      "Validation: Epoch [11], Batch [16/938], Loss: 0.5844422578811646\n",
      "Validation: Epoch [11], Batch [17/938], Loss: 0.30456534028053284\n",
      "Validation: Epoch [11], Batch [18/938], Loss: 0.6509567499160767\n",
      "Validation: Epoch [11], Batch [19/938], Loss: 0.3522089719772339\n",
      "Validation: Epoch [11], Batch [20/938], Loss: 0.49924665689468384\n",
      "Validation: Epoch [11], Batch [21/938], Loss: 0.4702596068382263\n",
      "Validation: Epoch [11], Batch [22/938], Loss: 0.5891354084014893\n",
      "Validation: Epoch [11], Batch [23/938], Loss: 0.5314444899559021\n",
      "Validation: Epoch [11], Batch [24/938], Loss: 0.48254019021987915\n",
      "Validation: Epoch [11], Batch [25/938], Loss: 0.3887493908405304\n",
      "Validation: Epoch [11], Batch [26/938], Loss: 0.5480906963348389\n",
      "Validation: Epoch [11], Batch [27/938], Loss: 0.4144981801509857\n",
      "Validation: Epoch [11], Batch [28/938], Loss: 0.42260056734085083\n",
      "Validation: Epoch [11], Batch [29/938], Loss: 0.6148836612701416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [30/938], Loss: 0.6481596231460571\n",
      "Validation: Epoch [11], Batch [31/938], Loss: 0.516215443611145\n",
      "Validation: Epoch [11], Batch [32/938], Loss: 0.4107585847377777\n",
      "Validation: Epoch [11], Batch [33/938], Loss: 0.4238465428352356\n",
      "Validation: Epoch [11], Batch [34/938], Loss: 0.39482516050338745\n",
      "Validation: Epoch [11], Batch [35/938], Loss: 0.4002186357975006\n",
      "Validation: Epoch [11], Batch [36/938], Loss: 0.7774678468704224\n",
      "Validation: Epoch [11], Batch [37/938], Loss: 0.5087205767631531\n",
      "Validation: Epoch [11], Batch [38/938], Loss: 0.6882696151733398\n",
      "Validation: Epoch [11], Batch [39/938], Loss: 0.3735380172729492\n",
      "Validation: Epoch [11], Batch [40/938], Loss: 0.5006253123283386\n",
      "Validation: Epoch [11], Batch [41/938], Loss: 0.4084421694278717\n",
      "Validation: Epoch [11], Batch [42/938], Loss: 0.4139675498008728\n",
      "Validation: Epoch [11], Batch [43/938], Loss: 0.49395012855529785\n",
      "Validation: Epoch [11], Batch [44/938], Loss: 0.4753187298774719\n",
      "Validation: Epoch [11], Batch [45/938], Loss: 0.32987162470817566\n",
      "Validation: Epoch [11], Batch [46/938], Loss: 0.4967249035835266\n",
      "Validation: Epoch [11], Batch [47/938], Loss: 0.5968267917633057\n",
      "Validation: Epoch [11], Batch [48/938], Loss: 0.46143442392349243\n",
      "Validation: Epoch [11], Batch [49/938], Loss: 0.6829262971878052\n",
      "Validation: Epoch [11], Batch [50/938], Loss: 0.6481742858886719\n",
      "Validation: Epoch [11], Batch [51/938], Loss: 0.4580443203449249\n",
      "Validation: Epoch [11], Batch [52/938], Loss: 0.4573501944541931\n",
      "Validation: Epoch [11], Batch [53/938], Loss: 0.5422945022583008\n",
      "Validation: Epoch [11], Batch [54/938], Loss: 0.5833874940872192\n",
      "Validation: Epoch [11], Batch [55/938], Loss: 0.6024886965751648\n",
      "Validation: Epoch [11], Batch [56/938], Loss: 0.45040932297706604\n",
      "Validation: Epoch [11], Batch [57/938], Loss: 0.5332291126251221\n",
      "Validation: Epoch [11], Batch [58/938], Loss: 0.6878810524940491\n",
      "Validation: Epoch [11], Batch [59/938], Loss: 0.4632285535335541\n",
      "Validation: Epoch [11], Batch [60/938], Loss: 0.6053062081336975\n",
      "Validation: Epoch [11], Batch [61/938], Loss: 0.5526493787765503\n",
      "Validation: Epoch [11], Batch [62/938], Loss: 0.502148449420929\n",
      "Validation: Epoch [11], Batch [63/938], Loss: 0.6396952867507935\n",
      "Validation: Epoch [11], Batch [64/938], Loss: 0.47152474522590637\n",
      "Validation: Epoch [11], Batch [65/938], Loss: 0.3191351592540741\n",
      "Validation: Epoch [11], Batch [66/938], Loss: 0.41908085346221924\n",
      "Validation: Epoch [11], Batch [67/938], Loss: 0.6017038822174072\n",
      "Validation: Epoch [11], Batch [68/938], Loss: 0.45183682441711426\n",
      "Validation: Epoch [11], Batch [69/938], Loss: 0.4173240065574646\n",
      "Validation: Epoch [11], Batch [70/938], Loss: 0.485319048166275\n",
      "Validation: Epoch [11], Batch [71/938], Loss: 0.5571995377540588\n",
      "Validation: Epoch [11], Batch [72/938], Loss: 0.5321585536003113\n",
      "Validation: Epoch [11], Batch [73/938], Loss: 0.5173152685165405\n",
      "Validation: Epoch [11], Batch [74/938], Loss: 0.6595264673233032\n",
      "Validation: Epoch [11], Batch [75/938], Loss: 0.3640984296798706\n",
      "Validation: Epoch [11], Batch [76/938], Loss: 0.42057088017463684\n",
      "Validation: Epoch [11], Batch [77/938], Loss: 0.5207654237747192\n",
      "Validation: Epoch [11], Batch [78/938], Loss: 0.4506463408470154\n",
      "Validation: Epoch [11], Batch [79/938], Loss: 0.4080040454864502\n",
      "Validation: Epoch [11], Batch [80/938], Loss: 0.5139334797859192\n",
      "Validation: Epoch [11], Batch [81/938], Loss: 0.7280265688896179\n",
      "Validation: Epoch [11], Batch [82/938], Loss: 0.4423244893550873\n",
      "Validation: Epoch [11], Batch [83/938], Loss: 0.5291694402694702\n",
      "Validation: Epoch [11], Batch [84/938], Loss: 0.6156647205352783\n",
      "Validation: Epoch [11], Batch [85/938], Loss: 0.45571017265319824\n",
      "Validation: Epoch [11], Batch [86/938], Loss: 0.37159019708633423\n",
      "Validation: Epoch [11], Batch [87/938], Loss: 0.6188125014305115\n",
      "Validation: Epoch [11], Batch [88/938], Loss: 0.37937694787979126\n",
      "Validation: Epoch [11], Batch [89/938], Loss: 0.5574110150337219\n",
      "Validation: Epoch [11], Batch [90/938], Loss: 0.3470150828361511\n",
      "Validation: Epoch [11], Batch [91/938], Loss: 0.5012327432632446\n",
      "Validation: Epoch [11], Batch [92/938], Loss: 0.37111008167266846\n",
      "Validation: Epoch [11], Batch [93/938], Loss: 0.7585664391517639\n",
      "Validation: Epoch [11], Batch [94/938], Loss: 0.4511072337627411\n",
      "Validation: Epoch [11], Batch [95/938], Loss: 0.48430484533309937\n",
      "Validation: Epoch [11], Batch [96/938], Loss: 0.46531549096107483\n",
      "Validation: Epoch [11], Batch [97/938], Loss: 0.32252779603004456\n",
      "Validation: Epoch [11], Batch [98/938], Loss: 0.48896345496177673\n",
      "Validation: Epoch [11], Batch [99/938], Loss: 0.6127285957336426\n",
      "Validation: Epoch [11], Batch [100/938], Loss: 1.016142725944519\n",
      "Validation: Epoch [11], Batch [101/938], Loss: 0.5259320735931396\n",
      "Validation: Epoch [11], Batch [102/938], Loss: 0.5750492811203003\n",
      "Validation: Epoch [11], Batch [103/938], Loss: 0.6241902709007263\n",
      "Validation: Epoch [11], Batch [104/938], Loss: 0.7163087725639343\n",
      "Validation: Epoch [11], Batch [105/938], Loss: 0.4574521780014038\n",
      "Validation: Epoch [11], Batch [106/938], Loss: 0.5803921222686768\n",
      "Validation: Epoch [11], Batch [107/938], Loss: 0.6345838308334351\n",
      "Validation: Epoch [11], Batch [108/938], Loss: 0.41033297777175903\n",
      "Validation: Epoch [11], Batch [109/938], Loss: 0.6629379391670227\n",
      "Validation: Epoch [11], Batch [110/938], Loss: 0.44249749183654785\n",
      "Validation: Epoch [11], Batch [111/938], Loss: 0.35270529985427856\n",
      "Validation: Epoch [11], Batch [112/938], Loss: 0.4276030361652374\n",
      "Validation: Epoch [11], Batch [113/938], Loss: 0.4547577500343323\n",
      "Validation: Epoch [11], Batch [114/938], Loss: 0.35985347628593445\n",
      "Validation: Epoch [11], Batch [115/938], Loss: 0.6684381365776062\n",
      "Validation: Epoch [11], Batch [116/938], Loss: 0.47616371512413025\n",
      "Validation: Epoch [11], Batch [117/938], Loss: 0.6666563153266907\n",
      "Validation: Epoch [11], Batch [118/938], Loss: 0.3628799021244049\n",
      "Validation: Epoch [11], Batch [119/938], Loss: 0.4950060248374939\n",
      "Validation: Epoch [11], Batch [120/938], Loss: 0.4247414767742157\n",
      "Validation: Epoch [11], Batch [121/938], Loss: 0.6009547114372253\n",
      "Validation: Epoch [11], Batch [122/938], Loss: 0.4586682617664337\n",
      "Validation: Epoch [11], Batch [123/938], Loss: 0.5454124808311462\n",
      "Validation: Epoch [11], Batch [124/938], Loss: 0.4625060558319092\n",
      "Validation: Epoch [11], Batch [125/938], Loss: 0.4996148645877838\n",
      "Validation: Epoch [11], Batch [126/938], Loss: 0.4431734085083008\n",
      "Validation: Epoch [11], Batch [127/938], Loss: 0.49941256642341614\n",
      "Validation: Epoch [11], Batch [128/938], Loss: 0.6069197058677673\n",
      "Validation: Epoch [11], Batch [129/938], Loss: 0.43692290782928467\n",
      "Validation: Epoch [11], Batch [130/938], Loss: 0.5353124737739563\n",
      "Validation: Epoch [11], Batch [131/938], Loss: 0.5956708192825317\n",
      "Validation: Epoch [11], Batch [132/938], Loss: 0.5908788442611694\n",
      "Validation: Epoch [11], Batch [133/938], Loss: 0.4677474796772003\n",
      "Validation: Epoch [11], Batch [134/938], Loss: 0.3480433523654938\n",
      "Validation: Epoch [11], Batch [135/938], Loss: 0.5062374472618103\n",
      "Validation: Epoch [11], Batch [136/938], Loss: 0.6900244951248169\n",
      "Validation: Epoch [11], Batch [137/938], Loss: 0.45928847789764404\n",
      "Validation: Epoch [11], Batch [138/938], Loss: 0.4850482642650604\n",
      "Validation: Epoch [11], Batch [139/938], Loss: 0.4139249920845032\n",
      "Validation: Epoch [11], Batch [140/938], Loss: 0.5044064521789551\n",
      "Validation: Epoch [11], Batch [141/938], Loss: 0.5949659943580627\n",
      "Validation: Epoch [11], Batch [142/938], Loss: 0.4510079324245453\n",
      "Validation: Epoch [11], Batch [143/938], Loss: 0.5205681324005127\n",
      "Validation: Epoch [11], Batch [144/938], Loss: 0.47398245334625244\n",
      "Validation: Epoch [11], Batch [145/938], Loss: 0.4568557143211365\n",
      "Validation: Epoch [11], Batch [146/938], Loss: 0.6111365556716919\n",
      "Validation: Epoch [11], Batch [147/938], Loss: 0.47298717498779297\n",
      "Validation: Epoch [11], Batch [148/938], Loss: 0.6922184228897095\n",
      "Validation: Epoch [11], Batch [149/938], Loss: 0.2914379835128784\n",
      "Validation: Epoch [11], Batch [150/938], Loss: 0.4125765860080719\n",
      "Validation: Epoch [11], Batch [151/938], Loss: 0.40985918045043945\n",
      "Validation: Epoch [11], Batch [152/938], Loss: 0.4295126795768738\n",
      "Validation: Epoch [11], Batch [153/938], Loss: 0.545103132724762\n",
      "Validation: Epoch [11], Batch [154/938], Loss: 0.5686367750167847\n",
      "Validation: Epoch [11], Batch [155/938], Loss: 0.5445019006729126\n",
      "Validation: Epoch [11], Batch [156/938], Loss: 0.6136012077331543\n",
      "Validation: Epoch [11], Batch [157/938], Loss: 0.30318886041641235\n",
      "Validation: Epoch [11], Batch [158/938], Loss: 0.44910234212875366\n",
      "Validation: Epoch [11], Batch [159/938], Loss: 0.32451120018959045\n",
      "Validation: Epoch [11], Batch [160/938], Loss: 0.45874783396720886\n",
      "Validation: Epoch [11], Batch [161/938], Loss: 0.5987591743469238\n",
      "Validation: Epoch [11], Batch [162/938], Loss: 0.5100256204605103\n",
      "Validation: Epoch [11], Batch [163/938], Loss: 0.5322320461273193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [164/938], Loss: 0.5388734340667725\n",
      "Validation: Epoch [11], Batch [165/938], Loss: 0.4641195237636566\n",
      "Validation: Epoch [11], Batch [166/938], Loss: 0.4480268061161041\n",
      "Validation: Epoch [11], Batch [167/938], Loss: 0.4545888900756836\n",
      "Validation: Epoch [11], Batch [168/938], Loss: 0.42934101819992065\n",
      "Validation: Epoch [11], Batch [169/938], Loss: 0.5029253959655762\n",
      "Validation: Epoch [11], Batch [170/938], Loss: 0.4984924793243408\n",
      "Validation: Epoch [11], Batch [171/938], Loss: 0.5617161393165588\n",
      "Validation: Epoch [11], Batch [172/938], Loss: 0.4733327031135559\n",
      "Validation: Epoch [11], Batch [173/938], Loss: 0.42119652032852173\n",
      "Validation: Epoch [11], Batch [174/938], Loss: 0.4991057515144348\n",
      "Validation: Epoch [11], Batch [175/938], Loss: 0.526561975479126\n",
      "Validation: Epoch [11], Batch [176/938], Loss: 0.5284168124198914\n",
      "Validation: Epoch [11], Batch [177/938], Loss: 0.4334273338317871\n",
      "Validation: Epoch [11], Batch [178/938], Loss: 0.47875216603279114\n",
      "Validation: Epoch [11], Batch [179/938], Loss: 0.5869030356407166\n",
      "Validation: Epoch [11], Batch [180/938], Loss: 0.4877080023288727\n",
      "Validation: Epoch [11], Batch [181/938], Loss: 0.3914218246936798\n",
      "Validation: Epoch [11], Batch [182/938], Loss: 0.5272666215896606\n",
      "Validation: Epoch [11], Batch [183/938], Loss: 0.4320835471153259\n",
      "Validation: Epoch [11], Batch [184/938], Loss: 0.5842587947845459\n",
      "Validation: Epoch [11], Batch [185/938], Loss: 0.43341735005378723\n",
      "Validation: Epoch [11], Batch [186/938], Loss: 0.5140154361724854\n",
      "Validation: Epoch [11], Batch [187/938], Loss: 0.6105507612228394\n",
      "Validation: Epoch [11], Batch [188/938], Loss: 0.3696279227733612\n",
      "Validation: Epoch [11], Batch [189/938], Loss: 0.6236635446548462\n",
      "Validation: Epoch [11], Batch [190/938], Loss: 0.603009045124054\n",
      "Validation: Epoch [11], Batch [191/938], Loss: 0.5538393259048462\n",
      "Validation: Epoch [11], Batch [192/938], Loss: 0.6247120499610901\n",
      "Validation: Epoch [11], Batch [193/938], Loss: 0.3739648759365082\n",
      "Validation: Epoch [11], Batch [194/938], Loss: 0.37791919708251953\n",
      "Validation: Epoch [11], Batch [195/938], Loss: 0.7346199750900269\n",
      "Validation: Epoch [11], Batch [196/938], Loss: 0.6473886966705322\n",
      "Validation: Epoch [11], Batch [197/938], Loss: 0.48554521799087524\n",
      "Validation: Epoch [11], Batch [198/938], Loss: 0.44842010736465454\n",
      "Validation: Epoch [11], Batch [199/938], Loss: 0.4648309648036957\n",
      "Validation: Epoch [11], Batch [200/938], Loss: 0.3410585820674896\n",
      "Validation: Epoch [11], Batch [201/938], Loss: 0.5092766284942627\n",
      "Validation: Epoch [11], Batch [202/938], Loss: 0.46714287996292114\n",
      "Validation: Epoch [11], Batch [203/938], Loss: 0.49385571479797363\n",
      "Validation: Epoch [11], Batch [204/938], Loss: 0.689440131187439\n",
      "Validation: Epoch [11], Batch [205/938], Loss: 0.5240895748138428\n",
      "Validation: Epoch [11], Batch [206/938], Loss: 0.716069757938385\n",
      "Validation: Epoch [11], Batch [207/938], Loss: 0.5827146768569946\n",
      "Validation: Epoch [11], Batch [208/938], Loss: 0.5014054179191589\n",
      "Validation: Epoch [11], Batch [209/938], Loss: 0.5573474168777466\n",
      "Validation: Epoch [11], Batch [210/938], Loss: 0.591090202331543\n",
      "Validation: Epoch [11], Batch [211/938], Loss: 0.6300406455993652\n",
      "Validation: Epoch [11], Batch [212/938], Loss: 0.578445553779602\n",
      "Validation: Epoch [11], Batch [213/938], Loss: 0.49685168266296387\n",
      "Validation: Epoch [11], Batch [214/938], Loss: 0.3817943334579468\n",
      "Validation: Epoch [11], Batch [215/938], Loss: 0.5025708079338074\n",
      "Validation: Epoch [11], Batch [216/938], Loss: 0.6367793679237366\n",
      "Validation: Epoch [11], Batch [217/938], Loss: 0.6127962470054626\n",
      "Validation: Epoch [11], Batch [218/938], Loss: 0.5513508319854736\n",
      "Validation: Epoch [11], Batch [219/938], Loss: 0.6240009069442749\n",
      "Validation: Epoch [11], Batch [220/938], Loss: 0.5596815347671509\n",
      "Validation: Epoch [11], Batch [221/938], Loss: 0.2844887375831604\n",
      "Validation: Epoch [11], Batch [222/938], Loss: 0.2862708568572998\n",
      "Validation: Epoch [11], Batch [223/938], Loss: 0.6909510493278503\n",
      "Validation: Epoch [11], Batch [224/938], Loss: 0.5380391478538513\n",
      "Validation: Epoch [11], Batch [225/938], Loss: 0.44178828597068787\n",
      "Validation: Epoch [11], Batch [226/938], Loss: 0.5775090456008911\n",
      "Validation: Epoch [11], Batch [227/938], Loss: 0.4793931245803833\n",
      "Validation: Epoch [11], Batch [228/938], Loss: 0.49087440967559814\n",
      "Validation: Epoch [11], Batch [229/938], Loss: 0.48582935333251953\n",
      "Validation: Epoch [11], Batch [230/938], Loss: 0.36657410860061646\n",
      "Validation: Epoch [11], Batch [231/938], Loss: 0.44769567251205444\n",
      "Validation: Epoch [11], Batch [232/938], Loss: 0.6283497214317322\n",
      "Validation: Epoch [11], Batch [233/938], Loss: 0.8062833547592163\n",
      "Validation: Epoch [11], Batch [234/938], Loss: 0.4417389929294586\n",
      "Validation: Epoch [11], Batch [235/938], Loss: 0.4888477921485901\n",
      "Validation: Epoch [11], Batch [236/938], Loss: 0.38666099309921265\n",
      "Validation: Epoch [11], Batch [237/938], Loss: 0.5055446624755859\n",
      "Validation: Epoch [11], Batch [238/938], Loss: 0.6449346542358398\n",
      "Validation: Epoch [11], Batch [239/938], Loss: 0.40415486693382263\n",
      "Validation: Epoch [11], Batch [240/938], Loss: 0.4059732258319855\n",
      "Validation: Epoch [11], Batch [241/938], Loss: 0.3962503671646118\n",
      "Validation: Epoch [11], Batch [242/938], Loss: 0.4757370948791504\n",
      "Validation: Epoch [11], Batch [243/938], Loss: 0.5619877576828003\n",
      "Validation: Epoch [11], Batch [244/938], Loss: 0.7376457452774048\n",
      "Validation: Epoch [11], Batch [245/938], Loss: 0.4391123056411743\n",
      "Validation: Epoch [11], Batch [246/938], Loss: 0.4386404752731323\n",
      "Validation: Epoch [11], Batch [247/938], Loss: 0.46394991874694824\n",
      "Validation: Epoch [11], Batch [248/938], Loss: 0.47360318899154663\n",
      "Validation: Epoch [11], Batch [249/938], Loss: 0.5921070575714111\n",
      "Validation: Epoch [11], Batch [250/938], Loss: 0.4052875339984894\n",
      "Validation: Epoch [11], Batch [251/938], Loss: 0.4467840790748596\n",
      "Validation: Epoch [11], Batch [252/938], Loss: 0.5205581188201904\n",
      "Validation: Epoch [11], Batch [253/938], Loss: 0.3929186463356018\n",
      "Validation: Epoch [11], Batch [254/938], Loss: 0.5971776247024536\n",
      "Validation: Epoch [11], Batch [255/938], Loss: 0.48054325580596924\n",
      "Validation: Epoch [11], Batch [256/938], Loss: 0.33312538266181946\n",
      "Validation: Epoch [11], Batch [257/938], Loss: 0.2972620725631714\n",
      "Validation: Epoch [11], Batch [258/938], Loss: 0.5369237661361694\n",
      "Validation: Epoch [11], Batch [259/938], Loss: 0.6222759485244751\n",
      "Validation: Epoch [11], Batch [260/938], Loss: 0.42547479271888733\n",
      "Validation: Epoch [11], Batch [261/938], Loss: 0.3817347586154938\n",
      "Validation: Epoch [11], Batch [262/938], Loss: 0.4886704087257385\n",
      "Validation: Epoch [11], Batch [263/938], Loss: 0.47176432609558105\n",
      "Validation: Epoch [11], Batch [264/938], Loss: 0.6236550211906433\n",
      "Validation: Epoch [11], Batch [265/938], Loss: 0.5944967865943909\n",
      "Validation: Epoch [11], Batch [266/938], Loss: 0.385097861289978\n",
      "Validation: Epoch [11], Batch [267/938], Loss: 0.3661714196205139\n",
      "Validation: Epoch [11], Batch [268/938], Loss: 0.5192559957504272\n",
      "Validation: Epoch [11], Batch [269/938], Loss: 0.4887668490409851\n",
      "Validation: Epoch [11], Batch [270/938], Loss: 0.39787179231643677\n",
      "Validation: Epoch [11], Batch [271/938], Loss: 0.5311359763145447\n",
      "Validation: Epoch [11], Batch [272/938], Loss: 0.34227290749549866\n",
      "Validation: Epoch [11], Batch [273/938], Loss: 0.6054843664169312\n",
      "Validation: Epoch [11], Batch [274/938], Loss: 0.6735482811927795\n",
      "Validation: Epoch [11], Batch [275/938], Loss: 0.5799274444580078\n",
      "Validation: Epoch [11], Batch [276/938], Loss: 0.5505547523498535\n",
      "Validation: Epoch [11], Batch [277/938], Loss: 0.49273577332496643\n",
      "Validation: Epoch [11], Batch [278/938], Loss: 0.46303653717041016\n",
      "Validation: Epoch [11], Batch [279/938], Loss: 0.3794185519218445\n",
      "Validation: Epoch [11], Batch [280/938], Loss: 0.3723748028278351\n",
      "Validation: Epoch [11], Batch [281/938], Loss: 0.2839638590812683\n",
      "Validation: Epoch [11], Batch [282/938], Loss: 0.5270382761955261\n",
      "Validation: Epoch [11], Batch [283/938], Loss: 0.6233021020889282\n",
      "Validation: Epoch [11], Batch [284/938], Loss: 0.40313783288002014\n",
      "Validation: Epoch [11], Batch [285/938], Loss: 0.538140058517456\n",
      "Validation: Epoch [11], Batch [286/938], Loss: 0.4105985760688782\n",
      "Validation: Epoch [11], Batch [287/938], Loss: 0.6135461926460266\n",
      "Validation: Epoch [11], Batch [288/938], Loss: 0.5786370038986206\n",
      "Validation: Epoch [11], Batch [289/938], Loss: 0.5286084413528442\n",
      "Validation: Epoch [11], Batch [290/938], Loss: 0.43842190504074097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [291/938], Loss: 0.3503531515598297\n",
      "Validation: Epoch [11], Batch [292/938], Loss: 0.6819909811019897\n",
      "Validation: Epoch [11], Batch [293/938], Loss: 0.4209461212158203\n",
      "Validation: Epoch [11], Batch [294/938], Loss: 0.4134292006492615\n",
      "Validation: Epoch [11], Batch [295/938], Loss: 0.46560895442962646\n",
      "Validation: Epoch [11], Batch [296/938], Loss: 0.33033448457717896\n",
      "Validation: Epoch [11], Batch [297/938], Loss: 0.5217176079750061\n",
      "Validation: Epoch [11], Batch [298/938], Loss: 0.3152868449687958\n",
      "Validation: Epoch [11], Batch [299/938], Loss: 0.588996410369873\n",
      "Validation: Epoch [11], Batch [300/938], Loss: 0.864571213722229\n",
      "Validation: Epoch [11], Batch [301/938], Loss: 0.37651360034942627\n",
      "Validation: Epoch [11], Batch [302/938], Loss: 0.565613865852356\n",
      "Validation: Epoch [11], Batch [303/938], Loss: 0.4125855565071106\n",
      "Validation: Epoch [11], Batch [304/938], Loss: 0.516461193561554\n",
      "Validation: Epoch [11], Batch [305/938], Loss: 0.33817189931869507\n",
      "Validation: Epoch [11], Batch [306/938], Loss: 0.37769395112991333\n",
      "Validation: Epoch [11], Batch [307/938], Loss: 0.37434133887290955\n",
      "Validation: Epoch [11], Batch [308/938], Loss: 0.4553769826889038\n",
      "Validation: Epoch [11], Batch [309/938], Loss: 0.42677634954452515\n",
      "Validation: Epoch [11], Batch [310/938], Loss: 0.4497656226158142\n",
      "Validation: Epoch [11], Batch [311/938], Loss: 0.5670340657234192\n",
      "Validation: Epoch [11], Batch [312/938], Loss: 0.40060609579086304\n",
      "Validation: Epoch [11], Batch [313/938], Loss: 0.4899137020111084\n",
      "Validation: Epoch [11], Batch [314/938], Loss: 0.6581964492797852\n",
      "Validation: Epoch [11], Batch [315/938], Loss: 0.5376181602478027\n",
      "Validation: Epoch [11], Batch [316/938], Loss: 0.39526331424713135\n",
      "Validation: Epoch [11], Batch [317/938], Loss: 0.6349859237670898\n",
      "Validation: Epoch [11], Batch [318/938], Loss: 0.5380502343177795\n",
      "Validation: Epoch [11], Batch [319/938], Loss: 0.33453369140625\n",
      "Validation: Epoch [11], Batch [320/938], Loss: 0.7324231863021851\n",
      "Validation: Epoch [11], Batch [321/938], Loss: 0.7022321224212646\n",
      "Validation: Epoch [11], Batch [322/938], Loss: 0.4361242949962616\n",
      "Validation: Epoch [11], Batch [323/938], Loss: 0.5431995987892151\n",
      "Validation: Epoch [11], Batch [324/938], Loss: 0.5626198053359985\n",
      "Validation: Epoch [11], Batch [325/938], Loss: 0.6355446577072144\n",
      "Validation: Epoch [11], Batch [326/938], Loss: 0.59244704246521\n",
      "Validation: Epoch [11], Batch [327/938], Loss: 0.35496601462364197\n",
      "Validation: Epoch [11], Batch [328/938], Loss: 0.4838871359825134\n",
      "Validation: Epoch [11], Batch [329/938], Loss: 0.36712586879730225\n",
      "Validation: Epoch [11], Batch [330/938], Loss: 0.5648139119148254\n",
      "Validation: Epoch [11], Batch [331/938], Loss: 0.5337991714477539\n",
      "Validation: Epoch [11], Batch [332/938], Loss: 0.5646916031837463\n",
      "Validation: Epoch [11], Batch [333/938], Loss: 0.512026309967041\n",
      "Validation: Epoch [11], Batch [334/938], Loss: 0.567802906036377\n",
      "Validation: Epoch [11], Batch [335/938], Loss: 0.44436225295066833\n",
      "Validation: Epoch [11], Batch [336/938], Loss: 0.43024611473083496\n",
      "Validation: Epoch [11], Batch [337/938], Loss: 0.6564289331436157\n",
      "Validation: Epoch [11], Batch [338/938], Loss: 0.5142067074775696\n",
      "Validation: Epoch [11], Batch [339/938], Loss: 0.5081948041915894\n",
      "Validation: Epoch [11], Batch [340/938], Loss: 0.6189720630645752\n",
      "Validation: Epoch [11], Batch [341/938], Loss: 0.6304125189781189\n",
      "Validation: Epoch [11], Batch [342/938], Loss: 0.3643781840801239\n",
      "Validation: Epoch [11], Batch [343/938], Loss: 0.6215246915817261\n",
      "Validation: Epoch [11], Batch [344/938], Loss: 0.5412477254867554\n",
      "Validation: Epoch [11], Batch [345/938], Loss: 0.43001240491867065\n",
      "Validation: Epoch [11], Batch [346/938], Loss: 0.4827639162540436\n",
      "Validation: Epoch [11], Batch [347/938], Loss: 0.47170260548591614\n",
      "Validation: Epoch [11], Batch [348/938], Loss: 0.3890798091888428\n",
      "Validation: Epoch [11], Batch [349/938], Loss: 0.4340672194957733\n",
      "Validation: Epoch [11], Batch [350/938], Loss: 0.3894740045070648\n",
      "Validation: Epoch [11], Batch [351/938], Loss: 0.5115382075309753\n",
      "Validation: Epoch [11], Batch [352/938], Loss: 0.510109007358551\n",
      "Validation: Epoch [11], Batch [353/938], Loss: 0.33606594800949097\n",
      "Validation: Epoch [11], Batch [354/938], Loss: 0.25132954120635986\n",
      "Validation: Epoch [11], Batch [355/938], Loss: 0.5017577409744263\n",
      "Validation: Epoch [11], Batch [356/938], Loss: 0.3783280551433563\n",
      "Validation: Epoch [11], Batch [357/938], Loss: 0.5125510692596436\n",
      "Validation: Epoch [11], Batch [358/938], Loss: 0.4410181939601898\n",
      "Validation: Epoch [11], Batch [359/938], Loss: 0.4806499779224396\n",
      "Validation: Epoch [11], Batch [360/938], Loss: 0.5707099437713623\n",
      "Validation: Epoch [11], Batch [361/938], Loss: 0.5131962895393372\n",
      "Validation: Epoch [11], Batch [362/938], Loss: 0.37493228912353516\n",
      "Validation: Epoch [11], Batch [363/938], Loss: 0.4482790231704712\n",
      "Validation: Epoch [11], Batch [364/938], Loss: 0.5632978677749634\n",
      "Validation: Epoch [11], Batch [365/938], Loss: 0.35889196395874023\n",
      "Validation: Epoch [11], Batch [366/938], Loss: 0.4578803479671478\n",
      "Validation: Epoch [11], Batch [367/938], Loss: 0.38994118571281433\n",
      "Validation: Epoch [11], Batch [368/938], Loss: 0.5658646821975708\n",
      "Validation: Epoch [11], Batch [369/938], Loss: 0.3739626705646515\n",
      "Validation: Epoch [11], Batch [370/938], Loss: 0.5383855104446411\n",
      "Validation: Epoch [11], Batch [371/938], Loss: 0.3409232497215271\n",
      "Validation: Epoch [11], Batch [372/938], Loss: 0.467043936252594\n",
      "Validation: Epoch [11], Batch [373/938], Loss: 0.6402300000190735\n",
      "Validation: Epoch [11], Batch [374/938], Loss: 0.5644665956497192\n",
      "Validation: Epoch [11], Batch [375/938], Loss: 0.5985167026519775\n",
      "Validation: Epoch [11], Batch [376/938], Loss: 0.31249871850013733\n",
      "Validation: Epoch [11], Batch [377/938], Loss: 0.570574164390564\n",
      "Validation: Epoch [11], Batch [378/938], Loss: 0.6111704111099243\n",
      "Validation: Epoch [11], Batch [379/938], Loss: 0.7234246730804443\n",
      "Validation: Epoch [11], Batch [380/938], Loss: 0.5786654949188232\n",
      "Validation: Epoch [11], Batch [381/938], Loss: 0.468540221452713\n",
      "Validation: Epoch [11], Batch [382/938], Loss: 0.4627775549888611\n",
      "Validation: Epoch [11], Batch [383/938], Loss: 0.5369095802307129\n",
      "Validation: Epoch [11], Batch [384/938], Loss: 0.6138432621955872\n",
      "Validation: Epoch [11], Batch [385/938], Loss: 0.4453386664390564\n",
      "Validation: Epoch [11], Batch [386/938], Loss: 0.6186048984527588\n",
      "Validation: Epoch [11], Batch [387/938], Loss: 0.5744950771331787\n",
      "Validation: Epoch [11], Batch [388/938], Loss: 0.6493647694587708\n",
      "Validation: Epoch [11], Batch [389/938], Loss: 0.5811315774917603\n",
      "Validation: Epoch [11], Batch [390/938], Loss: 0.6141791939735413\n",
      "Validation: Epoch [11], Batch [391/938], Loss: 0.39077267050743103\n",
      "Validation: Epoch [11], Batch [392/938], Loss: 0.5449585914611816\n",
      "Validation: Epoch [11], Batch [393/938], Loss: 0.4356797933578491\n",
      "Validation: Epoch [11], Batch [394/938], Loss: 0.45837652683258057\n",
      "Validation: Epoch [11], Batch [395/938], Loss: 0.4355373978614807\n",
      "Validation: Epoch [11], Batch [396/938], Loss: 0.4888109564781189\n",
      "Validation: Epoch [11], Batch [397/938], Loss: 0.38202863931655884\n",
      "Validation: Epoch [11], Batch [398/938], Loss: 0.582074761390686\n",
      "Validation: Epoch [11], Batch [399/938], Loss: 0.6293312311172485\n",
      "Validation: Epoch [11], Batch [400/938], Loss: 0.39543941617012024\n",
      "Validation: Epoch [11], Batch [401/938], Loss: 0.44667768478393555\n",
      "Validation: Epoch [11], Batch [402/938], Loss: 0.6232215762138367\n",
      "Validation: Epoch [11], Batch [403/938], Loss: 0.6684356927871704\n",
      "Validation: Epoch [11], Batch [404/938], Loss: 0.4110414981842041\n",
      "Validation: Epoch [11], Batch [405/938], Loss: 0.4621722400188446\n",
      "Validation: Epoch [11], Batch [406/938], Loss: 0.5808032751083374\n",
      "Validation: Epoch [11], Batch [407/938], Loss: 0.522854745388031\n",
      "Validation: Epoch [11], Batch [408/938], Loss: 0.6657701730728149\n",
      "Validation: Epoch [11], Batch [409/938], Loss: 0.47427400946617126\n",
      "Validation: Epoch [11], Batch [410/938], Loss: 0.34262970089912415\n",
      "Validation: Epoch [11], Batch [411/938], Loss: 0.46077194809913635\n",
      "Validation: Epoch [11], Batch [412/938], Loss: 0.771416425704956\n",
      "Validation: Epoch [11], Batch [413/938], Loss: 0.5220510363578796\n",
      "Validation: Epoch [11], Batch [414/938], Loss: 0.4251081943511963\n",
      "Validation: Epoch [11], Batch [415/938], Loss: 0.3962865471839905\n",
      "Validation: Epoch [11], Batch [416/938], Loss: 0.45973464846611023\n",
      "Validation: Epoch [11], Batch [417/938], Loss: 0.5327403545379639\n",
      "Validation: Epoch [11], Batch [418/938], Loss: 0.4569747745990753\n",
      "Validation: Epoch [11], Batch [419/938], Loss: 0.48340803384780884\n",
      "Validation: Epoch [11], Batch [420/938], Loss: 0.5606176257133484\n",
      "Validation: Epoch [11], Batch [421/938], Loss: 0.6810020208358765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [422/938], Loss: 0.4056706428527832\n",
      "Validation: Epoch [11], Batch [423/938], Loss: 0.5434876084327698\n",
      "Validation: Epoch [11], Batch [424/938], Loss: 0.3661714196205139\n",
      "Validation: Epoch [11], Batch [425/938], Loss: 0.5382213592529297\n",
      "Validation: Epoch [11], Batch [426/938], Loss: 0.43689942359924316\n",
      "Validation: Epoch [11], Batch [427/938], Loss: 0.35924163460731506\n",
      "Validation: Epoch [11], Batch [428/938], Loss: 0.47339192032814026\n",
      "Validation: Epoch [11], Batch [429/938], Loss: 0.6973016262054443\n",
      "Validation: Epoch [11], Batch [430/938], Loss: 0.5210421085357666\n",
      "Validation: Epoch [11], Batch [431/938], Loss: 0.6802366971969604\n",
      "Validation: Epoch [11], Batch [432/938], Loss: 0.6363468766212463\n",
      "Validation: Epoch [11], Batch [433/938], Loss: 0.5939042568206787\n",
      "Validation: Epoch [11], Batch [434/938], Loss: 0.4146149754524231\n",
      "Validation: Epoch [11], Batch [435/938], Loss: 0.6838756203651428\n",
      "Validation: Epoch [11], Batch [436/938], Loss: 0.6391739249229431\n",
      "Validation: Epoch [11], Batch [437/938], Loss: 0.4987664520740509\n",
      "Validation: Epoch [11], Batch [438/938], Loss: 0.4241926968097687\n",
      "Validation: Epoch [11], Batch [439/938], Loss: 0.3415781855583191\n",
      "Validation: Epoch [11], Batch [440/938], Loss: 0.5294103622436523\n",
      "Validation: Epoch [11], Batch [441/938], Loss: 0.46976593136787415\n",
      "Validation: Epoch [11], Batch [442/938], Loss: 0.4551563858985901\n",
      "Validation: Epoch [11], Batch [443/938], Loss: 0.546816885471344\n",
      "Validation: Epoch [11], Batch [444/938], Loss: 0.5458074808120728\n",
      "Validation: Epoch [11], Batch [445/938], Loss: 0.5583217740058899\n",
      "Validation: Epoch [11], Batch [446/938], Loss: 0.5386131405830383\n",
      "Validation: Epoch [11], Batch [447/938], Loss: 0.678616464138031\n",
      "Validation: Epoch [11], Batch [448/938], Loss: 0.6483924984931946\n",
      "Validation: Epoch [11], Batch [449/938], Loss: 0.6555013656616211\n",
      "Validation: Epoch [11], Batch [450/938], Loss: 0.578018844127655\n",
      "Validation: Epoch [11], Batch [451/938], Loss: 0.39403873682022095\n",
      "Validation: Epoch [11], Batch [452/938], Loss: 0.5622276067733765\n",
      "Validation: Epoch [11], Batch [453/938], Loss: 0.5786413550376892\n",
      "Validation: Epoch [11], Batch [454/938], Loss: 0.5084185600280762\n",
      "Validation: Epoch [11], Batch [455/938], Loss: 0.5535759925842285\n",
      "Validation: Epoch [11], Batch [456/938], Loss: 0.5434401631355286\n",
      "Validation: Epoch [11], Batch [457/938], Loss: 0.3611629009246826\n",
      "Validation: Epoch [11], Batch [458/938], Loss: 0.5984421968460083\n",
      "Validation: Epoch [11], Batch [459/938], Loss: 0.6615004539489746\n",
      "Validation: Epoch [11], Batch [460/938], Loss: 0.5644255876541138\n",
      "Validation: Epoch [11], Batch [461/938], Loss: 0.5786733627319336\n",
      "Validation: Epoch [11], Batch [462/938], Loss: 0.381214439868927\n",
      "Validation: Epoch [11], Batch [463/938], Loss: 0.6009510159492493\n",
      "Validation: Epoch [11], Batch [464/938], Loss: 0.47859880328178406\n",
      "Validation: Epoch [11], Batch [465/938], Loss: 0.3656173050403595\n",
      "Validation: Epoch [11], Batch [466/938], Loss: 0.45344680547714233\n",
      "Validation: Epoch [11], Batch [467/938], Loss: 0.5061366558074951\n",
      "Validation: Epoch [11], Batch [468/938], Loss: 0.7067825794219971\n",
      "Validation: Epoch [11], Batch [469/938], Loss: 0.5349124670028687\n",
      "Validation: Epoch [11], Batch [470/938], Loss: 0.3716304898262024\n",
      "Validation: Epoch [11], Batch [471/938], Loss: 0.41362518072128296\n",
      "Validation: Epoch [11], Batch [472/938], Loss: 0.48634442687034607\n",
      "Validation: Epoch [11], Batch [473/938], Loss: 0.48855000734329224\n",
      "Validation: Epoch [11], Batch [474/938], Loss: 0.33681994676589966\n",
      "Validation: Epoch [11], Batch [475/938], Loss: 0.5321328639984131\n",
      "Validation: Epoch [11], Batch [476/938], Loss: 0.5599699020385742\n",
      "Validation: Epoch [11], Batch [477/938], Loss: 0.418251633644104\n",
      "Validation: Epoch [11], Batch [478/938], Loss: 0.6189057230949402\n",
      "Validation: Epoch [11], Batch [479/938], Loss: 0.4770885109901428\n",
      "Validation: Epoch [11], Batch [480/938], Loss: 0.5556532144546509\n",
      "Validation: Epoch [11], Batch [481/938], Loss: 0.3878907561302185\n",
      "Validation: Epoch [11], Batch [482/938], Loss: 0.5165477991104126\n",
      "Validation: Epoch [11], Batch [483/938], Loss: 0.3465796709060669\n",
      "Validation: Epoch [11], Batch [484/938], Loss: 0.39662760496139526\n",
      "Validation: Epoch [11], Batch [485/938], Loss: 0.7444570064544678\n",
      "Validation: Epoch [11], Batch [486/938], Loss: 0.4733844995498657\n",
      "Validation: Epoch [11], Batch [487/938], Loss: 0.5576796531677246\n",
      "Validation: Epoch [11], Batch [488/938], Loss: 0.44808799028396606\n",
      "Validation: Epoch [11], Batch [489/938], Loss: 0.49219900369644165\n",
      "Validation: Epoch [11], Batch [490/938], Loss: 0.44829022884368896\n",
      "Validation: Epoch [11], Batch [491/938], Loss: 0.40215304493904114\n",
      "Validation: Epoch [11], Batch [492/938], Loss: 0.55250483751297\n",
      "Validation: Epoch [11], Batch [493/938], Loss: 0.510452151298523\n",
      "Validation: Epoch [11], Batch [494/938], Loss: 0.3530687987804413\n",
      "Validation: Epoch [11], Batch [495/938], Loss: 0.36694973707199097\n",
      "Validation: Epoch [11], Batch [496/938], Loss: 0.6153239011764526\n",
      "Validation: Epoch [11], Batch [497/938], Loss: 0.5707634091377258\n",
      "Validation: Epoch [11], Batch [498/938], Loss: 0.39532744884490967\n",
      "Validation: Epoch [11], Batch [499/938], Loss: 0.4590393304824829\n",
      "Validation: Epoch [11], Batch [500/938], Loss: 0.3677929937839508\n",
      "Validation: Epoch [11], Batch [501/938], Loss: 0.38896554708480835\n",
      "Validation: Epoch [11], Batch [502/938], Loss: 0.5247208476066589\n",
      "Validation: Epoch [11], Batch [503/938], Loss: 0.5124420523643494\n",
      "Validation: Epoch [11], Batch [504/938], Loss: 0.360371470451355\n",
      "Validation: Epoch [11], Batch [505/938], Loss: 0.6858541965484619\n",
      "Validation: Epoch [11], Batch [506/938], Loss: 0.5150708556175232\n",
      "Validation: Epoch [11], Batch [507/938], Loss: 0.6050467491149902\n",
      "Validation: Epoch [11], Batch [508/938], Loss: 0.5481633543968201\n",
      "Validation: Epoch [11], Batch [509/938], Loss: 0.6042056679725647\n",
      "Validation: Epoch [11], Batch [510/938], Loss: 0.6043499708175659\n",
      "Validation: Epoch [11], Batch [511/938], Loss: 0.46539515256881714\n",
      "Validation: Epoch [11], Batch [512/938], Loss: 0.4247887432575226\n",
      "Validation: Epoch [11], Batch [513/938], Loss: 0.5740963220596313\n",
      "Validation: Epoch [11], Batch [514/938], Loss: 0.5145207643508911\n",
      "Validation: Epoch [11], Batch [515/938], Loss: 0.5251299142837524\n",
      "Validation: Epoch [11], Batch [516/938], Loss: 0.4250865578651428\n",
      "Validation: Epoch [11], Batch [517/938], Loss: 0.5431234240531921\n",
      "Validation: Epoch [11], Batch [518/938], Loss: 0.6754215955734253\n",
      "Validation: Epoch [11], Batch [519/938], Loss: 0.5297219157218933\n",
      "Validation: Epoch [11], Batch [520/938], Loss: 0.44568777084350586\n",
      "Validation: Epoch [11], Batch [521/938], Loss: 0.5480282306671143\n",
      "Validation: Epoch [11], Batch [522/938], Loss: 0.570832371711731\n",
      "Validation: Epoch [11], Batch [523/938], Loss: 0.4178798496723175\n",
      "Validation: Epoch [11], Batch [524/938], Loss: 0.4569963812828064\n",
      "Validation: Epoch [11], Batch [525/938], Loss: 0.3694911599159241\n",
      "Validation: Epoch [11], Batch [526/938], Loss: 0.5394701957702637\n",
      "Validation: Epoch [11], Batch [527/938], Loss: 0.4660312831401825\n",
      "Validation: Epoch [11], Batch [528/938], Loss: 0.45934703946113586\n",
      "Validation: Epoch [11], Batch [529/938], Loss: 0.46248355507850647\n",
      "Validation: Epoch [11], Batch [530/938], Loss: 0.3844066262245178\n",
      "Validation: Epoch [11], Batch [531/938], Loss: 0.5659440755844116\n",
      "Validation: Epoch [11], Batch [532/938], Loss: 0.3917260766029358\n",
      "Validation: Epoch [11], Batch [533/938], Loss: 0.4918648302555084\n",
      "Validation: Epoch [11], Batch [534/938], Loss: 0.47352978587150574\n",
      "Validation: Epoch [11], Batch [535/938], Loss: 0.3840910792350769\n",
      "Validation: Epoch [11], Batch [536/938], Loss: 0.6389021277427673\n",
      "Validation: Epoch [11], Batch [537/938], Loss: 0.6699893474578857\n",
      "Validation: Epoch [11], Batch [538/938], Loss: 0.6978003978729248\n",
      "Validation: Epoch [11], Batch [539/938], Loss: 0.33580920100212097\n",
      "Validation: Epoch [11], Batch [540/938], Loss: 0.4825618863105774\n",
      "Validation: Epoch [11], Batch [541/938], Loss: 0.33025068044662476\n",
      "Validation: Epoch [11], Batch [542/938], Loss: 0.397358238697052\n",
      "Validation: Epoch [11], Batch [543/938], Loss: 0.47830379009246826\n",
      "Validation: Epoch [11], Batch [544/938], Loss: 0.5401417016983032\n",
      "Validation: Epoch [11], Batch [545/938], Loss: 0.41395431756973267\n",
      "Validation: Epoch [11], Batch [546/938], Loss: 0.6752881407737732\n",
      "Validation: Epoch [11], Batch [547/938], Loss: 0.37303394079208374\n",
      "Validation: Epoch [11], Batch [548/938], Loss: 0.5170629024505615\n",
      "Validation: Epoch [11], Batch [549/938], Loss: 0.5626270174980164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [550/938], Loss: 0.46788862347602844\n",
      "Validation: Epoch [11], Batch [551/938], Loss: 0.5408380627632141\n",
      "Validation: Epoch [11], Batch [552/938], Loss: 0.6012369394302368\n",
      "Validation: Epoch [11], Batch [553/938], Loss: 0.41355010867118835\n",
      "Validation: Epoch [11], Batch [554/938], Loss: 0.3921493887901306\n",
      "Validation: Epoch [11], Batch [555/938], Loss: 0.4986102283000946\n",
      "Validation: Epoch [11], Batch [556/938], Loss: 0.4051796793937683\n",
      "Validation: Epoch [11], Batch [557/938], Loss: 0.3660629093647003\n",
      "Validation: Epoch [11], Batch [558/938], Loss: 0.8249063491821289\n",
      "Validation: Epoch [11], Batch [559/938], Loss: 0.34694498777389526\n",
      "Validation: Epoch [11], Batch [560/938], Loss: 0.44193220138549805\n",
      "Validation: Epoch [11], Batch [561/938], Loss: 0.3869175910949707\n",
      "Validation: Epoch [11], Batch [562/938], Loss: 0.4380711317062378\n",
      "Validation: Epoch [11], Batch [563/938], Loss: 0.4597378969192505\n",
      "Validation: Epoch [11], Batch [564/938], Loss: 0.47141319513320923\n",
      "Validation: Epoch [11], Batch [565/938], Loss: 0.4662620425224304\n",
      "Validation: Epoch [11], Batch [566/938], Loss: 0.35463809967041016\n",
      "Validation: Epoch [11], Batch [567/938], Loss: 0.6106572151184082\n",
      "Validation: Epoch [11], Batch [568/938], Loss: 0.4263332486152649\n",
      "Validation: Epoch [11], Batch [569/938], Loss: 0.40915560722351074\n",
      "Validation: Epoch [11], Batch [570/938], Loss: 0.6330313086509705\n",
      "Validation: Epoch [11], Batch [571/938], Loss: 0.4673548936843872\n",
      "Validation: Epoch [11], Batch [572/938], Loss: 0.4707659184932709\n",
      "Validation: Epoch [11], Batch [573/938], Loss: 0.5498058795928955\n",
      "Validation: Epoch [11], Batch [574/938], Loss: 0.5104024410247803\n",
      "Validation: Epoch [11], Batch [575/938], Loss: 0.5621470212936401\n",
      "Validation: Epoch [11], Batch [576/938], Loss: 0.513449490070343\n",
      "Validation: Epoch [11], Batch [577/938], Loss: 0.5300624370574951\n",
      "Validation: Epoch [11], Batch [578/938], Loss: 0.8689841032028198\n",
      "Validation: Epoch [11], Batch [579/938], Loss: 0.5105923414230347\n",
      "Validation: Epoch [11], Batch [580/938], Loss: 0.4450533390045166\n",
      "Validation: Epoch [11], Batch [581/938], Loss: 0.48933443427085876\n",
      "Validation: Epoch [11], Batch [582/938], Loss: 0.4767570495605469\n",
      "Validation: Epoch [11], Batch [583/938], Loss: 0.574256420135498\n",
      "Validation: Epoch [11], Batch [584/938], Loss: 0.5025229454040527\n",
      "Validation: Epoch [11], Batch [585/938], Loss: 0.4309123158454895\n",
      "Validation: Epoch [11], Batch [586/938], Loss: 0.5638461112976074\n",
      "Validation: Epoch [11], Batch [587/938], Loss: 0.6302632093429565\n",
      "Validation: Epoch [11], Batch [588/938], Loss: 0.4480852484703064\n",
      "Validation: Epoch [11], Batch [589/938], Loss: 0.47463756799697876\n",
      "Validation: Epoch [11], Batch [590/938], Loss: 0.5804443955421448\n",
      "Validation: Epoch [11], Batch [591/938], Loss: 0.43635597825050354\n",
      "Validation: Epoch [11], Batch [592/938], Loss: 0.7654333710670471\n",
      "Validation: Epoch [11], Batch [593/938], Loss: 0.4223497211933136\n",
      "Validation: Epoch [11], Batch [594/938], Loss: 0.5535462498664856\n",
      "Validation: Epoch [11], Batch [595/938], Loss: 0.5442171096801758\n",
      "Validation: Epoch [11], Batch [596/938], Loss: 0.45564696192741394\n",
      "Validation: Epoch [11], Batch [597/938], Loss: 0.5110664367675781\n",
      "Validation: Epoch [11], Batch [598/938], Loss: 0.335010290145874\n",
      "Validation: Epoch [11], Batch [599/938], Loss: 0.550746500492096\n",
      "Validation: Epoch [11], Batch [600/938], Loss: 0.6888576745986938\n",
      "Validation: Epoch [11], Batch [601/938], Loss: 0.6289812326431274\n",
      "Validation: Epoch [11], Batch [602/938], Loss: 0.3841492533683777\n",
      "Validation: Epoch [11], Batch [603/938], Loss: 0.7325472831726074\n",
      "Validation: Epoch [11], Batch [604/938], Loss: 0.5632221698760986\n",
      "Validation: Epoch [11], Batch [605/938], Loss: 0.5213706493377686\n",
      "Validation: Epoch [11], Batch [606/938], Loss: 0.6640591025352478\n",
      "Validation: Epoch [11], Batch [607/938], Loss: 0.4596022069454193\n",
      "Validation: Epoch [11], Batch [608/938], Loss: 0.6235836744308472\n",
      "Validation: Epoch [11], Batch [609/938], Loss: 0.42318999767303467\n",
      "Validation: Epoch [11], Batch [610/938], Loss: 0.3796834945678711\n",
      "Validation: Epoch [11], Batch [611/938], Loss: 0.8214741349220276\n",
      "Validation: Epoch [11], Batch [612/938], Loss: 0.5124564170837402\n",
      "Validation: Epoch [11], Batch [613/938], Loss: 0.43503129482269287\n",
      "Validation: Epoch [11], Batch [614/938], Loss: 0.405079185962677\n",
      "Validation: Epoch [11], Batch [615/938], Loss: 0.6100250482559204\n",
      "Validation: Epoch [11], Batch [616/938], Loss: 0.6609989404678345\n",
      "Validation: Epoch [11], Batch [617/938], Loss: 0.584315299987793\n",
      "Validation: Epoch [11], Batch [618/938], Loss: 0.46585384011268616\n",
      "Validation: Epoch [11], Batch [619/938], Loss: 0.6073766350746155\n",
      "Validation: Epoch [11], Batch [620/938], Loss: 0.4781869053840637\n",
      "Validation: Epoch [11], Batch [621/938], Loss: 0.4787846505641937\n",
      "Validation: Epoch [11], Batch [622/938], Loss: 0.5968116521835327\n",
      "Validation: Epoch [11], Batch [623/938], Loss: 0.5916297435760498\n",
      "Validation: Epoch [11], Batch [624/938], Loss: 0.6323152780532837\n",
      "Validation: Epoch [11], Batch [625/938], Loss: 0.42989617586135864\n",
      "Validation: Epoch [11], Batch [626/938], Loss: 0.33484017848968506\n",
      "Validation: Epoch [11], Batch [627/938], Loss: 0.5154715776443481\n",
      "Validation: Epoch [11], Batch [628/938], Loss: 0.6742841005325317\n",
      "Validation: Epoch [11], Batch [629/938], Loss: 0.6867072582244873\n",
      "Validation: Epoch [11], Batch [630/938], Loss: 0.42127710580825806\n",
      "Validation: Epoch [11], Batch [631/938], Loss: 0.7262403964996338\n",
      "Validation: Epoch [11], Batch [632/938], Loss: 0.5135853290557861\n",
      "Validation: Epoch [11], Batch [633/938], Loss: 0.46289461851119995\n",
      "Validation: Epoch [11], Batch [634/938], Loss: 0.3632785379886627\n",
      "Validation: Epoch [11], Batch [635/938], Loss: 0.39442306756973267\n",
      "Validation: Epoch [11], Batch [636/938], Loss: 0.47480714321136475\n",
      "Validation: Epoch [11], Batch [637/938], Loss: 0.48769235610961914\n",
      "Validation: Epoch [11], Batch [638/938], Loss: 0.5076383352279663\n",
      "Validation: Epoch [11], Batch [639/938], Loss: 0.44328612089157104\n",
      "Validation: Epoch [11], Batch [640/938], Loss: 0.6908242702484131\n",
      "Validation: Epoch [11], Batch [641/938], Loss: 0.45197609066963196\n",
      "Validation: Epoch [11], Batch [642/938], Loss: 0.37338370084762573\n",
      "Validation: Epoch [11], Batch [643/938], Loss: 0.4558834433555603\n",
      "Validation: Epoch [11], Batch [644/938], Loss: 0.43927431106567383\n",
      "Validation: Epoch [11], Batch [645/938], Loss: 0.41393914818763733\n",
      "Validation: Epoch [11], Batch [646/938], Loss: 0.4880079925060272\n",
      "Validation: Epoch [11], Batch [647/938], Loss: 0.4176834225654602\n",
      "Validation: Epoch [11], Batch [648/938], Loss: 0.40655630826950073\n",
      "Validation: Epoch [11], Batch [649/938], Loss: 0.5417982339859009\n",
      "Validation: Epoch [11], Batch [650/938], Loss: 0.4232426583766937\n",
      "Validation: Epoch [11], Batch [651/938], Loss: 0.5282098054885864\n",
      "Validation: Epoch [11], Batch [652/938], Loss: 0.4954339861869812\n",
      "Validation: Epoch [11], Batch [653/938], Loss: 0.33109384775161743\n",
      "Validation: Epoch [11], Batch [654/938], Loss: 0.8014701008796692\n",
      "Validation: Epoch [11], Batch [655/938], Loss: 0.7005751729011536\n",
      "Validation: Epoch [11], Batch [656/938], Loss: 0.9362229108810425\n",
      "Validation: Epoch [11], Batch [657/938], Loss: 0.48391780257225037\n",
      "Validation: Epoch [11], Batch [658/938], Loss: 0.461875319480896\n",
      "Validation: Epoch [11], Batch [659/938], Loss: 0.5158724784851074\n",
      "Validation: Epoch [11], Batch [660/938], Loss: 0.2860013246536255\n",
      "Validation: Epoch [11], Batch [661/938], Loss: 0.5588298439979553\n",
      "Validation: Epoch [11], Batch [662/938], Loss: 0.49008941650390625\n",
      "Validation: Epoch [11], Batch [663/938], Loss: 0.6910125613212585\n",
      "Validation: Epoch [11], Batch [664/938], Loss: 0.40274935960769653\n",
      "Validation: Epoch [11], Batch [665/938], Loss: 0.6404520869255066\n",
      "Validation: Epoch [11], Batch [666/938], Loss: 0.46902695298194885\n",
      "Validation: Epoch [11], Batch [667/938], Loss: 0.44882863759994507\n",
      "Validation: Epoch [11], Batch [668/938], Loss: 0.48058614134788513\n",
      "Validation: Epoch [11], Batch [669/938], Loss: 0.5038437247276306\n",
      "Validation: Epoch [11], Batch [670/938], Loss: 0.43927332758903503\n",
      "Validation: Epoch [11], Batch [671/938], Loss: 0.43018609285354614\n",
      "Validation: Epoch [11], Batch [672/938], Loss: 0.35938704013824463\n",
      "Validation: Epoch [11], Batch [673/938], Loss: 0.4867441952228546\n",
      "Validation: Epoch [11], Batch [674/938], Loss: 0.4136312007904053\n",
      "Validation: Epoch [11], Batch [675/938], Loss: 0.4425366520881653\n",
      "Validation: Epoch [11], Batch [676/938], Loss: 0.319313108921051\n",
      "Validation: Epoch [11], Batch [677/938], Loss: 0.5484312176704407\n",
      "Validation: Epoch [11], Batch [678/938], Loss: 0.5377086400985718\n",
      "Validation: Epoch [11], Batch [679/938], Loss: 0.3855304718017578\n",
      "Validation: Epoch [11], Batch [680/938], Loss: 0.4361133575439453\n",
      "Validation: Epoch [11], Batch [681/938], Loss: 0.36035826802253723\n",
      "Validation: Epoch [11], Batch [682/938], Loss: 0.4342714548110962\n",
      "Validation: Epoch [11], Batch [683/938], Loss: 0.49182406067848206\n",
      "Validation: Epoch [11], Batch [684/938], Loss: 0.45502036809921265\n",
      "Validation: Epoch [11], Batch [685/938], Loss: 0.5911504030227661\n",
      "Validation: Epoch [11], Batch [686/938], Loss: 0.4121776819229126\n",
      "Validation: Epoch [11], Batch [687/938], Loss: 0.6646069288253784\n",
      "Validation: Epoch [11], Batch [688/938], Loss: 0.5505712032318115\n",
      "Validation: Epoch [11], Batch [689/938], Loss: 0.5277093648910522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [690/938], Loss: 0.41345101594924927\n",
      "Validation: Epoch [11], Batch [691/938], Loss: 0.321321576833725\n",
      "Validation: Epoch [11], Batch [692/938], Loss: 0.5148980021476746\n",
      "Validation: Epoch [11], Batch [693/938], Loss: 0.36987990140914917\n",
      "Validation: Epoch [11], Batch [694/938], Loss: 0.5021861791610718\n",
      "Validation: Epoch [11], Batch [695/938], Loss: 0.3663865923881531\n",
      "Validation: Epoch [11], Batch [696/938], Loss: 0.46464911103248596\n",
      "Validation: Epoch [11], Batch [697/938], Loss: 0.4280466139316559\n",
      "Validation: Epoch [11], Batch [698/938], Loss: 0.5832356810569763\n",
      "Validation: Epoch [11], Batch [699/938], Loss: 0.661545991897583\n",
      "Validation: Epoch [11], Batch [700/938], Loss: 0.540393590927124\n",
      "Validation: Epoch [11], Batch [701/938], Loss: 0.32337528467178345\n",
      "Validation: Epoch [11], Batch [702/938], Loss: 0.6585873365402222\n",
      "Validation: Epoch [11], Batch [703/938], Loss: 0.5563554763793945\n",
      "Validation: Epoch [11], Batch [704/938], Loss: 0.4798763692378998\n",
      "Validation: Epoch [11], Batch [705/938], Loss: 0.6386021971702576\n",
      "Validation: Epoch [11], Batch [706/938], Loss: 0.48555988073349\n",
      "Validation: Epoch [11], Batch [707/938], Loss: 0.6187053918838501\n",
      "Validation: Epoch [11], Batch [708/938], Loss: 0.435985803604126\n",
      "Validation: Epoch [11], Batch [709/938], Loss: 0.4637482464313507\n",
      "Validation: Epoch [11], Batch [710/938], Loss: 0.7574608325958252\n",
      "Validation: Epoch [11], Batch [711/938], Loss: 0.46537092328071594\n",
      "Validation: Epoch [11], Batch [712/938], Loss: 0.36408188939094543\n",
      "Validation: Epoch [11], Batch [713/938], Loss: 0.44980815052986145\n",
      "Validation: Epoch [11], Batch [714/938], Loss: 0.651132345199585\n",
      "Validation: Epoch [11], Batch [715/938], Loss: 0.5711053609848022\n",
      "Validation: Epoch [11], Batch [716/938], Loss: 0.6974931955337524\n",
      "Validation: Epoch [11], Batch [717/938], Loss: 0.5237153768539429\n",
      "Validation: Epoch [11], Batch [718/938], Loss: 0.3996793031692505\n",
      "Validation: Epoch [11], Batch [719/938], Loss: 0.3606460690498352\n",
      "Validation: Epoch [11], Batch [720/938], Loss: 0.3754502236843109\n",
      "Validation: Epoch [11], Batch [721/938], Loss: 0.7163926362991333\n",
      "Validation: Epoch [11], Batch [722/938], Loss: 0.5616247057914734\n",
      "Validation: Epoch [11], Batch [723/938], Loss: 0.4399499297142029\n",
      "Validation: Epoch [11], Batch [724/938], Loss: 0.6789186000823975\n",
      "Validation: Epoch [11], Batch [725/938], Loss: 0.33474212884902954\n",
      "Validation: Epoch [11], Batch [726/938], Loss: 0.809601902961731\n",
      "Validation: Epoch [11], Batch [727/938], Loss: 0.656435489654541\n",
      "Validation: Epoch [11], Batch [728/938], Loss: 0.3510378301143646\n",
      "Validation: Epoch [11], Batch [729/938], Loss: 0.5285995006561279\n",
      "Validation: Epoch [11], Batch [730/938], Loss: 0.5796990990638733\n",
      "Validation: Epoch [11], Batch [731/938], Loss: 0.6695977449417114\n",
      "Validation: Epoch [11], Batch [732/938], Loss: 0.7157928943634033\n",
      "Validation: Epoch [11], Batch [733/938], Loss: 0.4805036187171936\n",
      "Validation: Epoch [11], Batch [734/938], Loss: 0.5561840534210205\n",
      "Validation: Epoch [11], Batch [735/938], Loss: 0.41944676637649536\n",
      "Validation: Epoch [11], Batch [736/938], Loss: 0.6150449514389038\n",
      "Validation: Epoch [11], Batch [737/938], Loss: 0.548244059085846\n",
      "Validation: Epoch [11], Batch [738/938], Loss: 0.5501502752304077\n",
      "Validation: Epoch [11], Batch [739/938], Loss: 0.588357150554657\n",
      "Validation: Epoch [11], Batch [740/938], Loss: 0.6145187616348267\n",
      "Validation: Epoch [11], Batch [741/938], Loss: 0.4441831111907959\n",
      "Validation: Epoch [11], Batch [742/938], Loss: 0.4650953412055969\n",
      "Validation: Epoch [11], Batch [743/938], Loss: 0.48080214858055115\n",
      "Validation: Epoch [11], Batch [744/938], Loss: 0.6882405281066895\n",
      "Validation: Epoch [11], Batch [745/938], Loss: 0.4788655638694763\n",
      "Validation: Epoch [11], Batch [746/938], Loss: 0.6122778654098511\n",
      "Validation: Epoch [11], Batch [747/938], Loss: 0.7425967454910278\n",
      "Validation: Epoch [11], Batch [748/938], Loss: 0.44034630060195923\n",
      "Validation: Epoch [11], Batch [749/938], Loss: 0.43866991996765137\n",
      "Validation: Epoch [11], Batch [750/938], Loss: 0.3840169906616211\n",
      "Validation: Epoch [11], Batch [751/938], Loss: 0.5090357065200806\n",
      "Validation: Epoch [11], Batch [752/938], Loss: 0.5230485200881958\n",
      "Validation: Epoch [11], Batch [753/938], Loss: 0.38156336545944214\n",
      "Validation: Epoch [11], Batch [754/938], Loss: 0.352465957403183\n",
      "Validation: Epoch [11], Batch [755/938], Loss: 0.6737809181213379\n",
      "Validation: Epoch [11], Batch [756/938], Loss: 0.4758659303188324\n",
      "Validation: Epoch [11], Batch [757/938], Loss: 0.41701561212539673\n",
      "Validation: Epoch [11], Batch [758/938], Loss: 0.4206199645996094\n",
      "Validation: Epoch [11], Batch [759/938], Loss: 0.6022903323173523\n",
      "Validation: Epoch [11], Batch [760/938], Loss: 0.4460914134979248\n",
      "Validation: Epoch [11], Batch [761/938], Loss: 0.4665299952030182\n",
      "Validation: Epoch [11], Batch [762/938], Loss: 0.37927737832069397\n",
      "Validation: Epoch [11], Batch [763/938], Loss: 0.723029613494873\n",
      "Validation: Epoch [11], Batch [764/938], Loss: 0.4075484871864319\n",
      "Validation: Epoch [11], Batch [765/938], Loss: 0.6358577013015747\n",
      "Validation: Epoch [11], Batch [766/938], Loss: 0.585413932800293\n",
      "Validation: Epoch [11], Batch [767/938], Loss: 0.572873592376709\n",
      "Validation: Epoch [11], Batch [768/938], Loss: 0.35899585485458374\n",
      "Validation: Epoch [11], Batch [769/938], Loss: 0.5518016815185547\n",
      "Validation: Epoch [11], Batch [770/938], Loss: 0.625058650970459\n",
      "Validation: Epoch [11], Batch [771/938], Loss: 0.4923446476459503\n",
      "Validation: Epoch [11], Batch [772/938], Loss: 0.3627920150756836\n",
      "Validation: Epoch [11], Batch [773/938], Loss: 0.702608048915863\n",
      "Validation: Epoch [11], Batch [774/938], Loss: 0.4481922686100006\n",
      "Validation: Epoch [11], Batch [775/938], Loss: 0.5192891359329224\n",
      "Validation: Epoch [11], Batch [776/938], Loss: 0.527055025100708\n",
      "Validation: Epoch [11], Batch [777/938], Loss: 0.6359390020370483\n",
      "Validation: Epoch [11], Batch [778/938], Loss: 0.7776007056236267\n",
      "Validation: Epoch [11], Batch [779/938], Loss: 0.37284374237060547\n",
      "Validation: Epoch [11], Batch [780/938], Loss: 0.5101339221000671\n",
      "Validation: Epoch [11], Batch [781/938], Loss: 0.3178099989891052\n",
      "Validation: Epoch [11], Batch [782/938], Loss: 0.5344573855400085\n",
      "Validation: Epoch [11], Batch [783/938], Loss: 0.4936832785606384\n",
      "Validation: Epoch [11], Batch [784/938], Loss: 0.4841669499874115\n",
      "Validation: Epoch [11], Batch [785/938], Loss: 0.4755306541919708\n",
      "Validation: Epoch [11], Batch [786/938], Loss: 0.6225538849830627\n",
      "Validation: Epoch [11], Batch [787/938], Loss: 0.3984130322933197\n",
      "Validation: Epoch [11], Batch [788/938], Loss: 0.39659854769706726\n",
      "Validation: Epoch [11], Batch [789/938], Loss: 0.40630242228507996\n",
      "Validation: Epoch [11], Batch [790/938], Loss: 0.46509605646133423\n",
      "Validation: Epoch [11], Batch [791/938], Loss: 0.5264579057693481\n",
      "Validation: Epoch [11], Batch [792/938], Loss: 0.544546365737915\n",
      "Validation: Epoch [11], Batch [793/938], Loss: 0.4740108251571655\n",
      "Validation: Epoch [11], Batch [794/938], Loss: 0.571047306060791\n",
      "Validation: Epoch [11], Batch [795/938], Loss: 0.460614949464798\n",
      "Validation: Epoch [11], Batch [796/938], Loss: 0.501757025718689\n",
      "Validation: Epoch [11], Batch [797/938], Loss: 0.6599322557449341\n",
      "Validation: Epoch [11], Batch [798/938], Loss: 0.6295583248138428\n",
      "Validation: Epoch [11], Batch [799/938], Loss: 0.49272748827934265\n",
      "Validation: Epoch [11], Batch [800/938], Loss: 0.45550549030303955\n",
      "Validation: Epoch [11], Batch [801/938], Loss: 0.4725513756275177\n",
      "Validation: Epoch [11], Batch [802/938], Loss: 0.5292947292327881\n",
      "Validation: Epoch [11], Batch [803/938], Loss: 0.6456621885299683\n",
      "Validation: Epoch [11], Batch [804/938], Loss: 0.5227757692337036\n",
      "Validation: Epoch [11], Batch [805/938], Loss: 0.3496173620223999\n",
      "Validation: Epoch [11], Batch [806/938], Loss: 0.3769516944885254\n",
      "Validation: Epoch [11], Batch [807/938], Loss: 0.5867986083030701\n",
      "Validation: Epoch [11], Batch [808/938], Loss: 0.47048401832580566\n",
      "Validation: Epoch [11], Batch [809/938], Loss: 0.39914393424987793\n",
      "Validation: Epoch [11], Batch [810/938], Loss: 0.5273852348327637\n",
      "Validation: Epoch [11], Batch [811/938], Loss: 0.48064208030700684\n",
      "Validation: Epoch [11], Batch [812/938], Loss: 0.6497175693511963\n",
      "Validation: Epoch [11], Batch [813/938], Loss: 0.7097522616386414\n",
      "Validation: Epoch [11], Batch [814/938], Loss: 0.44083693623542786\n",
      "Validation: Epoch [11], Batch [815/938], Loss: 0.6054379940032959\n",
      "Validation: Epoch [11], Batch [816/938], Loss: 0.29315268993377686\n",
      "Validation: Epoch [11], Batch [817/938], Loss: 0.4041230082511902\n",
      "Validation: Epoch [11], Batch [818/938], Loss: 0.38013455271720886\n",
      "Validation: Epoch [11], Batch [819/938], Loss: 0.560639500617981\n",
      "Validation: Epoch [11], Batch [820/938], Loss: 0.5363987684249878\n",
      "Validation: Epoch [11], Batch [821/938], Loss: 0.5151795148849487\n",
      "Validation: Epoch [11], Batch [822/938], Loss: 0.4463083744049072\n",
      "Validation: Epoch [11], Batch [823/938], Loss: 0.3790917992591858\n",
      "Validation: Epoch [11], Batch [824/938], Loss: 0.47532597184181213\n",
      "Validation: Epoch [11], Batch [825/938], Loss: 0.5866988897323608\n",
      "Validation: Epoch [11], Batch [826/938], Loss: 0.27328723669052124\n",
      "Validation: Epoch [11], Batch [827/938], Loss: 0.6813980340957642\n",
      "Validation: Epoch [11], Batch [828/938], Loss: 0.4478291869163513\n",
      "Validation: Epoch [11], Batch [829/938], Loss: 0.5365036129951477\n",
      "Validation: Epoch [11], Batch [830/938], Loss: 0.38631314039230347\n",
      "Validation: Epoch [11], Batch [831/938], Loss: 0.5509400367736816\n",
      "Validation: Epoch [11], Batch [832/938], Loss: 0.48785120248794556\n",
      "Validation: Epoch [11], Batch [833/938], Loss: 0.42108041048049927\n",
      "Validation: Epoch [11], Batch [834/938], Loss: 0.8450645804405212\n",
      "Validation: Epoch [11], Batch [835/938], Loss: 0.3946976661682129\n",
      "Validation: Epoch [11], Batch [836/938], Loss: 0.611537516117096\n",
      "Validation: Epoch [11], Batch [837/938], Loss: 0.5489281415939331\n",
      "Validation: Epoch [11], Batch [838/938], Loss: 0.5335286855697632\n",
      "Validation: Epoch [11], Batch [839/938], Loss: 0.4845699369907379\n",
      "Validation: Epoch [11], Batch [840/938], Loss: 0.5472769141197205\n",
      "Validation: Epoch [11], Batch [841/938], Loss: 0.3782903850078583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [11], Batch [842/938], Loss: 0.462859570980072\n",
      "Validation: Epoch [11], Batch [843/938], Loss: 0.49141472578048706\n",
      "Validation: Epoch [11], Batch [844/938], Loss: 0.4808701276779175\n",
      "Validation: Epoch [11], Batch [845/938], Loss: 0.5296328067779541\n",
      "Validation: Epoch [11], Batch [846/938], Loss: 0.5813676714897156\n",
      "Validation: Epoch [11], Batch [847/938], Loss: 0.7665064334869385\n",
      "Validation: Epoch [11], Batch [848/938], Loss: 0.388181209564209\n",
      "Validation: Epoch [11], Batch [849/938], Loss: 0.4616994857788086\n",
      "Validation: Epoch [11], Batch [850/938], Loss: 0.5276156663894653\n",
      "Validation: Epoch [11], Batch [851/938], Loss: 0.5162931084632874\n",
      "Validation: Epoch [11], Batch [852/938], Loss: 0.4541025757789612\n",
      "Validation: Epoch [11], Batch [853/938], Loss: 0.49670225381851196\n",
      "Validation: Epoch [11], Batch [854/938], Loss: 0.46762701869010925\n",
      "Validation: Epoch [11], Batch [855/938], Loss: 0.49924027919769287\n",
      "Validation: Epoch [11], Batch [856/938], Loss: 0.436024010181427\n",
      "Validation: Epoch [11], Batch [857/938], Loss: 0.46987384557724\n",
      "Validation: Epoch [11], Batch [858/938], Loss: 0.34000012278556824\n",
      "Validation: Epoch [11], Batch [859/938], Loss: 0.5553855895996094\n",
      "Validation: Epoch [11], Batch [860/938], Loss: 0.4034922122955322\n",
      "Validation: Epoch [11], Batch [861/938], Loss: 0.5017507672309875\n",
      "Validation: Epoch [11], Batch [862/938], Loss: 0.4679397940635681\n",
      "Validation: Epoch [11], Batch [863/938], Loss: 0.4319489598274231\n",
      "Validation: Epoch [11], Batch [864/938], Loss: 0.5045830607414246\n",
      "Validation: Epoch [11], Batch [865/938], Loss: 0.3392346501350403\n",
      "Validation: Epoch [11], Batch [866/938], Loss: 0.5107358694076538\n",
      "Validation: Epoch [11], Batch [867/938], Loss: 0.6796616911888123\n",
      "Validation: Epoch [11], Batch [868/938], Loss: 0.42506808042526245\n",
      "Validation: Epoch [11], Batch [869/938], Loss: 0.40439295768737793\n",
      "Validation: Epoch [11], Batch [870/938], Loss: 0.42963749170303345\n",
      "Validation: Epoch [11], Batch [871/938], Loss: 0.492959588766098\n",
      "Validation: Epoch [11], Batch [872/938], Loss: 0.41086259484291077\n",
      "Validation: Epoch [11], Batch [873/938], Loss: 0.5172350406646729\n",
      "Validation: Epoch [11], Batch [874/938], Loss: 0.4403836727142334\n",
      "Validation: Epoch [11], Batch [875/938], Loss: 0.7362425327301025\n",
      "Validation: Epoch [11], Batch [876/938], Loss: 0.47533929347991943\n",
      "Validation: Epoch [11], Batch [877/938], Loss: 0.48888880014419556\n",
      "Validation: Epoch [11], Batch [878/938], Loss: 0.4234057664871216\n",
      "Validation: Epoch [11], Batch [879/938], Loss: 0.7272102236747742\n",
      "Validation: Epoch [11], Batch [880/938], Loss: 0.49070873856544495\n",
      "Validation: Epoch [11], Batch [881/938], Loss: 0.30719828605651855\n",
      "Validation: Epoch [11], Batch [882/938], Loss: 0.3586227297782898\n",
      "Validation: Epoch [11], Batch [883/938], Loss: 0.5828672647476196\n",
      "Validation: Epoch [11], Batch [884/938], Loss: 0.510692298412323\n",
      "Validation: Epoch [11], Batch [885/938], Loss: 0.3625381588935852\n",
      "Validation: Epoch [11], Batch [886/938], Loss: 0.5344062447547913\n",
      "Validation: Epoch [11], Batch [887/938], Loss: 0.4062620997428894\n",
      "Validation: Epoch [11], Batch [888/938], Loss: 0.3972173035144806\n",
      "Validation: Epoch [11], Batch [889/938], Loss: 0.7460230588912964\n",
      "Validation: Epoch [11], Batch [890/938], Loss: 0.344036340713501\n",
      "Validation: Epoch [11], Batch [891/938], Loss: 0.41368013620376587\n",
      "Validation: Epoch [11], Batch [892/938], Loss: 0.435845285654068\n",
      "Validation: Epoch [11], Batch [893/938], Loss: 0.4255812168121338\n",
      "Validation: Epoch [11], Batch [894/938], Loss: 0.4022488296031952\n",
      "Validation: Epoch [11], Batch [895/938], Loss: 0.48643574118614197\n",
      "Validation: Epoch [11], Batch [896/938], Loss: 0.45675599575042725\n",
      "Validation: Epoch [11], Batch [897/938], Loss: 0.7112112045288086\n",
      "Validation: Epoch [11], Batch [898/938], Loss: 0.3666071891784668\n",
      "Validation: Epoch [11], Batch [899/938], Loss: 0.4046526253223419\n",
      "Validation: Epoch [11], Batch [900/938], Loss: 0.4892323315143585\n",
      "Validation: Epoch [11], Batch [901/938], Loss: 0.5636806488037109\n",
      "Validation: Epoch [11], Batch [902/938], Loss: 0.6320360898971558\n",
      "Validation: Epoch [11], Batch [903/938], Loss: 0.3932598829269409\n",
      "Validation: Epoch [11], Batch [904/938], Loss: 0.3620849847793579\n",
      "Validation: Epoch [11], Batch [905/938], Loss: 0.41791900992393494\n",
      "Validation: Epoch [11], Batch [906/938], Loss: 0.5018627643585205\n",
      "Validation: Epoch [11], Batch [907/938], Loss: 0.41270774602890015\n",
      "Validation: Epoch [11], Batch [908/938], Loss: 0.4573921263217926\n",
      "Validation: Epoch [11], Batch [909/938], Loss: 0.4699357748031616\n",
      "Validation: Epoch [11], Batch [910/938], Loss: 0.5470195412635803\n",
      "Validation: Epoch [11], Batch [911/938], Loss: 0.4129725694656372\n",
      "Validation: Epoch [11], Batch [912/938], Loss: 0.5866824388504028\n",
      "Validation: Epoch [11], Batch [913/938], Loss: 0.5209068655967712\n",
      "Validation: Epoch [11], Batch [914/938], Loss: 0.32606056332588196\n",
      "Validation: Epoch [11], Batch [915/938], Loss: 0.57923424243927\n",
      "Validation: Epoch [11], Batch [916/938], Loss: 0.503795862197876\n",
      "Validation: Epoch [11], Batch [917/938], Loss: 0.5289177298545837\n",
      "Validation: Epoch [11], Batch [918/938], Loss: 0.7178871631622314\n",
      "Validation: Epoch [11], Batch [919/938], Loss: 0.3122780919075012\n",
      "Validation: Epoch [11], Batch [920/938], Loss: 0.5029973983764648\n",
      "Validation: Epoch [11], Batch [921/938], Loss: 0.6317362189292908\n",
      "Validation: Epoch [11], Batch [922/938], Loss: 0.511618435382843\n",
      "Validation: Epoch [11], Batch [923/938], Loss: 0.5140001773834229\n",
      "Validation: Epoch [11], Batch [924/938], Loss: 0.5170061588287354\n",
      "Validation: Epoch [11], Batch [925/938], Loss: 0.628652811050415\n",
      "Validation: Epoch [11], Batch [926/938], Loss: 0.46755751967430115\n",
      "Validation: Epoch [11], Batch [927/938], Loss: 0.676372766494751\n",
      "Validation: Epoch [11], Batch [928/938], Loss: 0.5379242897033691\n",
      "Validation: Epoch [11], Batch [929/938], Loss: 0.5983771085739136\n",
      "Validation: Epoch [11], Batch [930/938], Loss: 0.5099178552627563\n",
      "Validation: Epoch [11], Batch [931/938], Loss: 0.49555063247680664\n",
      "Validation: Epoch [11], Batch [932/938], Loss: 0.5493415594100952\n",
      "Validation: Epoch [11], Batch [933/938], Loss: 0.5709490180015564\n",
      "Validation: Epoch [11], Batch [934/938], Loss: 0.49506670236587524\n",
      "Validation: Epoch [11], Batch [935/938], Loss: 0.5048948526382446\n",
      "Validation: Epoch [11], Batch [936/938], Loss: 0.4216192364692688\n",
      "Validation: Epoch [11], Batch [937/938], Loss: 0.4768274426460266\n",
      "Validation: Epoch [11], Batch [938/938], Loss: 0.2890191972255707\n",
      "Accuracy of test set: 0.8266333333333333\n",
      "Train: Epoch [12], Batch [1/938], Loss: 0.6783577799797058\n",
      "Train: Epoch [12], Batch [2/938], Loss: 0.5514482259750366\n",
      "Train: Epoch [12], Batch [3/938], Loss: 0.5785712003707886\n",
      "Train: Epoch [12], Batch [4/938], Loss: 0.45096704363822937\n",
      "Train: Epoch [12], Batch [5/938], Loss: 0.4956095516681671\n",
      "Train: Epoch [12], Batch [6/938], Loss: 0.5236520171165466\n",
      "Train: Epoch [12], Batch [7/938], Loss: 0.6174180507659912\n",
      "Train: Epoch [12], Batch [8/938], Loss: 0.3716072738170624\n",
      "Train: Epoch [12], Batch [9/938], Loss: 0.7171560525894165\n",
      "Train: Epoch [12], Batch [10/938], Loss: 0.6746044158935547\n",
      "Train: Epoch [12], Batch [11/938], Loss: 0.4557662308216095\n",
      "Train: Epoch [12], Batch [12/938], Loss: 0.5112579464912415\n",
      "Train: Epoch [12], Batch [13/938], Loss: 0.4251149296760559\n",
      "Train: Epoch [12], Batch [14/938], Loss: 0.49385106563568115\n",
      "Train: Epoch [12], Batch [15/938], Loss: 0.5435675382614136\n",
      "Train: Epoch [12], Batch [16/938], Loss: 0.4656633734703064\n",
      "Train: Epoch [12], Batch [17/938], Loss: 0.4495396018028259\n",
      "Train: Epoch [12], Batch [18/938], Loss: 0.6162114143371582\n",
      "Train: Epoch [12], Batch [19/938], Loss: 0.4021758437156677\n",
      "Train: Epoch [12], Batch [20/938], Loss: 0.3758334517478943\n",
      "Train: Epoch [12], Batch [21/938], Loss: 0.5262401103973389\n",
      "Train: Epoch [12], Batch [22/938], Loss: 0.5083192586898804\n",
      "Train: Epoch [12], Batch [23/938], Loss: 0.6392135620117188\n",
      "Train: Epoch [12], Batch [24/938], Loss: 0.617232084274292\n",
      "Train: Epoch [12], Batch [25/938], Loss: 0.48343953490257263\n",
      "Train: Epoch [12], Batch [26/938], Loss: 0.4133814573287964\n",
      "Train: Epoch [12], Batch [27/938], Loss: 0.4221133589744568\n",
      "Train: Epoch [12], Batch [28/938], Loss: 0.4608186185359955\n",
      "Train: Epoch [12], Batch [29/938], Loss: 0.5586225986480713\n",
      "Train: Epoch [12], Batch [30/938], Loss: 0.48220932483673096\n",
      "Train: Epoch [12], Batch [31/938], Loss: 0.5910173654556274\n",
      "Train: Epoch [12], Batch [32/938], Loss: 0.4501500725746155\n",
      "Train: Epoch [12], Batch [33/938], Loss: 0.505933403968811\n",
      "Train: Epoch [12], Batch [34/938], Loss: 0.42517730593681335\n",
      "Train: Epoch [12], Batch [35/938], Loss: 0.48651427030563354\n",
      "Train: Epoch [12], Batch [36/938], Loss: 0.3948994278907776\n",
      "Train: Epoch [12], Batch [37/938], Loss: 0.4546980559825897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [38/938], Loss: 0.536941409111023\n",
      "Train: Epoch [12], Batch [39/938], Loss: 0.5786120891571045\n",
      "Train: Epoch [12], Batch [40/938], Loss: 0.5230151414871216\n",
      "Train: Epoch [12], Batch [41/938], Loss: 0.5331698656082153\n",
      "Train: Epoch [12], Batch [42/938], Loss: 0.6995043754577637\n",
      "Train: Epoch [12], Batch [43/938], Loss: 0.4831959903240204\n",
      "Train: Epoch [12], Batch [44/938], Loss: 0.4258042275905609\n",
      "Train: Epoch [12], Batch [45/938], Loss: 0.7741563320159912\n",
      "Train: Epoch [12], Batch [46/938], Loss: 0.4919806718826294\n",
      "Train: Epoch [12], Batch [47/938], Loss: 0.39951467514038086\n",
      "Train: Epoch [12], Batch [48/938], Loss: 0.5989686250686646\n",
      "Train: Epoch [12], Batch [49/938], Loss: 0.7931638956069946\n",
      "Train: Epoch [12], Batch [50/938], Loss: 0.7073498964309692\n",
      "Train: Epoch [12], Batch [51/938], Loss: 0.328111857175827\n",
      "Train: Epoch [12], Batch [52/938], Loss: 0.42042115330696106\n",
      "Train: Epoch [12], Batch [53/938], Loss: 0.4185527563095093\n",
      "Train: Epoch [12], Batch [54/938], Loss: 0.478750079870224\n",
      "Train: Epoch [12], Batch [55/938], Loss: 0.4061998724937439\n",
      "Train: Epoch [12], Batch [56/938], Loss: 0.5631723403930664\n",
      "Train: Epoch [12], Batch [57/938], Loss: 0.5863538384437561\n",
      "Train: Epoch [12], Batch [58/938], Loss: 0.8593989610671997\n",
      "Train: Epoch [12], Batch [59/938], Loss: 0.5597352981567383\n",
      "Train: Epoch [12], Batch [60/938], Loss: 0.49515703320503235\n",
      "Train: Epoch [12], Batch [61/938], Loss: 0.5317821502685547\n",
      "Train: Epoch [12], Batch [62/938], Loss: 0.5240845084190369\n",
      "Train: Epoch [12], Batch [63/938], Loss: 0.4621734023094177\n",
      "Train: Epoch [12], Batch [64/938], Loss: 0.509492039680481\n",
      "Train: Epoch [12], Batch [65/938], Loss: 0.4732406735420227\n",
      "Train: Epoch [12], Batch [66/938], Loss: 0.6250032186508179\n",
      "Train: Epoch [12], Batch [67/938], Loss: 0.6412210464477539\n",
      "Train: Epoch [12], Batch [68/938], Loss: 0.47726327180862427\n",
      "Train: Epoch [12], Batch [69/938], Loss: 0.585598349571228\n",
      "Train: Epoch [12], Batch [70/938], Loss: 0.6405818462371826\n",
      "Train: Epoch [12], Batch [71/938], Loss: 0.3457966446876526\n",
      "Train: Epoch [12], Batch [72/938], Loss: 0.49473559856414795\n",
      "Train: Epoch [12], Batch [73/938], Loss: 0.5987744331359863\n",
      "Train: Epoch [12], Batch [74/938], Loss: 0.37895625829696655\n",
      "Train: Epoch [12], Batch [75/938], Loss: 0.5024355053901672\n",
      "Train: Epoch [12], Batch [76/938], Loss: 0.4725225567817688\n",
      "Train: Epoch [12], Batch [77/938], Loss: 0.36664652824401855\n",
      "Train: Epoch [12], Batch [78/938], Loss: 0.23695962131023407\n",
      "Train: Epoch [12], Batch [79/938], Loss: 0.5595188736915588\n",
      "Train: Epoch [12], Batch [80/938], Loss: 0.31560561060905457\n",
      "Train: Epoch [12], Batch [81/938], Loss: 0.43363863229751587\n",
      "Train: Epoch [12], Batch [82/938], Loss: 0.5036976337432861\n",
      "Train: Epoch [12], Batch [83/938], Loss: 0.47171348333358765\n",
      "Train: Epoch [12], Batch [84/938], Loss: 0.5226089954376221\n",
      "Train: Epoch [12], Batch [85/938], Loss: 0.7265380620956421\n",
      "Train: Epoch [12], Batch [86/938], Loss: 0.45078837871551514\n",
      "Train: Epoch [12], Batch [87/938], Loss: 0.39618349075317383\n",
      "Train: Epoch [12], Batch [88/938], Loss: 0.3838520348072052\n",
      "Train: Epoch [12], Batch [89/938], Loss: 0.3457525372505188\n",
      "Train: Epoch [12], Batch [90/938], Loss: 0.8129827976226807\n",
      "Train: Epoch [12], Batch [91/938], Loss: 0.5667283535003662\n",
      "Train: Epoch [12], Batch [92/938], Loss: 0.7145434617996216\n",
      "Train: Epoch [12], Batch [93/938], Loss: 0.4723610579967499\n",
      "Train: Epoch [12], Batch [94/938], Loss: 0.47925668954849243\n",
      "Train: Epoch [12], Batch [95/938], Loss: 0.5518303513526917\n",
      "Train: Epoch [12], Batch [96/938], Loss: 0.4936618506908417\n",
      "Train: Epoch [12], Batch [97/938], Loss: 0.43864789605140686\n",
      "Train: Epoch [12], Batch [98/938], Loss: 0.5576508045196533\n",
      "Train: Epoch [12], Batch [99/938], Loss: 0.5219672918319702\n",
      "Train: Epoch [12], Batch [100/938], Loss: 0.32739049196243286\n",
      "Train: Epoch [12], Batch [101/938], Loss: 0.4501075744628906\n",
      "Train: Epoch [12], Batch [102/938], Loss: 0.4992940425872803\n",
      "Train: Epoch [12], Batch [103/938], Loss: 0.41718602180480957\n",
      "Train: Epoch [12], Batch [104/938], Loss: 0.5492835640907288\n",
      "Train: Epoch [12], Batch [105/938], Loss: 0.5068333745002747\n",
      "Train: Epoch [12], Batch [106/938], Loss: 0.4460223317146301\n",
      "Train: Epoch [12], Batch [107/938], Loss: 0.7084908485412598\n",
      "Train: Epoch [12], Batch [108/938], Loss: 0.39499354362487793\n",
      "Train: Epoch [12], Batch [109/938], Loss: 0.5587005615234375\n",
      "Train: Epoch [12], Batch [110/938], Loss: 0.26818668842315674\n",
      "Train: Epoch [12], Batch [111/938], Loss: 0.3473671078681946\n",
      "Train: Epoch [12], Batch [112/938], Loss: 0.6846628189086914\n",
      "Train: Epoch [12], Batch [113/938], Loss: 0.5384464263916016\n",
      "Train: Epoch [12], Batch [114/938], Loss: 0.5473753809928894\n",
      "Train: Epoch [12], Batch [115/938], Loss: 0.527808427810669\n",
      "Train: Epoch [12], Batch [116/938], Loss: 0.5219595432281494\n",
      "Train: Epoch [12], Batch [117/938], Loss: 0.3004249930381775\n",
      "Train: Epoch [12], Batch [118/938], Loss: 0.49242979288101196\n",
      "Train: Epoch [12], Batch [119/938], Loss: 0.490378201007843\n",
      "Train: Epoch [12], Batch [120/938], Loss: 0.5985051989555359\n",
      "Train: Epoch [12], Batch [121/938], Loss: 0.44433140754699707\n",
      "Train: Epoch [12], Batch [122/938], Loss: 0.839122474193573\n",
      "Train: Epoch [12], Batch [123/938], Loss: 0.6465678215026855\n",
      "Train: Epoch [12], Batch [124/938], Loss: 0.5807651281356812\n",
      "Train: Epoch [12], Batch [125/938], Loss: 0.5481265783309937\n",
      "Train: Epoch [12], Batch [126/938], Loss: 0.5154052972793579\n",
      "Train: Epoch [12], Batch [127/938], Loss: 0.5773839950561523\n",
      "Train: Epoch [12], Batch [128/938], Loss: 0.30474328994750977\n",
      "Train: Epoch [12], Batch [129/938], Loss: 0.5534327030181885\n",
      "Train: Epoch [12], Batch [130/938], Loss: 0.6471625566482544\n",
      "Train: Epoch [12], Batch [131/938], Loss: 0.49699464440345764\n",
      "Train: Epoch [12], Batch [132/938], Loss: 0.642750084400177\n",
      "Train: Epoch [12], Batch [133/938], Loss: 0.5554090738296509\n",
      "Train: Epoch [12], Batch [134/938], Loss: 0.4147379398345947\n",
      "Train: Epoch [12], Batch [135/938], Loss: 0.555511474609375\n",
      "Train: Epoch [12], Batch [136/938], Loss: 0.429162859916687\n",
      "Train: Epoch [12], Batch [137/938], Loss: 0.49703001976013184\n",
      "Train: Epoch [12], Batch [138/938], Loss: 0.42573994398117065\n",
      "Train: Epoch [12], Batch [139/938], Loss: 0.4698669910430908\n",
      "Train: Epoch [12], Batch [140/938], Loss: 0.5339210033416748\n",
      "Train: Epoch [12], Batch [141/938], Loss: 0.3314034044742584\n",
      "Train: Epoch [12], Batch [142/938], Loss: 0.49067676067352295\n",
      "Train: Epoch [12], Batch [143/938], Loss: 0.5803476572036743\n",
      "Train: Epoch [12], Batch [144/938], Loss: 0.5801357626914978\n",
      "Train: Epoch [12], Batch [145/938], Loss: 0.48197686672210693\n",
      "Train: Epoch [12], Batch [146/938], Loss: 0.5358428955078125\n",
      "Train: Epoch [12], Batch [147/938], Loss: 0.33727383613586426\n",
      "Train: Epoch [12], Batch [148/938], Loss: 0.4604235887527466\n",
      "Train: Epoch [12], Batch [149/938], Loss: 0.7701694965362549\n",
      "Train: Epoch [12], Batch [150/938], Loss: 0.5534135103225708\n",
      "Train: Epoch [12], Batch [151/938], Loss: 0.6470270752906799\n",
      "Train: Epoch [12], Batch [152/938], Loss: 0.4887145161628723\n",
      "Train: Epoch [12], Batch [153/938], Loss: 0.41891735792160034\n",
      "Train: Epoch [12], Batch [154/938], Loss: 0.5385550260543823\n",
      "Train: Epoch [12], Batch [155/938], Loss: 0.5202609896659851\n",
      "Train: Epoch [12], Batch [156/938], Loss: 0.5379165410995483\n",
      "Train: Epoch [12], Batch [157/938], Loss: 0.4495859742164612\n",
      "Train: Epoch [12], Batch [158/938], Loss: 0.5541958808898926\n",
      "Train: Epoch [12], Batch [159/938], Loss: 0.5734778642654419\n",
      "Train: Epoch [12], Batch [160/938], Loss: 0.4085102677345276\n",
      "Train: Epoch [12], Batch [161/938], Loss: 0.5851873755455017\n",
      "Train: Epoch [12], Batch [162/938], Loss: 0.4375501573085785\n",
      "Train: Epoch [12], Batch [163/938], Loss: 0.37496471405029297\n",
      "Train: Epoch [12], Batch [164/938], Loss: 0.38980740308761597\n",
      "Train: Epoch [12], Batch [165/938], Loss: 0.7036679983139038\n",
      "Train: Epoch [12], Batch [166/938], Loss: 0.295076459646225\n",
      "Train: Epoch [12], Batch [167/938], Loss: 0.6093108057975769\n",
      "Train: Epoch [12], Batch [168/938], Loss: 0.4281386733055115\n",
      "Train: Epoch [12], Batch [169/938], Loss: 0.6520397067070007\n",
      "Train: Epoch [12], Batch [170/938], Loss: 0.6056909561157227\n",
      "Train: Epoch [12], Batch [171/938], Loss: 0.7189453840255737\n",
      "Train: Epoch [12], Batch [172/938], Loss: 0.5787082314491272\n",
      "Train: Epoch [12], Batch [173/938], Loss: 0.33571597933769226\n",
      "Train: Epoch [12], Batch [174/938], Loss: 0.6244978904724121\n",
      "Train: Epoch [12], Batch [175/938], Loss: 0.40012162923812866\n",
      "Train: Epoch [12], Batch [176/938], Loss: 0.5022914409637451\n",
      "Train: Epoch [12], Batch [177/938], Loss: 0.6567074656486511\n",
      "Train: Epoch [12], Batch [178/938], Loss: 0.4467853307723999\n",
      "Train: Epoch [12], Batch [179/938], Loss: 0.5227028131484985\n",
      "Train: Epoch [12], Batch [180/938], Loss: 0.5279536247253418\n",
      "Train: Epoch [12], Batch [181/938], Loss: 0.5174163579940796\n",
      "Train: Epoch [12], Batch [182/938], Loss: 0.40131980180740356\n",
      "Train: Epoch [12], Batch [183/938], Loss: 0.49041086435317993\n",
      "Train: Epoch [12], Batch [184/938], Loss: 0.444155216217041\n",
      "Train: Epoch [12], Batch [185/938], Loss: 0.45031824707984924\n",
      "Train: Epoch [12], Batch [186/938], Loss: 0.5409709215164185\n",
      "Train: Epoch [12], Batch [187/938], Loss: 0.6456876993179321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [188/938], Loss: 0.5046017169952393\n",
      "Train: Epoch [12], Batch [189/938], Loss: 0.45715636014938354\n",
      "Train: Epoch [12], Batch [190/938], Loss: 0.5622272491455078\n",
      "Train: Epoch [12], Batch [191/938], Loss: 0.4204983115196228\n",
      "Train: Epoch [12], Batch [192/938], Loss: 0.4803301990032196\n",
      "Train: Epoch [12], Batch [193/938], Loss: 0.49152010679244995\n",
      "Train: Epoch [12], Batch [194/938], Loss: 0.6064703464508057\n",
      "Train: Epoch [12], Batch [195/938], Loss: 0.4723004102706909\n",
      "Train: Epoch [12], Batch [196/938], Loss: 0.5041561126708984\n",
      "Train: Epoch [12], Batch [197/938], Loss: 0.3152044713497162\n",
      "Train: Epoch [12], Batch [198/938], Loss: 0.3908877968788147\n",
      "Train: Epoch [12], Batch [199/938], Loss: 0.45482227206230164\n",
      "Train: Epoch [12], Batch [200/938], Loss: 0.6284310221672058\n",
      "Train: Epoch [12], Batch [201/938], Loss: 0.4714936912059784\n",
      "Train: Epoch [12], Batch [202/938], Loss: 0.5569655299186707\n",
      "Train: Epoch [12], Batch [203/938], Loss: 0.4865032732486725\n",
      "Train: Epoch [12], Batch [204/938], Loss: 0.5366341471672058\n",
      "Train: Epoch [12], Batch [205/938], Loss: 0.7559535503387451\n",
      "Train: Epoch [12], Batch [206/938], Loss: 0.5004541277885437\n",
      "Train: Epoch [12], Batch [207/938], Loss: 0.5625472664833069\n",
      "Train: Epoch [12], Batch [208/938], Loss: 0.38900214433670044\n",
      "Train: Epoch [12], Batch [209/938], Loss: 0.5561661124229431\n",
      "Train: Epoch [12], Batch [210/938], Loss: 0.3688769042491913\n",
      "Train: Epoch [12], Batch [211/938], Loss: 0.3581864833831787\n",
      "Train: Epoch [12], Batch [212/938], Loss: 0.4663677215576172\n",
      "Train: Epoch [12], Batch [213/938], Loss: 0.42663854360580444\n",
      "Train: Epoch [12], Batch [214/938], Loss: 0.5869817733764648\n",
      "Train: Epoch [12], Batch [215/938], Loss: 0.4168885350227356\n",
      "Train: Epoch [12], Batch [216/938], Loss: 0.6415871381759644\n",
      "Train: Epoch [12], Batch [217/938], Loss: 0.5973800420761108\n",
      "Train: Epoch [12], Batch [218/938], Loss: 0.6322438716888428\n",
      "Train: Epoch [12], Batch [219/938], Loss: 0.4564148187637329\n",
      "Train: Epoch [12], Batch [220/938], Loss: 0.38985416293144226\n",
      "Train: Epoch [12], Batch [221/938], Loss: 0.5150750875473022\n",
      "Train: Epoch [12], Batch [222/938], Loss: 0.6333067417144775\n",
      "Train: Epoch [12], Batch [223/938], Loss: 0.4715609550476074\n",
      "Train: Epoch [12], Batch [224/938], Loss: 0.6831088066101074\n",
      "Train: Epoch [12], Batch [225/938], Loss: 0.5272680521011353\n",
      "Train: Epoch [12], Batch [226/938], Loss: 0.646538257598877\n",
      "Train: Epoch [12], Batch [227/938], Loss: 0.3598470687866211\n",
      "Train: Epoch [12], Batch [228/938], Loss: 0.4374029040336609\n",
      "Train: Epoch [12], Batch [229/938], Loss: 0.42935648560523987\n",
      "Train: Epoch [12], Batch [230/938], Loss: 0.4718552231788635\n",
      "Train: Epoch [12], Batch [231/938], Loss: 0.5980229377746582\n",
      "Train: Epoch [12], Batch [232/938], Loss: 0.49297967553138733\n",
      "Train: Epoch [12], Batch [233/938], Loss: 0.6058790683746338\n",
      "Train: Epoch [12], Batch [234/938], Loss: 0.49644067883491516\n",
      "Train: Epoch [12], Batch [235/938], Loss: 0.5178995132446289\n",
      "Train: Epoch [12], Batch [236/938], Loss: 0.7017443180084229\n",
      "Train: Epoch [12], Batch [237/938], Loss: 0.3955777883529663\n",
      "Train: Epoch [12], Batch [238/938], Loss: 0.7056909799575806\n",
      "Train: Epoch [12], Batch [239/938], Loss: 0.48871147632598877\n",
      "Train: Epoch [12], Batch [240/938], Loss: 0.38204020261764526\n",
      "Train: Epoch [12], Batch [241/938], Loss: 0.49407321214675903\n",
      "Train: Epoch [12], Batch [242/938], Loss: 0.42101389169692993\n",
      "Train: Epoch [12], Batch [243/938], Loss: 0.6383049488067627\n",
      "Train: Epoch [12], Batch [244/938], Loss: 0.6457918286323547\n",
      "Train: Epoch [12], Batch [245/938], Loss: 0.5182960629463196\n",
      "Train: Epoch [12], Batch [246/938], Loss: 0.5838385820388794\n",
      "Train: Epoch [12], Batch [247/938], Loss: 0.6698299646377563\n",
      "Train: Epoch [12], Batch [248/938], Loss: 0.5107929706573486\n",
      "Train: Epoch [12], Batch [249/938], Loss: 0.4592198431491852\n",
      "Train: Epoch [12], Batch [250/938], Loss: 0.4404968321323395\n",
      "Train: Epoch [12], Batch [251/938], Loss: 0.47250908613204956\n",
      "Train: Epoch [12], Batch [252/938], Loss: 0.5153916478157043\n",
      "Train: Epoch [12], Batch [253/938], Loss: 0.4505815804004669\n",
      "Train: Epoch [12], Batch [254/938], Loss: 0.5261878371238708\n",
      "Train: Epoch [12], Batch [255/938], Loss: 0.3882334530353546\n",
      "Train: Epoch [12], Batch [256/938], Loss: 0.5712532997131348\n",
      "Train: Epoch [12], Batch [257/938], Loss: 0.44677427411079407\n",
      "Train: Epoch [12], Batch [258/938], Loss: 0.4131542146205902\n",
      "Train: Epoch [12], Batch [259/938], Loss: 0.5985076427459717\n",
      "Train: Epoch [12], Batch [260/938], Loss: 0.6374933123588562\n",
      "Train: Epoch [12], Batch [261/938], Loss: 0.6556684970855713\n",
      "Train: Epoch [12], Batch [262/938], Loss: 0.5856341123580933\n",
      "Train: Epoch [12], Batch [263/938], Loss: 0.6664674878120422\n",
      "Train: Epoch [12], Batch [264/938], Loss: 0.5726106762886047\n",
      "Train: Epoch [12], Batch [265/938], Loss: 0.7069366574287415\n",
      "Train: Epoch [12], Batch [266/938], Loss: 0.44713592529296875\n",
      "Train: Epoch [12], Batch [267/938], Loss: 0.597113847732544\n",
      "Train: Epoch [12], Batch [268/938], Loss: 0.5767838358879089\n",
      "Train: Epoch [12], Batch [269/938], Loss: 0.40942034125328064\n",
      "Train: Epoch [12], Batch [270/938], Loss: 0.3829035460948944\n",
      "Train: Epoch [12], Batch [271/938], Loss: 0.497664213180542\n",
      "Train: Epoch [12], Batch [272/938], Loss: 0.5554976463317871\n",
      "Train: Epoch [12], Batch [273/938], Loss: 0.4464532136917114\n",
      "Train: Epoch [12], Batch [274/938], Loss: 0.5237026214599609\n",
      "Train: Epoch [12], Batch [275/938], Loss: 0.41486072540283203\n",
      "Train: Epoch [12], Batch [276/938], Loss: 0.5156213641166687\n",
      "Train: Epoch [12], Batch [277/938], Loss: 0.6800826787948608\n",
      "Train: Epoch [12], Batch [278/938], Loss: 0.47435981035232544\n",
      "Train: Epoch [12], Batch [279/938], Loss: 0.633952260017395\n",
      "Train: Epoch [12], Batch [280/938], Loss: 0.5286374688148499\n",
      "Train: Epoch [12], Batch [281/938], Loss: 0.7483391761779785\n",
      "Train: Epoch [12], Batch [282/938], Loss: 0.4818549156188965\n",
      "Train: Epoch [12], Batch [283/938], Loss: 0.5324068069458008\n",
      "Train: Epoch [12], Batch [284/938], Loss: 0.5178908109664917\n",
      "Train: Epoch [12], Batch [285/938], Loss: 0.4731895327568054\n",
      "Train: Epoch [12], Batch [286/938], Loss: 0.6681636571884155\n",
      "Train: Epoch [12], Batch [287/938], Loss: 0.5452306866645813\n",
      "Train: Epoch [12], Batch [288/938], Loss: 0.5575607419013977\n",
      "Train: Epoch [12], Batch [289/938], Loss: 0.6034095883369446\n",
      "Train: Epoch [12], Batch [290/938], Loss: 0.5990751385688782\n",
      "Train: Epoch [12], Batch [291/938], Loss: 0.6175020337104797\n",
      "Train: Epoch [12], Batch [292/938], Loss: 0.5536460876464844\n",
      "Train: Epoch [12], Batch [293/938], Loss: 0.46293431520462036\n",
      "Train: Epoch [12], Batch [294/938], Loss: 0.44627752900123596\n",
      "Train: Epoch [12], Batch [295/938], Loss: 0.36341536045074463\n",
      "Train: Epoch [12], Batch [296/938], Loss: 0.35968032479286194\n",
      "Train: Epoch [12], Batch [297/938], Loss: 0.6835577487945557\n",
      "Train: Epoch [12], Batch [298/938], Loss: 0.6033180356025696\n",
      "Train: Epoch [12], Batch [299/938], Loss: 0.6291253566741943\n",
      "Train: Epoch [12], Batch [300/938], Loss: 0.38705694675445557\n",
      "Train: Epoch [12], Batch [301/938], Loss: 0.5387851595878601\n",
      "Train: Epoch [12], Batch [302/938], Loss: 0.664193868637085\n",
      "Train: Epoch [12], Batch [303/938], Loss: 0.46841752529144287\n",
      "Train: Epoch [12], Batch [304/938], Loss: 0.5676965713500977\n",
      "Train: Epoch [12], Batch [305/938], Loss: 0.4315856695175171\n",
      "Train: Epoch [12], Batch [306/938], Loss: 0.5001116991043091\n",
      "Train: Epoch [12], Batch [307/938], Loss: 0.5834782123565674\n",
      "Train: Epoch [12], Batch [308/938], Loss: 0.43780720233917236\n",
      "Train: Epoch [12], Batch [309/938], Loss: 0.47776591777801514\n",
      "Train: Epoch [12], Batch [310/938], Loss: 0.3575853705406189\n",
      "Train: Epoch [12], Batch [311/938], Loss: 0.40237361192703247\n",
      "Train: Epoch [12], Batch [312/938], Loss: 0.5552594065666199\n",
      "Train: Epoch [12], Batch [313/938], Loss: 0.4407607913017273\n",
      "Train: Epoch [12], Batch [314/938], Loss: 0.7930326461791992\n",
      "Train: Epoch [12], Batch [315/938], Loss: 0.46450379490852356\n",
      "Train: Epoch [12], Batch [316/938], Loss: 0.5094872117042542\n",
      "Train: Epoch [12], Batch [317/938], Loss: 0.6477063894271851\n",
      "Train: Epoch [12], Batch [318/938], Loss: 0.48821812868118286\n",
      "Train: Epoch [12], Batch [319/938], Loss: 0.6606311798095703\n",
      "Train: Epoch [12], Batch [320/938], Loss: 0.5487231016159058\n",
      "Train: Epoch [12], Batch [321/938], Loss: 0.3307115435600281\n",
      "Train: Epoch [12], Batch [322/938], Loss: 0.47685569524765015\n",
      "Train: Epoch [12], Batch [323/938], Loss: 0.4104425311088562\n",
      "Train: Epoch [12], Batch [324/938], Loss: 0.42383402585983276\n",
      "Train: Epoch [12], Batch [325/938], Loss: 0.45192474126815796\n",
      "Train: Epoch [12], Batch [326/938], Loss: 0.5366388559341431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [327/938], Loss: 0.34829992055892944\n",
      "Train: Epoch [12], Batch [328/938], Loss: 0.5785597562789917\n",
      "Train: Epoch [12], Batch [329/938], Loss: 0.4274034798145294\n",
      "Train: Epoch [12], Batch [330/938], Loss: 0.47823500633239746\n",
      "Train: Epoch [12], Batch [331/938], Loss: 0.5267417430877686\n",
      "Train: Epoch [12], Batch [332/938], Loss: 0.7574867010116577\n",
      "Train: Epoch [12], Batch [333/938], Loss: 0.6553511023521423\n",
      "Train: Epoch [12], Batch [334/938], Loss: 0.4005565345287323\n",
      "Train: Epoch [12], Batch [335/938], Loss: 0.656398355960846\n",
      "Train: Epoch [12], Batch [336/938], Loss: 0.7842991352081299\n",
      "Train: Epoch [12], Batch [337/938], Loss: 0.5512082576751709\n",
      "Train: Epoch [12], Batch [338/938], Loss: 0.6907908916473389\n",
      "Train: Epoch [12], Batch [339/938], Loss: 0.3862067461013794\n",
      "Train: Epoch [12], Batch [340/938], Loss: 0.3755994737148285\n",
      "Train: Epoch [12], Batch [341/938], Loss: 0.5036954879760742\n",
      "Train: Epoch [12], Batch [342/938], Loss: 0.4317607283592224\n",
      "Train: Epoch [12], Batch [343/938], Loss: 0.9303178787231445\n",
      "Train: Epoch [12], Batch [344/938], Loss: 0.33996593952178955\n",
      "Train: Epoch [12], Batch [345/938], Loss: 0.45808249711990356\n",
      "Train: Epoch [12], Batch [346/938], Loss: 0.41313445568084717\n",
      "Train: Epoch [12], Batch [347/938], Loss: 0.48995473980903625\n",
      "Train: Epoch [12], Batch [348/938], Loss: 0.4950588047504425\n",
      "Train: Epoch [12], Batch [349/938], Loss: 0.3878747820854187\n",
      "Train: Epoch [12], Batch [350/938], Loss: 0.4958329498767853\n",
      "Train: Epoch [12], Batch [351/938], Loss: 0.35091134905815125\n",
      "Train: Epoch [12], Batch [352/938], Loss: 0.7729330062866211\n",
      "Train: Epoch [12], Batch [353/938], Loss: 0.7655085325241089\n",
      "Train: Epoch [12], Batch [354/938], Loss: 0.5217187404632568\n",
      "Train: Epoch [12], Batch [355/938], Loss: 0.5814475417137146\n",
      "Train: Epoch [12], Batch [356/938], Loss: 0.44364285469055176\n",
      "Train: Epoch [12], Batch [357/938], Loss: 0.583365797996521\n",
      "Train: Epoch [12], Batch [358/938], Loss: 0.6412742137908936\n",
      "Train: Epoch [12], Batch [359/938], Loss: 0.3919181227684021\n",
      "Train: Epoch [12], Batch [360/938], Loss: 0.49384212493896484\n",
      "Train: Epoch [12], Batch [361/938], Loss: 0.5988547205924988\n",
      "Train: Epoch [12], Batch [362/938], Loss: 0.5565010905265808\n",
      "Train: Epoch [12], Batch [363/938], Loss: 0.30121710896492004\n",
      "Train: Epoch [12], Batch [364/938], Loss: 0.6981968879699707\n",
      "Train: Epoch [12], Batch [365/938], Loss: 0.5170936584472656\n",
      "Train: Epoch [12], Batch [366/938], Loss: 0.41018131375312805\n",
      "Train: Epoch [12], Batch [367/938], Loss: 0.4377649426460266\n",
      "Train: Epoch [12], Batch [368/938], Loss: 0.5332715511322021\n",
      "Train: Epoch [12], Batch [369/938], Loss: 0.5711349844932556\n",
      "Train: Epoch [12], Batch [370/938], Loss: 0.6089515089988708\n",
      "Train: Epoch [12], Batch [371/938], Loss: 0.5653146505355835\n",
      "Train: Epoch [12], Batch [372/938], Loss: 0.5694005489349365\n",
      "Train: Epoch [12], Batch [373/938], Loss: 0.40437716245651245\n",
      "Train: Epoch [12], Batch [374/938], Loss: 0.5415428280830383\n",
      "Train: Epoch [12], Batch [375/938], Loss: 0.5219920873641968\n",
      "Train: Epoch [12], Batch [376/938], Loss: 0.545403003692627\n",
      "Train: Epoch [12], Batch [377/938], Loss: 0.5934845209121704\n",
      "Train: Epoch [12], Batch [378/938], Loss: 0.5398877859115601\n",
      "Train: Epoch [12], Batch [379/938], Loss: 0.5920245051383972\n",
      "Train: Epoch [12], Batch [380/938], Loss: 0.6184500455856323\n",
      "Train: Epoch [12], Batch [381/938], Loss: 0.44230902194976807\n",
      "Train: Epoch [12], Batch [382/938], Loss: 0.43603086471557617\n",
      "Train: Epoch [12], Batch [383/938], Loss: 0.5477490425109863\n",
      "Train: Epoch [12], Batch [384/938], Loss: 0.45237261056900024\n",
      "Train: Epoch [12], Batch [385/938], Loss: 0.44625601172447205\n",
      "Train: Epoch [12], Batch [386/938], Loss: 0.6379929780960083\n",
      "Train: Epoch [12], Batch [387/938], Loss: 0.451613187789917\n",
      "Train: Epoch [12], Batch [388/938], Loss: 0.42750534415245056\n",
      "Train: Epoch [12], Batch [389/938], Loss: 0.39425456523895264\n",
      "Train: Epoch [12], Batch [390/938], Loss: 0.4638521671295166\n",
      "Train: Epoch [12], Batch [391/938], Loss: 0.6292779445648193\n",
      "Train: Epoch [12], Batch [392/938], Loss: 0.4941664934158325\n",
      "Train: Epoch [12], Batch [393/938], Loss: 0.4381333887577057\n",
      "Train: Epoch [12], Batch [394/938], Loss: 0.2805752754211426\n",
      "Train: Epoch [12], Batch [395/938], Loss: 0.5437244176864624\n",
      "Train: Epoch [12], Batch [396/938], Loss: 0.3613908290863037\n",
      "Train: Epoch [12], Batch [397/938], Loss: 0.5455194115638733\n",
      "Train: Epoch [12], Batch [398/938], Loss: 0.5635911822319031\n",
      "Train: Epoch [12], Batch [399/938], Loss: 0.2815820574760437\n",
      "Train: Epoch [12], Batch [400/938], Loss: 0.5327744483947754\n",
      "Train: Epoch [12], Batch [401/938], Loss: 0.4279172718524933\n",
      "Train: Epoch [12], Batch [402/938], Loss: 0.3755081295967102\n",
      "Train: Epoch [12], Batch [403/938], Loss: 0.4297627806663513\n",
      "Train: Epoch [12], Batch [404/938], Loss: 0.44741177558898926\n",
      "Train: Epoch [12], Batch [405/938], Loss: 0.4464871883392334\n",
      "Train: Epoch [12], Batch [406/938], Loss: 0.5766727924346924\n",
      "Train: Epoch [12], Batch [407/938], Loss: 0.46489769220352173\n",
      "Train: Epoch [12], Batch [408/938], Loss: 0.27120906114578247\n",
      "Train: Epoch [12], Batch [409/938], Loss: 0.5794565677642822\n",
      "Train: Epoch [12], Batch [410/938], Loss: 0.6397024989128113\n",
      "Train: Epoch [12], Batch [411/938], Loss: 0.6489891409873962\n",
      "Train: Epoch [12], Batch [412/938], Loss: 0.4521366357803345\n",
      "Train: Epoch [12], Batch [413/938], Loss: 0.7277220487594604\n",
      "Train: Epoch [12], Batch [414/938], Loss: 0.29555612802505493\n",
      "Train: Epoch [12], Batch [415/938], Loss: 0.6127649545669556\n",
      "Train: Epoch [12], Batch [416/938], Loss: 0.4494105577468872\n",
      "Train: Epoch [12], Batch [417/938], Loss: 0.49635398387908936\n",
      "Train: Epoch [12], Batch [418/938], Loss: 0.32293272018432617\n",
      "Train: Epoch [12], Batch [419/938], Loss: 0.597047746181488\n",
      "Train: Epoch [12], Batch [420/938], Loss: 0.4478303790092468\n",
      "Train: Epoch [12], Batch [421/938], Loss: 0.46951058506965637\n",
      "Train: Epoch [12], Batch [422/938], Loss: 0.37017279863357544\n",
      "Train: Epoch [12], Batch [423/938], Loss: 0.4662443995475769\n",
      "Train: Epoch [12], Batch [424/938], Loss: 0.4676804542541504\n",
      "Train: Epoch [12], Batch [425/938], Loss: 0.3673091232776642\n",
      "Train: Epoch [12], Batch [426/938], Loss: 0.5998247861862183\n",
      "Train: Epoch [12], Batch [427/938], Loss: 0.6435481309890747\n",
      "Train: Epoch [12], Batch [428/938], Loss: 0.6427133083343506\n",
      "Train: Epoch [12], Batch [429/938], Loss: 0.4173482656478882\n",
      "Train: Epoch [12], Batch [430/938], Loss: 0.5351073145866394\n",
      "Train: Epoch [12], Batch [431/938], Loss: 0.6278649568557739\n",
      "Train: Epoch [12], Batch [432/938], Loss: 0.3628939092159271\n",
      "Train: Epoch [12], Batch [433/938], Loss: 0.3058500587940216\n",
      "Train: Epoch [12], Batch [434/938], Loss: 0.5611555576324463\n",
      "Train: Epoch [12], Batch [435/938], Loss: 0.4202984571456909\n",
      "Train: Epoch [12], Batch [436/938], Loss: 0.6094878911972046\n",
      "Train: Epoch [12], Batch [437/938], Loss: 0.6033983826637268\n",
      "Train: Epoch [12], Batch [438/938], Loss: 0.5675684809684753\n",
      "Train: Epoch [12], Batch [439/938], Loss: 0.4135192036628723\n",
      "Train: Epoch [12], Batch [440/938], Loss: 0.49532946944236755\n",
      "Train: Epoch [12], Batch [441/938], Loss: 0.5898431539535522\n",
      "Train: Epoch [12], Batch [442/938], Loss: 0.4291878342628479\n",
      "Train: Epoch [12], Batch [443/938], Loss: 0.5371573567390442\n",
      "Train: Epoch [12], Batch [444/938], Loss: 0.5098953247070312\n",
      "Train: Epoch [12], Batch [445/938], Loss: 0.5095113515853882\n",
      "Train: Epoch [12], Batch [446/938], Loss: 0.6423467397689819\n",
      "Train: Epoch [12], Batch [447/938], Loss: 0.49527597427368164\n",
      "Train: Epoch [12], Batch [448/938], Loss: 0.4737837612628937\n",
      "Train: Epoch [12], Batch [449/938], Loss: 0.7078131437301636\n",
      "Train: Epoch [12], Batch [450/938], Loss: 0.40144047141075134\n",
      "Train: Epoch [12], Batch [451/938], Loss: 0.49983030557632446\n",
      "Train: Epoch [12], Batch [452/938], Loss: 0.38788101077079773\n",
      "Train: Epoch [12], Batch [453/938], Loss: 0.3989441394805908\n",
      "Train: Epoch [12], Batch [454/938], Loss: 0.37712711095809937\n",
      "Train: Epoch [12], Batch [455/938], Loss: 0.4835561513900757\n",
      "Train: Epoch [12], Batch [456/938], Loss: 0.5295659303665161\n",
      "Train: Epoch [12], Batch [457/938], Loss: 0.47295650839805603\n",
      "Train: Epoch [12], Batch [458/938], Loss: 0.7973675727844238\n",
      "Train: Epoch [12], Batch [459/938], Loss: 0.469105064868927\n",
      "Train: Epoch [12], Batch [460/938], Loss: 0.48258912563323975\n",
      "Train: Epoch [12], Batch [461/938], Loss: 0.4197264611721039\n",
      "Train: Epoch [12], Batch [462/938], Loss: 0.47905054688453674\n",
      "Train: Epoch [12], Batch [463/938], Loss: 0.33930742740631104\n",
      "Train: Epoch [12], Batch [464/938], Loss: 0.5155391693115234\n",
      "Train: Epoch [12], Batch [465/938], Loss: 0.6691340208053589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [466/938], Loss: 0.5810937881469727\n",
      "Train: Epoch [12], Batch [467/938], Loss: 0.5216103196144104\n",
      "Train: Epoch [12], Batch [468/938], Loss: 0.44436705112457275\n",
      "Train: Epoch [12], Batch [469/938], Loss: 0.3646610975265503\n",
      "Train: Epoch [12], Batch [470/938], Loss: 0.37217527627944946\n",
      "Train: Epoch [12], Batch [471/938], Loss: 0.48091772198677063\n",
      "Train: Epoch [12], Batch [472/938], Loss: 0.4317013621330261\n",
      "Train: Epoch [12], Batch [473/938], Loss: 0.35747700929641724\n",
      "Train: Epoch [12], Batch [474/938], Loss: 0.572079062461853\n",
      "Train: Epoch [12], Batch [475/938], Loss: 0.5805292129516602\n",
      "Train: Epoch [12], Batch [476/938], Loss: 0.6162910461425781\n",
      "Train: Epoch [12], Batch [477/938], Loss: 0.4199354946613312\n",
      "Train: Epoch [12], Batch [478/938], Loss: 0.4306539297103882\n",
      "Train: Epoch [12], Batch [479/938], Loss: 0.3953995108604431\n",
      "Train: Epoch [12], Batch [480/938], Loss: 0.36338263750076294\n",
      "Train: Epoch [12], Batch [481/938], Loss: 0.5460483431816101\n",
      "Train: Epoch [12], Batch [482/938], Loss: 0.4232481122016907\n",
      "Train: Epoch [12], Batch [483/938], Loss: 0.5330309271812439\n",
      "Train: Epoch [12], Batch [484/938], Loss: 0.3687516450881958\n",
      "Train: Epoch [12], Batch [485/938], Loss: 0.4583951234817505\n",
      "Train: Epoch [12], Batch [486/938], Loss: 0.6073445677757263\n",
      "Train: Epoch [12], Batch [487/938], Loss: 0.5693073272705078\n",
      "Train: Epoch [12], Batch [488/938], Loss: 0.34325745701789856\n",
      "Train: Epoch [12], Batch [489/938], Loss: 0.4074065089225769\n",
      "Train: Epoch [12], Batch [490/938], Loss: 0.4387222230434418\n",
      "Train: Epoch [12], Batch [491/938], Loss: 0.35850149393081665\n",
      "Train: Epoch [12], Batch [492/938], Loss: 0.5912021398544312\n",
      "Train: Epoch [12], Batch [493/938], Loss: 0.5922728776931763\n",
      "Train: Epoch [12], Batch [494/938], Loss: 0.5252360105514526\n",
      "Train: Epoch [12], Batch [495/938], Loss: 0.5736686587333679\n",
      "Train: Epoch [12], Batch [496/938], Loss: 0.5410200953483582\n",
      "Train: Epoch [12], Batch [497/938], Loss: 0.41891777515411377\n",
      "Train: Epoch [12], Batch [498/938], Loss: 0.5369778871536255\n",
      "Train: Epoch [12], Batch [499/938], Loss: 0.467458575963974\n",
      "Train: Epoch [12], Batch [500/938], Loss: 0.4641479551792145\n",
      "Train: Epoch [12], Batch [501/938], Loss: 0.44540268182754517\n",
      "Train: Epoch [12], Batch [502/938], Loss: 0.6394634246826172\n",
      "Train: Epoch [12], Batch [503/938], Loss: 0.7444157600402832\n",
      "Train: Epoch [12], Batch [504/938], Loss: 0.7329005002975464\n",
      "Train: Epoch [12], Batch [505/938], Loss: 0.38877445459365845\n",
      "Train: Epoch [12], Batch [506/938], Loss: 0.4989778995513916\n",
      "Train: Epoch [12], Batch [507/938], Loss: 0.5194994211196899\n",
      "Train: Epoch [12], Batch [508/938], Loss: 0.6423344612121582\n",
      "Train: Epoch [12], Batch [509/938], Loss: 0.564700186252594\n",
      "Train: Epoch [12], Batch [510/938], Loss: 0.5155203342437744\n",
      "Train: Epoch [12], Batch [511/938], Loss: 0.4489132761955261\n",
      "Train: Epoch [12], Batch [512/938], Loss: 0.29922351241111755\n",
      "Train: Epoch [12], Batch [513/938], Loss: 0.5816642045974731\n",
      "Train: Epoch [12], Batch [514/938], Loss: 0.36964961886405945\n",
      "Train: Epoch [12], Batch [515/938], Loss: 0.4401854872703552\n",
      "Train: Epoch [12], Batch [516/938], Loss: 0.49642717838287354\n",
      "Train: Epoch [12], Batch [517/938], Loss: 0.42057356238365173\n",
      "Train: Epoch [12], Batch [518/938], Loss: 0.4260760545730591\n",
      "Train: Epoch [12], Batch [519/938], Loss: 0.4765012264251709\n",
      "Train: Epoch [12], Batch [520/938], Loss: 0.444358229637146\n",
      "Train: Epoch [12], Batch [521/938], Loss: 0.43526238203048706\n",
      "Train: Epoch [12], Batch [522/938], Loss: 0.6826574802398682\n",
      "Train: Epoch [12], Batch [523/938], Loss: 0.4001767933368683\n",
      "Train: Epoch [12], Batch [524/938], Loss: 0.6046171188354492\n",
      "Train: Epoch [12], Batch [525/938], Loss: 0.44748416543006897\n",
      "Train: Epoch [12], Batch [526/938], Loss: 0.5477616190910339\n",
      "Train: Epoch [12], Batch [527/938], Loss: 0.47774404287338257\n",
      "Train: Epoch [12], Batch [528/938], Loss: 0.41612574458122253\n",
      "Train: Epoch [12], Batch [529/938], Loss: 0.5531858801841736\n",
      "Train: Epoch [12], Batch [530/938], Loss: 0.5333964824676514\n",
      "Train: Epoch [12], Batch [531/938], Loss: 0.7020397186279297\n",
      "Train: Epoch [12], Batch [532/938], Loss: 0.7758916020393372\n",
      "Train: Epoch [12], Batch [533/938], Loss: 0.34799259901046753\n",
      "Train: Epoch [12], Batch [534/938], Loss: 0.3606162965297699\n",
      "Train: Epoch [12], Batch [535/938], Loss: 0.26160115003585815\n",
      "Train: Epoch [12], Batch [536/938], Loss: 0.4696407616138458\n",
      "Train: Epoch [12], Batch [537/938], Loss: 0.49469998478889465\n",
      "Train: Epoch [12], Batch [538/938], Loss: 0.44194844365119934\n",
      "Train: Epoch [12], Batch [539/938], Loss: 0.4766002595424652\n",
      "Train: Epoch [12], Batch [540/938], Loss: 0.57593834400177\n",
      "Train: Epoch [12], Batch [541/938], Loss: 0.5652317404747009\n",
      "Train: Epoch [12], Batch [542/938], Loss: 0.49895790219306946\n",
      "Train: Epoch [12], Batch [543/938], Loss: 0.4720844626426697\n",
      "Train: Epoch [12], Batch [544/938], Loss: 0.5305270552635193\n",
      "Train: Epoch [12], Batch [545/938], Loss: 0.39140114188194275\n",
      "Train: Epoch [12], Batch [546/938], Loss: 0.45173192024230957\n",
      "Train: Epoch [12], Batch [547/938], Loss: 0.3782595098018646\n",
      "Train: Epoch [12], Batch [548/938], Loss: 0.6593873500823975\n",
      "Train: Epoch [12], Batch [549/938], Loss: 0.40147972106933594\n",
      "Train: Epoch [12], Batch [550/938], Loss: 0.5298919677734375\n",
      "Train: Epoch [12], Batch [551/938], Loss: 0.23875263333320618\n",
      "Train: Epoch [12], Batch [552/938], Loss: 0.5946354866027832\n",
      "Train: Epoch [12], Batch [553/938], Loss: 0.5278730392456055\n",
      "Train: Epoch [12], Batch [554/938], Loss: 0.38476619124412537\n",
      "Train: Epoch [12], Batch [555/938], Loss: 0.41384628415107727\n",
      "Train: Epoch [12], Batch [556/938], Loss: 0.4534509778022766\n",
      "Train: Epoch [12], Batch [557/938], Loss: 0.49684926867485046\n",
      "Train: Epoch [12], Batch [558/938], Loss: 0.31619513034820557\n",
      "Train: Epoch [12], Batch [559/938], Loss: 0.5359947681427002\n",
      "Train: Epoch [12], Batch [560/938], Loss: 0.3853422701358795\n",
      "Train: Epoch [12], Batch [561/938], Loss: 0.5526690483093262\n",
      "Train: Epoch [12], Batch [562/938], Loss: 0.47379982471466064\n",
      "Train: Epoch [12], Batch [563/938], Loss: 0.3988240957260132\n",
      "Train: Epoch [12], Batch [564/938], Loss: 0.5156007409095764\n",
      "Train: Epoch [12], Batch [565/938], Loss: 0.4760091304779053\n",
      "Train: Epoch [12], Batch [566/938], Loss: 0.5000919103622437\n",
      "Train: Epoch [12], Batch [567/938], Loss: 0.43596670031547546\n",
      "Train: Epoch [12], Batch [568/938], Loss: 0.5350713729858398\n",
      "Train: Epoch [12], Batch [569/938], Loss: 0.4000434875488281\n",
      "Train: Epoch [12], Batch [570/938], Loss: 0.34678712487220764\n",
      "Train: Epoch [12], Batch [571/938], Loss: 0.586843729019165\n",
      "Train: Epoch [12], Batch [572/938], Loss: 0.4588198959827423\n",
      "Train: Epoch [12], Batch [573/938], Loss: 0.7062516808509827\n",
      "Train: Epoch [12], Batch [574/938], Loss: 0.3625527620315552\n",
      "Train: Epoch [12], Batch [575/938], Loss: 0.5799686908721924\n",
      "Train: Epoch [12], Batch [576/938], Loss: 0.4672721028327942\n",
      "Train: Epoch [12], Batch [577/938], Loss: 0.5918013453483582\n",
      "Train: Epoch [12], Batch [578/938], Loss: 0.5163355469703674\n",
      "Train: Epoch [12], Batch [579/938], Loss: 0.450120210647583\n",
      "Train: Epoch [12], Batch [580/938], Loss: 0.7739378213882446\n",
      "Train: Epoch [12], Batch [581/938], Loss: 0.37256115674972534\n",
      "Train: Epoch [12], Batch [582/938], Loss: 0.35602790117263794\n",
      "Train: Epoch [12], Batch [583/938], Loss: 0.6844795942306519\n",
      "Train: Epoch [12], Batch [584/938], Loss: 0.48953813314437866\n",
      "Train: Epoch [12], Batch [585/938], Loss: 0.5794190764427185\n",
      "Train: Epoch [12], Batch [586/938], Loss: 0.374170184135437\n",
      "Train: Epoch [12], Batch [587/938], Loss: 0.4639222025871277\n",
      "Train: Epoch [12], Batch [588/938], Loss: 0.5726163387298584\n",
      "Train: Epoch [12], Batch [589/938], Loss: 0.3382646143436432\n",
      "Train: Epoch [12], Batch [590/938], Loss: 0.40801411867141724\n",
      "Train: Epoch [12], Batch [591/938], Loss: 0.3769925534725189\n",
      "Train: Epoch [12], Batch [592/938], Loss: 0.5473060607910156\n",
      "Train: Epoch [12], Batch [593/938], Loss: 0.36831265687942505\n",
      "Train: Epoch [12], Batch [594/938], Loss: 0.8056814074516296\n",
      "Train: Epoch [12], Batch [595/938], Loss: 0.5239120721817017\n",
      "Train: Epoch [12], Batch [596/938], Loss: 0.6521526575088501\n",
      "Train: Epoch [12], Batch [597/938], Loss: 0.45200836658477783\n",
      "Train: Epoch [12], Batch [598/938], Loss: 0.5037702918052673\n",
      "Train: Epoch [12], Batch [599/938], Loss: 0.37670570611953735\n",
      "Train: Epoch [12], Batch [600/938], Loss: 0.674370288848877\n",
      "Train: Epoch [12], Batch [601/938], Loss: 0.2787562310695648\n",
      "Train: Epoch [12], Batch [602/938], Loss: 0.39520883560180664\n",
      "Train: Epoch [12], Batch [603/938], Loss: 0.46013879776000977\n",
      "Train: Epoch [12], Batch [604/938], Loss: 0.44838324189186096\n",
      "Train: Epoch [12], Batch [605/938], Loss: 0.45553264021873474\n",
      "Train: Epoch [12], Batch [606/938], Loss: 0.44157737493515015\n",
      "Train: Epoch [12], Batch [607/938], Loss: 0.575543224811554\n",
      "Train: Epoch [12], Batch [608/938], Loss: 0.5346310138702393\n",
      "Train: Epoch [12], Batch [609/938], Loss: 0.6140214204788208\n",
      "Train: Epoch [12], Batch [610/938], Loss: 0.7138551473617554\n",
      "Train: Epoch [12], Batch [611/938], Loss: 0.40617501735687256\n",
      "Train: Epoch [12], Batch [612/938], Loss: 0.3922000229358673\n",
      "Train: Epoch [12], Batch [613/938], Loss: 0.3221000134944916\n",
      "Train: Epoch [12], Batch [614/938], Loss: 0.28082358837127686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [615/938], Loss: 0.4900868535041809\n",
      "Train: Epoch [12], Batch [616/938], Loss: 0.4936068058013916\n",
      "Train: Epoch [12], Batch [617/938], Loss: 0.49861598014831543\n",
      "Train: Epoch [12], Batch [618/938], Loss: 0.7377437353134155\n",
      "Train: Epoch [12], Batch [619/938], Loss: 0.6887170076370239\n",
      "Train: Epoch [12], Batch [620/938], Loss: 0.7346686124801636\n",
      "Train: Epoch [12], Batch [621/938], Loss: 0.6845954060554504\n",
      "Train: Epoch [12], Batch [622/938], Loss: 0.54726243019104\n",
      "Train: Epoch [12], Batch [623/938], Loss: 0.5235610008239746\n",
      "Train: Epoch [12], Batch [624/938], Loss: 0.6863114833831787\n",
      "Train: Epoch [12], Batch [625/938], Loss: 0.41118165850639343\n",
      "Train: Epoch [12], Batch [626/938], Loss: 0.7147496938705444\n",
      "Train: Epoch [12], Batch [627/938], Loss: 0.5709962248802185\n",
      "Train: Epoch [12], Batch [628/938], Loss: 0.4505223035812378\n",
      "Train: Epoch [12], Batch [629/938], Loss: 0.4118509292602539\n",
      "Train: Epoch [12], Batch [630/938], Loss: 0.4396781325340271\n",
      "Train: Epoch [12], Batch [631/938], Loss: 0.5177873373031616\n",
      "Train: Epoch [12], Batch [632/938], Loss: 0.43014705181121826\n",
      "Train: Epoch [12], Batch [633/938], Loss: 0.48256582021713257\n",
      "Train: Epoch [12], Batch [634/938], Loss: 0.6486670970916748\n",
      "Train: Epoch [12], Batch [635/938], Loss: 0.30838286876678467\n",
      "Train: Epoch [12], Batch [636/938], Loss: 0.5456748008728027\n",
      "Train: Epoch [12], Batch [637/938], Loss: 0.4705662131309509\n",
      "Train: Epoch [12], Batch [638/938], Loss: 0.4230037331581116\n",
      "Train: Epoch [12], Batch [639/938], Loss: 0.3056771159172058\n",
      "Train: Epoch [12], Batch [640/938], Loss: 0.6253902912139893\n",
      "Train: Epoch [12], Batch [641/938], Loss: 0.35258960723876953\n",
      "Train: Epoch [12], Batch [642/938], Loss: 0.32589152455329895\n",
      "Train: Epoch [12], Batch [643/938], Loss: 0.262965589761734\n",
      "Train: Epoch [12], Batch [644/938], Loss: 0.5689013004302979\n",
      "Train: Epoch [12], Batch [645/938], Loss: 0.6096621751785278\n",
      "Train: Epoch [12], Batch [646/938], Loss: 0.44469505548477173\n",
      "Train: Epoch [12], Batch [647/938], Loss: 0.5112833976745605\n",
      "Train: Epoch [12], Batch [648/938], Loss: 0.36008530855178833\n",
      "Train: Epoch [12], Batch [649/938], Loss: 0.7397398948669434\n",
      "Train: Epoch [12], Batch [650/938], Loss: 0.3778744339942932\n",
      "Train: Epoch [12], Batch [651/938], Loss: 0.35985907912254333\n",
      "Train: Epoch [12], Batch [652/938], Loss: 0.31241315603256226\n",
      "Train: Epoch [12], Batch [653/938], Loss: 0.44920969009399414\n",
      "Train: Epoch [12], Batch [654/938], Loss: 0.5362791419029236\n",
      "Train: Epoch [12], Batch [655/938], Loss: 0.4323490858078003\n",
      "Train: Epoch [12], Batch [656/938], Loss: 0.4360780119895935\n",
      "Train: Epoch [12], Batch [657/938], Loss: 0.5594110488891602\n",
      "Train: Epoch [12], Batch [658/938], Loss: 0.4945973753929138\n",
      "Train: Epoch [12], Batch [659/938], Loss: 0.5014063715934753\n",
      "Train: Epoch [12], Batch [660/938], Loss: 0.6454465389251709\n",
      "Train: Epoch [12], Batch [661/938], Loss: 0.47999775409698486\n",
      "Train: Epoch [12], Batch [662/938], Loss: 0.43920058012008667\n",
      "Train: Epoch [12], Batch [663/938], Loss: 0.5817925333976746\n",
      "Train: Epoch [12], Batch [664/938], Loss: 0.3285708427429199\n",
      "Train: Epoch [12], Batch [665/938], Loss: 0.6164294481277466\n",
      "Train: Epoch [12], Batch [666/938], Loss: 0.6334066390991211\n",
      "Train: Epoch [12], Batch [667/938], Loss: 0.5596956014633179\n",
      "Train: Epoch [12], Batch [668/938], Loss: 0.4253236651420593\n",
      "Train: Epoch [12], Batch [669/938], Loss: 0.47409874200820923\n",
      "Train: Epoch [12], Batch [670/938], Loss: 0.6089152097702026\n",
      "Train: Epoch [12], Batch [671/938], Loss: 0.3747151494026184\n",
      "Train: Epoch [12], Batch [672/938], Loss: 0.4924079179763794\n",
      "Train: Epoch [12], Batch [673/938], Loss: 0.5266624093055725\n",
      "Train: Epoch [12], Batch [674/938], Loss: 0.5442895889282227\n",
      "Train: Epoch [12], Batch [675/938], Loss: 0.7481527328491211\n",
      "Train: Epoch [12], Batch [676/938], Loss: 0.6217008829116821\n",
      "Train: Epoch [12], Batch [677/938], Loss: 0.5465921759605408\n",
      "Train: Epoch [12], Batch [678/938], Loss: 0.5196167230606079\n",
      "Train: Epoch [12], Batch [679/938], Loss: 0.5188099145889282\n",
      "Train: Epoch [12], Batch [680/938], Loss: 0.4949294328689575\n",
      "Train: Epoch [12], Batch [681/938], Loss: 0.3807338774204254\n",
      "Train: Epoch [12], Batch [682/938], Loss: 0.3532405495643616\n",
      "Train: Epoch [12], Batch [683/938], Loss: 0.43445971608161926\n",
      "Train: Epoch [12], Batch [684/938], Loss: 0.44801753759384155\n",
      "Train: Epoch [12], Batch [685/938], Loss: 0.3742583394050598\n",
      "Train: Epoch [12], Batch [686/938], Loss: 0.41470879316329956\n",
      "Train: Epoch [12], Batch [687/938], Loss: 0.4809960424900055\n",
      "Train: Epoch [12], Batch [688/938], Loss: 0.3759758770465851\n",
      "Train: Epoch [12], Batch [689/938], Loss: 0.4121192693710327\n",
      "Train: Epoch [12], Batch [690/938], Loss: 0.48543721437454224\n",
      "Train: Epoch [12], Batch [691/938], Loss: 0.5033472776412964\n",
      "Train: Epoch [12], Batch [692/938], Loss: 0.5872491598129272\n",
      "Train: Epoch [12], Batch [693/938], Loss: 0.4318491816520691\n",
      "Train: Epoch [12], Batch [694/938], Loss: 0.29290205240249634\n",
      "Train: Epoch [12], Batch [695/938], Loss: 0.4322509467601776\n",
      "Train: Epoch [12], Batch [696/938], Loss: 0.6167514324188232\n",
      "Train: Epoch [12], Batch [697/938], Loss: 0.39642518758773804\n",
      "Train: Epoch [12], Batch [698/938], Loss: 0.45457571744918823\n",
      "Train: Epoch [12], Batch [699/938], Loss: 0.4696071743965149\n",
      "Train: Epoch [12], Batch [700/938], Loss: 0.5518677830696106\n",
      "Train: Epoch [12], Batch [701/938], Loss: 0.5563860535621643\n",
      "Train: Epoch [12], Batch [702/938], Loss: 0.45549091696739197\n",
      "Train: Epoch [12], Batch [703/938], Loss: 0.36958375573158264\n",
      "Train: Epoch [12], Batch [704/938], Loss: 0.5902539491653442\n",
      "Train: Epoch [12], Batch [705/938], Loss: 0.5068260431289673\n",
      "Train: Epoch [12], Batch [706/938], Loss: 0.5869085788726807\n",
      "Train: Epoch [12], Batch [707/938], Loss: 0.8695370554924011\n",
      "Train: Epoch [12], Batch [708/938], Loss: 0.5491034388542175\n",
      "Train: Epoch [12], Batch [709/938], Loss: 0.6350970268249512\n",
      "Train: Epoch [12], Batch [710/938], Loss: 0.5061855316162109\n",
      "Train: Epoch [12], Batch [711/938], Loss: 0.5514413118362427\n",
      "Train: Epoch [12], Batch [712/938], Loss: 0.5973511934280396\n",
      "Train: Epoch [12], Batch [713/938], Loss: 0.49549752473831177\n",
      "Train: Epoch [12], Batch [714/938], Loss: 0.5274389982223511\n",
      "Train: Epoch [12], Batch [715/938], Loss: 0.4718289375305176\n",
      "Train: Epoch [12], Batch [716/938], Loss: 0.45362040400505066\n",
      "Train: Epoch [12], Batch [717/938], Loss: 0.6489808559417725\n",
      "Train: Epoch [12], Batch [718/938], Loss: 0.26287683844566345\n",
      "Train: Epoch [12], Batch [719/938], Loss: 0.5135716199874878\n",
      "Train: Epoch [12], Batch [720/938], Loss: 0.45981651544570923\n",
      "Train: Epoch [12], Batch [721/938], Loss: 0.4764699637889862\n",
      "Train: Epoch [12], Batch [722/938], Loss: 0.4061667025089264\n",
      "Train: Epoch [12], Batch [723/938], Loss: 0.4983387291431427\n",
      "Train: Epoch [12], Batch [724/938], Loss: 0.38132137060165405\n",
      "Train: Epoch [12], Batch [725/938], Loss: 0.4336452782154083\n",
      "Train: Epoch [12], Batch [726/938], Loss: 0.619704008102417\n",
      "Train: Epoch [12], Batch [727/938], Loss: 0.41396212577819824\n",
      "Train: Epoch [12], Batch [728/938], Loss: 0.7035703659057617\n",
      "Train: Epoch [12], Batch [729/938], Loss: 0.47823935747146606\n",
      "Train: Epoch [12], Batch [730/938], Loss: 0.4625885486602783\n",
      "Train: Epoch [12], Batch [731/938], Loss: 0.42385774850845337\n",
      "Train: Epoch [12], Batch [732/938], Loss: 0.5834037065505981\n",
      "Train: Epoch [12], Batch [733/938], Loss: 0.5634687542915344\n",
      "Train: Epoch [12], Batch [734/938], Loss: 0.609149694442749\n",
      "Train: Epoch [12], Batch [735/938], Loss: 0.5043621063232422\n",
      "Train: Epoch [12], Batch [736/938], Loss: 0.48791414499282837\n",
      "Train: Epoch [12], Batch [737/938], Loss: 0.5873340964317322\n",
      "Train: Epoch [12], Batch [738/938], Loss: 0.31487661600112915\n",
      "Train: Epoch [12], Batch [739/938], Loss: 0.5841064453125\n",
      "Train: Epoch [12], Batch [740/938], Loss: 0.4710078835487366\n",
      "Train: Epoch [12], Batch [741/938], Loss: 0.41697511076927185\n",
      "Train: Epoch [12], Batch [742/938], Loss: 0.406364768743515\n",
      "Train: Epoch [12], Batch [743/938], Loss: 0.5974169969558716\n",
      "Train: Epoch [12], Batch [744/938], Loss: 0.6779780983924866\n",
      "Train: Epoch [12], Batch [745/938], Loss: 0.45613348484039307\n",
      "Train: Epoch [12], Batch [746/938], Loss: 0.4479832649230957\n",
      "Train: Epoch [12], Batch [747/938], Loss: 0.488983154296875\n",
      "Train: Epoch [12], Batch [748/938], Loss: 0.467794269323349\n",
      "Train: Epoch [12], Batch [749/938], Loss: 0.35175395011901855\n",
      "Train: Epoch [12], Batch [750/938], Loss: 0.3653130829334259\n",
      "Train: Epoch [12], Batch [751/938], Loss: 0.49584081768989563\n",
      "Train: Epoch [12], Batch [752/938], Loss: 0.42251965403556824\n",
      "Train: Epoch [12], Batch [753/938], Loss: 0.5297644734382629\n",
      "Train: Epoch [12], Batch [754/938], Loss: 0.40232375264167786\n",
      "Train: Epoch [12], Batch [755/938], Loss: 0.42454859614372253\n",
      "Train: Epoch [12], Batch [756/938], Loss: 0.5255248546600342\n",
      "Train: Epoch [12], Batch [757/938], Loss: 0.5404742956161499\n",
      "Train: Epoch [12], Batch [758/938], Loss: 0.608605146408081\n",
      "Train: Epoch [12], Batch [759/938], Loss: 0.633727490901947\n",
      "Train: Epoch [12], Batch [760/938], Loss: 0.4011736512184143\n",
      "Train: Epoch [12], Batch [761/938], Loss: 0.69356369972229\n",
      "Train: Epoch [12], Batch [762/938], Loss: 0.30638736486434937\n",
      "Train: Epoch [12], Batch [763/938], Loss: 0.488884299993515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [764/938], Loss: 0.3194155693054199\n",
      "Train: Epoch [12], Batch [765/938], Loss: 0.5198314189910889\n",
      "Train: Epoch [12], Batch [766/938], Loss: 0.5394489765167236\n",
      "Train: Epoch [12], Batch [767/938], Loss: 0.5209444761276245\n",
      "Train: Epoch [12], Batch [768/938], Loss: 0.5747684836387634\n",
      "Train: Epoch [12], Batch [769/938], Loss: 0.346558541059494\n",
      "Train: Epoch [12], Batch [770/938], Loss: 0.450740784406662\n",
      "Train: Epoch [12], Batch [771/938], Loss: 0.5325800180435181\n",
      "Train: Epoch [12], Batch [772/938], Loss: 0.5203146934509277\n",
      "Train: Epoch [12], Batch [773/938], Loss: 0.5389221906661987\n",
      "Train: Epoch [12], Batch [774/938], Loss: 0.692847728729248\n",
      "Train: Epoch [12], Batch [775/938], Loss: 0.574970006942749\n",
      "Train: Epoch [12], Batch [776/938], Loss: 0.5492011308670044\n",
      "Train: Epoch [12], Batch [777/938], Loss: 0.7658582925796509\n",
      "Train: Epoch [12], Batch [778/938], Loss: 0.5687872171401978\n",
      "Train: Epoch [12], Batch [779/938], Loss: 0.37883615493774414\n",
      "Train: Epoch [12], Batch [780/938], Loss: 0.3286549746990204\n",
      "Train: Epoch [12], Batch [781/938], Loss: 0.5343867540359497\n",
      "Train: Epoch [12], Batch [782/938], Loss: 0.5425941944122314\n",
      "Train: Epoch [12], Batch [783/938], Loss: 0.533042848110199\n",
      "Train: Epoch [12], Batch [784/938], Loss: 0.605234682559967\n",
      "Train: Epoch [12], Batch [785/938], Loss: 0.438241183757782\n",
      "Train: Epoch [12], Batch [786/938], Loss: 0.46628880500793457\n",
      "Train: Epoch [12], Batch [787/938], Loss: 0.4375338554382324\n",
      "Train: Epoch [12], Batch [788/938], Loss: 0.37061214447021484\n",
      "Train: Epoch [12], Batch [789/938], Loss: 0.6270776987075806\n",
      "Train: Epoch [12], Batch [790/938], Loss: 0.5012613534927368\n",
      "Train: Epoch [12], Batch [791/938], Loss: 0.7263933420181274\n",
      "Train: Epoch [12], Batch [792/938], Loss: 0.4474487602710724\n",
      "Train: Epoch [12], Batch [793/938], Loss: 0.32948970794677734\n",
      "Train: Epoch [12], Batch [794/938], Loss: 0.46490180492401123\n",
      "Train: Epoch [12], Batch [795/938], Loss: 0.5016332268714905\n",
      "Train: Epoch [12], Batch [796/938], Loss: 0.4527727961540222\n",
      "Train: Epoch [12], Batch [797/938], Loss: 0.42447391152381897\n",
      "Train: Epoch [12], Batch [798/938], Loss: 0.4940621852874756\n",
      "Train: Epoch [12], Batch [799/938], Loss: 0.44085317850112915\n",
      "Train: Epoch [12], Batch [800/938], Loss: 0.4226405620574951\n",
      "Train: Epoch [12], Batch [801/938], Loss: 0.37943127751350403\n",
      "Train: Epoch [12], Batch [802/938], Loss: 0.42551037669181824\n",
      "Train: Epoch [12], Batch [803/938], Loss: 0.465313196182251\n",
      "Train: Epoch [12], Batch [804/938], Loss: 0.44684508442878723\n",
      "Train: Epoch [12], Batch [805/938], Loss: 0.34368786215782166\n",
      "Train: Epoch [12], Batch [806/938], Loss: 0.3917335271835327\n",
      "Train: Epoch [12], Batch [807/938], Loss: 0.4073545038700104\n",
      "Train: Epoch [12], Batch [808/938], Loss: 0.7358169555664062\n",
      "Train: Epoch [12], Batch [809/938], Loss: 0.6335537433624268\n",
      "Train: Epoch [12], Batch [810/938], Loss: 0.3419749140739441\n",
      "Train: Epoch [12], Batch [811/938], Loss: 0.3329954743385315\n",
      "Train: Epoch [12], Batch [812/938], Loss: 0.38181090354919434\n",
      "Train: Epoch [12], Batch [813/938], Loss: 0.35531216859817505\n",
      "Train: Epoch [12], Batch [814/938], Loss: 0.4681153893470764\n",
      "Train: Epoch [12], Batch [815/938], Loss: 0.3862444758415222\n",
      "Train: Epoch [12], Batch [816/938], Loss: 0.6586022973060608\n",
      "Train: Epoch [12], Batch [817/938], Loss: 0.6517248153686523\n",
      "Train: Epoch [12], Batch [818/938], Loss: 0.5153186917304993\n",
      "Train: Epoch [12], Batch [819/938], Loss: 0.44218969345092773\n",
      "Train: Epoch [12], Batch [820/938], Loss: 0.41782817244529724\n",
      "Train: Epoch [12], Batch [821/938], Loss: 0.5028930902481079\n",
      "Train: Epoch [12], Batch [822/938], Loss: 0.37634608149528503\n",
      "Train: Epoch [12], Batch [823/938], Loss: 0.6669811010360718\n",
      "Train: Epoch [12], Batch [824/938], Loss: 0.6656348705291748\n",
      "Train: Epoch [12], Batch [825/938], Loss: 0.38507407903671265\n",
      "Train: Epoch [12], Batch [826/938], Loss: 0.655771017074585\n",
      "Train: Epoch [12], Batch [827/938], Loss: 0.5737336874008179\n",
      "Train: Epoch [12], Batch [828/938], Loss: 0.5117157101631165\n",
      "Train: Epoch [12], Batch [829/938], Loss: 0.3878638744354248\n",
      "Train: Epoch [12], Batch [830/938], Loss: 0.433500200510025\n",
      "Train: Epoch [12], Batch [831/938], Loss: 0.4137257933616638\n",
      "Train: Epoch [12], Batch [832/938], Loss: 0.5739492177963257\n",
      "Train: Epoch [12], Batch [833/938], Loss: 0.5241401195526123\n",
      "Train: Epoch [12], Batch [834/938], Loss: 0.45764416456222534\n",
      "Train: Epoch [12], Batch [835/938], Loss: 0.33986836671829224\n",
      "Train: Epoch [12], Batch [836/938], Loss: 0.4313295781612396\n",
      "Train: Epoch [12], Batch [837/938], Loss: 0.635949969291687\n",
      "Train: Epoch [12], Batch [838/938], Loss: 0.6795043349266052\n",
      "Train: Epoch [12], Batch [839/938], Loss: 0.5158206224441528\n",
      "Train: Epoch [12], Batch [840/938], Loss: 0.47378379106521606\n",
      "Train: Epoch [12], Batch [841/938], Loss: 0.613595724105835\n",
      "Train: Epoch [12], Batch [842/938], Loss: 0.5877865552902222\n",
      "Train: Epoch [12], Batch [843/938], Loss: 0.3968741297721863\n",
      "Train: Epoch [12], Batch [844/938], Loss: 0.5057637691497803\n",
      "Train: Epoch [12], Batch [845/938], Loss: 0.5171201825141907\n",
      "Train: Epoch [12], Batch [846/938], Loss: 0.6006466150283813\n",
      "Train: Epoch [12], Batch [847/938], Loss: 0.4466002285480499\n",
      "Train: Epoch [12], Batch [848/938], Loss: 0.4051245152950287\n",
      "Train: Epoch [12], Batch [849/938], Loss: 0.45155614614486694\n",
      "Train: Epoch [12], Batch [850/938], Loss: 0.5618512630462646\n",
      "Train: Epoch [12], Batch [851/938], Loss: 0.5419625043869019\n",
      "Train: Epoch [12], Batch [852/938], Loss: 0.5109861493110657\n",
      "Train: Epoch [12], Batch [853/938], Loss: 0.5900675058364868\n",
      "Train: Epoch [12], Batch [854/938], Loss: 0.45209020376205444\n",
      "Train: Epoch [12], Batch [855/938], Loss: 0.5169618129730225\n",
      "Train: Epoch [12], Batch [856/938], Loss: 0.5977862477302551\n",
      "Train: Epoch [12], Batch [857/938], Loss: 0.33616775274276733\n",
      "Train: Epoch [12], Batch [858/938], Loss: 0.40266865491867065\n",
      "Train: Epoch [12], Batch [859/938], Loss: 0.6497950553894043\n",
      "Train: Epoch [12], Batch [860/938], Loss: 0.486689954996109\n",
      "Train: Epoch [12], Batch [861/938], Loss: 0.43091821670532227\n",
      "Train: Epoch [12], Batch [862/938], Loss: 0.5098311305046082\n",
      "Train: Epoch [12], Batch [863/938], Loss: 0.561899721622467\n",
      "Train: Epoch [12], Batch [864/938], Loss: 0.5554804801940918\n",
      "Train: Epoch [12], Batch [865/938], Loss: 0.4065859913825989\n",
      "Train: Epoch [12], Batch [866/938], Loss: 0.5116578340530396\n",
      "Train: Epoch [12], Batch [867/938], Loss: 0.4949759244918823\n",
      "Train: Epoch [12], Batch [868/938], Loss: 0.33379918336868286\n",
      "Train: Epoch [12], Batch [869/938], Loss: 0.311797171831131\n",
      "Train: Epoch [12], Batch [870/938], Loss: 0.41736695170402527\n",
      "Train: Epoch [12], Batch [871/938], Loss: 0.6059154272079468\n",
      "Train: Epoch [12], Batch [872/938], Loss: 0.3196150064468384\n",
      "Train: Epoch [12], Batch [873/938], Loss: 0.6516757011413574\n",
      "Train: Epoch [12], Batch [874/938], Loss: 0.5198239088058472\n",
      "Train: Epoch [12], Batch [875/938], Loss: 0.565673828125\n",
      "Train: Epoch [12], Batch [876/938], Loss: 0.42721569538116455\n",
      "Train: Epoch [12], Batch [877/938], Loss: 0.5068080425262451\n",
      "Train: Epoch [12], Batch [878/938], Loss: 0.6327025890350342\n",
      "Train: Epoch [12], Batch [879/938], Loss: 0.36228787899017334\n",
      "Train: Epoch [12], Batch [880/938], Loss: 0.39799338579177856\n",
      "Train: Epoch [12], Batch [881/938], Loss: 0.5551459789276123\n",
      "Train: Epoch [12], Batch [882/938], Loss: 0.40724778175354004\n",
      "Train: Epoch [12], Batch [883/938], Loss: 0.4613097310066223\n",
      "Train: Epoch [12], Batch [884/938], Loss: 0.46899059414863586\n",
      "Train: Epoch [12], Batch [885/938], Loss: 0.3100111484527588\n",
      "Train: Epoch [12], Batch [886/938], Loss: 0.5334709286689758\n",
      "Train: Epoch [12], Batch [887/938], Loss: 0.5521167516708374\n",
      "Train: Epoch [12], Batch [888/938], Loss: 0.651753842830658\n",
      "Train: Epoch [12], Batch [889/938], Loss: 0.4599204957485199\n",
      "Train: Epoch [12], Batch [890/938], Loss: 0.5934712886810303\n",
      "Train: Epoch [12], Batch [891/938], Loss: 0.6040788888931274\n",
      "Train: Epoch [12], Batch [892/938], Loss: 0.5255658626556396\n",
      "Train: Epoch [12], Batch [893/938], Loss: 0.3079203963279724\n",
      "Train: Epoch [12], Batch [894/938], Loss: 0.48216724395751953\n",
      "Train: Epoch [12], Batch [895/938], Loss: 0.5401950478553772\n",
      "Train: Epoch [12], Batch [896/938], Loss: 0.5139073729515076\n",
      "Train: Epoch [12], Batch [897/938], Loss: 0.6791983246803284\n",
      "Train: Epoch [12], Batch [898/938], Loss: 0.47451692819595337\n",
      "Train: Epoch [12], Batch [899/938], Loss: 0.46755653619766235\n",
      "Train: Epoch [12], Batch [900/938], Loss: 0.6138066649436951\n",
      "Train: Epoch [12], Batch [901/938], Loss: 0.538785457611084\n",
      "Train: Epoch [12], Batch [902/938], Loss: 0.6454798579216003\n",
      "Train: Epoch [12], Batch [903/938], Loss: 0.7093098163604736\n",
      "Train: Epoch [12], Batch [904/938], Loss: 0.5753180384635925\n",
      "Train: Epoch [12], Batch [905/938], Loss: 0.6382046937942505\n",
      "Train: Epoch [12], Batch [906/938], Loss: 0.488847553730011\n",
      "Train: Epoch [12], Batch [907/938], Loss: 0.6793955564498901\n",
      "Train: Epoch [12], Batch [908/938], Loss: 0.5171308517456055\n",
      "Train: Epoch [12], Batch [909/938], Loss: 0.459425687789917\n",
      "Train: Epoch [12], Batch [910/938], Loss: 0.3921011686325073\n",
      "Train: Epoch [12], Batch [911/938], Loss: 0.34504300355911255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [12], Batch [912/938], Loss: 0.47146230936050415\n",
      "Train: Epoch [12], Batch [913/938], Loss: 0.5615019798278809\n",
      "Train: Epoch [12], Batch [914/938], Loss: 0.47172728180885315\n",
      "Train: Epoch [12], Batch [915/938], Loss: 0.5269980430603027\n",
      "Train: Epoch [12], Batch [916/938], Loss: 0.46895620226860046\n",
      "Train: Epoch [12], Batch [917/938], Loss: 0.4498194456100464\n",
      "Train: Epoch [12], Batch [918/938], Loss: 0.4279036521911621\n",
      "Train: Epoch [12], Batch [919/938], Loss: 0.4044598340988159\n",
      "Train: Epoch [12], Batch [920/938], Loss: 0.5126932859420776\n",
      "Train: Epoch [12], Batch [921/938], Loss: 0.45068925619125366\n",
      "Train: Epoch [12], Batch [922/938], Loss: 0.5114014744758606\n",
      "Train: Epoch [12], Batch [923/938], Loss: 0.5465212464332581\n",
      "Train: Epoch [12], Batch [924/938], Loss: 0.4283095598220825\n",
      "Train: Epoch [12], Batch [925/938], Loss: 0.3905852437019348\n",
      "Train: Epoch [12], Batch [926/938], Loss: 0.35701984167099\n",
      "Train: Epoch [12], Batch [927/938], Loss: 0.5306911468505859\n",
      "Train: Epoch [12], Batch [928/938], Loss: 0.43974846601486206\n",
      "Train: Epoch [12], Batch [929/938], Loss: 0.6629738807678223\n",
      "Train: Epoch [12], Batch [930/938], Loss: 0.5482369661331177\n",
      "Train: Epoch [12], Batch [931/938], Loss: 0.6425067186355591\n",
      "Train: Epoch [12], Batch [932/938], Loss: 0.4903595447540283\n",
      "Train: Epoch [12], Batch [933/938], Loss: 0.3804611265659332\n",
      "Train: Epoch [12], Batch [934/938], Loss: 0.3278657793998718\n",
      "Train: Epoch [12], Batch [935/938], Loss: 0.6620018482208252\n",
      "Train: Epoch [12], Batch [936/938], Loss: 0.6028395891189575\n",
      "Train: Epoch [12], Batch [937/938], Loss: 0.5991994142532349\n",
      "Train: Epoch [12], Batch [938/938], Loss: 0.7557129859924316\n",
      "Accuracy of train set: 0.82555\n",
      "Validation: Epoch [12], Batch [1/938], Loss: 0.47309133410453796\n",
      "Validation: Epoch [12], Batch [2/938], Loss: 0.5190933346748352\n",
      "Validation: Epoch [12], Batch [3/938], Loss: 0.4351102113723755\n",
      "Validation: Epoch [12], Batch [4/938], Loss: 0.6332712173461914\n",
      "Validation: Epoch [12], Batch [5/938], Loss: 0.6576381325721741\n",
      "Validation: Epoch [12], Batch [6/938], Loss: 0.462053120136261\n",
      "Validation: Epoch [12], Batch [7/938], Loss: 0.43492481112480164\n",
      "Validation: Epoch [12], Batch [8/938], Loss: 0.527463972568512\n",
      "Validation: Epoch [12], Batch [9/938], Loss: 0.6431211829185486\n",
      "Validation: Epoch [12], Batch [10/938], Loss: 0.5456581115722656\n",
      "Validation: Epoch [12], Batch [11/938], Loss: 0.38102561235427856\n",
      "Validation: Epoch [12], Batch [12/938], Loss: 0.5451356768608093\n",
      "Validation: Epoch [12], Batch [13/938], Loss: 0.4701305031776428\n",
      "Validation: Epoch [12], Batch [14/938], Loss: 0.5957694053649902\n",
      "Validation: Epoch [12], Batch [15/938], Loss: 0.5804804563522339\n",
      "Validation: Epoch [12], Batch [16/938], Loss: 0.7616915106773376\n",
      "Validation: Epoch [12], Batch [17/938], Loss: 0.47636377811431885\n",
      "Validation: Epoch [12], Batch [18/938], Loss: 0.47562116384506226\n",
      "Validation: Epoch [12], Batch [19/938], Loss: 0.4714219570159912\n",
      "Validation: Epoch [12], Batch [20/938], Loss: 0.4961128532886505\n",
      "Validation: Epoch [12], Batch [21/938], Loss: 0.4869861304759979\n",
      "Validation: Epoch [12], Batch [22/938], Loss: 0.6238002777099609\n",
      "Validation: Epoch [12], Batch [23/938], Loss: 0.439569354057312\n",
      "Validation: Epoch [12], Batch [24/938], Loss: 0.3950612545013428\n",
      "Validation: Epoch [12], Batch [25/938], Loss: 0.37408578395843506\n",
      "Validation: Epoch [12], Batch [26/938], Loss: 0.5222969651222229\n",
      "Validation: Epoch [12], Batch [27/938], Loss: 0.4987381100654602\n",
      "Validation: Epoch [12], Batch [28/938], Loss: 0.3855058252811432\n",
      "Validation: Epoch [12], Batch [29/938], Loss: 0.7041319608688354\n",
      "Validation: Epoch [12], Batch [30/938], Loss: 0.46337196230888367\n",
      "Validation: Epoch [12], Batch [31/938], Loss: 0.39545178413391113\n",
      "Validation: Epoch [12], Batch [32/938], Loss: 0.43207064270973206\n",
      "Validation: Epoch [12], Batch [33/938], Loss: 0.4308093190193176\n",
      "Validation: Epoch [12], Batch [34/938], Loss: 0.41667088866233826\n",
      "Validation: Epoch [12], Batch [35/938], Loss: 0.4477350115776062\n",
      "Validation: Epoch [12], Batch [36/938], Loss: 0.3514192998409271\n",
      "Validation: Epoch [12], Batch [37/938], Loss: 0.551027774810791\n",
      "Validation: Epoch [12], Batch [38/938], Loss: 0.5119357705116272\n",
      "Validation: Epoch [12], Batch [39/938], Loss: 0.577173113822937\n",
      "Validation: Epoch [12], Batch [40/938], Loss: 0.3278312087059021\n",
      "Validation: Epoch [12], Batch [41/938], Loss: 0.620850682258606\n",
      "Validation: Epoch [12], Batch [42/938], Loss: 0.4772917628288269\n",
      "Validation: Epoch [12], Batch [43/938], Loss: 0.46211063861846924\n",
      "Validation: Epoch [12], Batch [44/938], Loss: 0.6820613145828247\n",
      "Validation: Epoch [12], Batch [45/938], Loss: 0.535515308380127\n",
      "Validation: Epoch [12], Batch [46/938], Loss: 0.6051421761512756\n",
      "Validation: Epoch [12], Batch [47/938], Loss: 0.7169641256332397\n",
      "Validation: Epoch [12], Batch [48/938], Loss: 0.46261096000671387\n",
      "Validation: Epoch [12], Batch [49/938], Loss: 0.5728724598884583\n",
      "Validation: Epoch [12], Batch [50/938], Loss: 0.589975118637085\n",
      "Validation: Epoch [12], Batch [51/938], Loss: 0.43312937021255493\n",
      "Validation: Epoch [12], Batch [52/938], Loss: 0.2070842981338501\n",
      "Validation: Epoch [12], Batch [53/938], Loss: 0.6249525547027588\n",
      "Validation: Epoch [12], Batch [54/938], Loss: 0.4530296325683594\n",
      "Validation: Epoch [12], Batch [55/938], Loss: 0.2670665979385376\n",
      "Validation: Epoch [12], Batch [56/938], Loss: 0.48636627197265625\n",
      "Validation: Epoch [12], Batch [57/938], Loss: 0.5390146374702454\n",
      "Validation: Epoch [12], Batch [58/938], Loss: 0.830034613609314\n",
      "Validation: Epoch [12], Batch [59/938], Loss: 0.4373518228530884\n",
      "Validation: Epoch [12], Batch [60/938], Loss: 0.43207427859306335\n",
      "Validation: Epoch [12], Batch [61/938], Loss: 0.4550076127052307\n",
      "Validation: Epoch [12], Batch [62/938], Loss: 0.41124778985977173\n",
      "Validation: Epoch [12], Batch [63/938], Loss: 0.4941479563713074\n",
      "Validation: Epoch [12], Batch [64/938], Loss: 0.6033027768135071\n",
      "Validation: Epoch [12], Batch [65/938], Loss: 0.5598889589309692\n",
      "Validation: Epoch [12], Batch [66/938], Loss: 0.5194268226623535\n",
      "Validation: Epoch [12], Batch [67/938], Loss: 0.3329952359199524\n",
      "Validation: Epoch [12], Batch [68/938], Loss: 0.3255612850189209\n",
      "Validation: Epoch [12], Batch [69/938], Loss: 0.4813198447227478\n",
      "Validation: Epoch [12], Batch [70/938], Loss: 0.7693943977355957\n",
      "Validation: Epoch [12], Batch [71/938], Loss: 0.4967864751815796\n",
      "Validation: Epoch [12], Batch [72/938], Loss: 0.479054719209671\n",
      "Validation: Epoch [12], Batch [73/938], Loss: 0.4656001925468445\n",
      "Validation: Epoch [12], Batch [74/938], Loss: 0.5038699507713318\n",
      "Validation: Epoch [12], Batch [75/938], Loss: 0.565224289894104\n",
      "Validation: Epoch [12], Batch [76/938], Loss: 0.5112342834472656\n",
      "Validation: Epoch [12], Batch [77/938], Loss: 0.5227893590927124\n",
      "Validation: Epoch [12], Batch [78/938], Loss: 0.525389552116394\n",
      "Validation: Epoch [12], Batch [79/938], Loss: 0.3730618953704834\n",
      "Validation: Epoch [12], Batch [80/938], Loss: 0.5905493497848511\n",
      "Validation: Epoch [12], Batch [81/938], Loss: 0.5393989086151123\n",
      "Validation: Epoch [12], Batch [82/938], Loss: 0.4109765887260437\n",
      "Validation: Epoch [12], Batch [83/938], Loss: 0.41026419401168823\n",
      "Validation: Epoch [12], Batch [84/938], Loss: 0.4105934500694275\n",
      "Validation: Epoch [12], Batch [85/938], Loss: 0.4273409843444824\n",
      "Validation: Epoch [12], Batch [86/938], Loss: 0.5133082866668701\n",
      "Validation: Epoch [12], Batch [87/938], Loss: 0.6073415875434875\n",
      "Validation: Epoch [12], Batch [88/938], Loss: 0.3776022791862488\n",
      "Validation: Epoch [12], Batch [89/938], Loss: 0.37277546525001526\n",
      "Validation: Epoch [12], Batch [90/938], Loss: 0.4022024869918823\n",
      "Validation: Epoch [12], Batch [91/938], Loss: 0.40866920351982117\n",
      "Validation: Epoch [12], Batch [92/938], Loss: 0.3944343626499176\n",
      "Validation: Epoch [12], Batch [93/938], Loss: 0.5125629305839539\n",
      "Validation: Epoch [12], Batch [94/938], Loss: 0.5607157945632935\n",
      "Validation: Epoch [12], Batch [95/938], Loss: 0.36389416456222534\n",
      "Validation: Epoch [12], Batch [96/938], Loss: 0.44559451937675476\n",
      "Validation: Epoch [12], Batch [97/938], Loss: 0.40462827682495117\n",
      "Validation: Epoch [12], Batch [98/938], Loss: 0.46875089406967163\n",
      "Validation: Epoch [12], Batch [99/938], Loss: 0.6090705394744873\n",
      "Validation: Epoch [12], Batch [100/938], Loss: 0.5258989334106445\n",
      "Validation: Epoch [12], Batch [101/938], Loss: 0.41611766815185547\n",
      "Validation: Epoch [12], Batch [102/938], Loss: 0.49323156476020813\n",
      "Validation: Epoch [12], Batch [103/938], Loss: 0.5185141563415527\n",
      "Validation: Epoch [12], Batch [104/938], Loss: 0.3879772424697876\n",
      "Validation: Epoch [12], Batch [105/938], Loss: 0.45107829570770264\n",
      "Validation: Epoch [12], Batch [106/938], Loss: 0.45522427558898926\n",
      "Validation: Epoch [12], Batch [107/938], Loss: 0.6417505741119385\n",
      "Validation: Epoch [12], Batch [108/938], Loss: 0.7017922401428223\n",
      "Validation: Epoch [12], Batch [109/938], Loss: 0.3457386791706085\n",
      "Validation: Epoch [12], Batch [110/938], Loss: 0.5060808658599854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [111/938], Loss: 0.4192061424255371\n",
      "Validation: Epoch [12], Batch [112/938], Loss: 0.8503559827804565\n",
      "Validation: Epoch [12], Batch [113/938], Loss: 0.44892939925193787\n",
      "Validation: Epoch [12], Batch [114/938], Loss: 0.27823883295059204\n",
      "Validation: Epoch [12], Batch [115/938], Loss: 0.4605225920677185\n",
      "Validation: Epoch [12], Batch [116/938], Loss: 0.40068915486335754\n",
      "Validation: Epoch [12], Batch [117/938], Loss: 0.5012196898460388\n",
      "Validation: Epoch [12], Batch [118/938], Loss: 0.3407633602619171\n",
      "Validation: Epoch [12], Batch [119/938], Loss: 0.29396164417266846\n",
      "Validation: Epoch [12], Batch [120/938], Loss: 0.5671820640563965\n",
      "Validation: Epoch [12], Batch [121/938], Loss: 0.5399985313415527\n",
      "Validation: Epoch [12], Batch [122/938], Loss: 0.5733041763305664\n",
      "Validation: Epoch [12], Batch [123/938], Loss: 0.43548503518104553\n",
      "Validation: Epoch [12], Batch [124/938], Loss: 0.6219035387039185\n",
      "Validation: Epoch [12], Batch [125/938], Loss: 0.4033716917037964\n",
      "Validation: Epoch [12], Batch [126/938], Loss: 0.31623414158821106\n",
      "Validation: Epoch [12], Batch [127/938], Loss: 0.5677473545074463\n",
      "Validation: Epoch [12], Batch [128/938], Loss: 0.5728645324707031\n",
      "Validation: Epoch [12], Batch [129/938], Loss: 0.5114905834197998\n",
      "Validation: Epoch [12], Batch [130/938], Loss: 0.5572878122329712\n",
      "Validation: Epoch [12], Batch [131/938], Loss: 0.4139344096183777\n",
      "Validation: Epoch [12], Batch [132/938], Loss: 0.3185085356235504\n",
      "Validation: Epoch [12], Batch [133/938], Loss: 0.46130916476249695\n",
      "Validation: Epoch [12], Batch [134/938], Loss: 0.3974740207195282\n",
      "Validation: Epoch [12], Batch [135/938], Loss: 0.38347867131233215\n",
      "Validation: Epoch [12], Batch [136/938], Loss: 0.5862231254577637\n",
      "Validation: Epoch [12], Batch [137/938], Loss: 0.4746917188167572\n",
      "Validation: Epoch [12], Batch [138/938], Loss: 0.5532023906707764\n",
      "Validation: Epoch [12], Batch [139/938], Loss: 0.4374253749847412\n",
      "Validation: Epoch [12], Batch [140/938], Loss: 0.4788300096988678\n",
      "Validation: Epoch [12], Batch [141/938], Loss: 0.6440168619155884\n",
      "Validation: Epoch [12], Batch [142/938], Loss: 0.4516628384590149\n",
      "Validation: Epoch [12], Batch [143/938], Loss: 0.7993491888046265\n",
      "Validation: Epoch [12], Batch [144/938], Loss: 0.5991953611373901\n",
      "Validation: Epoch [12], Batch [145/938], Loss: 0.43822458386421204\n",
      "Validation: Epoch [12], Batch [146/938], Loss: 0.45022323727607727\n",
      "Validation: Epoch [12], Batch [147/938], Loss: 0.7135597467422485\n",
      "Validation: Epoch [12], Batch [148/938], Loss: 0.3544704020023346\n",
      "Validation: Epoch [12], Batch [149/938], Loss: 0.3996761441230774\n",
      "Validation: Epoch [12], Batch [150/938], Loss: 0.5131906270980835\n",
      "Validation: Epoch [12], Batch [151/938], Loss: 0.3612782955169678\n",
      "Validation: Epoch [12], Batch [152/938], Loss: 0.41000819206237793\n",
      "Validation: Epoch [12], Batch [153/938], Loss: 0.5668976306915283\n",
      "Validation: Epoch [12], Batch [154/938], Loss: 0.5276197195053101\n",
      "Validation: Epoch [12], Batch [155/938], Loss: 0.5630781650543213\n",
      "Validation: Epoch [12], Batch [156/938], Loss: 0.2918131649494171\n",
      "Validation: Epoch [12], Batch [157/938], Loss: 0.564142107963562\n",
      "Validation: Epoch [12], Batch [158/938], Loss: 0.3336195945739746\n",
      "Validation: Epoch [12], Batch [159/938], Loss: 0.553808867931366\n",
      "Validation: Epoch [12], Batch [160/938], Loss: 0.556679368019104\n",
      "Validation: Epoch [12], Batch [161/938], Loss: 0.510021984577179\n",
      "Validation: Epoch [12], Batch [162/938], Loss: 0.40893858671188354\n",
      "Validation: Epoch [12], Batch [163/938], Loss: 0.5330787897109985\n",
      "Validation: Epoch [12], Batch [164/938], Loss: 0.4074002504348755\n",
      "Validation: Epoch [12], Batch [165/938], Loss: 0.4195452928543091\n",
      "Validation: Epoch [12], Batch [166/938], Loss: 0.4030548632144928\n",
      "Validation: Epoch [12], Batch [167/938], Loss: 0.4604582190513611\n",
      "Validation: Epoch [12], Batch [168/938], Loss: 0.481347918510437\n",
      "Validation: Epoch [12], Batch [169/938], Loss: 0.4921375513076782\n",
      "Validation: Epoch [12], Batch [170/938], Loss: 0.4910014569759369\n",
      "Validation: Epoch [12], Batch [171/938], Loss: 0.4646652936935425\n",
      "Validation: Epoch [12], Batch [172/938], Loss: 0.2619204819202423\n",
      "Validation: Epoch [12], Batch [173/938], Loss: 0.48339205980300903\n",
      "Validation: Epoch [12], Batch [174/938], Loss: 0.44750696420669556\n",
      "Validation: Epoch [12], Batch [175/938], Loss: 0.5484803915023804\n",
      "Validation: Epoch [12], Batch [176/938], Loss: 0.5403467416763306\n",
      "Validation: Epoch [12], Batch [177/938], Loss: 0.7021639347076416\n",
      "Validation: Epoch [12], Batch [178/938], Loss: 0.3752845525741577\n",
      "Validation: Epoch [12], Batch [179/938], Loss: 0.497211754322052\n",
      "Validation: Epoch [12], Batch [180/938], Loss: 0.6312561631202698\n",
      "Validation: Epoch [12], Batch [181/938], Loss: 0.6328417062759399\n",
      "Validation: Epoch [12], Batch [182/938], Loss: 0.45146238803863525\n",
      "Validation: Epoch [12], Batch [183/938], Loss: 0.4576517939567566\n",
      "Validation: Epoch [12], Batch [184/938], Loss: 0.3288969397544861\n",
      "Validation: Epoch [12], Batch [185/938], Loss: 0.6485766172409058\n",
      "Validation: Epoch [12], Batch [186/938], Loss: 0.5123653411865234\n",
      "Validation: Epoch [12], Batch [187/938], Loss: 0.5530471801757812\n",
      "Validation: Epoch [12], Batch [188/938], Loss: 0.48243388533592224\n",
      "Validation: Epoch [12], Batch [189/938], Loss: 0.6178990602493286\n",
      "Validation: Epoch [12], Batch [190/938], Loss: 0.33249813318252563\n",
      "Validation: Epoch [12], Batch [191/938], Loss: 0.41752660274505615\n",
      "Validation: Epoch [12], Batch [192/938], Loss: 0.43811625242233276\n",
      "Validation: Epoch [12], Batch [193/938], Loss: 0.6785674691200256\n",
      "Validation: Epoch [12], Batch [194/938], Loss: 0.3825731575489044\n",
      "Validation: Epoch [12], Batch [195/938], Loss: 0.3673636317253113\n",
      "Validation: Epoch [12], Batch [196/938], Loss: 0.4528436064720154\n",
      "Validation: Epoch [12], Batch [197/938], Loss: 0.4349803924560547\n",
      "Validation: Epoch [12], Batch [198/938], Loss: 0.5492647886276245\n",
      "Validation: Epoch [12], Batch [199/938], Loss: 0.4541977047920227\n",
      "Validation: Epoch [12], Batch [200/938], Loss: 0.4669981300830841\n",
      "Validation: Epoch [12], Batch [201/938], Loss: 0.5478989481925964\n",
      "Validation: Epoch [12], Batch [202/938], Loss: 0.5644220113754272\n",
      "Validation: Epoch [12], Batch [203/938], Loss: 0.43329811096191406\n",
      "Validation: Epoch [12], Batch [204/938], Loss: 0.40802282094955444\n",
      "Validation: Epoch [12], Batch [205/938], Loss: 0.537774920463562\n",
      "Validation: Epoch [12], Batch [206/938], Loss: 0.37797608971595764\n",
      "Validation: Epoch [12], Batch [207/938], Loss: 0.44685351848602295\n",
      "Validation: Epoch [12], Batch [208/938], Loss: 0.5707417726516724\n",
      "Validation: Epoch [12], Batch [209/938], Loss: 0.5407532453536987\n",
      "Validation: Epoch [12], Batch [210/938], Loss: 0.5887391567230225\n",
      "Validation: Epoch [12], Batch [211/938], Loss: 0.29530516266822815\n",
      "Validation: Epoch [12], Batch [212/938], Loss: 0.5775130987167358\n",
      "Validation: Epoch [12], Batch [213/938], Loss: 0.5279384851455688\n",
      "Validation: Epoch [12], Batch [214/938], Loss: 0.5085822343826294\n",
      "Validation: Epoch [12], Batch [215/938], Loss: 0.5126109719276428\n",
      "Validation: Epoch [12], Batch [216/938], Loss: 0.524154543876648\n",
      "Validation: Epoch [12], Batch [217/938], Loss: 0.41667410731315613\n",
      "Validation: Epoch [12], Batch [218/938], Loss: 0.5300262570381165\n",
      "Validation: Epoch [12], Batch [219/938], Loss: 0.35801824927330017\n",
      "Validation: Epoch [12], Batch [220/938], Loss: 0.3920665681362152\n",
      "Validation: Epoch [12], Batch [221/938], Loss: 0.36732521653175354\n",
      "Validation: Epoch [12], Batch [222/938], Loss: 0.5049887299537659\n",
      "Validation: Epoch [12], Batch [223/938], Loss: 0.4398021996021271\n",
      "Validation: Epoch [12], Batch [224/938], Loss: 0.7242093086242676\n",
      "Validation: Epoch [12], Batch [225/938], Loss: 0.5485222935676575\n",
      "Validation: Epoch [12], Batch [226/938], Loss: 0.5141240358352661\n",
      "Validation: Epoch [12], Batch [227/938], Loss: 0.5462260246276855\n",
      "Validation: Epoch [12], Batch [228/938], Loss: 0.38092368841171265\n",
      "Validation: Epoch [12], Batch [229/938], Loss: 0.43963634967803955\n",
      "Validation: Epoch [12], Batch [230/938], Loss: 0.3377634882926941\n",
      "Validation: Epoch [12], Batch [231/938], Loss: 0.49556511640548706\n",
      "Validation: Epoch [12], Batch [232/938], Loss: 0.43939122557640076\n",
      "Validation: Epoch [12], Batch [233/938], Loss: 0.7353432178497314\n",
      "Validation: Epoch [12], Batch [234/938], Loss: 0.5702273845672607\n",
      "Validation: Epoch [12], Batch [235/938], Loss: 0.5126765370368958\n",
      "Validation: Epoch [12], Batch [236/938], Loss: 0.40214037895202637\n",
      "Validation: Epoch [12], Batch [237/938], Loss: 0.3824555277824402\n",
      "Validation: Epoch [12], Batch [238/938], Loss: 0.43604952096939087\n",
      "Validation: Epoch [12], Batch [239/938], Loss: 0.2654726803302765\n",
      "Validation: Epoch [12], Batch [240/938], Loss: 0.45217615365982056\n",
      "Validation: Epoch [12], Batch [241/938], Loss: 0.4003269672393799\n",
      "Validation: Epoch [12], Batch [242/938], Loss: 0.44883620738983154\n",
      "Validation: Epoch [12], Batch [243/938], Loss: 0.3502061367034912\n",
      "Validation: Epoch [12], Batch [244/938], Loss: 0.5065194368362427\n",
      "Validation: Epoch [12], Batch [245/938], Loss: 0.3222635090351105\n",
      "Validation: Epoch [12], Batch [246/938], Loss: 0.6824193596839905\n",
      "Validation: Epoch [12], Batch [247/938], Loss: 0.4770575165748596\n",
      "Validation: Epoch [12], Batch [248/938], Loss: 0.30973419547080994\n",
      "Validation: Epoch [12], Batch [249/938], Loss: 0.4451470375061035\n",
      "Validation: Epoch [12], Batch [250/938], Loss: 0.738391637802124\n",
      "Validation: Epoch [12], Batch [251/938], Loss: 0.47833824157714844\n",
      "Validation: Epoch [12], Batch [252/938], Loss: 0.46892455220222473\n",
      "Validation: Epoch [12], Batch [253/938], Loss: 0.5792277455329895\n",
      "Validation: Epoch [12], Batch [254/938], Loss: 0.5498186349868774\n",
      "Validation: Epoch [12], Batch [255/938], Loss: 0.6294408440589905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [256/938], Loss: 0.3956284523010254\n",
      "Validation: Epoch [12], Batch [257/938], Loss: 0.5135353803634644\n",
      "Validation: Epoch [12], Batch [258/938], Loss: 0.4921359717845917\n",
      "Validation: Epoch [12], Batch [259/938], Loss: 0.6891658306121826\n",
      "Validation: Epoch [12], Batch [260/938], Loss: 0.3591955900192261\n",
      "Validation: Epoch [12], Batch [261/938], Loss: 0.6466639041900635\n",
      "Validation: Epoch [12], Batch [262/938], Loss: 0.35414963960647583\n",
      "Validation: Epoch [12], Batch [263/938], Loss: 0.5153056979179382\n",
      "Validation: Epoch [12], Batch [264/938], Loss: 0.6512562036514282\n",
      "Validation: Epoch [12], Batch [265/938], Loss: 0.49743184447288513\n",
      "Validation: Epoch [12], Batch [266/938], Loss: 0.4121227562427521\n",
      "Validation: Epoch [12], Batch [267/938], Loss: 0.516642689704895\n",
      "Validation: Epoch [12], Batch [268/938], Loss: 0.453529417514801\n",
      "Validation: Epoch [12], Batch [269/938], Loss: 0.5782709121704102\n",
      "Validation: Epoch [12], Batch [270/938], Loss: 0.3422200381755829\n",
      "Validation: Epoch [12], Batch [271/938], Loss: 0.34132707118988037\n",
      "Validation: Epoch [12], Batch [272/938], Loss: 0.3984985649585724\n",
      "Validation: Epoch [12], Batch [273/938], Loss: 0.5335268378257751\n",
      "Validation: Epoch [12], Batch [274/938], Loss: 0.5110011100769043\n",
      "Validation: Epoch [12], Batch [275/938], Loss: 0.4130009710788727\n",
      "Validation: Epoch [12], Batch [276/938], Loss: 0.48113515973091125\n",
      "Validation: Epoch [12], Batch [277/938], Loss: 0.4350169897079468\n",
      "Validation: Epoch [12], Batch [278/938], Loss: 0.32170698046684265\n",
      "Validation: Epoch [12], Batch [279/938], Loss: 0.35243475437164307\n",
      "Validation: Epoch [12], Batch [280/938], Loss: 0.4666505753993988\n",
      "Validation: Epoch [12], Batch [281/938], Loss: 0.6898194551467896\n",
      "Validation: Epoch [12], Batch [282/938], Loss: 0.5367686748504639\n",
      "Validation: Epoch [12], Batch [283/938], Loss: 0.49070408940315247\n",
      "Validation: Epoch [12], Batch [284/938], Loss: 0.4248550534248352\n",
      "Validation: Epoch [12], Batch [285/938], Loss: 0.407630980014801\n",
      "Validation: Epoch [12], Batch [286/938], Loss: 0.462673157453537\n",
      "Validation: Epoch [12], Batch [287/938], Loss: 0.5025614500045776\n",
      "Validation: Epoch [12], Batch [288/938], Loss: 0.32796114683151245\n",
      "Validation: Epoch [12], Batch [289/938], Loss: 0.558802604675293\n",
      "Validation: Epoch [12], Batch [290/938], Loss: 0.42032498121261597\n",
      "Validation: Epoch [12], Batch [291/938], Loss: 0.4137873649597168\n",
      "Validation: Epoch [12], Batch [292/938], Loss: 0.41399720311164856\n",
      "Validation: Epoch [12], Batch [293/938], Loss: 0.5358982086181641\n",
      "Validation: Epoch [12], Batch [294/938], Loss: 0.5110410451889038\n",
      "Validation: Epoch [12], Batch [295/938], Loss: 0.4531165063381195\n",
      "Validation: Epoch [12], Batch [296/938], Loss: 0.2776888906955719\n",
      "Validation: Epoch [12], Batch [297/938], Loss: 0.2987975478172302\n",
      "Validation: Epoch [12], Batch [298/938], Loss: 0.6261430978775024\n",
      "Validation: Epoch [12], Batch [299/938], Loss: 0.4759137034416199\n",
      "Validation: Epoch [12], Batch [300/938], Loss: 0.5137550234794617\n",
      "Validation: Epoch [12], Batch [301/938], Loss: 0.47435981035232544\n",
      "Validation: Epoch [12], Batch [302/938], Loss: 0.5367804169654846\n",
      "Validation: Epoch [12], Batch [303/938], Loss: 0.6554696559906006\n",
      "Validation: Epoch [12], Batch [304/938], Loss: 0.5396807193756104\n",
      "Validation: Epoch [12], Batch [305/938], Loss: 0.3717268705368042\n",
      "Validation: Epoch [12], Batch [306/938], Loss: 0.5143091678619385\n",
      "Validation: Epoch [12], Batch [307/938], Loss: 0.4600948393344879\n",
      "Validation: Epoch [12], Batch [308/938], Loss: 0.5497767329216003\n",
      "Validation: Epoch [12], Batch [309/938], Loss: 0.3507991433143616\n",
      "Validation: Epoch [12], Batch [310/938], Loss: 0.38326510787010193\n",
      "Validation: Epoch [12], Batch [311/938], Loss: 0.5300164222717285\n",
      "Validation: Epoch [12], Batch [312/938], Loss: 0.6794964075088501\n",
      "Validation: Epoch [12], Batch [313/938], Loss: 0.44430941343307495\n",
      "Validation: Epoch [12], Batch [314/938], Loss: 0.5353968143463135\n",
      "Validation: Epoch [12], Batch [315/938], Loss: 0.4717043340206146\n",
      "Validation: Epoch [12], Batch [316/938], Loss: 0.40245822072029114\n",
      "Validation: Epoch [12], Batch [317/938], Loss: 0.3469848334789276\n",
      "Validation: Epoch [12], Batch [318/938], Loss: 0.5056695938110352\n",
      "Validation: Epoch [12], Batch [319/938], Loss: 0.6193704605102539\n",
      "Validation: Epoch [12], Batch [320/938], Loss: 0.42273277044296265\n",
      "Validation: Epoch [12], Batch [321/938], Loss: 0.2576282024383545\n",
      "Validation: Epoch [12], Batch [322/938], Loss: 0.4996836483478546\n",
      "Validation: Epoch [12], Batch [323/938], Loss: 0.4787721037864685\n",
      "Validation: Epoch [12], Batch [324/938], Loss: 0.3681311309337616\n",
      "Validation: Epoch [12], Batch [325/938], Loss: 0.5048527121543884\n",
      "Validation: Epoch [12], Batch [326/938], Loss: 0.44905799627304077\n",
      "Validation: Epoch [12], Batch [327/938], Loss: 0.44811856746673584\n",
      "Validation: Epoch [12], Batch [328/938], Loss: 0.40094003081321716\n",
      "Validation: Epoch [12], Batch [329/938], Loss: 0.40799790620803833\n",
      "Validation: Epoch [12], Batch [330/938], Loss: 0.48356813192367554\n",
      "Validation: Epoch [12], Batch [331/938], Loss: 0.5270947813987732\n",
      "Validation: Epoch [12], Batch [332/938], Loss: 0.392433762550354\n",
      "Validation: Epoch [12], Batch [333/938], Loss: 0.4614097476005554\n",
      "Validation: Epoch [12], Batch [334/938], Loss: 0.662655234336853\n",
      "Validation: Epoch [12], Batch [335/938], Loss: 0.7252460718154907\n",
      "Validation: Epoch [12], Batch [336/938], Loss: 0.6533256769180298\n",
      "Validation: Epoch [12], Batch [337/938], Loss: 0.4214778542518616\n",
      "Validation: Epoch [12], Batch [338/938], Loss: 0.3772546052932739\n",
      "Validation: Epoch [12], Batch [339/938], Loss: 0.37858152389526367\n",
      "Validation: Epoch [12], Batch [340/938], Loss: 0.38360700011253357\n",
      "Validation: Epoch [12], Batch [341/938], Loss: 0.39667192101478577\n",
      "Validation: Epoch [12], Batch [342/938], Loss: 0.5692830681800842\n",
      "Validation: Epoch [12], Batch [343/938], Loss: 0.655429482460022\n",
      "Validation: Epoch [12], Batch [344/938], Loss: 0.4888058304786682\n",
      "Validation: Epoch [12], Batch [345/938], Loss: 0.5313447713851929\n",
      "Validation: Epoch [12], Batch [346/938], Loss: 0.3902304172515869\n",
      "Validation: Epoch [12], Batch [347/938], Loss: 0.5015683174133301\n",
      "Validation: Epoch [12], Batch [348/938], Loss: 0.47056320309638977\n",
      "Validation: Epoch [12], Batch [349/938], Loss: 0.4021952152252197\n",
      "Validation: Epoch [12], Batch [350/938], Loss: 0.42095237970352173\n",
      "Validation: Epoch [12], Batch [351/938], Loss: 0.4258747100830078\n",
      "Validation: Epoch [12], Batch [352/938], Loss: 0.36512282490730286\n",
      "Validation: Epoch [12], Batch [353/938], Loss: 0.7242673635482788\n",
      "Validation: Epoch [12], Batch [354/938], Loss: 0.2661188840866089\n",
      "Validation: Epoch [12], Batch [355/938], Loss: 0.5218470096588135\n",
      "Validation: Epoch [12], Batch [356/938], Loss: 0.598500669002533\n",
      "Validation: Epoch [12], Batch [357/938], Loss: 0.384989857673645\n",
      "Validation: Epoch [12], Batch [358/938], Loss: 0.6956934928894043\n",
      "Validation: Epoch [12], Batch [359/938], Loss: 0.4103800356388092\n",
      "Validation: Epoch [12], Batch [360/938], Loss: 0.4899067282676697\n",
      "Validation: Epoch [12], Batch [361/938], Loss: 0.5602563619613647\n",
      "Validation: Epoch [12], Batch [362/938], Loss: 0.5937917232513428\n",
      "Validation: Epoch [12], Batch [363/938], Loss: 0.48094695806503296\n",
      "Validation: Epoch [12], Batch [364/938], Loss: 0.441359281539917\n",
      "Validation: Epoch [12], Batch [365/938], Loss: 0.29741349816322327\n",
      "Validation: Epoch [12], Batch [366/938], Loss: 0.5649317502975464\n",
      "Validation: Epoch [12], Batch [367/938], Loss: 0.45572614669799805\n",
      "Validation: Epoch [12], Batch [368/938], Loss: 0.4968370795249939\n",
      "Validation: Epoch [12], Batch [369/938], Loss: 0.3579104542732239\n",
      "Validation: Epoch [12], Batch [370/938], Loss: 0.43836069107055664\n",
      "Validation: Epoch [12], Batch [371/938], Loss: 0.5015237331390381\n",
      "Validation: Epoch [12], Batch [372/938], Loss: 0.45365116000175476\n",
      "Validation: Epoch [12], Batch [373/938], Loss: 0.7034744024276733\n",
      "Validation: Epoch [12], Batch [374/938], Loss: 0.5652226209640503\n",
      "Validation: Epoch [12], Batch [375/938], Loss: 0.6626467108726501\n",
      "Validation: Epoch [12], Batch [376/938], Loss: 0.4036359190940857\n",
      "Validation: Epoch [12], Batch [377/938], Loss: 0.3348410725593567\n",
      "Validation: Epoch [12], Batch [378/938], Loss: 0.3467938005924225\n",
      "Validation: Epoch [12], Batch [379/938], Loss: 0.5282202959060669\n",
      "Validation: Epoch [12], Batch [380/938], Loss: 0.42136892676353455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [381/938], Loss: 0.5428732633590698\n",
      "Validation: Epoch [12], Batch [382/938], Loss: 0.41920924186706543\n",
      "Validation: Epoch [12], Batch [383/938], Loss: 0.3361489176750183\n",
      "Validation: Epoch [12], Batch [384/938], Loss: 0.4687066078186035\n",
      "Validation: Epoch [12], Batch [385/938], Loss: 0.5249376893043518\n",
      "Validation: Epoch [12], Batch [386/938], Loss: 0.47883108258247375\n",
      "Validation: Epoch [12], Batch [387/938], Loss: 0.4135138988494873\n",
      "Validation: Epoch [12], Batch [388/938], Loss: 0.6664576530456543\n",
      "Validation: Epoch [12], Batch [389/938], Loss: 0.4647209048271179\n",
      "Validation: Epoch [12], Batch [390/938], Loss: 0.4000716209411621\n",
      "Validation: Epoch [12], Batch [391/938], Loss: 0.4234549403190613\n",
      "Validation: Epoch [12], Batch [392/938], Loss: 0.41568508744239807\n",
      "Validation: Epoch [12], Batch [393/938], Loss: 0.5372060537338257\n",
      "Validation: Epoch [12], Batch [394/938], Loss: 0.3954654335975647\n",
      "Validation: Epoch [12], Batch [395/938], Loss: 0.62799072265625\n",
      "Validation: Epoch [12], Batch [396/938], Loss: 0.5187538862228394\n",
      "Validation: Epoch [12], Batch [397/938], Loss: 0.4279487729072571\n",
      "Validation: Epoch [12], Batch [398/938], Loss: 0.5777701735496521\n",
      "Validation: Epoch [12], Batch [399/938], Loss: 0.4088923931121826\n",
      "Validation: Epoch [12], Batch [400/938], Loss: 0.4913509488105774\n",
      "Validation: Epoch [12], Batch [401/938], Loss: 0.3142830729484558\n",
      "Validation: Epoch [12], Batch [402/938], Loss: 0.6256284713745117\n",
      "Validation: Epoch [12], Batch [403/938], Loss: 0.5088393092155457\n",
      "Validation: Epoch [12], Batch [404/938], Loss: 0.40318286418914795\n",
      "Validation: Epoch [12], Batch [405/938], Loss: 0.556978166103363\n",
      "Validation: Epoch [12], Batch [406/938], Loss: 0.48791417479515076\n",
      "Validation: Epoch [12], Batch [407/938], Loss: 0.5010037422180176\n",
      "Validation: Epoch [12], Batch [408/938], Loss: 0.45792001485824585\n",
      "Validation: Epoch [12], Batch [409/938], Loss: 0.516107439994812\n",
      "Validation: Epoch [12], Batch [410/938], Loss: 0.6946203708648682\n",
      "Validation: Epoch [12], Batch [411/938], Loss: 0.5234411358833313\n",
      "Validation: Epoch [12], Batch [412/938], Loss: 0.5415518283843994\n",
      "Validation: Epoch [12], Batch [413/938], Loss: 0.5365419983863831\n",
      "Validation: Epoch [12], Batch [414/938], Loss: 0.5513684153556824\n",
      "Validation: Epoch [12], Batch [415/938], Loss: 0.4749188721179962\n",
      "Validation: Epoch [12], Batch [416/938], Loss: 0.42495155334472656\n",
      "Validation: Epoch [12], Batch [417/938], Loss: 0.7271699905395508\n",
      "Validation: Epoch [12], Batch [418/938], Loss: 0.49427977204322815\n",
      "Validation: Epoch [12], Batch [419/938], Loss: 0.31553518772125244\n",
      "Validation: Epoch [12], Batch [420/938], Loss: 0.402202308177948\n",
      "Validation: Epoch [12], Batch [421/938], Loss: 0.4741242527961731\n",
      "Validation: Epoch [12], Batch [422/938], Loss: 0.3717731535434723\n",
      "Validation: Epoch [12], Batch [423/938], Loss: 0.39530259370803833\n",
      "Validation: Epoch [12], Batch [424/938], Loss: 0.4882649779319763\n",
      "Validation: Epoch [12], Batch [425/938], Loss: 0.45625534653663635\n",
      "Validation: Epoch [12], Batch [426/938], Loss: 0.4610118865966797\n",
      "Validation: Epoch [12], Batch [427/938], Loss: 0.5889496803283691\n",
      "Validation: Epoch [12], Batch [428/938], Loss: 0.5396273136138916\n",
      "Validation: Epoch [12], Batch [429/938], Loss: 0.5723453164100647\n",
      "Validation: Epoch [12], Batch [430/938], Loss: 0.465351402759552\n",
      "Validation: Epoch [12], Batch [431/938], Loss: 0.4526374340057373\n",
      "Validation: Epoch [12], Batch [432/938], Loss: 0.560081422328949\n",
      "Validation: Epoch [12], Batch [433/938], Loss: 0.6265380382537842\n",
      "Validation: Epoch [12], Batch [434/938], Loss: 0.3887559473514557\n",
      "Validation: Epoch [12], Batch [435/938], Loss: 0.35462453961372375\n",
      "Validation: Epoch [12], Batch [436/938], Loss: 0.5084348320960999\n",
      "Validation: Epoch [12], Batch [437/938], Loss: 0.48949822783470154\n",
      "Validation: Epoch [12], Batch [438/938], Loss: 0.5871941447257996\n",
      "Validation: Epoch [12], Batch [439/938], Loss: 0.5128893852233887\n",
      "Validation: Epoch [12], Batch [440/938], Loss: 0.3731732666492462\n",
      "Validation: Epoch [12], Batch [441/938], Loss: 0.41416263580322266\n",
      "Validation: Epoch [12], Batch [442/938], Loss: 0.545217752456665\n",
      "Validation: Epoch [12], Batch [443/938], Loss: 0.6327786445617676\n",
      "Validation: Epoch [12], Batch [444/938], Loss: 0.383323609828949\n",
      "Validation: Epoch [12], Batch [445/938], Loss: 0.3772662281990051\n",
      "Validation: Epoch [12], Batch [446/938], Loss: 0.5092625021934509\n",
      "Validation: Epoch [12], Batch [447/938], Loss: 0.3963911235332489\n",
      "Validation: Epoch [12], Batch [448/938], Loss: 0.6899018287658691\n",
      "Validation: Epoch [12], Batch [449/938], Loss: 0.7323237657546997\n",
      "Validation: Epoch [12], Batch [450/938], Loss: 0.42972666025161743\n",
      "Validation: Epoch [12], Batch [451/938], Loss: 0.446328729391098\n",
      "Validation: Epoch [12], Batch [452/938], Loss: 0.5103014707565308\n",
      "Validation: Epoch [12], Batch [453/938], Loss: 0.44752806425094604\n",
      "Validation: Epoch [12], Batch [454/938], Loss: 0.6016222834587097\n",
      "Validation: Epoch [12], Batch [455/938], Loss: 0.4384073317050934\n",
      "Validation: Epoch [12], Batch [456/938], Loss: 0.509516716003418\n",
      "Validation: Epoch [12], Batch [457/938], Loss: 0.4007309377193451\n",
      "Validation: Epoch [12], Batch [458/938], Loss: 0.3438066840171814\n",
      "Validation: Epoch [12], Batch [459/938], Loss: 0.6165944337844849\n",
      "Validation: Epoch [12], Batch [460/938], Loss: 0.5834834575653076\n",
      "Validation: Epoch [12], Batch [461/938], Loss: 0.4770013093948364\n",
      "Validation: Epoch [12], Batch [462/938], Loss: 0.5807026624679565\n",
      "Validation: Epoch [12], Batch [463/938], Loss: 0.6386592984199524\n",
      "Validation: Epoch [12], Batch [464/938], Loss: 0.35965973138809204\n",
      "Validation: Epoch [12], Batch [465/938], Loss: 0.4398341476917267\n",
      "Validation: Epoch [12], Batch [466/938], Loss: 0.5371403694152832\n",
      "Validation: Epoch [12], Batch [467/938], Loss: 0.5358145236968994\n",
      "Validation: Epoch [12], Batch [468/938], Loss: 0.49424564838409424\n",
      "Validation: Epoch [12], Batch [469/938], Loss: 0.37417322397232056\n",
      "Validation: Epoch [12], Batch [470/938], Loss: 0.4382738471031189\n",
      "Validation: Epoch [12], Batch [471/938], Loss: 0.4754023551940918\n",
      "Validation: Epoch [12], Batch [472/938], Loss: 0.32452642917633057\n",
      "Validation: Epoch [12], Batch [473/938], Loss: 0.6338698863983154\n",
      "Validation: Epoch [12], Batch [474/938], Loss: 0.5385828614234924\n",
      "Validation: Epoch [12], Batch [475/938], Loss: 0.5908247828483582\n",
      "Validation: Epoch [12], Batch [476/938], Loss: 0.4630032479763031\n",
      "Validation: Epoch [12], Batch [477/938], Loss: 0.6096078157424927\n",
      "Validation: Epoch [12], Batch [478/938], Loss: 0.574714183807373\n",
      "Validation: Epoch [12], Batch [479/938], Loss: 0.49847128987312317\n",
      "Validation: Epoch [12], Batch [480/938], Loss: 0.43116021156311035\n",
      "Validation: Epoch [12], Batch [481/938], Loss: 0.48037636280059814\n",
      "Validation: Epoch [12], Batch [482/938], Loss: 0.6295688152313232\n",
      "Validation: Epoch [12], Batch [483/938], Loss: 0.4835320711135864\n",
      "Validation: Epoch [12], Batch [484/938], Loss: 0.4447323679924011\n",
      "Validation: Epoch [12], Batch [485/938], Loss: 0.6467658281326294\n",
      "Validation: Epoch [12], Batch [486/938], Loss: 0.3830958902835846\n",
      "Validation: Epoch [12], Batch [487/938], Loss: 0.42542698979377747\n",
      "Validation: Epoch [12], Batch [488/938], Loss: 0.5819162726402283\n",
      "Validation: Epoch [12], Batch [489/938], Loss: 0.6940592527389526\n",
      "Validation: Epoch [12], Batch [490/938], Loss: 0.6377478837966919\n",
      "Validation: Epoch [12], Batch [491/938], Loss: 0.42187464237213135\n",
      "Validation: Epoch [12], Batch [492/938], Loss: 0.4709169566631317\n",
      "Validation: Epoch [12], Batch [493/938], Loss: 0.6073710322380066\n",
      "Validation: Epoch [12], Batch [494/938], Loss: 0.7261361479759216\n",
      "Validation: Epoch [12], Batch [495/938], Loss: 0.4688867926597595\n",
      "Validation: Epoch [12], Batch [496/938], Loss: 0.2752370238304138\n",
      "Validation: Epoch [12], Batch [497/938], Loss: 0.45773521065711975\n",
      "Validation: Epoch [12], Batch [498/938], Loss: 0.29098397493362427\n",
      "Validation: Epoch [12], Batch [499/938], Loss: 0.6797456741333008\n",
      "Validation: Epoch [12], Batch [500/938], Loss: 0.3935721516609192\n",
      "Validation: Epoch [12], Batch [501/938], Loss: 0.6949445009231567\n",
      "Validation: Epoch [12], Batch [502/938], Loss: 0.593181848526001\n",
      "Validation: Epoch [12], Batch [503/938], Loss: 0.42233002185821533\n",
      "Validation: Epoch [12], Batch [504/938], Loss: 0.6561286449432373\n",
      "Validation: Epoch [12], Batch [505/938], Loss: 0.35665419697761536\n",
      "Validation: Epoch [12], Batch [506/938], Loss: 0.41477692127227783\n",
      "Validation: Epoch [12], Batch [507/938], Loss: 0.5901349186897278\n",
      "Validation: Epoch [12], Batch [508/938], Loss: 0.637057363986969\n",
      "Validation: Epoch [12], Batch [509/938], Loss: 0.5061482191085815\n",
      "Validation: Epoch [12], Batch [510/938], Loss: 0.3415856659412384\n",
      "Validation: Epoch [12], Batch [511/938], Loss: 0.4444672465324402\n",
      "Validation: Epoch [12], Batch [512/938], Loss: 0.43406808376312256\n",
      "Validation: Epoch [12], Batch [513/938], Loss: 0.39264777302742004\n",
      "Validation: Epoch [12], Batch [514/938], Loss: 0.45736461877822876\n",
      "Validation: Epoch [12], Batch [515/938], Loss: 0.345104843378067\n",
      "Validation: Epoch [12], Batch [516/938], Loss: 0.5903348922729492\n",
      "Validation: Epoch [12], Batch [517/938], Loss: 0.48168355226516724\n",
      "Validation: Epoch [12], Batch [518/938], Loss: 0.649874210357666\n",
      "Validation: Epoch [12], Batch [519/938], Loss: 0.5127453207969666\n",
      "Validation: Epoch [12], Batch [520/938], Loss: 0.5557494163513184\n",
      "Validation: Epoch [12], Batch [521/938], Loss: 0.46392717957496643\n",
      "Validation: Epoch [12], Batch [522/938], Loss: 0.5103194713592529\n",
      "Validation: Epoch [12], Batch [523/938], Loss: 0.6409004926681519\n",
      "Validation: Epoch [12], Batch [524/938], Loss: 0.5058075189590454\n",
      "Validation: Epoch [12], Batch [525/938], Loss: 0.6580455899238586\n",
      "Validation: Epoch [12], Batch [526/938], Loss: 0.6345764994621277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [527/938], Loss: 0.4241207242012024\n",
      "Validation: Epoch [12], Batch [528/938], Loss: 0.42455169558525085\n",
      "Validation: Epoch [12], Batch [529/938], Loss: 0.41257578134536743\n",
      "Validation: Epoch [12], Batch [530/938], Loss: 0.6360594034194946\n",
      "Validation: Epoch [12], Batch [531/938], Loss: 0.3857538104057312\n",
      "Validation: Epoch [12], Batch [532/938], Loss: 0.5221572518348694\n",
      "Validation: Epoch [12], Batch [533/938], Loss: 0.6493067741394043\n",
      "Validation: Epoch [12], Batch [534/938], Loss: 0.3803367614746094\n",
      "Validation: Epoch [12], Batch [535/938], Loss: 0.3283976912498474\n",
      "Validation: Epoch [12], Batch [536/938], Loss: 0.4980316162109375\n",
      "Validation: Epoch [12], Batch [537/938], Loss: 0.49142518639564514\n",
      "Validation: Epoch [12], Batch [538/938], Loss: 0.35019999742507935\n",
      "Validation: Epoch [12], Batch [539/938], Loss: 0.6058328151702881\n",
      "Validation: Epoch [12], Batch [540/938], Loss: 0.4300205111503601\n",
      "Validation: Epoch [12], Batch [541/938], Loss: 0.4082910418510437\n",
      "Validation: Epoch [12], Batch [542/938], Loss: 0.4431889057159424\n",
      "Validation: Epoch [12], Batch [543/938], Loss: 0.4942070245742798\n",
      "Validation: Epoch [12], Batch [544/938], Loss: 0.4880353808403015\n",
      "Validation: Epoch [12], Batch [545/938], Loss: 0.44056928157806396\n",
      "Validation: Epoch [12], Batch [546/938], Loss: 0.40496742725372314\n",
      "Validation: Epoch [12], Batch [547/938], Loss: 0.566447377204895\n",
      "Validation: Epoch [12], Batch [548/938], Loss: 0.39855819940567017\n",
      "Validation: Epoch [12], Batch [549/938], Loss: 0.4469906687736511\n",
      "Validation: Epoch [12], Batch [550/938], Loss: 0.4032508134841919\n",
      "Validation: Epoch [12], Batch [551/938], Loss: 0.38800182938575745\n",
      "Validation: Epoch [12], Batch [552/938], Loss: 0.36391276121139526\n",
      "Validation: Epoch [12], Batch [553/938], Loss: 0.343020498752594\n",
      "Validation: Epoch [12], Batch [554/938], Loss: 0.5394943356513977\n",
      "Validation: Epoch [12], Batch [555/938], Loss: 0.4121406078338623\n",
      "Validation: Epoch [12], Batch [556/938], Loss: 0.3641345500946045\n",
      "Validation: Epoch [12], Batch [557/938], Loss: 0.4369197487831116\n",
      "Validation: Epoch [12], Batch [558/938], Loss: 0.3765052855014801\n",
      "Validation: Epoch [12], Batch [559/938], Loss: 0.34142959117889404\n",
      "Validation: Epoch [12], Batch [560/938], Loss: 0.3633825182914734\n",
      "Validation: Epoch [12], Batch [561/938], Loss: 0.5544506907463074\n",
      "Validation: Epoch [12], Batch [562/938], Loss: 0.6266571283340454\n",
      "Validation: Epoch [12], Batch [563/938], Loss: 0.7106149196624756\n",
      "Validation: Epoch [12], Batch [564/938], Loss: 0.4828612804412842\n",
      "Validation: Epoch [12], Batch [565/938], Loss: 0.33281195163726807\n",
      "Validation: Epoch [12], Batch [566/938], Loss: 0.4442368745803833\n",
      "Validation: Epoch [12], Batch [567/938], Loss: 0.6494497060775757\n",
      "Validation: Epoch [12], Batch [568/938], Loss: 0.4073425829410553\n",
      "Validation: Epoch [12], Batch [569/938], Loss: 0.47945451736450195\n",
      "Validation: Epoch [12], Batch [570/938], Loss: 0.7955017685890198\n",
      "Validation: Epoch [12], Batch [571/938], Loss: 0.5015291571617126\n",
      "Validation: Epoch [12], Batch [572/938], Loss: 0.4743915796279907\n",
      "Validation: Epoch [12], Batch [573/938], Loss: 0.3276331424713135\n",
      "Validation: Epoch [12], Batch [574/938], Loss: 0.40732109546661377\n",
      "Validation: Epoch [12], Batch [575/938], Loss: 0.49295857548713684\n",
      "Validation: Epoch [12], Batch [576/938], Loss: 0.5443681478500366\n",
      "Validation: Epoch [12], Batch [577/938], Loss: 0.5310600996017456\n",
      "Validation: Epoch [12], Batch [578/938], Loss: 0.3747161626815796\n",
      "Validation: Epoch [12], Batch [579/938], Loss: 0.7041040062904358\n",
      "Validation: Epoch [12], Batch [580/938], Loss: 0.6343533992767334\n",
      "Validation: Epoch [12], Batch [581/938], Loss: 0.44233912229537964\n",
      "Validation: Epoch [12], Batch [582/938], Loss: 0.5809683799743652\n",
      "Validation: Epoch [12], Batch [583/938], Loss: 0.4512542188167572\n",
      "Validation: Epoch [12], Batch [584/938], Loss: 0.8320053219795227\n",
      "Validation: Epoch [12], Batch [585/938], Loss: 0.3876098096370697\n",
      "Validation: Epoch [12], Batch [586/938], Loss: 0.46230635046958923\n",
      "Validation: Epoch [12], Batch [587/938], Loss: 0.460615873336792\n",
      "Validation: Epoch [12], Batch [588/938], Loss: 0.6182122826576233\n",
      "Validation: Epoch [12], Batch [589/938], Loss: 0.5597327947616577\n",
      "Validation: Epoch [12], Batch [590/938], Loss: 0.35452333092689514\n",
      "Validation: Epoch [12], Batch [591/938], Loss: 0.5200587511062622\n",
      "Validation: Epoch [12], Batch [592/938], Loss: 0.4848584830760956\n",
      "Validation: Epoch [12], Batch [593/938], Loss: 0.5861243009567261\n",
      "Validation: Epoch [12], Batch [594/938], Loss: 0.34021201729774475\n",
      "Validation: Epoch [12], Batch [595/938], Loss: 0.4539481997489929\n",
      "Validation: Epoch [12], Batch [596/938], Loss: 0.6122819781303406\n",
      "Validation: Epoch [12], Batch [597/938], Loss: 0.4297839105129242\n",
      "Validation: Epoch [12], Batch [598/938], Loss: 0.45709043741226196\n",
      "Validation: Epoch [12], Batch [599/938], Loss: 0.40023428201675415\n",
      "Validation: Epoch [12], Batch [600/938], Loss: 0.3569347858428955\n",
      "Validation: Epoch [12], Batch [601/938], Loss: 0.48393410444259644\n",
      "Validation: Epoch [12], Batch [602/938], Loss: 0.32931840419769287\n",
      "Validation: Epoch [12], Batch [603/938], Loss: 0.48891234397888184\n",
      "Validation: Epoch [12], Batch [604/938], Loss: 0.5106073021888733\n",
      "Validation: Epoch [12], Batch [605/938], Loss: 0.5005524158477783\n",
      "Validation: Epoch [12], Batch [606/938], Loss: 0.30052676796913147\n",
      "Validation: Epoch [12], Batch [607/938], Loss: 0.4702381491661072\n",
      "Validation: Epoch [12], Batch [608/938], Loss: 0.3758211135864258\n",
      "Validation: Epoch [12], Batch [609/938], Loss: 0.5697327852249146\n",
      "Validation: Epoch [12], Batch [610/938], Loss: 0.46410292387008667\n",
      "Validation: Epoch [12], Batch [611/938], Loss: 0.4097049832344055\n",
      "Validation: Epoch [12], Batch [612/938], Loss: 0.3437018394470215\n",
      "Validation: Epoch [12], Batch [613/938], Loss: 0.4932307004928589\n",
      "Validation: Epoch [12], Batch [614/938], Loss: 0.5298458337783813\n",
      "Validation: Epoch [12], Batch [615/938], Loss: 0.4052848219871521\n",
      "Validation: Epoch [12], Batch [616/938], Loss: 0.6422876715660095\n",
      "Validation: Epoch [12], Batch [617/938], Loss: 0.47734057903289795\n",
      "Validation: Epoch [12], Batch [618/938], Loss: 0.5171051621437073\n",
      "Validation: Epoch [12], Batch [619/938], Loss: 0.6130173802375793\n",
      "Validation: Epoch [12], Batch [620/938], Loss: 0.739629328250885\n",
      "Validation: Epoch [12], Batch [621/938], Loss: 0.4582529664039612\n",
      "Validation: Epoch [12], Batch [622/938], Loss: 0.8303128480911255\n",
      "Validation: Epoch [12], Batch [623/938], Loss: 0.7887967824935913\n",
      "Validation: Epoch [12], Batch [624/938], Loss: 0.49931612610816956\n",
      "Validation: Epoch [12], Batch [625/938], Loss: 0.38803011178970337\n",
      "Validation: Epoch [12], Batch [626/938], Loss: 0.672492265701294\n",
      "Validation: Epoch [12], Batch [627/938], Loss: 0.5193583965301514\n",
      "Validation: Epoch [12], Batch [628/938], Loss: 0.4383223056793213\n",
      "Validation: Epoch [12], Batch [629/938], Loss: 0.491728276014328\n",
      "Validation: Epoch [12], Batch [630/938], Loss: 0.3744373619556427\n",
      "Validation: Epoch [12], Batch [631/938], Loss: 0.4775471091270447\n",
      "Validation: Epoch [12], Batch [632/938], Loss: 0.44014960527420044\n",
      "Validation: Epoch [12], Batch [633/938], Loss: 0.5198490619659424\n",
      "Validation: Epoch [12], Batch [634/938], Loss: 0.6112309694290161\n",
      "Validation: Epoch [12], Batch [635/938], Loss: 0.578173041343689\n",
      "Validation: Epoch [12], Batch [636/938], Loss: 0.4876195192337036\n",
      "Validation: Epoch [12], Batch [637/938], Loss: 0.5393919348716736\n",
      "Validation: Epoch [12], Batch [638/938], Loss: 0.5487784743309021\n",
      "Validation: Epoch [12], Batch [639/938], Loss: 0.4299054443836212\n",
      "Validation: Epoch [12], Batch [640/938], Loss: 0.4541683793067932\n",
      "Validation: Epoch [12], Batch [641/938], Loss: 0.41803818941116333\n",
      "Validation: Epoch [12], Batch [642/938], Loss: 0.46479707956314087\n",
      "Validation: Epoch [12], Batch [643/938], Loss: 0.5158078670501709\n",
      "Validation: Epoch [12], Batch [644/938], Loss: 0.608461856842041\n",
      "Validation: Epoch [12], Batch [645/938], Loss: 0.569466233253479\n",
      "Validation: Epoch [12], Batch [646/938], Loss: 0.819743275642395\n",
      "Validation: Epoch [12], Batch [647/938], Loss: 0.23563730716705322\n",
      "Validation: Epoch [12], Batch [648/938], Loss: 0.4895281195640564\n",
      "Validation: Epoch [12], Batch [649/938], Loss: 0.38589316606521606\n",
      "Validation: Epoch [12], Batch [650/938], Loss: 0.42130541801452637\n",
      "Validation: Epoch [12], Batch [651/938], Loss: 0.4626502990722656\n",
      "Validation: Epoch [12], Batch [652/938], Loss: 0.4852156341075897\n",
      "Validation: Epoch [12], Batch [653/938], Loss: 0.6326215267181396\n",
      "Validation: Epoch [12], Batch [654/938], Loss: 0.5008168816566467\n",
      "Validation: Epoch [12], Batch [655/938], Loss: 0.3814128637313843\n",
      "Validation: Epoch [12], Batch [656/938], Loss: 0.4868520200252533\n",
      "Validation: Epoch [12], Batch [657/938], Loss: 0.2818449139595032\n",
      "Validation: Epoch [12], Batch [658/938], Loss: 0.4275733232498169\n",
      "Validation: Epoch [12], Batch [659/938], Loss: 0.609103798866272\n",
      "Validation: Epoch [12], Batch [660/938], Loss: 0.4395926892757416\n",
      "Validation: Epoch [12], Batch [661/938], Loss: 0.35021451115608215\n",
      "Validation: Epoch [12], Batch [662/938], Loss: 0.5736225843429565\n",
      "Validation: Epoch [12], Batch [663/938], Loss: 0.4709039628505707\n",
      "Validation: Epoch [12], Batch [664/938], Loss: 0.4217456877231598\n",
      "Validation: Epoch [12], Batch [665/938], Loss: 0.41958826780319214\n",
      "Validation: Epoch [12], Batch [666/938], Loss: 0.7745208740234375\n",
      "Validation: Epoch [12], Batch [667/938], Loss: 0.4758126437664032\n",
      "Validation: Epoch [12], Batch [668/938], Loss: 0.37428396940231323\n",
      "Validation: Epoch [12], Batch [669/938], Loss: 0.411947101354599\n",
      "Validation: Epoch [12], Batch [670/938], Loss: 0.3567522168159485\n",
      "Validation: Epoch [12], Batch [671/938], Loss: 0.518362820148468\n",
      "Validation: Epoch [12], Batch [672/938], Loss: 0.3932599723339081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [673/938], Loss: 0.5772342681884766\n",
      "Validation: Epoch [12], Batch [674/938], Loss: 0.5289426445960999\n",
      "Validation: Epoch [12], Batch [675/938], Loss: 0.4354560971260071\n",
      "Validation: Epoch [12], Batch [676/938], Loss: 0.3941965401172638\n",
      "Validation: Epoch [12], Batch [677/938], Loss: 0.5403537154197693\n",
      "Validation: Epoch [12], Batch [678/938], Loss: 0.5116264820098877\n",
      "Validation: Epoch [12], Batch [679/938], Loss: 0.5306766033172607\n",
      "Validation: Epoch [12], Batch [680/938], Loss: 0.6389268636703491\n",
      "Validation: Epoch [12], Batch [681/938], Loss: 0.6258851289749146\n",
      "Validation: Epoch [12], Batch [682/938], Loss: 0.33975088596343994\n",
      "Validation: Epoch [12], Batch [683/938], Loss: 0.4418697953224182\n",
      "Validation: Epoch [12], Batch [684/938], Loss: 0.2518637478351593\n",
      "Validation: Epoch [12], Batch [685/938], Loss: 0.4586544930934906\n",
      "Validation: Epoch [12], Batch [686/938], Loss: 0.3647755980491638\n",
      "Validation: Epoch [12], Batch [687/938], Loss: 0.4585772752761841\n",
      "Validation: Epoch [12], Batch [688/938], Loss: 0.39530813694000244\n",
      "Validation: Epoch [12], Batch [689/938], Loss: 0.5963250994682312\n",
      "Validation: Epoch [12], Batch [690/938], Loss: 0.5348156690597534\n",
      "Validation: Epoch [12], Batch [691/938], Loss: 0.3770076036453247\n",
      "Validation: Epoch [12], Batch [692/938], Loss: 0.5713913440704346\n",
      "Validation: Epoch [12], Batch [693/938], Loss: 0.37939977645874023\n",
      "Validation: Epoch [12], Batch [694/938], Loss: 0.37472325563430786\n",
      "Validation: Epoch [12], Batch [695/938], Loss: 0.5278247594833374\n",
      "Validation: Epoch [12], Batch [696/938], Loss: 0.46014338731765747\n",
      "Validation: Epoch [12], Batch [697/938], Loss: 0.508782148361206\n",
      "Validation: Epoch [12], Batch [698/938], Loss: 0.30069464445114136\n",
      "Validation: Epoch [12], Batch [699/938], Loss: 0.4876566529273987\n",
      "Validation: Epoch [12], Batch [700/938], Loss: 0.3414555788040161\n",
      "Validation: Epoch [12], Batch [701/938], Loss: 0.39357900619506836\n",
      "Validation: Epoch [12], Batch [702/938], Loss: 0.45502859354019165\n",
      "Validation: Epoch [12], Batch [703/938], Loss: 0.49881887435913086\n",
      "Validation: Epoch [12], Batch [704/938], Loss: 0.43914470076560974\n",
      "Validation: Epoch [12], Batch [705/938], Loss: 0.6646010875701904\n",
      "Validation: Epoch [12], Batch [706/938], Loss: 0.539789617061615\n",
      "Validation: Epoch [12], Batch [707/938], Loss: 0.42831265926361084\n",
      "Validation: Epoch [12], Batch [708/938], Loss: 0.4801373779773712\n",
      "Validation: Epoch [12], Batch [709/938], Loss: 0.44098973274230957\n",
      "Validation: Epoch [12], Batch [710/938], Loss: 0.35766133666038513\n",
      "Validation: Epoch [12], Batch [711/938], Loss: 0.5701170563697815\n",
      "Validation: Epoch [12], Batch [712/938], Loss: 0.434395432472229\n",
      "Validation: Epoch [12], Batch [713/938], Loss: 0.42075198888778687\n",
      "Validation: Epoch [12], Batch [714/938], Loss: 0.5024785995483398\n",
      "Validation: Epoch [12], Batch [715/938], Loss: 0.46518343687057495\n",
      "Validation: Epoch [12], Batch [716/938], Loss: 0.4989909529685974\n",
      "Validation: Epoch [12], Batch [717/938], Loss: 0.5981613397598267\n",
      "Validation: Epoch [12], Batch [718/938], Loss: 0.5385441780090332\n",
      "Validation: Epoch [12], Batch [719/938], Loss: 0.5593122243881226\n",
      "Validation: Epoch [12], Batch [720/938], Loss: 0.3998613953590393\n",
      "Validation: Epoch [12], Batch [721/938], Loss: 0.3067657947540283\n",
      "Validation: Epoch [12], Batch [722/938], Loss: 0.4208308458328247\n",
      "Validation: Epoch [12], Batch [723/938], Loss: 0.5032628178596497\n",
      "Validation: Epoch [12], Batch [724/938], Loss: 0.638565182685852\n",
      "Validation: Epoch [12], Batch [725/938], Loss: 0.4072844386100769\n",
      "Validation: Epoch [12], Batch [726/938], Loss: 0.3684259057044983\n",
      "Validation: Epoch [12], Batch [727/938], Loss: 0.4663922190666199\n",
      "Validation: Epoch [12], Batch [728/938], Loss: 0.5586373209953308\n",
      "Validation: Epoch [12], Batch [729/938], Loss: 0.602605938911438\n",
      "Validation: Epoch [12], Batch [730/938], Loss: 0.4104077219963074\n",
      "Validation: Epoch [12], Batch [731/938], Loss: 0.5984783172607422\n",
      "Validation: Epoch [12], Batch [732/938], Loss: 0.45070120692253113\n",
      "Validation: Epoch [12], Batch [733/938], Loss: 0.5276412963867188\n",
      "Validation: Epoch [12], Batch [734/938], Loss: 0.6421846151351929\n",
      "Validation: Epoch [12], Batch [735/938], Loss: 0.3043425679206848\n",
      "Validation: Epoch [12], Batch [736/938], Loss: 0.6051056385040283\n",
      "Validation: Epoch [12], Batch [737/938], Loss: 0.2924403250217438\n",
      "Validation: Epoch [12], Batch [738/938], Loss: 0.5104793310165405\n",
      "Validation: Epoch [12], Batch [739/938], Loss: 0.4136400818824768\n",
      "Validation: Epoch [12], Batch [740/938], Loss: 0.49003520607948303\n",
      "Validation: Epoch [12], Batch [741/938], Loss: 0.3636576533317566\n",
      "Validation: Epoch [12], Batch [742/938], Loss: 0.38661813735961914\n",
      "Validation: Epoch [12], Batch [743/938], Loss: 0.4740754961967468\n",
      "Validation: Epoch [12], Batch [744/938], Loss: 0.4962448179721832\n",
      "Validation: Epoch [12], Batch [745/938], Loss: 0.4158998429775238\n",
      "Validation: Epoch [12], Batch [746/938], Loss: 0.46091747283935547\n",
      "Validation: Epoch [12], Batch [747/938], Loss: 0.3309846520423889\n",
      "Validation: Epoch [12], Batch [748/938], Loss: 0.4493883550167084\n",
      "Validation: Epoch [12], Batch [749/938], Loss: 0.3697497546672821\n",
      "Validation: Epoch [12], Batch [750/938], Loss: 0.590036153793335\n",
      "Validation: Epoch [12], Batch [751/938], Loss: 0.5037791728973389\n",
      "Validation: Epoch [12], Batch [752/938], Loss: 0.468213826417923\n",
      "Validation: Epoch [12], Batch [753/938], Loss: 0.49463680386543274\n",
      "Validation: Epoch [12], Batch [754/938], Loss: 0.5713878273963928\n",
      "Validation: Epoch [12], Batch [755/938], Loss: 0.3728995621204376\n",
      "Validation: Epoch [12], Batch [756/938], Loss: 0.459272563457489\n",
      "Validation: Epoch [12], Batch [757/938], Loss: 0.27862533926963806\n",
      "Validation: Epoch [12], Batch [758/938], Loss: 0.5515750646591187\n",
      "Validation: Epoch [12], Batch [759/938], Loss: 0.41986528038978577\n",
      "Validation: Epoch [12], Batch [760/938], Loss: 0.6538876295089722\n",
      "Validation: Epoch [12], Batch [761/938], Loss: 0.44273313879966736\n",
      "Validation: Epoch [12], Batch [762/938], Loss: 0.590866208076477\n",
      "Validation: Epoch [12], Batch [763/938], Loss: 0.423255056142807\n",
      "Validation: Epoch [12], Batch [764/938], Loss: 0.49532580375671387\n",
      "Validation: Epoch [12], Batch [765/938], Loss: 0.31890466809272766\n",
      "Validation: Epoch [12], Batch [766/938], Loss: 0.4070414900779724\n",
      "Validation: Epoch [12], Batch [767/938], Loss: 0.49248459935188293\n",
      "Validation: Epoch [12], Batch [768/938], Loss: 0.5203567147254944\n",
      "Validation: Epoch [12], Batch [769/938], Loss: 0.3774082362651825\n",
      "Validation: Epoch [12], Batch [770/938], Loss: 0.4506617784500122\n",
      "Validation: Epoch [12], Batch [771/938], Loss: 0.44461333751678467\n",
      "Validation: Epoch [12], Batch [772/938], Loss: 0.5277206897735596\n",
      "Validation: Epoch [12], Batch [773/938], Loss: 0.5227385759353638\n",
      "Validation: Epoch [12], Batch [774/938], Loss: 0.5023815631866455\n",
      "Validation: Epoch [12], Batch [775/938], Loss: 0.40993747115135193\n",
      "Validation: Epoch [12], Batch [776/938], Loss: 0.33809444308280945\n",
      "Validation: Epoch [12], Batch [777/938], Loss: 0.5610690712928772\n",
      "Validation: Epoch [12], Batch [778/938], Loss: 0.433258056640625\n",
      "Validation: Epoch [12], Batch [779/938], Loss: 0.45579275488853455\n",
      "Validation: Epoch [12], Batch [780/938], Loss: 0.3458704352378845\n",
      "Validation: Epoch [12], Batch [781/938], Loss: 0.4009470045566559\n",
      "Validation: Epoch [12], Batch [782/938], Loss: 0.31922730803489685\n",
      "Validation: Epoch [12], Batch [783/938], Loss: 0.5097771883010864\n",
      "Validation: Epoch [12], Batch [784/938], Loss: 0.581290602684021\n",
      "Validation: Epoch [12], Batch [785/938], Loss: 0.4964984953403473\n",
      "Validation: Epoch [12], Batch [786/938], Loss: 0.47638386487960815\n",
      "Validation: Epoch [12], Batch [787/938], Loss: 0.5467912554740906\n",
      "Validation: Epoch [12], Batch [788/938], Loss: 0.5597007870674133\n",
      "Validation: Epoch [12], Batch [789/938], Loss: 0.43867143988609314\n",
      "Validation: Epoch [12], Batch [790/938], Loss: 0.31600284576416016\n",
      "Validation: Epoch [12], Batch [791/938], Loss: 0.47057417035102844\n",
      "Validation: Epoch [12], Batch [792/938], Loss: 0.6050246953964233\n",
      "Validation: Epoch [12], Batch [793/938], Loss: 0.42473214864730835\n",
      "Validation: Epoch [12], Batch [794/938], Loss: 0.4771006405353546\n",
      "Validation: Epoch [12], Batch [795/938], Loss: 0.58678138256073\n",
      "Validation: Epoch [12], Batch [796/938], Loss: 0.5543146133422852\n",
      "Validation: Epoch [12], Batch [797/938], Loss: 0.4146013855934143\n",
      "Validation: Epoch [12], Batch [798/938], Loss: 0.37939247488975525\n",
      "Validation: Epoch [12], Batch [799/938], Loss: 0.3882104158401489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [800/938], Loss: 0.4188442826271057\n",
      "Validation: Epoch [12], Batch [801/938], Loss: 0.5345818996429443\n",
      "Validation: Epoch [12], Batch [802/938], Loss: 0.41490787267684937\n",
      "Validation: Epoch [12], Batch [803/938], Loss: 0.5480442047119141\n",
      "Validation: Epoch [12], Batch [804/938], Loss: 0.41220954060554504\n",
      "Validation: Epoch [12], Batch [805/938], Loss: 0.5781494379043579\n",
      "Validation: Epoch [12], Batch [806/938], Loss: 0.4258992373943329\n",
      "Validation: Epoch [12], Batch [807/938], Loss: 0.362567275762558\n",
      "Validation: Epoch [12], Batch [808/938], Loss: 0.32353585958480835\n",
      "Validation: Epoch [12], Batch [809/938], Loss: 0.37683406472206116\n",
      "Validation: Epoch [12], Batch [810/938], Loss: 0.36441683769226074\n",
      "Validation: Epoch [12], Batch [811/938], Loss: 0.3737921416759491\n",
      "Validation: Epoch [12], Batch [812/938], Loss: 0.46268242597579956\n",
      "Validation: Epoch [12], Batch [813/938], Loss: 0.436641126871109\n",
      "Validation: Epoch [12], Batch [814/938], Loss: 0.3700023293495178\n",
      "Validation: Epoch [12], Batch [815/938], Loss: 0.3936467468738556\n",
      "Validation: Epoch [12], Batch [816/938], Loss: 0.30648475885391235\n",
      "Validation: Epoch [12], Batch [817/938], Loss: 0.5516719818115234\n",
      "Validation: Epoch [12], Batch [818/938], Loss: 0.33076372742652893\n",
      "Validation: Epoch [12], Batch [819/938], Loss: 0.5444432497024536\n",
      "Validation: Epoch [12], Batch [820/938], Loss: 0.3781769275665283\n",
      "Validation: Epoch [12], Batch [821/938], Loss: 0.5015584230422974\n",
      "Validation: Epoch [12], Batch [822/938], Loss: 0.4779333472251892\n",
      "Validation: Epoch [12], Batch [823/938], Loss: 0.4178723990917206\n",
      "Validation: Epoch [12], Batch [824/938], Loss: 0.5655826926231384\n",
      "Validation: Epoch [12], Batch [825/938], Loss: 0.4904214143753052\n",
      "Validation: Epoch [12], Batch [826/938], Loss: 0.4860541820526123\n",
      "Validation: Epoch [12], Batch [827/938], Loss: 0.45593833923339844\n",
      "Validation: Epoch [12], Batch [828/938], Loss: 0.6290995478630066\n",
      "Validation: Epoch [12], Batch [829/938], Loss: 0.39922040700912476\n",
      "Validation: Epoch [12], Batch [830/938], Loss: 0.4740401804447174\n",
      "Validation: Epoch [12], Batch [831/938], Loss: 0.3912777304649353\n",
      "Validation: Epoch [12], Batch [832/938], Loss: 0.5167578458786011\n",
      "Validation: Epoch [12], Batch [833/938], Loss: 0.53997802734375\n",
      "Validation: Epoch [12], Batch [834/938], Loss: 0.23739241063594818\n",
      "Validation: Epoch [12], Batch [835/938], Loss: 0.5347923636436462\n",
      "Validation: Epoch [12], Batch [836/938], Loss: 0.4603026509284973\n",
      "Validation: Epoch [12], Batch [837/938], Loss: 0.4502566456794739\n",
      "Validation: Epoch [12], Batch [838/938], Loss: 0.6703884601593018\n",
      "Validation: Epoch [12], Batch [839/938], Loss: 0.5005993247032166\n",
      "Validation: Epoch [12], Batch [840/938], Loss: 0.5174276828765869\n",
      "Validation: Epoch [12], Batch [841/938], Loss: 0.6009994745254517\n",
      "Validation: Epoch [12], Batch [842/938], Loss: 0.4270424544811249\n",
      "Validation: Epoch [12], Batch [843/938], Loss: 0.43253275752067566\n",
      "Validation: Epoch [12], Batch [844/938], Loss: 0.4025821089744568\n",
      "Validation: Epoch [12], Batch [845/938], Loss: 0.48283153772354126\n",
      "Validation: Epoch [12], Batch [846/938], Loss: 0.4630414843559265\n",
      "Validation: Epoch [12], Batch [847/938], Loss: 0.44096770882606506\n",
      "Validation: Epoch [12], Batch [848/938], Loss: 0.28705984354019165\n",
      "Validation: Epoch [12], Batch [849/938], Loss: 0.35424181818962097\n",
      "Validation: Epoch [12], Batch [850/938], Loss: 0.5543326139450073\n",
      "Validation: Epoch [12], Batch [851/938], Loss: 0.5663497447967529\n",
      "Validation: Epoch [12], Batch [852/938], Loss: 0.5431627035140991\n",
      "Validation: Epoch [12], Batch [853/938], Loss: 0.43520885705947876\n",
      "Validation: Epoch [12], Batch [854/938], Loss: 0.39771485328674316\n",
      "Validation: Epoch [12], Batch [855/938], Loss: 0.47894734144210815\n",
      "Validation: Epoch [12], Batch [856/938], Loss: 0.3602195382118225\n",
      "Validation: Epoch [12], Batch [857/938], Loss: 0.577226996421814\n",
      "Validation: Epoch [12], Batch [858/938], Loss: 0.4813286066055298\n",
      "Validation: Epoch [12], Batch [859/938], Loss: 0.6325665712356567\n",
      "Validation: Epoch [12], Batch [860/938], Loss: 0.35844120383262634\n",
      "Validation: Epoch [12], Batch [861/938], Loss: 0.39937257766723633\n",
      "Validation: Epoch [12], Batch [862/938], Loss: 0.49902379512786865\n",
      "Validation: Epoch [12], Batch [863/938], Loss: 0.488343745470047\n",
      "Validation: Epoch [12], Batch [864/938], Loss: 0.5043675899505615\n",
      "Validation: Epoch [12], Batch [865/938], Loss: 0.3482406735420227\n",
      "Validation: Epoch [12], Batch [866/938], Loss: 0.4593622088432312\n",
      "Validation: Epoch [12], Batch [867/938], Loss: 0.38000091910362244\n",
      "Validation: Epoch [12], Batch [868/938], Loss: 0.5257836580276489\n",
      "Validation: Epoch [12], Batch [869/938], Loss: 0.5329994559288025\n",
      "Validation: Epoch [12], Batch [870/938], Loss: 0.3704386353492737\n",
      "Validation: Epoch [12], Batch [871/938], Loss: 0.41472798585891724\n",
      "Validation: Epoch [12], Batch [872/938], Loss: 0.7648848295211792\n",
      "Validation: Epoch [12], Batch [873/938], Loss: 0.41522017121315\n",
      "Validation: Epoch [12], Batch [874/938], Loss: 0.3912687301635742\n",
      "Validation: Epoch [12], Batch [875/938], Loss: 0.5621533393859863\n",
      "Validation: Epoch [12], Batch [876/938], Loss: 0.532794177532196\n",
      "Validation: Epoch [12], Batch [877/938], Loss: 0.5113895535469055\n",
      "Validation: Epoch [12], Batch [878/938], Loss: 0.4610866606235504\n",
      "Validation: Epoch [12], Batch [879/938], Loss: 0.5436127185821533\n",
      "Validation: Epoch [12], Batch [880/938], Loss: 0.5864316821098328\n",
      "Validation: Epoch [12], Batch [881/938], Loss: 0.39489316940307617\n",
      "Validation: Epoch [12], Batch [882/938], Loss: 0.47655969858169556\n",
      "Validation: Epoch [12], Batch [883/938], Loss: 0.6962693929672241\n",
      "Validation: Epoch [12], Batch [884/938], Loss: 0.39925628900527954\n",
      "Validation: Epoch [12], Batch [885/938], Loss: 0.44917750358581543\n",
      "Validation: Epoch [12], Batch [886/938], Loss: 0.4432185888290405\n",
      "Validation: Epoch [12], Batch [887/938], Loss: 0.6351210474967957\n",
      "Validation: Epoch [12], Batch [888/938], Loss: 0.29524195194244385\n",
      "Validation: Epoch [12], Batch [889/938], Loss: 0.44022318720817566\n",
      "Validation: Epoch [12], Batch [890/938], Loss: 0.7662964463233948\n",
      "Validation: Epoch [12], Batch [891/938], Loss: 0.3945923447608948\n",
      "Validation: Epoch [12], Batch [892/938], Loss: 0.44187432527542114\n",
      "Validation: Epoch [12], Batch [893/938], Loss: 0.43515869975090027\n",
      "Validation: Epoch [12], Batch [894/938], Loss: 0.43531912565231323\n",
      "Validation: Epoch [12], Batch [895/938], Loss: 0.5159997940063477\n",
      "Validation: Epoch [12], Batch [896/938], Loss: 0.5431303381919861\n",
      "Validation: Epoch [12], Batch [897/938], Loss: 0.47687816619873047\n",
      "Validation: Epoch [12], Batch [898/938], Loss: 0.2666857838630676\n",
      "Validation: Epoch [12], Batch [899/938], Loss: 0.4267019033432007\n",
      "Validation: Epoch [12], Batch [900/938], Loss: 0.5472730398178101\n",
      "Validation: Epoch [12], Batch [901/938], Loss: 0.4591013193130493\n",
      "Validation: Epoch [12], Batch [902/938], Loss: 0.49434489011764526\n",
      "Validation: Epoch [12], Batch [903/938], Loss: 0.4467819333076477\n",
      "Validation: Epoch [12], Batch [904/938], Loss: 0.44241684675216675\n",
      "Validation: Epoch [12], Batch [905/938], Loss: 0.4618256688117981\n",
      "Validation: Epoch [12], Batch [906/938], Loss: 0.6537455916404724\n",
      "Validation: Epoch [12], Batch [907/938], Loss: 0.425880491733551\n",
      "Validation: Epoch [12], Batch [908/938], Loss: 0.35157227516174316\n",
      "Validation: Epoch [12], Batch [909/938], Loss: 0.5531492233276367\n",
      "Validation: Epoch [12], Batch [910/938], Loss: 0.38059529662132263\n",
      "Validation: Epoch [12], Batch [911/938], Loss: 0.6631453633308411\n",
      "Validation: Epoch [12], Batch [912/938], Loss: 0.3894488215446472\n",
      "Validation: Epoch [12], Batch [913/938], Loss: 0.6289626359939575\n",
      "Validation: Epoch [12], Batch [914/938], Loss: 0.41455018520355225\n",
      "Validation: Epoch [12], Batch [915/938], Loss: 0.5134176015853882\n",
      "Validation: Epoch [12], Batch [916/938], Loss: 0.40243199467658997\n",
      "Validation: Epoch [12], Batch [917/938], Loss: 0.44895708560943604\n",
      "Validation: Epoch [12], Batch [918/938], Loss: 0.3403977155685425\n",
      "Validation: Epoch [12], Batch [919/938], Loss: 0.5125203132629395\n",
      "Validation: Epoch [12], Batch [920/938], Loss: 0.6366888284683228\n",
      "Validation: Epoch [12], Batch [921/938], Loss: 0.3357818126678467\n",
      "Validation: Epoch [12], Batch [922/938], Loss: 0.5357184410095215\n",
      "Validation: Epoch [12], Batch [923/938], Loss: 0.5977949500083923\n",
      "Validation: Epoch [12], Batch [924/938], Loss: 0.627208948135376\n",
      "Validation: Epoch [12], Batch [925/938], Loss: 0.4568255841732025\n",
      "Validation: Epoch [12], Batch [926/938], Loss: 0.8459197282791138\n",
      "Validation: Epoch [12], Batch [927/938], Loss: 0.47879865765571594\n",
      "Validation: Epoch [12], Batch [928/938], Loss: 0.5901060104370117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [12], Batch [929/938], Loss: 0.3720358610153198\n",
      "Validation: Epoch [12], Batch [930/938], Loss: 0.42867398262023926\n",
      "Validation: Epoch [12], Batch [931/938], Loss: 0.5470726490020752\n",
      "Validation: Epoch [12], Batch [932/938], Loss: 0.4153997302055359\n",
      "Validation: Epoch [12], Batch [933/938], Loss: 0.7263887524604797\n",
      "Validation: Epoch [12], Batch [934/938], Loss: 0.35021066665649414\n",
      "Validation: Epoch [12], Batch [935/938], Loss: 0.37565910816192627\n",
      "Validation: Epoch [12], Batch [936/938], Loss: 0.38427475094795227\n",
      "Validation: Epoch [12], Batch [937/938], Loss: 0.5330410003662109\n",
      "Validation: Epoch [12], Batch [938/938], Loss: 0.7106366157531738\n",
      "Accuracy of test set: 0.8339666666666666\n",
      "Train: Epoch [13], Batch [1/938], Loss: 0.4727715849876404\n",
      "Train: Epoch [13], Batch [2/938], Loss: 0.332550585269928\n",
      "Train: Epoch [13], Batch [3/938], Loss: 0.5505654811859131\n",
      "Train: Epoch [13], Batch [4/938], Loss: 0.45614683628082275\n",
      "Train: Epoch [13], Batch [5/938], Loss: 0.5846505165100098\n",
      "Train: Epoch [13], Batch [6/938], Loss: 0.5241285562515259\n",
      "Train: Epoch [13], Batch [7/938], Loss: 0.5614380836486816\n",
      "Train: Epoch [13], Batch [8/938], Loss: 0.4464152455329895\n",
      "Train: Epoch [13], Batch [9/938], Loss: 0.4723016917705536\n",
      "Train: Epoch [13], Batch [10/938], Loss: 0.44976961612701416\n",
      "Train: Epoch [13], Batch [11/938], Loss: 0.5501326322555542\n",
      "Train: Epoch [13], Batch [12/938], Loss: 0.39270544052124023\n",
      "Train: Epoch [13], Batch [13/938], Loss: 0.5068181157112122\n",
      "Train: Epoch [13], Batch [14/938], Loss: 0.3840629458427429\n",
      "Train: Epoch [13], Batch [15/938], Loss: 0.40389642119407654\n",
      "Train: Epoch [13], Batch [16/938], Loss: 0.40188637375831604\n",
      "Train: Epoch [13], Batch [17/938], Loss: 0.7095882892608643\n",
      "Train: Epoch [13], Batch [18/938], Loss: 0.65357506275177\n",
      "Train: Epoch [13], Batch [19/938], Loss: 0.43414244055747986\n",
      "Train: Epoch [13], Batch [20/938], Loss: 0.6259679794311523\n",
      "Train: Epoch [13], Batch [21/938], Loss: 0.6760934591293335\n",
      "Train: Epoch [13], Batch [22/938], Loss: 0.4633168578147888\n",
      "Train: Epoch [13], Batch [23/938], Loss: 0.3918015956878662\n",
      "Train: Epoch [13], Batch [24/938], Loss: 0.3888150751590729\n",
      "Train: Epoch [13], Batch [25/938], Loss: 0.518322229385376\n",
      "Train: Epoch [13], Batch [26/938], Loss: 0.5979805588722229\n",
      "Train: Epoch [13], Batch [27/938], Loss: 0.5851321816444397\n",
      "Train: Epoch [13], Batch [28/938], Loss: 0.41985130310058594\n",
      "Train: Epoch [13], Batch [29/938], Loss: 0.5857857465744019\n",
      "Train: Epoch [13], Batch [30/938], Loss: 0.6196154952049255\n",
      "Train: Epoch [13], Batch [31/938], Loss: 0.4409430921077728\n",
      "Train: Epoch [13], Batch [32/938], Loss: 0.5665708780288696\n",
      "Train: Epoch [13], Batch [33/938], Loss: 0.5342684984207153\n",
      "Train: Epoch [13], Batch [34/938], Loss: 0.43767249584198\n",
      "Train: Epoch [13], Batch [35/938], Loss: 0.5787902474403381\n",
      "Train: Epoch [13], Batch [36/938], Loss: 0.5680966377258301\n",
      "Train: Epoch [13], Batch [37/938], Loss: 0.622715950012207\n",
      "Train: Epoch [13], Batch [38/938], Loss: 0.41435882449150085\n",
      "Train: Epoch [13], Batch [39/938], Loss: 0.4108338952064514\n",
      "Train: Epoch [13], Batch [40/938], Loss: 0.39341461658477783\n",
      "Train: Epoch [13], Batch [41/938], Loss: 0.4612511694431305\n",
      "Train: Epoch [13], Batch [42/938], Loss: 0.64723801612854\n",
      "Train: Epoch [13], Batch [43/938], Loss: 0.43171921372413635\n",
      "Train: Epoch [13], Batch [44/938], Loss: 0.438960462808609\n",
      "Train: Epoch [13], Batch [45/938], Loss: 0.4709217846393585\n",
      "Train: Epoch [13], Batch [46/938], Loss: 0.4515112042427063\n",
      "Train: Epoch [13], Batch [47/938], Loss: 0.435808390378952\n",
      "Train: Epoch [13], Batch [48/938], Loss: 0.39373475313186646\n",
      "Train: Epoch [13], Batch [49/938], Loss: 0.5985895395278931\n",
      "Train: Epoch [13], Batch [50/938], Loss: 0.4138755202293396\n",
      "Train: Epoch [13], Batch [51/938], Loss: 0.5204957723617554\n",
      "Train: Epoch [13], Batch [52/938], Loss: 0.4820183217525482\n",
      "Train: Epoch [13], Batch [53/938], Loss: 0.48428019881248474\n",
      "Train: Epoch [13], Batch [54/938], Loss: 0.4723014235496521\n",
      "Train: Epoch [13], Batch [55/938], Loss: 0.30398672819137573\n",
      "Train: Epoch [13], Batch [56/938], Loss: 0.4929197430610657\n",
      "Train: Epoch [13], Batch [57/938], Loss: 0.4143146574497223\n",
      "Train: Epoch [13], Batch [58/938], Loss: 0.4776989817619324\n",
      "Train: Epoch [13], Batch [59/938], Loss: 0.44121426343917847\n",
      "Train: Epoch [13], Batch [60/938], Loss: 0.44540032744407654\n",
      "Train: Epoch [13], Batch [61/938], Loss: 0.513334333896637\n",
      "Train: Epoch [13], Batch [62/938], Loss: 0.4949760138988495\n",
      "Train: Epoch [13], Batch [63/938], Loss: 0.510874330997467\n",
      "Train: Epoch [13], Batch [64/938], Loss: 0.6162541508674622\n",
      "Train: Epoch [13], Batch [65/938], Loss: 0.5376058220863342\n",
      "Train: Epoch [13], Batch [66/938], Loss: 0.45995479822158813\n",
      "Train: Epoch [13], Batch [67/938], Loss: 0.4298016130924225\n",
      "Train: Epoch [13], Batch [68/938], Loss: 0.5968766808509827\n",
      "Train: Epoch [13], Batch [69/938], Loss: 0.33194541931152344\n",
      "Train: Epoch [13], Batch [70/938], Loss: 0.6198627352714539\n",
      "Train: Epoch [13], Batch [71/938], Loss: 0.4153369963169098\n",
      "Train: Epoch [13], Batch [72/938], Loss: 0.41799044609069824\n",
      "Train: Epoch [13], Batch [73/938], Loss: 0.4671429693698883\n",
      "Train: Epoch [13], Batch [74/938], Loss: 0.5405409336090088\n",
      "Train: Epoch [13], Batch [75/938], Loss: 0.47965526580810547\n",
      "Train: Epoch [13], Batch [76/938], Loss: 0.45174312591552734\n",
      "Train: Epoch [13], Batch [77/938], Loss: 0.3749362826347351\n",
      "Train: Epoch [13], Batch [78/938], Loss: 0.47400081157684326\n",
      "Train: Epoch [13], Batch [79/938], Loss: 0.33125627040863037\n",
      "Train: Epoch [13], Batch [80/938], Loss: 0.6333718299865723\n",
      "Train: Epoch [13], Batch [81/938], Loss: 0.6276829838752747\n",
      "Train: Epoch [13], Batch [82/938], Loss: 0.48388999700546265\n",
      "Train: Epoch [13], Batch [83/938], Loss: 0.5408725738525391\n",
      "Train: Epoch [13], Batch [84/938], Loss: 0.6110839247703552\n",
      "Train: Epoch [13], Batch [85/938], Loss: 0.561208963394165\n",
      "Train: Epoch [13], Batch [86/938], Loss: 0.5505265593528748\n",
      "Train: Epoch [13], Batch [87/938], Loss: 0.621885359287262\n",
      "Train: Epoch [13], Batch [88/938], Loss: 0.5878491401672363\n",
      "Train: Epoch [13], Batch [89/938], Loss: 0.5716778039932251\n",
      "Train: Epoch [13], Batch [90/938], Loss: 0.5224123001098633\n",
      "Train: Epoch [13], Batch [91/938], Loss: 0.5696839094161987\n",
      "Train: Epoch [13], Batch [92/938], Loss: 0.6141765713691711\n",
      "Train: Epoch [13], Batch [93/938], Loss: 0.4167514443397522\n",
      "Train: Epoch [13], Batch [94/938], Loss: 0.5634493827819824\n",
      "Train: Epoch [13], Batch [95/938], Loss: 0.5046149492263794\n",
      "Train: Epoch [13], Batch [96/938], Loss: 0.37920480966567993\n",
      "Train: Epoch [13], Batch [97/938], Loss: 0.4210459589958191\n",
      "Train: Epoch [13], Batch [98/938], Loss: 0.5210088491439819\n",
      "Train: Epoch [13], Batch [99/938], Loss: 0.4094786047935486\n",
      "Train: Epoch [13], Batch [100/938], Loss: 0.4183928966522217\n",
      "Train: Epoch [13], Batch [101/938], Loss: 0.45542651414871216\n",
      "Train: Epoch [13], Batch [102/938], Loss: 0.5733860731124878\n",
      "Train: Epoch [13], Batch [103/938], Loss: 0.5323961973190308\n",
      "Train: Epoch [13], Batch [104/938], Loss: 0.458710253238678\n",
      "Train: Epoch [13], Batch [105/938], Loss: 0.38132625818252563\n",
      "Train: Epoch [13], Batch [106/938], Loss: 0.4341876208782196\n",
      "Train: Epoch [13], Batch [107/938], Loss: 0.5537680387496948\n",
      "Train: Epoch [13], Batch [108/938], Loss: 0.4219621419906616\n",
      "Train: Epoch [13], Batch [109/938], Loss: 0.5163198113441467\n",
      "Train: Epoch [13], Batch [110/938], Loss: 0.6155779361724854\n",
      "Train: Epoch [13], Batch [111/938], Loss: 0.37789949774742126\n",
      "Train: Epoch [13], Batch [112/938], Loss: 0.25374966859817505\n",
      "Train: Epoch [13], Batch [113/938], Loss: 0.5207631587982178\n",
      "Train: Epoch [13], Batch [114/938], Loss: 0.3841964900493622\n",
      "Train: Epoch [13], Batch [115/938], Loss: 0.6297858357429504\n",
      "Train: Epoch [13], Batch [116/938], Loss: 0.6956958174705505\n",
      "Train: Epoch [13], Batch [117/938], Loss: 0.42295679450035095\n",
      "Train: Epoch [13], Batch [118/938], Loss: 0.4590175151824951\n",
      "Train: Epoch [13], Batch [119/938], Loss: 0.4208703637123108\n",
      "Train: Epoch [13], Batch [120/938], Loss: 0.6247518658638\n",
      "Train: Epoch [13], Batch [121/938], Loss: 0.5964373350143433\n",
      "Train: Epoch [13], Batch [122/938], Loss: 0.345713347196579\n",
      "Train: Epoch [13], Batch [123/938], Loss: 0.5569195747375488\n",
      "Train: Epoch [13], Batch [124/938], Loss: 0.577006995677948\n",
      "Train: Epoch [13], Batch [125/938], Loss: 0.4334506094455719\n",
      "Train: Epoch [13], Batch [126/938], Loss: 0.4260898232460022\n",
      "Train: Epoch [13], Batch [127/938], Loss: 0.5095123052597046\n",
      "Train: Epoch [13], Batch [128/938], Loss: 0.5142210721969604\n",
      "Train: Epoch [13], Batch [129/938], Loss: 0.46159833669662476\n",
      "Train: Epoch [13], Batch [130/938], Loss: 0.6120320558547974\n",
      "Train: Epoch [13], Batch [131/938], Loss: 0.774963915348053\n",
      "Train: Epoch [13], Batch [132/938], Loss: 0.5638197660446167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [13], Batch [133/938], Loss: 0.4799498915672302\n",
      "Train: Epoch [13], Batch [134/938], Loss: 0.4189527928829193\n",
      "Train: Epoch [13], Batch [135/938], Loss: 0.6985770463943481\n",
      "Train: Epoch [13], Batch [136/938], Loss: 0.4888429045677185\n",
      "Train: Epoch [13], Batch [137/938], Loss: 0.46448612213134766\n",
      "Train: Epoch [13], Batch [138/938], Loss: 0.3463113307952881\n",
      "Train: Epoch [13], Batch [139/938], Loss: 0.535152018070221\n",
      "Train: Epoch [13], Batch [140/938], Loss: 0.4005701243877411\n",
      "Train: Epoch [13], Batch [141/938], Loss: 0.7014068365097046\n",
      "Train: Epoch [13], Batch [142/938], Loss: 0.5061749815940857\n",
      "Train: Epoch [13], Batch [143/938], Loss: 0.4316130578517914\n",
      "Train: Epoch [13], Batch [144/938], Loss: 0.35858792066574097\n",
      "Train: Epoch [13], Batch [145/938], Loss: 0.33208659291267395\n",
      "Train: Epoch [13], Batch [146/938], Loss: 0.4407670795917511\n",
      "Train: Epoch [13], Batch [147/938], Loss: 0.5358877778053284\n",
      "Train: Epoch [13], Batch [148/938], Loss: 0.5066076517105103\n",
      "Train: Epoch [13], Batch [149/938], Loss: 0.4880193769931793\n",
      "Train: Epoch [13], Batch [150/938], Loss: 0.40817704796791077\n",
      "Train: Epoch [13], Batch [151/938], Loss: 0.4510158598423004\n",
      "Train: Epoch [13], Batch [152/938], Loss: 0.4600599408149719\n",
      "Train: Epoch [13], Batch [153/938], Loss: 0.37006962299346924\n",
      "Train: Epoch [13], Batch [154/938], Loss: 0.3987673223018646\n",
      "Train: Epoch [13], Batch [155/938], Loss: 0.5140224099159241\n",
      "Train: Epoch [13], Batch [156/938], Loss: 0.5524007678031921\n",
      "Train: Epoch [13], Batch [157/938], Loss: 0.5301811695098877\n",
      "Train: Epoch [13], Batch [158/938], Loss: 0.536047101020813\n",
      "Train: Epoch [13], Batch [159/938], Loss: 0.4319675862789154\n",
      "Train: Epoch [13], Batch [160/938], Loss: 0.42437389492988586\n",
      "Train: Epoch [13], Batch [161/938], Loss: 0.4521375000476837\n",
      "Train: Epoch [13], Batch [162/938], Loss: 0.6672350168228149\n",
      "Train: Epoch [13], Batch [163/938], Loss: 0.5061909556388855\n",
      "Train: Epoch [13], Batch [164/938], Loss: 0.5096524953842163\n",
      "Train: Epoch [13], Batch [165/938], Loss: 0.5921616554260254\n",
      "Train: Epoch [13], Batch [166/938], Loss: 0.38479697704315186\n",
      "Train: Epoch [13], Batch [167/938], Loss: 0.42179813981056213\n",
      "Train: Epoch [13], Batch [168/938], Loss: 0.5264148712158203\n",
      "Train: Epoch [13], Batch [169/938], Loss: 0.47862088680267334\n",
      "Train: Epoch [13], Batch [170/938], Loss: 0.5098260641098022\n",
      "Train: Epoch [13], Batch [171/938], Loss: 0.4734915494918823\n",
      "Train: Epoch [13], Batch [172/938], Loss: 0.3007079064846039\n",
      "Train: Epoch [13], Batch [173/938], Loss: 0.5296422243118286\n",
      "Train: Epoch [13], Batch [174/938], Loss: 0.6838618516921997\n",
      "Train: Epoch [13], Batch [175/938], Loss: 0.8056419491767883\n",
      "Train: Epoch [13], Batch [176/938], Loss: 0.324457585811615\n",
      "Train: Epoch [13], Batch [177/938], Loss: 0.4792611002922058\n",
      "Train: Epoch [13], Batch [178/938], Loss: 0.7033635377883911\n",
      "Train: Epoch [13], Batch [179/938], Loss: 0.267837792634964\n",
      "Train: Epoch [13], Batch [180/938], Loss: 0.404951274394989\n",
      "Train: Epoch [13], Batch [181/938], Loss: 0.5610775947570801\n",
      "Train: Epoch [13], Batch [182/938], Loss: 0.7049494981765747\n",
      "Train: Epoch [13], Batch [183/938], Loss: 0.3682228922843933\n",
      "Train: Epoch [13], Batch [184/938], Loss: 0.20429767668247223\n",
      "Train: Epoch [13], Batch [185/938], Loss: 0.3309834599494934\n",
      "Train: Epoch [13], Batch [186/938], Loss: 0.6232355833053589\n",
      "Train: Epoch [13], Batch [187/938], Loss: 0.48550426959991455\n",
      "Train: Epoch [13], Batch [188/938], Loss: 0.5289977192878723\n",
      "Train: Epoch [13], Batch [189/938], Loss: 0.5432037115097046\n",
      "Train: Epoch [13], Batch [190/938], Loss: 0.5771744251251221\n",
      "Train: Epoch [13], Batch [191/938], Loss: 0.6436303853988647\n",
      "Train: Epoch [13], Batch [192/938], Loss: 0.3596501350402832\n",
      "Train: Epoch [13], Batch [193/938], Loss: 0.5148380398750305\n",
      "Train: Epoch [13], Batch [194/938], Loss: 0.6500567197799683\n",
      "Train: Epoch [13], Batch [195/938], Loss: 0.3381159007549286\n",
      "Train: Epoch [13], Batch [196/938], Loss: 0.5486748218536377\n",
      "Train: Epoch [13], Batch [197/938], Loss: 0.5047799944877625\n",
      "Train: Epoch [13], Batch [198/938], Loss: 0.38766270875930786\n",
      "Train: Epoch [13], Batch [199/938], Loss: 0.48748651146888733\n",
      "Train: Epoch [13], Batch [200/938], Loss: 0.4805421233177185\n",
      "Train: Epoch [13], Batch [201/938], Loss: 0.6259384155273438\n",
      "Train: Epoch [13], Batch [202/938], Loss: 0.6927443742752075\n",
      "Train: Epoch [13], Batch [203/938], Loss: 0.28117823600769043\n",
      "Train: Epoch [13], Batch [204/938], Loss: 0.4557758569717407\n",
      "Train: Epoch [13], Batch [205/938], Loss: 0.3923492133617401\n",
      "Train: Epoch [13], Batch [206/938], Loss: 0.5302424430847168\n",
      "Train: Epoch [13], Batch [207/938], Loss: 0.5823448896408081\n",
      "Train: Epoch [13], Batch [208/938], Loss: 0.5207058787345886\n",
      "Train: Epoch [13], Batch [209/938], Loss: 0.48149439692497253\n",
      "Train: Epoch [13], Batch [210/938], Loss: 0.5176225304603577\n",
      "Train: Epoch [13], Batch [211/938], Loss: 0.7279114127159119\n",
      "Train: Epoch [13], Batch [212/938], Loss: 0.6277377009391785\n",
      "Train: Epoch [13], Batch [213/938], Loss: 0.5445613861083984\n",
      "Train: Epoch [13], Batch [214/938], Loss: 0.6954277753829956\n",
      "Train: Epoch [13], Batch [215/938], Loss: 0.5531860589981079\n",
      "Train: Epoch [13], Batch [216/938], Loss: 0.5577319860458374\n",
      "Train: Epoch [13], Batch [217/938], Loss: 0.6505433320999146\n",
      "Train: Epoch [13], Batch [218/938], Loss: 0.576743483543396\n",
      "Train: Epoch [13], Batch [219/938], Loss: 0.6488265991210938\n",
      "Train: Epoch [13], Batch [220/938], Loss: 0.5240205526351929\n",
      "Train: Epoch [13], Batch [221/938], Loss: 0.5756850838661194\n",
      "Train: Epoch [13], Batch [222/938], Loss: 0.39464256167411804\n",
      "Train: Epoch [13], Batch [223/938], Loss: 0.4539601802825928\n",
      "Train: Epoch [13], Batch [224/938], Loss: 0.5232650637626648\n",
      "Train: Epoch [13], Batch [225/938], Loss: 0.3391782343387604\n",
      "Train: Epoch [13], Batch [226/938], Loss: 0.43087491393089294\n",
      "Train: Epoch [13], Batch [227/938], Loss: 0.5032153129577637\n",
      "Train: Epoch [13], Batch [228/938], Loss: 0.5592753887176514\n",
      "Train: Epoch [13], Batch [229/938], Loss: 0.522713303565979\n",
      "Train: Epoch [13], Batch [230/938], Loss: 0.5449865460395813\n",
      "Train: Epoch [13], Batch [231/938], Loss: 0.5382635593414307\n",
      "Train: Epoch [13], Batch [232/938], Loss: 0.42358461022377014\n",
      "Train: Epoch [13], Batch [233/938], Loss: 0.31884804368019104\n",
      "Train: Epoch [13], Batch [234/938], Loss: 0.6635251045227051\n",
      "Train: Epoch [13], Batch [235/938], Loss: 0.4133679270744324\n",
      "Train: Epoch [13], Batch [236/938], Loss: 0.6470680236816406\n",
      "Train: Epoch [13], Batch [237/938], Loss: 0.6622879505157471\n",
      "Train: Epoch [13], Batch [238/938], Loss: 0.44538700580596924\n",
      "Train: Epoch [13], Batch [239/938], Loss: 0.4783266484737396\n",
      "Train: Epoch [13], Batch [240/938], Loss: 0.42977607250213623\n",
      "Train: Epoch [13], Batch [241/938], Loss: 0.5923540592193604\n",
      "Train: Epoch [13], Batch [242/938], Loss: 0.4766134023666382\n",
      "Train: Epoch [13], Batch [243/938], Loss: 0.4291948974132538\n",
      "Train: Epoch [13], Batch [244/938], Loss: 0.48592329025268555\n",
      "Train: Epoch [13], Batch [245/938], Loss: 0.5710940957069397\n",
      "Train: Epoch [13], Batch [246/938], Loss: 0.3492189049720764\n",
      "Train: Epoch [13], Batch [247/938], Loss: 0.5618759393692017\n",
      "Train: Epoch [13], Batch [248/938], Loss: 0.311914324760437\n",
      "Train: Epoch [13], Batch [249/938], Loss: 0.4788717031478882\n",
      "Train: Epoch [13], Batch [250/938], Loss: 0.5447811484336853\n",
      "Train: Epoch [13], Batch [251/938], Loss: 0.39801108837127686\n",
      "Train: Epoch [13], Batch [252/938], Loss: 0.4729185104370117\n",
      "Train: Epoch [13], Batch [253/938], Loss: 0.6993950009346008\n",
      "Train: Epoch [13], Batch [254/938], Loss: 0.39465808868408203\n",
      "Train: Epoch [13], Batch [255/938], Loss: 0.34752804040908813\n",
      "Train: Epoch [13], Batch [256/938], Loss: 0.3808280825614929\n",
      "Train: Epoch [13], Batch [257/938], Loss: 0.5390095710754395\n",
      "Train: Epoch [13], Batch [258/938], Loss: 0.43056920170783997\n",
      "Train: Epoch [13], Batch [259/938], Loss: 0.32623404264450073\n",
      "Train: Epoch [13], Batch [260/938], Loss: 0.6526867151260376\n",
      "Train: Epoch [13], Batch [261/938], Loss: 0.4987744688987732\n",
      "Train: Epoch [13], Batch [262/938], Loss: 0.7133795022964478\n",
      "Train: Epoch [13], Batch [263/938], Loss: 0.36539292335510254\n",
      "Train: Epoch [13], Batch [264/938], Loss: 0.4463999271392822\n",
      "Train: Epoch [13], Batch [265/938], Loss: 0.27957576513290405\n",
      "Train: Epoch [13], Batch [266/938], Loss: 0.516481876373291\n",
      "Train: Epoch [13], Batch [267/938], Loss: 0.4522780776023865\n",
      "Train: Epoch [13], Batch [268/938], Loss: 0.33134859800338745\n",
      "Train: Epoch [13], Batch [269/938], Loss: 0.5169165134429932\n",
      "Train: Epoch [13], Batch [270/938], Loss: 0.47987422347068787\n",
      "Train: Epoch [13], Batch [271/938], Loss: 0.4372246563434601\n",
      "Train: Epoch [13], Batch [272/938], Loss: 0.38017305731773376\n",
      "Train: Epoch [13], Batch [273/938], Loss: 0.434512197971344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [13], Batch [274/938], Loss: 0.5981211066246033\n",
      "Train: Epoch [13], Batch [275/938], Loss: 0.6051722764968872\n",
      "Train: Epoch [13], Batch [276/938], Loss: 0.5940863490104675\n",
      "Train: Epoch [13], Batch [277/938], Loss: 0.5798618793487549\n",
      "Train: Epoch [13], Batch [278/938], Loss: 0.46312403678894043\n",
      "Train: Epoch [13], Batch [279/938], Loss: 0.4160844683647156\n",
      "Train: Epoch [13], Batch [280/938], Loss: 0.43343567848205566\n",
      "Train: Epoch [13], Batch [281/938], Loss: 0.3940892815589905\n",
      "Train: Epoch [13], Batch [282/938], Loss: 0.42728179693222046\n",
      "Train: Epoch [13], Batch [283/938], Loss: 0.3655017018318176\n",
      "Train: Epoch [13], Batch [284/938], Loss: 0.5255588293075562\n",
      "Train: Epoch [13], Batch [285/938], Loss: 0.5129255652427673\n",
      "Train: Epoch [13], Batch [286/938], Loss: 0.6820756196975708\n",
      "Train: Epoch [13], Batch [287/938], Loss: 0.4569304287433624\n",
      "Train: Epoch [13], Batch [288/938], Loss: 0.42831119894981384\n",
      "Train: Epoch [13], Batch [289/938], Loss: 0.377772718667984\n",
      "Train: Epoch [13], Batch [290/938], Loss: 0.3507227301597595\n",
      "Train: Epoch [13], Batch [291/938], Loss: 0.520615816116333\n",
      "Train: Epoch [13], Batch [292/938], Loss: 0.35522496700286865\n",
      "Train: Epoch [13], Batch [293/938], Loss: 0.6928122639656067\n",
      "Train: Epoch [13], Batch [294/938], Loss: 0.5497726202011108\n",
      "Train: Epoch [13], Batch [295/938], Loss: 0.5051189661026001\n",
      "Train: Epoch [13], Batch [296/938], Loss: 0.3571856915950775\n",
      "Train: Epoch [13], Batch [297/938], Loss: 0.3685780465602875\n",
      "Train: Epoch [13], Batch [298/938], Loss: 0.5480614304542542\n",
      "Train: Epoch [13], Batch [299/938], Loss: 0.5173884630203247\n",
      "Train: Epoch [13], Batch [300/938], Loss: 0.5514222979545593\n",
      "Train: Epoch [13], Batch [301/938], Loss: 0.6297877430915833\n",
      "Train: Epoch [13], Batch [302/938], Loss: 0.5575032234191895\n",
      "Train: Epoch [13], Batch [303/938], Loss: 0.5015482902526855\n",
      "Train: Epoch [13], Batch [304/938], Loss: 0.2920253872871399\n",
      "Train: Epoch [13], Batch [305/938], Loss: 0.6264983415603638\n",
      "Train: Epoch [13], Batch [306/938], Loss: 0.6790776252746582\n",
      "Train: Epoch [13], Batch [307/938], Loss: 0.3863416612148285\n",
      "Train: Epoch [13], Batch [308/938], Loss: 0.5126348733901978\n",
      "Train: Epoch [13], Batch [309/938], Loss: 0.44968611001968384\n",
      "Train: Epoch [13], Batch [310/938], Loss: 0.40143632888793945\n",
      "Train: Epoch [13], Batch [311/938], Loss: 0.5949205160140991\n",
      "Train: Epoch [13], Batch [312/938], Loss: 0.5630403757095337\n",
      "Train: Epoch [13], Batch [313/938], Loss: 0.5878100991249084\n",
      "Train: Epoch [13], Batch [314/938], Loss: 0.4759480953216553\n",
      "Train: Epoch [13], Batch [315/938], Loss: 0.512177050113678\n",
      "Train: Epoch [13], Batch [316/938], Loss: 0.3815269470214844\n",
      "Train: Epoch [13], Batch [317/938], Loss: 0.5709813237190247\n",
      "Train: Epoch [13], Batch [318/938], Loss: 0.47857198119163513\n",
      "Train: Epoch [13], Batch [319/938], Loss: 0.4607465863227844\n",
      "Train: Epoch [13], Batch [320/938], Loss: 0.5022374391555786\n",
      "Train: Epoch [13], Batch [321/938], Loss: 0.31377720832824707\n",
      "Train: Epoch [13], Batch [322/938], Loss: 0.47652488946914673\n",
      "Train: Epoch [13], Batch [323/938], Loss: 0.3721419870853424\n",
      "Train: Epoch [13], Batch [324/938], Loss: 0.39347875118255615\n",
      "Train: Epoch [13], Batch [325/938], Loss: 0.4812784492969513\n",
      "Train: Epoch [13], Batch [326/938], Loss: 0.39982178807258606\n",
      "Train: Epoch [13], Batch [327/938], Loss: 0.4219553470611572\n",
      "Train: Epoch [13], Batch [328/938], Loss: 0.45486071705818176\n",
      "Train: Epoch [13], Batch [329/938], Loss: 0.5099575519561768\n",
      "Train: Epoch [13], Batch [330/938], Loss: 0.4182995557785034\n",
      "Train: Epoch [13], Batch [331/938], Loss: 0.48739999532699585\n",
      "Train: Epoch [13], Batch [332/938], Loss: 0.36061710119247437\n",
      "Train: Epoch [13], Batch [333/938], Loss: 0.4290478825569153\n",
      "Train: Epoch [13], Batch [334/938], Loss: 0.5837270021438599\n",
      "Train: Epoch [13], Batch [335/938], Loss: 0.31212127208709717\n",
      "Train: Epoch [13], Batch [336/938], Loss: 0.3006550073623657\n",
      "Train: Epoch [13], Batch [337/938], Loss: 0.4418010115623474\n",
      "Train: Epoch [13], Batch [338/938], Loss: 0.6450420022010803\n",
      "Train: Epoch [13], Batch [339/938], Loss: 0.43849891424179077\n",
      "Train: Epoch [13], Batch [340/938], Loss: 0.7013053894042969\n",
      "Train: Epoch [13], Batch [341/938], Loss: 0.4882749915122986\n",
      "Train: Epoch [13], Batch [342/938], Loss: 0.4880816340446472\n",
      "Train: Epoch [13], Batch [343/938], Loss: 0.3659120798110962\n",
      "Train: Epoch [13], Batch [344/938], Loss: 0.4808766543865204\n",
      "Train: Epoch [13], Batch [345/938], Loss: 0.42103201150894165\n",
      "Train: Epoch [13], Batch [346/938], Loss: 0.4692404568195343\n",
      "Train: Epoch [13], Batch [347/938], Loss: 0.4563332200050354\n",
      "Train: Epoch [13], Batch [348/938], Loss: 0.5523847341537476\n",
      "Train: Epoch [13], Batch [349/938], Loss: 0.518028736114502\n",
      "Train: Epoch [13], Batch [350/938], Loss: 0.5923510789871216\n",
      "Train: Epoch [13], Batch [351/938], Loss: 0.4800933301448822\n",
      "Train: Epoch [13], Batch [352/938], Loss: 0.34864282608032227\n",
      "Train: Epoch [13], Batch [353/938], Loss: 0.40897196531295776\n",
      "Train: Epoch [13], Batch [354/938], Loss: 0.31647205352783203\n",
      "Train: Epoch [13], Batch [355/938], Loss: 0.31969237327575684\n",
      "Train: Epoch [13], Batch [356/938], Loss: 0.33593758940696716\n",
      "Train: Epoch [13], Batch [357/938], Loss: 0.6052311658859253\n",
      "Train: Epoch [13], Batch [358/938], Loss: 0.4465128183364868\n",
      "Train: Epoch [13], Batch [359/938], Loss: 0.4497436583042145\n",
      "Train: Epoch [13], Batch [360/938], Loss: 0.5010465383529663\n",
      "Train: Epoch [13], Batch [361/938], Loss: 0.4008672833442688\n",
      "Train: Epoch [13], Batch [362/938], Loss: 0.5249637365341187\n",
      "Train: Epoch [13], Batch [363/938], Loss: 0.48826128244400024\n",
      "Train: Epoch [13], Batch [364/938], Loss: 0.3724634349346161\n",
      "Train: Epoch [13], Batch [365/938], Loss: 0.5011276006698608\n",
      "Train: Epoch [13], Batch [366/938], Loss: 0.5528814792633057\n",
      "Train: Epoch [13], Batch [367/938], Loss: 0.3518691956996918\n",
      "Train: Epoch [13], Batch [368/938], Loss: 0.3933449983596802\n",
      "Train: Epoch [13], Batch [369/938], Loss: 0.5791662335395813\n",
      "Train: Epoch [13], Batch [370/938], Loss: 0.3284001350402832\n",
      "Train: Epoch [13], Batch [371/938], Loss: 0.5056298971176147\n",
      "Train: Epoch [13], Batch [372/938], Loss: 0.35941824316978455\n",
      "Train: Epoch [13], Batch [373/938], Loss: 0.30384838581085205\n",
      "Train: Epoch [13], Batch [374/938], Loss: 0.262147456407547\n",
      "Train: Epoch [13], Batch [375/938], Loss: 0.5033870935440063\n",
      "Train: Epoch [13], Batch [376/938], Loss: 0.5344066619873047\n",
      "Train: Epoch [13], Batch [377/938], Loss: 0.3087347149848938\n",
      "Train: Epoch [13], Batch [378/938], Loss: 0.41806596517562866\n",
      "Train: Epoch [13], Batch [379/938], Loss: 0.5530394315719604\n",
      "Train: Epoch [13], Batch [380/938], Loss: 0.5372278690338135\n",
      "Train: Epoch [13], Batch [381/938], Loss: 0.7685983180999756\n",
      "Train: Epoch [13], Batch [382/938], Loss: 0.37518608570098877\n",
      "Train: Epoch [13], Batch [383/938], Loss: 0.45217761397361755\n",
      "Train: Epoch [13], Batch [384/938], Loss: 0.6360616683959961\n",
      "Train: Epoch [13], Batch [385/938], Loss: 0.21305058896541595\n",
      "Train: Epoch [13], Batch [386/938], Loss: 0.5802102088928223\n",
      "Train: Epoch [13], Batch [387/938], Loss: 0.4032118320465088\n",
      "Train: Epoch [13], Batch [388/938], Loss: 0.37880808115005493\n",
      "Train: Epoch [13], Batch [389/938], Loss: 0.3024902939796448\n",
      "Train: Epoch [13], Batch [390/938], Loss: 0.4359486401081085\n",
      "Train: Epoch [13], Batch [391/938], Loss: 0.453675240278244\n",
      "Train: Epoch [13], Batch [392/938], Loss: 0.3575151562690735\n",
      "Train: Epoch [13], Batch [393/938], Loss: 0.6300898790359497\n",
      "Train: Epoch [13], Batch [394/938], Loss: 0.38134774565696716\n",
      "Train: Epoch [13], Batch [395/938], Loss: 0.5034462213516235\n",
      "Train: Epoch [13], Batch [396/938], Loss: 0.31534481048583984\n",
      "Train: Epoch [13], Batch [397/938], Loss: 0.49863559007644653\n",
      "Train: Epoch [13], Batch [398/938], Loss: 0.51566481590271\n",
      "Train: Epoch [13], Batch [399/938], Loss: 0.5307091474533081\n",
      "Train: Epoch [13], Batch [400/938], Loss: 0.45758455991744995\n",
      "Train: Epoch [13], Batch [401/938], Loss: 0.7829904556274414\n",
      "Train: Epoch [13], Batch [402/938], Loss: 0.45075809955596924\n",
      "Train: Epoch [13], Batch [403/938], Loss: 0.4759458303451538\n",
      "Train: Epoch [13], Batch [404/938], Loss: 0.45037010312080383\n",
      "Train: Epoch [13], Batch [405/938], Loss: 0.39995452761650085\n",
      "Train: Epoch [13], Batch [406/938], Loss: 0.538377046585083\n",
      "Train: Epoch [13], Batch [407/938], Loss: 0.6086870431900024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [13], Batch [408/938], Loss: 0.6664464473724365\n",
      "Train: Epoch [13], Batch [409/938], Loss: 0.5808892846107483\n",
      "Train: Epoch [13], Batch [410/938], Loss: 0.2804657220840454\n",
      "Train: Epoch [13], Batch [411/938], Loss: 0.7171589136123657\n",
      "Train: Epoch [13], Batch [412/938], Loss: 0.465248167514801\n",
      "Train: Epoch [13], Batch [413/938], Loss: 0.4918677508831024\n",
      "Train: Epoch [13], Batch [414/938], Loss: 0.727569580078125\n",
      "Train: Epoch [13], Batch [415/938], Loss: 0.40802133083343506\n",
      "Train: Epoch [13], Batch [416/938], Loss: 0.28510239720344543\n",
      "Train: Epoch [13], Batch [417/938], Loss: 0.24217787384986877\n",
      "Train: Epoch [13], Batch [418/938], Loss: 0.5479633808135986\n",
      "Train: Epoch [13], Batch [419/938], Loss: 0.556378185749054\n",
      "Train: Epoch [13], Batch [420/938], Loss: 0.24153253436088562\n",
      "Train: Epoch [13], Batch [421/938], Loss: 0.5934633016586304\n",
      "Train: Epoch [13], Batch [422/938], Loss: 0.29605281352996826\n",
      "Train: Epoch [13], Batch [423/938], Loss: 0.7309448719024658\n",
      "Train: Epoch [13], Batch [424/938], Loss: 0.3457403779029846\n",
      "Train: Epoch [13], Batch [425/938], Loss: 0.4016101062297821\n",
      "Train: Epoch [13], Batch [426/938], Loss: 0.5811672806739807\n",
      "Train: Epoch [13], Batch [427/938], Loss: 0.530104398727417\n",
      "Train: Epoch [13], Batch [428/938], Loss: 0.33741500973701477\n",
      "Train: Epoch [13], Batch [429/938], Loss: 0.6151242256164551\n",
      "Train: Epoch [13], Batch [430/938], Loss: 0.45859503746032715\n",
      "Train: Epoch [13], Batch [431/938], Loss: 0.3133113384246826\n",
      "Train: Epoch [13], Batch [432/938], Loss: 0.5912971496582031\n",
      "Train: Epoch [13], Batch [433/938], Loss: 0.4618462324142456\n",
      "Train: Epoch [13], Batch [434/938], Loss: 0.5112924575805664\n",
      "Train: Epoch [13], Batch [435/938], Loss: 0.612205445766449\n",
      "Train: Epoch [13], Batch [436/938], Loss: 0.44903406500816345\n",
      "Train: Epoch [13], Batch [437/938], Loss: 0.6157211661338806\n",
      "Train: Epoch [13], Batch [438/938], Loss: 0.40277910232543945\n",
      "Train: Epoch [13], Batch [439/938], Loss: 0.39239078760147095\n",
      "Train: Epoch [13], Batch [440/938], Loss: 0.4824013113975525\n",
      "Train: Epoch [13], Batch [441/938], Loss: 0.6668272018432617\n",
      "Train: Epoch [13], Batch [442/938], Loss: 0.3550049662590027\n",
      "Train: Epoch [13], Batch [443/938], Loss: 0.522278368473053\n",
      "Train: Epoch [13], Batch [444/938], Loss: 0.41457152366638184\n",
      "Train: Epoch [13], Batch [445/938], Loss: 0.39733973145484924\n",
      "Train: Epoch [13], Batch [446/938], Loss: 0.3730096220970154\n",
      "Train: Epoch [13], Batch [447/938], Loss: 0.6899316906929016\n",
      "Train: Epoch [13], Batch [448/938], Loss: 0.38092249631881714\n",
      "Train: Epoch [13], Batch [449/938], Loss: 0.34667831659317017\n",
      "Train: Epoch [13], Batch [450/938], Loss: 0.27495449781417847\n",
      "Train: Epoch [13], Batch [451/938], Loss: 0.6786454319953918\n",
      "Train: Epoch [13], Batch [452/938], Loss: 0.5371838808059692\n",
      "Train: Epoch [13], Batch [453/938], Loss: 0.6325608491897583\n",
      "Train: Epoch [13], Batch [454/938], Loss: 0.7316498756408691\n",
      "Train: Epoch [13], Batch [455/938], Loss: 0.5334987640380859\n",
      "Train: Epoch [13], Batch [456/938], Loss: 0.5273150205612183\n",
      "Train: Epoch [13], Batch [457/938], Loss: 0.5579942464828491\n",
      "Train: Epoch [13], Batch [458/938], Loss: 0.6012498140335083\n",
      "Train: Epoch [13], Batch [459/938], Loss: 0.42384999990463257\n",
      "Train: Epoch [13], Batch [460/938], Loss: 0.31719842553138733\n",
      "Train: Epoch [13], Batch [461/938], Loss: 0.5105898380279541\n",
      "Train: Epoch [13], Batch [462/938], Loss: 0.3941023647785187\n",
      "Train: Epoch [13], Batch [463/938], Loss: 0.5622726678848267\n",
      "Train: Epoch [13], Batch [464/938], Loss: 0.36341696977615356\n",
      "Train: Epoch [13], Batch [465/938], Loss: 0.5620605945587158\n",
      "Train: Epoch [13], Batch [466/938], Loss: 0.4795304834842682\n",
      "Train: Epoch [13], Batch [467/938], Loss: 0.6066162586212158\n",
      "Train: Epoch [13], Batch [468/938], Loss: 0.624072253704071\n",
      "Train: Epoch [13], Batch [469/938], Loss: 0.43674126267433167\n",
      "Train: Epoch [13], Batch [470/938], Loss: 0.515190064907074\n",
      "Train: Epoch [13], Batch [471/938], Loss: 0.48068341612815857\n",
      "Train: Epoch [13], Batch [472/938], Loss: 0.46341121196746826\n",
      "Train: Epoch [13], Batch [473/938], Loss: 0.5433187484741211\n",
      "Train: Epoch [13], Batch [474/938], Loss: 0.5310063362121582\n",
      "Train: Epoch [13], Batch [475/938], Loss: 0.7983041405677795\n",
      "Train: Epoch [13], Batch [476/938], Loss: 0.3388322591781616\n",
      "Train: Epoch [13], Batch [477/938], Loss: 0.2833302617073059\n",
      "Train: Epoch [13], Batch [478/938], Loss: 0.4349476397037506\n",
      "Train: Epoch [13], Batch [479/938], Loss: 0.4314802289009094\n",
      "Train: Epoch [13], Batch [480/938], Loss: 0.5498387813568115\n",
      "Train: Epoch [13], Batch [481/938], Loss: 0.2717772126197815\n",
      "Train: Epoch [13], Batch [482/938], Loss: 0.5559741258621216\n",
      "Train: Epoch [13], Batch [483/938], Loss: 0.6234486103057861\n",
      "Train: Epoch [13], Batch [484/938], Loss: 0.5013078451156616\n",
      "Train: Epoch [13], Batch [485/938], Loss: 0.3558316230773926\n",
      "Train: Epoch [13], Batch [486/938], Loss: 0.31969910860061646\n",
      "Train: Epoch [13], Batch [487/938], Loss: 0.35789754986763\n",
      "Train: Epoch [13], Batch [488/938], Loss: 0.4693637490272522\n",
      "Train: Epoch [13], Batch [489/938], Loss: 0.5055404305458069\n",
      "Train: Epoch [13], Batch [490/938], Loss: 0.6497905254364014\n",
      "Train: Epoch [13], Batch [491/938], Loss: 0.4717845618724823\n",
      "Train: Epoch [13], Batch [492/938], Loss: 0.5496823787689209\n",
      "Train: Epoch [13], Batch [493/938], Loss: 0.7785497903823853\n",
      "Train: Epoch [13], Batch [494/938], Loss: 0.40621596574783325\n",
      "Train: Epoch [13], Batch [495/938], Loss: 0.8089587092399597\n",
      "Train: Epoch [13], Batch [496/938], Loss: 0.8561069369316101\n",
      "Train: Epoch [13], Batch [497/938], Loss: 0.3393334746360779\n",
      "Train: Epoch [13], Batch [498/938], Loss: 0.5143769383430481\n",
      "Train: Epoch [13], Batch [499/938], Loss: 0.30797550082206726\n",
      "Train: Epoch [13], Batch [500/938], Loss: 0.3825910687446594\n",
      "Train: Epoch [13], Batch [501/938], Loss: 0.49684590101242065\n",
      "Train: Epoch [13], Batch [502/938], Loss: 0.5117958784103394\n",
      "Train: Epoch [13], Batch [503/938], Loss: 0.47039899230003357\n",
      "Train: Epoch [13], Batch [504/938], Loss: 0.36337944865226746\n",
      "Train: Epoch [13], Batch [505/938], Loss: 0.4308897852897644\n",
      "Train: Epoch [13], Batch [506/938], Loss: 0.6204840540885925\n",
      "Train: Epoch [13], Batch [507/938], Loss: 0.6089426875114441\n",
      "Train: Epoch [13], Batch [508/938], Loss: 0.40853437781333923\n",
      "Train: Epoch [13], Batch [509/938], Loss: 0.43284499645233154\n",
      "Train: Epoch [13], Batch [510/938], Loss: 0.5358611345291138\n",
      "Train: Epoch [13], Batch [511/938], Loss: 0.5182314515113831\n",
      "Train: Epoch [13], Batch [512/938], Loss: 0.6540624499320984\n",
      "Train: Epoch [13], Batch [513/938], Loss: 0.4045056700706482\n",
      "Train: Epoch [13], Batch [514/938], Loss: 0.45295873284339905\n",
      "Train: Epoch [13], Batch [515/938], Loss: 0.5978624820709229\n",
      "Train: Epoch [13], Batch [516/938], Loss: 0.3672073483467102\n",
      "Train: Epoch [13], Batch [517/938], Loss: 0.679302990436554\n",
      "Train: Epoch [13], Batch [518/938], Loss: 0.4034724235534668\n",
      "Train: Epoch [13], Batch [519/938], Loss: 0.429159939289093\n",
      "Train: Epoch [13], Batch [520/938], Loss: 0.4585336148738861\n",
      "Train: Epoch [13], Batch [521/938], Loss: 0.49720528721809387\n",
      "Train: Epoch [13], Batch [522/938], Loss: 0.5130946040153503\n",
      "Train: Epoch [13], Batch [523/938], Loss: 0.3527780771255493\n",
      "Train: Epoch [13], Batch [524/938], Loss: 0.31469592452049255\n",
      "Train: Epoch [13], Batch [525/938], Loss: 0.45334506034851074\n",
      "Train: Epoch [13], Batch [526/938], Loss: 0.320138156414032\n",
      "Train: Epoch [13], Batch [527/938], Loss: 0.6561017036437988\n",
      "Train: Epoch [13], Batch [528/938], Loss: 0.703708291053772\n",
      "Train: Epoch [13], Batch [529/938], Loss: 0.4305146336555481\n",
      "Train: Epoch [13], Batch [530/938], Loss: 0.4975000023841858\n",
      "Train: Epoch [13], Batch [531/938], Loss: 0.523181676864624\n",
      "Train: Epoch [13], Batch [532/938], Loss: 0.49799978733062744\n",
      "Train: Epoch [13], Batch [533/938], Loss: 0.4853742718696594\n",
      "Train: Epoch [13], Batch [534/938], Loss: 0.42253854870796204\n",
      "Train: Epoch [13], Batch [535/938], Loss: 0.4265219271183014\n",
      "Train: Epoch [13], Batch [536/938], Loss: 0.42327699065208435\n",
      "Train: Epoch [13], Batch [537/938], Loss: 0.41161003708839417\n",
      "Train: Epoch [13], Batch [538/938], Loss: 0.5244366526603699\n",
      "Train: Epoch [13], Batch [539/938], Loss: 0.6368001699447632\n",
      "Train: Epoch [13], Batch [540/938], Loss: 0.5380005240440369\n",
      "Train: Epoch [13], Batch [541/938], Loss: 0.4522024393081665\n",
      "Train: Epoch [13], Batch [542/938], Loss: 0.5546869039535522\n",
      "Train: Epoch [13], Batch [543/938], Loss: 0.4577726721763611\n",
      "Train: Epoch [13], Batch [544/938], Loss: 0.502781867980957\n",
      "Train: Epoch [13], Batch [545/938], Loss: 0.45812851190567017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [13], Batch [546/938], Loss: 0.6537649631500244\n",
      "Train: Epoch [13], Batch [547/938], Loss: 0.5858346223831177\n",
      "Train: Epoch [13], Batch [548/938], Loss: 0.40818583965301514\n",
      "Train: Epoch [13], Batch [549/938], Loss: 0.4800646901130676\n",
      "Train: Epoch [13], Batch [550/938], Loss: 0.7064366340637207\n",
      "Train: Epoch [13], Batch [551/938], Loss: 0.8558143377304077\n",
      "Train: Epoch [13], Batch [552/938], Loss: 0.4767521917819977\n",
      "Train: Epoch [13], Batch [553/938], Loss: 0.6204821467399597\n",
      "Train: Epoch [13], Batch [554/938], Loss: 0.5238134264945984\n",
      "Train: Epoch [13], Batch [555/938], Loss: 0.39178645610809326\n",
      "Train: Epoch [13], Batch [556/938], Loss: 0.31487837433815\n",
      "Train: Epoch [13], Batch [557/938], Loss: 0.5230244398117065\n",
      "Train: Epoch [13], Batch [558/938], Loss: 0.638114869594574\n",
      "Train: Epoch [13], Batch [559/938], Loss: 0.6195977926254272\n",
      "Train: Epoch [13], Batch [560/938], Loss: 0.3105059862136841\n",
      "Train: Epoch [13], Batch [561/938], Loss: 0.44111329317092896\n",
      "Train: Epoch [13], Batch [562/938], Loss: 0.7493875622749329\n",
      "Train: Epoch [13], Batch [563/938], Loss: 0.5462673306465149\n",
      "Train: Epoch [13], Batch [564/938], Loss: 0.5333210229873657\n",
      "Train: Epoch [13], Batch [565/938], Loss: 0.5309488773345947\n",
      "Train: Epoch [13], Batch [566/938], Loss: 0.5117828249931335\n",
      "Train: Epoch [13], Batch [567/938], Loss: 0.31950604915618896\n",
      "Train: Epoch [13], Batch [568/938], Loss: 0.4461301267147064\n",
      "Train: Epoch [13], Batch [569/938], Loss: 0.347123384475708\n",
      "Train: Epoch [13], Batch [570/938], Loss: 0.45076799392700195\n",
      "Train: Epoch [13], Batch [571/938], Loss: 0.5067710876464844\n",
      "Train: Epoch [13], Batch [572/938], Loss: 0.36653560400009155\n",
      "Train: Epoch [13], Batch [573/938], Loss: 0.3732554316520691\n",
      "Train: Epoch [13], Batch [574/938], Loss: 0.5582118630409241\n",
      "Train: Epoch [13], Batch [575/938], Loss: 0.4819546639919281\n",
      "Train: Epoch [13], Batch [576/938], Loss: 0.4835493266582489\n",
      "Train: Epoch [13], Batch [577/938], Loss: 0.398750364780426\n",
      "Train: Epoch [13], Batch [578/938], Loss: 0.3144306540489197\n",
      "Train: Epoch [13], Batch [579/938], Loss: 0.5277162790298462\n",
      "Train: Epoch [13], Batch [580/938], Loss: 0.5574334859848022\n",
      "Train: Epoch [13], Batch [581/938], Loss: 0.4142304062843323\n",
      "Train: Epoch [13], Batch [582/938], Loss: 0.46485739946365356\n",
      "Train: Epoch [13], Batch [583/938], Loss: 0.547295331954956\n",
      "Train: Epoch [13], Batch [584/938], Loss: 0.4333854913711548\n",
      "Train: Epoch [13], Batch [585/938], Loss: 0.34394288063049316\n",
      "Train: Epoch [13], Batch [586/938], Loss: 0.453910768032074\n",
      "Train: Epoch [13], Batch [587/938], Loss: 0.6694527864456177\n",
      "Train: Epoch [13], Batch [588/938], Loss: 0.4656825661659241\n",
      "Train: Epoch [13], Batch [589/938], Loss: 0.4442724585533142\n",
      "Train: Epoch [13], Batch [590/938], Loss: 0.6518144607543945\n",
      "Train: Epoch [13], Batch [591/938], Loss: 0.48529595136642456\n",
      "Train: Epoch [13], Batch [592/938], Loss: 0.4242226481437683\n",
      "Train: Epoch [13], Batch [593/938], Loss: 0.5284963250160217\n",
      "Train: Epoch [13], Batch [594/938], Loss: 0.7353518605232239\n",
      "Train: Epoch [13], Batch [595/938], Loss: 0.45188868045806885\n",
      "Train: Epoch [13], Batch [596/938], Loss: 0.41027969121932983\n",
      "Train: Epoch [13], Batch [597/938], Loss: 0.5119390487670898\n",
      "Train: Epoch [13], Batch [598/938], Loss: 0.4891428053379059\n",
      "Train: Epoch [13], Batch [599/938], Loss: 0.6322900056838989\n",
      "Train: Epoch [13], Batch [600/938], Loss: 0.33744674921035767\n",
      "Train: Epoch [13], Batch [601/938], Loss: 0.33162033557891846\n",
      "Train: Epoch [13], Batch [602/938], Loss: 0.6077290773391724\n",
      "Train: Epoch [13], Batch [603/938], Loss: 0.3524268567562103\n",
      "Train: Epoch [13], Batch [604/938], Loss: 0.5196864008903503\n",
      "Train: Epoch [13], Batch [605/938], Loss: 0.5033543705940247\n",
      "Train: Epoch [13], Batch [606/938], Loss: 0.5368204116821289\n",
      "Train: Epoch [13], Batch [607/938], Loss: 0.4192180633544922\n",
      "Train: Epoch [13], Batch [608/938], Loss: 0.5083481669425964\n",
      "Train: Epoch [13], Batch [609/938], Loss: 0.5196793079376221\n",
      "Train: Epoch [13], Batch [610/938], Loss: 0.304097056388855\n",
      "Train: Epoch [13], Batch [611/938], Loss: 0.4210686683654785\n",
      "Train: Epoch [13], Batch [612/938], Loss: 0.5491774082183838\n",
      "Train: Epoch [13], Batch [613/938], Loss: 0.4999288320541382\n",
      "Train: Epoch [13], Batch [614/938], Loss: 0.5844542384147644\n",
      "Train: Epoch [13], Batch [615/938], Loss: 0.6066321730613708\n",
      "Train: Epoch [13], Batch [616/938], Loss: 0.39523231983184814\n",
      "Train: Epoch [13], Batch [617/938], Loss: 0.5788434743881226\n",
      "Train: Epoch [13], Batch [618/938], Loss: 0.6353952288627625\n",
      "Train: Epoch [13], Batch [619/938], Loss: 0.5657432079315186\n",
      "Train: Epoch [13], Batch [620/938], Loss: 0.47438114881515503\n",
      "Train: Epoch [13], Batch [621/938], Loss: 0.44101962447166443\n",
      "Train: Epoch [13], Batch [622/938], Loss: 0.5044379234313965\n",
      "Train: Epoch [13], Batch [623/938], Loss: 0.5141596794128418\n",
      "Train: Epoch [13], Batch [624/938], Loss: 0.42994165420532227\n",
      "Train: Epoch [13], Batch [625/938], Loss: 0.49859505891799927\n",
      "Train: Epoch [13], Batch [626/938], Loss: 0.4968465566635132\n",
      "Train: Epoch [13], Batch [627/938], Loss: 0.42475008964538574\n",
      "Train: Epoch [13], Batch [628/938], Loss: 0.5409454703330994\n",
      "Train: Epoch [13], Batch [629/938], Loss: 0.5775243043899536\n",
      "Train: Epoch [13], Batch [630/938], Loss: 0.4511745870113373\n",
      "Train: Epoch [13], Batch [631/938], Loss: 0.35280197858810425\n",
      "Train: Epoch [13], Batch [632/938], Loss: 0.5026426911354065\n",
      "Train: Epoch [13], Batch [633/938], Loss: 0.479826420545578\n",
      "Train: Epoch [13], Batch [634/938], Loss: 0.37010082602500916\n",
      "Train: Epoch [13], Batch [635/938], Loss: 0.46594664454460144\n",
      "Train: Epoch [13], Batch [636/938], Loss: 0.5694523453712463\n",
      "Train: Epoch [13], Batch [637/938], Loss: 0.46815770864486694\n",
      "Train: Epoch [13], Batch [638/938], Loss: 0.5076471567153931\n",
      "Train: Epoch [13], Batch [639/938], Loss: 0.7239689826965332\n",
      "Train: Epoch [13], Batch [640/938], Loss: 0.45271873474121094\n",
      "Train: Epoch [13], Batch [641/938], Loss: 0.34948843717575073\n",
      "Train: Epoch [13], Batch [642/938], Loss: 0.47509491443634033\n",
      "Train: Epoch [13], Batch [643/938], Loss: 0.5724663734436035\n",
      "Train: Epoch [13], Batch [644/938], Loss: 0.5102674961090088\n",
      "Train: Epoch [13], Batch [645/938], Loss: 0.6202453970909119\n",
      "Train: Epoch [13], Batch [646/938], Loss: 0.4112335443496704\n",
      "Train: Epoch [13], Batch [647/938], Loss: 0.374616414308548\n",
      "Train: Epoch [13], Batch [648/938], Loss: 0.4432361125946045\n",
      "Train: Epoch [13], Batch [649/938], Loss: 0.7879682779312134\n",
      "Train: Epoch [13], Batch [650/938], Loss: 0.5203260779380798\n",
      "Train: Epoch [13], Batch [651/938], Loss: 0.416294127702713\n",
      "Train: Epoch [13], Batch [652/938], Loss: 0.6839280724525452\n",
      "Train: Epoch [13], Batch [653/938], Loss: 0.3734644949436188\n",
      "Train: Epoch [13], Batch [654/938], Loss: 0.38753122091293335\n",
      "Train: Epoch [13], Batch [655/938], Loss: 0.40610983967781067\n",
      "Train: Epoch [13], Batch [656/938], Loss: 0.43521666526794434\n",
      "Train: Epoch [13], Batch [657/938], Loss: 0.46760183572769165\n",
      "Train: Epoch [13], Batch [658/938], Loss: 0.3570542335510254\n",
      "Train: Epoch [13], Batch [659/938], Loss: 0.7726835608482361\n",
      "Train: Epoch [13], Batch [660/938], Loss: 0.35965028405189514\n",
      "Train: Epoch [13], Batch [661/938], Loss: 0.45008718967437744\n",
      "Train: Epoch [13], Batch [662/938], Loss: 0.5460688471794128\n",
      "Train: Epoch [13], Batch [663/938], Loss: 0.6981648802757263\n",
      "Train: Epoch [13], Batch [664/938], Loss: 0.37920883297920227\n",
      "Train: Epoch [13], Batch [665/938], Loss: 0.483062744140625\n",
      "Train: Epoch [13], Batch [666/938], Loss: 0.40729373693466187\n",
      "Train: Epoch [13], Batch [667/938], Loss: 0.47782355546951294\n",
      "Train: Epoch [13], Batch [668/938], Loss: 0.33102619647979736\n",
      "Train: Epoch [13], Batch [669/938], Loss: 0.3556959629058838\n",
      "Train: Epoch [13], Batch [670/938], Loss: 0.6184766292572021\n",
      "Train: Epoch [13], Batch [671/938], Loss: 0.48899924755096436\n",
      "Train: Epoch [13], Batch [672/938], Loss: 0.4868270456790924\n",
      "Train: Epoch [13], Batch [673/938], Loss: 0.46183374524116516\n",
      "Train: Epoch [13], Batch [674/938], Loss: 0.47841763496398926\n",
      "Train: Epoch [13], Batch [675/938], Loss: 0.4887717366218567\n",
      "Train: Epoch [13], Batch [676/938], Loss: 0.5461947917938232\n",
      "Train: Epoch [13], Batch [677/938], Loss: 0.4768204092979431\n",
      "Train: Epoch [13], Batch [678/938], Loss: 0.5365160703659058\n",
      "Train: Epoch [13], Batch [679/938], Loss: 0.5753016471862793\n",
      "Train: Epoch [13], Batch [680/938], Loss: 0.36308470368385315\n",
      "Train: Epoch [13], Batch [681/938], Loss: 0.5860798358917236\n",
      "Train: Epoch [13], Batch [682/938], Loss: 0.3557579219341278\n",
      "Train: Epoch [13], Batch [683/938], Loss: 0.4299159049987793\n",
      "Train: Epoch [13], Batch [684/938], Loss: 0.42846012115478516\n",
      "Train: Epoch [13], Batch [685/938], Loss: 0.447476863861084\n",
      "Train: Epoch [13], Batch [686/938], Loss: 0.429805189371109\n",
      "Train: Epoch [13], Batch [687/938], Loss: 0.43166619539260864\n",
      "Train: Epoch [13], Batch [688/938], Loss: 0.5353698134422302\n",
      "Train: Epoch [13], Batch [689/938], Loss: 0.5661104917526245\n",
      "Train: Epoch [13], Batch [690/938], Loss: 0.479693740606308\n",
      "Train: Epoch [13], Batch [691/938], Loss: 0.3810989558696747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [13], Batch [692/938], Loss: 0.2687971293926239\n",
      "Train: Epoch [13], Batch [693/938], Loss: 0.47476696968078613\n",
      "Train: Epoch [13], Batch [694/938], Loss: 0.5260617136955261\n",
      "Train: Epoch [13], Batch [695/938], Loss: 0.44158807396888733\n",
      "Train: Epoch [13], Batch [696/938], Loss: 0.6819312572479248\n",
      "Train: Epoch [13], Batch [697/938], Loss: 0.6084134578704834\n",
      "Train: Epoch [13], Batch [698/938], Loss: 0.4996783137321472\n",
      "Train: Epoch [13], Batch [699/938], Loss: 0.4009396731853485\n",
      "Train: Epoch [13], Batch [700/938], Loss: 0.3374234139919281\n",
      "Train: Epoch [13], Batch [701/938], Loss: 0.574550986289978\n",
      "Train: Epoch [13], Batch [702/938], Loss: 0.364676833152771\n",
      "Train: Epoch [13], Batch [703/938], Loss: 0.28393009305000305\n",
      "Train: Epoch [13], Batch [704/938], Loss: 0.2944234609603882\n",
      "Train: Epoch [13], Batch [705/938], Loss: 0.3719269931316376\n",
      "Train: Epoch [13], Batch [706/938], Loss: 0.558223307132721\n",
      "Train: Epoch [13], Batch [707/938], Loss: 0.4619004726409912\n",
      "Train: Epoch [13], Batch [708/938], Loss: 0.3839462995529175\n",
      "Train: Epoch [13], Batch [709/938], Loss: 0.6620063781738281\n",
      "Train: Epoch [13], Batch [710/938], Loss: 0.6172646284103394\n",
      "Train: Epoch [13], Batch [711/938], Loss: 0.69330894947052\n",
      "Train: Epoch [13], Batch [712/938], Loss: 0.4058155119419098\n",
      "Train: Epoch [13], Batch [713/938], Loss: 0.3972388505935669\n",
      "Train: Epoch [13], Batch [714/938], Loss: 0.4567677974700928\n",
      "Train: Epoch [13], Batch [715/938], Loss: 0.39718523621559143\n",
      "Train: Epoch [13], Batch [716/938], Loss: 0.45124804973602295\n",
      "Train: Epoch [13], Batch [717/938], Loss: 0.613933801651001\n",
      "Train: Epoch [13], Batch [718/938], Loss: 0.5667468905448914\n",
      "Train: Epoch [13], Batch [719/938], Loss: 0.5213383436203003\n",
      "Train: Epoch [13], Batch [720/938], Loss: 0.5239230394363403\n",
      "Train: Epoch [13], Batch [721/938], Loss: 0.5449835062026978\n",
      "Train: Epoch [13], Batch [722/938], Loss: 0.4865516722202301\n",
      "Train: Epoch [13], Batch [723/938], Loss: 0.505828320980072\n",
      "Train: Epoch [13], Batch [724/938], Loss: 0.5009347796440125\n",
      "Train: Epoch [13], Batch [725/938], Loss: 0.29300206899642944\n",
      "Train: Epoch [13], Batch [726/938], Loss: 0.540032148361206\n",
      "Train: Epoch [13], Batch [727/938], Loss: 0.4378892183303833\n",
      "Train: Epoch [13], Batch [728/938], Loss: 0.5440056324005127\n",
      "Train: Epoch [13], Batch [729/938], Loss: 0.448411226272583\n",
      "Train: Epoch [13], Batch [730/938], Loss: 0.4128207564353943\n",
      "Train: Epoch [13], Batch [731/938], Loss: 0.40909242630004883\n",
      "Train: Epoch [13], Batch [732/938], Loss: 0.5460319519042969\n",
      "Train: Epoch [13], Batch [733/938], Loss: 0.6112399101257324\n",
      "Train: Epoch [13], Batch [734/938], Loss: 0.47938457131385803\n",
      "Train: Epoch [13], Batch [735/938], Loss: 0.606099009513855\n",
      "Train: Epoch [13], Batch [736/938], Loss: 0.47629833221435547\n",
      "Train: Epoch [13], Batch [737/938], Loss: 0.5564493536949158\n",
      "Train: Epoch [13], Batch [738/938], Loss: 0.37430107593536377\n",
      "Train: Epoch [13], Batch [739/938], Loss: 0.532878041267395\n",
      "Train: Epoch [13], Batch [740/938], Loss: 0.41929078102111816\n",
      "Train: Epoch [13], Batch [741/938], Loss: 0.33047932386398315\n",
      "Train: Epoch [13], Batch [742/938], Loss: 0.540597677230835\n",
      "Train: Epoch [13], Batch [743/938], Loss: 0.6289063096046448\n",
      "Train: Epoch [13], Batch [744/938], Loss: 0.4338582456111908\n",
      "Train: Epoch [13], Batch [745/938], Loss: 0.3992488384246826\n",
      "Train: Epoch [13], Batch [746/938], Loss: 0.7715088725090027\n",
      "Train: Epoch [13], Batch [747/938], Loss: 0.39944231510162354\n",
      "Train: Epoch [13], Batch [748/938], Loss: 0.40298160910606384\n",
      "Train: Epoch [13], Batch [749/938], Loss: 0.4317026734352112\n",
      "Train: Epoch [13], Batch [750/938], Loss: 0.37404757738113403\n",
      "Train: Epoch [13], Batch [751/938], Loss: 0.4064614176750183\n",
      "Train: Epoch [13], Batch [752/938], Loss: 0.537684440612793\n",
      "Train: Epoch [13], Batch [753/938], Loss: 0.5872787833213806\n",
      "Train: Epoch [13], Batch [754/938], Loss: 0.42453986406326294\n",
      "Train: Epoch [13], Batch [755/938], Loss: 0.610969066619873\n",
      "Train: Epoch [13], Batch [756/938], Loss: 0.7064424157142639\n",
      "Train: Epoch [13], Batch [757/938], Loss: 0.42750751972198486\n",
      "Train: Epoch [13], Batch [758/938], Loss: 0.37053585052490234\n",
      "Train: Epoch [13], Batch [759/938], Loss: 0.354539692401886\n",
      "Train: Epoch [13], Batch [760/938], Loss: 0.5990347862243652\n",
      "Train: Epoch [13], Batch [761/938], Loss: 0.40603065490722656\n",
      "Train: Epoch [13], Batch [762/938], Loss: 0.46220827102661133\n",
      "Train: Epoch [13], Batch [763/938], Loss: 0.4557706117630005\n",
      "Train: Epoch [13], Batch [764/938], Loss: 0.6208359599113464\n",
      "Train: Epoch [13], Batch [765/938], Loss: 0.5275248289108276\n",
      "Train: Epoch [13], Batch [766/938], Loss: 0.44698452949523926\n",
      "Train: Epoch [13], Batch [767/938], Loss: 0.44899389147758484\n",
      "Train: Epoch [13], Batch [768/938], Loss: 0.7121061086654663\n",
      "Train: Epoch [13], Batch [769/938], Loss: 0.43265676498413086\n",
      "Train: Epoch [13], Batch [770/938], Loss: 0.4487383961677551\n",
      "Train: Epoch [13], Batch [771/938], Loss: 0.7272170782089233\n",
      "Train: Epoch [13], Batch [772/938], Loss: 0.5502734184265137\n",
      "Train: Epoch [13], Batch [773/938], Loss: 0.5590803027153015\n",
      "Train: Epoch [13], Batch [774/938], Loss: 0.582784116268158\n",
      "Train: Epoch [13], Batch [775/938], Loss: 0.4516521990299225\n",
      "Train: Epoch [13], Batch [776/938], Loss: 0.5280672311782837\n",
      "Train: Epoch [13], Batch [777/938], Loss: 0.38983970880508423\n",
      "Train: Epoch [13], Batch [778/938], Loss: 0.450779527425766\n",
      "Train: Epoch [13], Batch [779/938], Loss: 0.554179847240448\n",
      "Train: Epoch [13], Batch [780/938], Loss: 0.5193794965744019\n",
      "Train: Epoch [13], Batch [781/938], Loss: 0.4704216718673706\n",
      "Train: Epoch [13], Batch [782/938], Loss: 0.5019652247428894\n",
      "Train: Epoch [13], Batch [783/938], Loss: 0.7237375974655151\n",
      "Train: Epoch [13], Batch [784/938], Loss: 0.41883429884910583\n",
      "Train: Epoch [13], Batch [785/938], Loss: 0.4099113643169403\n",
      "Train: Epoch [13], Batch [786/938], Loss: 0.5212068557739258\n",
      "Train: Epoch [13], Batch [787/938], Loss: 0.4705747961997986\n",
      "Train: Epoch [13], Batch [788/938], Loss: 0.44856205582618713\n",
      "Train: Epoch [13], Batch [789/938], Loss: 0.3476428985595703\n",
      "Train: Epoch [13], Batch [790/938], Loss: 0.3704533576965332\n",
      "Train: Epoch [13], Batch [791/938], Loss: 0.35079333186149597\n",
      "Train: Epoch [13], Batch [792/938], Loss: 0.508976936340332\n",
      "Train: Epoch [13], Batch [793/938], Loss: 0.3674260377883911\n",
      "Train: Epoch [13], Batch [794/938], Loss: 0.5371860265731812\n",
      "Train: Epoch [13], Batch [795/938], Loss: 0.494812935590744\n",
      "Train: Epoch [13], Batch [796/938], Loss: 0.30153530836105347\n",
      "Train: Epoch [13], Batch [797/938], Loss: 0.48122653365135193\n",
      "Train: Epoch [13], Batch [798/938], Loss: 0.6441291570663452\n",
      "Train: Epoch [13], Batch [799/938], Loss: 0.3628029525279999\n",
      "Train: Epoch [13], Batch [800/938], Loss: 0.2846589684486389\n",
      "Train: Epoch [13], Batch [801/938], Loss: 0.531913161277771\n",
      "Train: Epoch [13], Batch [802/938], Loss: 0.4634861350059509\n",
      "Train: Epoch [13], Batch [803/938], Loss: 0.38216274976730347\n",
      "Train: Epoch [13], Batch [804/938], Loss: 0.351563036441803\n",
      "Train: Epoch [13], Batch [805/938], Loss: 0.5292981863021851\n",
      "Train: Epoch [13], Batch [806/938], Loss: 0.43055784702301025\n",
      "Train: Epoch [13], Batch [807/938], Loss: 0.5041429996490479\n",
      "Train: Epoch [13], Batch [808/938], Loss: 0.5310718417167664\n",
      "Train: Epoch [13], Batch [809/938], Loss: 0.36462685465812683\n",
      "Train: Epoch [13], Batch [810/938], Loss: 0.45132529735565186\n",
      "Train: Epoch [13], Batch [811/938], Loss: 0.47577935457229614\n",
      "Train: Epoch [13], Batch [812/938], Loss: 0.4672975540161133\n",
      "Train: Epoch [13], Batch [813/938], Loss: 0.39209485054016113\n",
      "Train: Epoch [13], Batch [814/938], Loss: 0.5238465070724487\n",
      "Train: Epoch [13], Batch [815/938], Loss: 0.5247529745101929\n",
      "Train: Epoch [13], Batch [816/938], Loss: 0.5699813365936279\n",
      "Train: Epoch [13], Batch [817/938], Loss: 0.39208072423934937\n",
      "Train: Epoch [13], Batch [818/938], Loss: 0.34423261880874634\n",
      "Train: Epoch [13], Batch [819/938], Loss: 0.47851869463920593\n",
      "Train: Epoch [13], Batch [820/938], Loss: 0.41753509640693665\n",
      "Train: Epoch [13], Batch [821/938], Loss: 0.5959726572036743\n",
      "Train: Epoch [13], Batch [822/938], Loss: 0.6033309102058411\n",
      "Train: Epoch [13], Batch [823/938], Loss: 0.522297739982605\n",
      "Train: Epoch [13], Batch [824/938], Loss: 0.41766974329948425\n",
      "Train: Epoch [13], Batch [825/938], Loss: 0.4751589000225067\n",
      "Train: Epoch [13], Batch [826/938], Loss: 0.4948040843009949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [13], Batch [827/938], Loss: 0.326373815536499\n",
      "Train: Epoch [13], Batch [828/938], Loss: 0.47658565640449524\n",
      "Train: Epoch [13], Batch [829/938], Loss: 0.37139713764190674\n",
      "Train: Epoch [13], Batch [830/938], Loss: 0.5165755152702332\n",
      "Train: Epoch [13], Batch [831/938], Loss: 0.4820851683616638\n",
      "Train: Epoch [13], Batch [832/938], Loss: 0.44031447172164917\n",
      "Train: Epoch [13], Batch [833/938], Loss: 0.5664010047912598\n",
      "Train: Epoch [13], Batch [834/938], Loss: 0.5575462579727173\n",
      "Train: Epoch [13], Batch [835/938], Loss: 0.5360046625137329\n",
      "Train: Epoch [13], Batch [836/938], Loss: 0.7413188219070435\n",
      "Train: Epoch [13], Batch [837/938], Loss: 0.8214329481124878\n",
      "Train: Epoch [13], Batch [838/938], Loss: 0.38903647661209106\n",
      "Train: Epoch [13], Batch [839/938], Loss: 0.4532316327095032\n",
      "Train: Epoch [13], Batch [840/938], Loss: 0.48306968808174133\n",
      "Train: Epoch [13], Batch [841/938], Loss: 0.4251776337623596\n",
      "Train: Epoch [13], Batch [842/938], Loss: 0.5923817753791809\n",
      "Train: Epoch [13], Batch [843/938], Loss: 0.3436209261417389\n",
      "Train: Epoch [13], Batch [844/938], Loss: 0.4355374574661255\n",
      "Train: Epoch [13], Batch [845/938], Loss: 0.6544194221496582\n",
      "Train: Epoch [13], Batch [846/938], Loss: 0.4206664264202118\n",
      "Train: Epoch [13], Batch [847/938], Loss: 0.581018328666687\n",
      "Train: Epoch [13], Batch [848/938], Loss: 0.631782591342926\n",
      "Train: Epoch [13], Batch [849/938], Loss: 0.46959924697875977\n",
      "Train: Epoch [13], Batch [850/938], Loss: 0.5108160972595215\n",
      "Train: Epoch [13], Batch [851/938], Loss: 0.5896022915840149\n",
      "Train: Epoch [13], Batch [852/938], Loss: 0.579596757888794\n",
      "Train: Epoch [13], Batch [853/938], Loss: 0.47976040840148926\n",
      "Train: Epoch [13], Batch [854/938], Loss: 0.5454990863800049\n",
      "Train: Epoch [13], Batch [855/938], Loss: 0.40425270795822144\n",
      "Train: Epoch [13], Batch [856/938], Loss: 0.23607565462589264\n",
      "Train: Epoch [13], Batch [857/938], Loss: 0.5414932370185852\n",
      "Train: Epoch [13], Batch [858/938], Loss: 0.3468063175678253\n",
      "Train: Epoch [13], Batch [859/938], Loss: 0.4566342532634735\n",
      "Train: Epoch [13], Batch [860/938], Loss: 0.44589686393737793\n",
      "Train: Epoch [13], Batch [861/938], Loss: 0.6509711742401123\n",
      "Train: Epoch [13], Batch [862/938], Loss: 0.474143922328949\n",
      "Train: Epoch [13], Batch [863/938], Loss: 0.6344189643859863\n",
      "Train: Epoch [13], Batch [864/938], Loss: 0.7074071168899536\n",
      "Train: Epoch [13], Batch [865/938], Loss: 0.44806236028671265\n",
      "Train: Epoch [13], Batch [866/938], Loss: 0.5715056657791138\n",
      "Train: Epoch [13], Batch [867/938], Loss: 0.4588237404823303\n",
      "Train: Epoch [13], Batch [868/938], Loss: 0.4922613203525543\n",
      "Train: Epoch [13], Batch [869/938], Loss: 0.37143102288246155\n",
      "Train: Epoch [13], Batch [870/938], Loss: 0.5020047426223755\n",
      "Train: Epoch [13], Batch [871/938], Loss: 0.4165310561656952\n",
      "Train: Epoch [13], Batch [872/938], Loss: 0.5349617004394531\n",
      "Train: Epoch [13], Batch [873/938], Loss: 0.5122712850570679\n",
      "Train: Epoch [13], Batch [874/938], Loss: 0.45817768573760986\n",
      "Train: Epoch [13], Batch [875/938], Loss: 0.6236820220947266\n",
      "Train: Epoch [13], Batch [876/938], Loss: 0.37491145730018616\n",
      "Train: Epoch [13], Batch [877/938], Loss: 0.5971800684928894\n",
      "Train: Epoch [13], Batch [878/938], Loss: 0.5344031453132629\n",
      "Train: Epoch [13], Batch [879/938], Loss: 0.5403986573219299\n",
      "Train: Epoch [13], Batch [880/938], Loss: 0.46228447556495667\n",
      "Train: Epoch [13], Batch [881/938], Loss: 0.5319477319717407\n",
      "Train: Epoch [13], Batch [882/938], Loss: 0.271950364112854\n",
      "Train: Epoch [13], Batch [883/938], Loss: 0.728622555732727\n",
      "Train: Epoch [13], Batch [884/938], Loss: 0.47912269830703735\n",
      "Train: Epoch [13], Batch [885/938], Loss: 0.4026413559913635\n",
      "Train: Epoch [13], Batch [886/938], Loss: 0.45633557438850403\n",
      "Train: Epoch [13], Batch [887/938], Loss: 0.5303223729133606\n",
      "Train: Epoch [13], Batch [888/938], Loss: 0.4245094060897827\n",
      "Train: Epoch [13], Batch [889/938], Loss: 0.27687880396842957\n",
      "Train: Epoch [13], Batch [890/938], Loss: 0.6314159631729126\n",
      "Train: Epoch [13], Batch [891/938], Loss: 0.3548295795917511\n",
      "Train: Epoch [13], Batch [892/938], Loss: 0.5472592115402222\n",
      "Train: Epoch [13], Batch [893/938], Loss: 0.498440146446228\n",
      "Train: Epoch [13], Batch [894/938], Loss: 0.5551332235336304\n",
      "Train: Epoch [13], Batch [895/938], Loss: 0.3804429769515991\n",
      "Train: Epoch [13], Batch [896/938], Loss: 0.24377885460853577\n",
      "Train: Epoch [13], Batch [897/938], Loss: 0.47039270401000977\n",
      "Train: Epoch [13], Batch [898/938], Loss: 0.5653052926063538\n",
      "Train: Epoch [13], Batch [899/938], Loss: 0.27375173568725586\n",
      "Train: Epoch [13], Batch [900/938], Loss: 0.45670732855796814\n",
      "Train: Epoch [13], Batch [901/938], Loss: 0.4769108295440674\n",
      "Train: Epoch [13], Batch [902/938], Loss: 0.6102380752563477\n",
      "Train: Epoch [13], Batch [903/938], Loss: 0.35600727796554565\n",
      "Train: Epoch [13], Batch [904/938], Loss: 0.631549060344696\n",
      "Train: Epoch [13], Batch [905/938], Loss: 0.3559964895248413\n",
      "Train: Epoch [13], Batch [906/938], Loss: 0.5125285387039185\n",
      "Train: Epoch [13], Batch [907/938], Loss: 0.3092183470726013\n",
      "Train: Epoch [13], Batch [908/938], Loss: 0.5355504751205444\n",
      "Train: Epoch [13], Batch [909/938], Loss: 0.5092788934707642\n",
      "Train: Epoch [13], Batch [910/938], Loss: 0.5044493675231934\n",
      "Train: Epoch [13], Batch [911/938], Loss: 0.5350720286369324\n",
      "Train: Epoch [13], Batch [912/938], Loss: 0.4494279623031616\n",
      "Train: Epoch [13], Batch [913/938], Loss: 0.42453640699386597\n",
      "Train: Epoch [13], Batch [914/938], Loss: 0.4102283716201782\n",
      "Train: Epoch [13], Batch [915/938], Loss: 0.6850490570068359\n",
      "Train: Epoch [13], Batch [916/938], Loss: 0.5246461629867554\n",
      "Train: Epoch [13], Batch [917/938], Loss: 0.43306225538253784\n",
      "Train: Epoch [13], Batch [918/938], Loss: 0.5163476467132568\n",
      "Train: Epoch [13], Batch [919/938], Loss: 0.34500205516815186\n",
      "Train: Epoch [13], Batch [920/938], Loss: 0.7121711373329163\n",
      "Train: Epoch [13], Batch [921/938], Loss: 0.42282068729400635\n",
      "Train: Epoch [13], Batch [922/938], Loss: 0.47101378440856934\n",
      "Train: Epoch [13], Batch [923/938], Loss: 0.3090744912624359\n",
      "Train: Epoch [13], Batch [924/938], Loss: 0.5414291620254517\n",
      "Train: Epoch [13], Batch [925/938], Loss: 0.42942315340042114\n",
      "Train: Epoch [13], Batch [926/938], Loss: 0.4553077220916748\n",
      "Train: Epoch [13], Batch [927/938], Loss: 0.6273115873336792\n",
      "Train: Epoch [13], Batch [928/938], Loss: 0.6585544347763062\n",
      "Train: Epoch [13], Batch [929/938], Loss: 0.5075860023498535\n",
      "Train: Epoch [13], Batch [930/938], Loss: 0.48102670907974243\n",
      "Train: Epoch [13], Batch [931/938], Loss: 0.4589325189590454\n",
      "Train: Epoch [13], Batch [932/938], Loss: 0.4845033288002014\n",
      "Train: Epoch [13], Batch [933/938], Loss: 0.3477277457714081\n",
      "Train: Epoch [13], Batch [934/938], Loss: 0.3385675847530365\n",
      "Train: Epoch [13], Batch [935/938], Loss: 0.4403493404388428\n",
      "Train: Epoch [13], Batch [936/938], Loss: 0.5379518270492554\n",
      "Train: Epoch [13], Batch [937/938], Loss: 0.540732741355896\n",
      "Train: Epoch [13], Batch [938/938], Loss: 0.5363790988922119\n",
      "Accuracy of train set: 0.8306833333333333\n",
      "Validation: Epoch [13], Batch [1/938], Loss: 0.5577378273010254\n",
      "Validation: Epoch [13], Batch [2/938], Loss: 0.5428037643432617\n",
      "Validation: Epoch [13], Batch [3/938], Loss: 0.4467664957046509\n",
      "Validation: Epoch [13], Batch [4/938], Loss: 0.38793787360191345\n",
      "Validation: Epoch [13], Batch [5/938], Loss: 0.5646581649780273\n",
      "Validation: Epoch [13], Batch [6/938], Loss: 0.4365384578704834\n",
      "Validation: Epoch [13], Batch [7/938], Loss: 0.4357459247112274\n",
      "Validation: Epoch [13], Batch [8/938], Loss: 0.6406733989715576\n",
      "Validation: Epoch [13], Batch [9/938], Loss: 0.32514870166778564\n",
      "Validation: Epoch [13], Batch [10/938], Loss: 0.38401365280151367\n",
      "Validation: Epoch [13], Batch [11/938], Loss: 0.5486881732940674\n",
      "Validation: Epoch [13], Batch [12/938], Loss: 0.5263981819152832\n",
      "Validation: Epoch [13], Batch [13/938], Loss: 0.4490147829055786\n",
      "Validation: Epoch [13], Batch [14/938], Loss: 0.6483091115951538\n",
      "Validation: Epoch [13], Batch [15/938], Loss: 0.35874873399734497\n",
      "Validation: Epoch [13], Batch [16/938], Loss: 0.5617608428001404\n",
      "Validation: Epoch [13], Batch [17/938], Loss: 0.4325113296508789\n",
      "Validation: Epoch [13], Batch [18/938], Loss: 0.49387210607528687\n",
      "Validation: Epoch [13], Batch [19/938], Loss: 0.665094256401062\n",
      "Validation: Epoch [13], Batch [20/938], Loss: 0.4386030435562134\n",
      "Validation: Epoch [13], Batch [21/938], Loss: 0.4202768802642822\n",
      "Validation: Epoch [13], Batch [22/938], Loss: 0.5354958176612854\n",
      "Validation: Epoch [13], Batch [23/938], Loss: 0.5860885381698608\n",
      "Validation: Epoch [13], Batch [24/938], Loss: 0.458023339509964\n",
      "Validation: Epoch [13], Batch [25/938], Loss: 0.7251856923103333\n",
      "Validation: Epoch [13], Batch [26/938], Loss: 0.4584800899028778\n",
      "Validation: Epoch [13], Batch [27/938], Loss: 0.31276434659957886\n",
      "Validation: Epoch [13], Batch [28/938], Loss: 0.4590074419975281\n",
      "Validation: Epoch [13], Batch [29/938], Loss: 0.5184407830238342\n",
      "Validation: Epoch [13], Batch [30/938], Loss: 0.6828155517578125\n",
      "Validation: Epoch [13], Batch [31/938], Loss: 0.40045350790023804\n",
      "Validation: Epoch [13], Batch [32/938], Loss: 0.4787178337574005\n",
      "Validation: Epoch [13], Batch [33/938], Loss: 0.37480488419532776\n",
      "Validation: Epoch [13], Batch [34/938], Loss: 0.5661615133285522\n",
      "Validation: Epoch [13], Batch [35/938], Loss: 0.6506142616271973\n",
      "Validation: Epoch [13], Batch [36/938], Loss: 0.42638444900512695\n",
      "Validation: Epoch [13], Batch [37/938], Loss: 0.6170308589935303\n",
      "Validation: Epoch [13], Batch [38/938], Loss: 0.526305079460144\n",
      "Validation: Epoch [13], Batch [39/938], Loss: 0.6664076447486877\n",
      "Validation: Epoch [13], Batch [40/938], Loss: 0.4376910328865051\n",
      "Validation: Epoch [13], Batch [41/938], Loss: 0.4157490134239197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [42/938], Loss: 0.5185744762420654\n",
      "Validation: Epoch [13], Batch [43/938], Loss: 0.3665024936199188\n",
      "Validation: Epoch [13], Batch [44/938], Loss: 0.4463500380516052\n",
      "Validation: Epoch [13], Batch [45/938], Loss: 0.5062111020088196\n",
      "Validation: Epoch [13], Batch [46/938], Loss: 0.5498558282852173\n",
      "Validation: Epoch [13], Batch [47/938], Loss: 0.47574663162231445\n",
      "Validation: Epoch [13], Batch [48/938], Loss: 0.24849238991737366\n",
      "Validation: Epoch [13], Batch [49/938], Loss: 0.568023681640625\n",
      "Validation: Epoch [13], Batch [50/938], Loss: 0.41582638025283813\n",
      "Validation: Epoch [13], Batch [51/938], Loss: 0.47398841381073\n",
      "Validation: Epoch [13], Batch [52/938], Loss: 0.5885143280029297\n",
      "Validation: Epoch [13], Batch [53/938], Loss: 0.38985419273376465\n",
      "Validation: Epoch [13], Batch [54/938], Loss: 0.33923646807670593\n",
      "Validation: Epoch [13], Batch [55/938], Loss: 0.3820255398750305\n",
      "Validation: Epoch [13], Batch [56/938], Loss: 0.6767915487289429\n",
      "Validation: Epoch [13], Batch [57/938], Loss: 0.46826106309890747\n",
      "Validation: Epoch [13], Batch [58/938], Loss: 0.4577295184135437\n",
      "Validation: Epoch [13], Batch [59/938], Loss: 0.29279834032058716\n",
      "Validation: Epoch [13], Batch [60/938], Loss: 0.5340405702590942\n",
      "Validation: Epoch [13], Batch [61/938], Loss: 0.5197272896766663\n",
      "Validation: Epoch [13], Batch [62/938], Loss: 0.4906092584133148\n",
      "Validation: Epoch [13], Batch [63/938], Loss: 0.6548463106155396\n",
      "Validation: Epoch [13], Batch [64/938], Loss: 0.6466624736785889\n",
      "Validation: Epoch [13], Batch [65/938], Loss: 0.5031289458274841\n",
      "Validation: Epoch [13], Batch [66/938], Loss: 0.5060960650444031\n",
      "Validation: Epoch [13], Batch [67/938], Loss: 0.3948119580745697\n",
      "Validation: Epoch [13], Batch [68/938], Loss: 0.4516792297363281\n",
      "Validation: Epoch [13], Batch [69/938], Loss: 0.47035840153694153\n",
      "Validation: Epoch [13], Batch [70/938], Loss: 0.44002729654312134\n",
      "Validation: Epoch [13], Batch [71/938], Loss: 0.6641465425491333\n",
      "Validation: Epoch [13], Batch [72/938], Loss: 0.28268933296203613\n",
      "Validation: Epoch [13], Batch [73/938], Loss: 0.5404549837112427\n",
      "Validation: Epoch [13], Batch [74/938], Loss: 0.5757128000259399\n",
      "Validation: Epoch [13], Batch [75/938], Loss: 0.487246036529541\n",
      "Validation: Epoch [13], Batch [76/938], Loss: 0.43064379692077637\n",
      "Validation: Epoch [13], Batch [77/938], Loss: 0.4551502466201782\n",
      "Validation: Epoch [13], Batch [78/938], Loss: 0.40222445130348206\n",
      "Validation: Epoch [13], Batch [79/938], Loss: 0.6395819187164307\n",
      "Validation: Epoch [13], Batch [80/938], Loss: 0.8272334337234497\n",
      "Validation: Epoch [13], Batch [81/938], Loss: 0.4831923842430115\n",
      "Validation: Epoch [13], Batch [82/938], Loss: 0.21708637475967407\n",
      "Validation: Epoch [13], Batch [83/938], Loss: 0.5127426981925964\n",
      "Validation: Epoch [13], Batch [84/938], Loss: 0.3657432198524475\n",
      "Validation: Epoch [13], Batch [85/938], Loss: 0.47296327352523804\n",
      "Validation: Epoch [13], Batch [86/938], Loss: 0.6078799962997437\n",
      "Validation: Epoch [13], Batch [87/938], Loss: 0.3808310925960541\n",
      "Validation: Epoch [13], Batch [88/938], Loss: 0.35244789719581604\n",
      "Validation: Epoch [13], Batch [89/938], Loss: 0.520677924156189\n",
      "Validation: Epoch [13], Batch [90/938], Loss: 0.5587241649627686\n",
      "Validation: Epoch [13], Batch [91/938], Loss: 0.36914440989494324\n",
      "Validation: Epoch [13], Batch [92/938], Loss: 0.36913827061653137\n",
      "Validation: Epoch [13], Batch [93/938], Loss: 0.5224093198776245\n",
      "Validation: Epoch [13], Batch [94/938], Loss: 0.4283056855201721\n",
      "Validation: Epoch [13], Batch [95/938], Loss: 0.5034642219543457\n",
      "Validation: Epoch [13], Batch [96/938], Loss: 0.34322935342788696\n",
      "Validation: Epoch [13], Batch [97/938], Loss: 0.4720860421657562\n",
      "Validation: Epoch [13], Batch [98/938], Loss: 0.43945568799972534\n",
      "Validation: Epoch [13], Batch [99/938], Loss: 0.6491202116012573\n",
      "Validation: Epoch [13], Batch [100/938], Loss: 0.7009187936782837\n",
      "Validation: Epoch [13], Batch [101/938], Loss: 0.5438982248306274\n",
      "Validation: Epoch [13], Batch [102/938], Loss: 0.3561253547668457\n",
      "Validation: Epoch [13], Batch [103/938], Loss: 0.3140995502471924\n",
      "Validation: Epoch [13], Batch [104/938], Loss: 0.5795127153396606\n",
      "Validation: Epoch [13], Batch [105/938], Loss: 0.37087857723236084\n",
      "Validation: Epoch [13], Batch [106/938], Loss: 0.3989017605781555\n",
      "Validation: Epoch [13], Batch [107/938], Loss: 0.4858747720718384\n",
      "Validation: Epoch [13], Batch [108/938], Loss: 0.3238493800163269\n",
      "Validation: Epoch [13], Batch [109/938], Loss: 0.3951888680458069\n",
      "Validation: Epoch [13], Batch [110/938], Loss: 0.5342347025871277\n",
      "Validation: Epoch [13], Batch [111/938], Loss: 0.47843968868255615\n",
      "Validation: Epoch [13], Batch [112/938], Loss: 0.5119091272354126\n",
      "Validation: Epoch [13], Batch [113/938], Loss: 0.5107583999633789\n",
      "Validation: Epoch [13], Batch [114/938], Loss: 0.5844954252243042\n",
      "Validation: Epoch [13], Batch [115/938], Loss: 0.5396407842636108\n",
      "Validation: Epoch [13], Batch [116/938], Loss: 0.6490575671195984\n",
      "Validation: Epoch [13], Batch [117/938], Loss: 0.4965468645095825\n",
      "Validation: Epoch [13], Batch [118/938], Loss: 0.46364906430244446\n",
      "Validation: Epoch [13], Batch [119/938], Loss: 0.40635254979133606\n",
      "Validation: Epoch [13], Batch [120/938], Loss: 0.4266270399093628\n",
      "Validation: Epoch [13], Batch [121/938], Loss: 0.5505056381225586\n",
      "Validation: Epoch [13], Batch [122/938], Loss: 0.49619653820991516\n",
      "Validation: Epoch [13], Batch [123/938], Loss: 0.47811323404312134\n",
      "Validation: Epoch [13], Batch [124/938], Loss: 0.5104513168334961\n",
      "Validation: Epoch [13], Batch [125/938], Loss: 0.47943955659866333\n",
      "Validation: Epoch [13], Batch [126/938], Loss: 0.342227041721344\n",
      "Validation: Epoch [13], Batch [127/938], Loss: 0.3809848129749298\n",
      "Validation: Epoch [13], Batch [128/938], Loss: 0.718981146812439\n",
      "Validation: Epoch [13], Batch [129/938], Loss: 0.4968889057636261\n",
      "Validation: Epoch [13], Batch [130/938], Loss: 0.3015354573726654\n",
      "Validation: Epoch [13], Batch [131/938], Loss: 0.598944902420044\n",
      "Validation: Epoch [13], Batch [132/938], Loss: 0.5851024389266968\n",
      "Validation: Epoch [13], Batch [133/938], Loss: 0.6896746158599854\n",
      "Validation: Epoch [13], Batch [134/938], Loss: 0.5873777866363525\n",
      "Validation: Epoch [13], Batch [135/938], Loss: 0.4942939877510071\n",
      "Validation: Epoch [13], Batch [136/938], Loss: 0.5025126934051514\n",
      "Validation: Epoch [13], Batch [137/938], Loss: 0.4809299111366272\n",
      "Validation: Epoch [13], Batch [138/938], Loss: 0.7069438099861145\n",
      "Validation: Epoch [13], Batch [139/938], Loss: 0.5874499082565308\n",
      "Validation: Epoch [13], Batch [140/938], Loss: 0.479790598154068\n",
      "Validation: Epoch [13], Batch [141/938], Loss: 0.5734364986419678\n",
      "Validation: Epoch [13], Batch [142/938], Loss: 0.40173932909965515\n",
      "Validation: Epoch [13], Batch [143/938], Loss: 0.5410664081573486\n",
      "Validation: Epoch [13], Batch [144/938], Loss: 0.5788717269897461\n",
      "Validation: Epoch [13], Batch [145/938], Loss: 0.5321823358535767\n",
      "Validation: Epoch [13], Batch [146/938], Loss: 0.4935908019542694\n",
      "Validation: Epoch [13], Batch [147/938], Loss: 0.5047144889831543\n",
      "Validation: Epoch [13], Batch [148/938], Loss: 0.41283637285232544\n",
      "Validation: Epoch [13], Batch [149/938], Loss: 0.4331018924713135\n",
      "Validation: Epoch [13], Batch [150/938], Loss: 0.6414769291877747\n",
      "Validation: Epoch [13], Batch [151/938], Loss: 0.5196376442909241\n",
      "Validation: Epoch [13], Batch [152/938], Loss: 0.5460141897201538\n",
      "Validation: Epoch [13], Batch [153/938], Loss: 0.5835900902748108\n",
      "Validation: Epoch [13], Batch [154/938], Loss: 0.4453723430633545\n",
      "Validation: Epoch [13], Batch [155/938], Loss: 0.4461638927459717\n",
      "Validation: Epoch [13], Batch [156/938], Loss: 0.4088035225868225\n",
      "Validation: Epoch [13], Batch [157/938], Loss: 0.3635672628879547\n",
      "Validation: Epoch [13], Batch [158/938], Loss: 0.4766506552696228\n",
      "Validation: Epoch [13], Batch [159/938], Loss: 0.43996697664260864\n",
      "Validation: Epoch [13], Batch [160/938], Loss: 0.5232189893722534\n",
      "Validation: Epoch [13], Batch [161/938], Loss: 0.6103991270065308\n",
      "Validation: Epoch [13], Batch [162/938], Loss: 0.5041744709014893\n",
      "Validation: Epoch [13], Batch [163/938], Loss: 0.37816962599754333\n",
      "Validation: Epoch [13], Batch [164/938], Loss: 0.6386196613311768\n",
      "Validation: Epoch [13], Batch [165/938], Loss: 0.5156711339950562\n",
      "Validation: Epoch [13], Batch [166/938], Loss: 0.396013468503952\n",
      "Validation: Epoch [13], Batch [167/938], Loss: 0.7454040050506592\n",
      "Validation: Epoch [13], Batch [168/938], Loss: 0.42041486501693726\n",
      "Validation: Epoch [13], Batch [169/938], Loss: 0.5245884656906128\n",
      "Validation: Epoch [13], Batch [170/938], Loss: 0.5482562780380249\n",
      "Validation: Epoch [13], Batch [171/938], Loss: 0.4001990258693695\n",
      "Validation: Epoch [13], Batch [172/938], Loss: 0.39621779322624207\n",
      "Validation: Epoch [13], Batch [173/938], Loss: 0.6889940500259399\n",
      "Validation: Epoch [13], Batch [174/938], Loss: 0.5302133560180664\n",
      "Validation: Epoch [13], Batch [175/938], Loss: 0.5238075852394104\n",
      "Validation: Epoch [13], Batch [176/938], Loss: 0.5125178694725037\n",
      "Validation: Epoch [13], Batch [177/938], Loss: 0.5971506237983704\n",
      "Validation: Epoch [13], Batch [178/938], Loss: 0.3469467759132385\n",
      "Validation: Epoch [13], Batch [179/938], Loss: 0.5963040590286255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [180/938], Loss: 0.549820601940155\n",
      "Validation: Epoch [13], Batch [181/938], Loss: 0.4288560748100281\n",
      "Validation: Epoch [13], Batch [182/938], Loss: 0.5752226710319519\n",
      "Validation: Epoch [13], Batch [183/938], Loss: 0.5470833778381348\n",
      "Validation: Epoch [13], Batch [184/938], Loss: 0.5605705380439758\n",
      "Validation: Epoch [13], Batch [185/938], Loss: 0.4543440639972687\n",
      "Validation: Epoch [13], Batch [186/938], Loss: 0.6000648736953735\n",
      "Validation: Epoch [13], Batch [187/938], Loss: 0.712289571762085\n",
      "Validation: Epoch [13], Batch [188/938], Loss: 0.6449517607688904\n",
      "Validation: Epoch [13], Batch [189/938], Loss: 0.4320816695690155\n",
      "Validation: Epoch [13], Batch [190/938], Loss: 0.3868435025215149\n",
      "Validation: Epoch [13], Batch [191/938], Loss: 0.5840252041816711\n",
      "Validation: Epoch [13], Batch [192/938], Loss: 0.6137773990631104\n",
      "Validation: Epoch [13], Batch [193/938], Loss: 0.40707188844680786\n",
      "Validation: Epoch [13], Batch [194/938], Loss: 0.53411865234375\n",
      "Validation: Epoch [13], Batch [195/938], Loss: 0.5268732309341431\n",
      "Validation: Epoch [13], Batch [196/938], Loss: 0.520332932472229\n",
      "Validation: Epoch [13], Batch [197/938], Loss: 0.6618669629096985\n",
      "Validation: Epoch [13], Batch [198/938], Loss: 0.37686875462532043\n",
      "Validation: Epoch [13], Batch [199/938], Loss: 0.5350732803344727\n",
      "Validation: Epoch [13], Batch [200/938], Loss: 0.48906591534614563\n",
      "Validation: Epoch [13], Batch [201/938], Loss: 0.4950105845928192\n",
      "Validation: Epoch [13], Batch [202/938], Loss: 0.401262491941452\n",
      "Validation: Epoch [13], Batch [203/938], Loss: 0.6384517550468445\n",
      "Validation: Epoch [13], Batch [204/938], Loss: 0.49754995107650757\n",
      "Validation: Epoch [13], Batch [205/938], Loss: 0.4777306318283081\n",
      "Validation: Epoch [13], Batch [206/938], Loss: 0.6075645089149475\n",
      "Validation: Epoch [13], Batch [207/938], Loss: 0.5452840924263\n",
      "Validation: Epoch [13], Batch [208/938], Loss: 0.5377529263496399\n",
      "Validation: Epoch [13], Batch [209/938], Loss: 0.3129139542579651\n",
      "Validation: Epoch [13], Batch [210/938], Loss: 0.3596728444099426\n",
      "Validation: Epoch [13], Batch [211/938], Loss: 0.8061245679855347\n",
      "Validation: Epoch [13], Batch [212/938], Loss: 0.40942680835723877\n",
      "Validation: Epoch [13], Batch [213/938], Loss: 0.3448564410209656\n",
      "Validation: Epoch [13], Batch [214/938], Loss: 0.4565226435661316\n",
      "Validation: Epoch [13], Batch [215/938], Loss: 0.535004734992981\n",
      "Validation: Epoch [13], Batch [216/938], Loss: 0.4275292158126831\n",
      "Validation: Epoch [13], Batch [217/938], Loss: 0.40341052412986755\n",
      "Validation: Epoch [13], Batch [218/938], Loss: 0.522944450378418\n",
      "Validation: Epoch [13], Batch [219/938], Loss: 0.5804747343063354\n",
      "Validation: Epoch [13], Batch [220/938], Loss: 0.50914466381073\n",
      "Validation: Epoch [13], Batch [221/938], Loss: 0.32645633816719055\n",
      "Validation: Epoch [13], Batch [222/938], Loss: 0.40126633644104004\n",
      "Validation: Epoch [13], Batch [223/938], Loss: 0.4330998957157135\n",
      "Validation: Epoch [13], Batch [224/938], Loss: 0.4622146785259247\n",
      "Validation: Epoch [13], Batch [225/938], Loss: 0.34364402294158936\n",
      "Validation: Epoch [13], Batch [226/938], Loss: 0.3938561975955963\n",
      "Validation: Epoch [13], Batch [227/938], Loss: 0.5257220268249512\n",
      "Validation: Epoch [13], Batch [228/938], Loss: 0.3112277388572693\n",
      "Validation: Epoch [13], Batch [229/938], Loss: 0.45861876010894775\n",
      "Validation: Epoch [13], Batch [230/938], Loss: 0.33965814113616943\n",
      "Validation: Epoch [13], Batch [231/938], Loss: 0.39426833391189575\n",
      "Validation: Epoch [13], Batch [232/938], Loss: 0.46136176586151123\n",
      "Validation: Epoch [13], Batch [233/938], Loss: 0.47874715924263\n",
      "Validation: Epoch [13], Batch [234/938], Loss: 0.7809600234031677\n",
      "Validation: Epoch [13], Batch [235/938], Loss: 0.6292707324028015\n",
      "Validation: Epoch [13], Batch [236/938], Loss: 0.6098008751869202\n",
      "Validation: Epoch [13], Batch [237/938], Loss: 0.33913037180900574\n",
      "Validation: Epoch [13], Batch [238/938], Loss: 0.5047549605369568\n",
      "Validation: Epoch [13], Batch [239/938], Loss: 0.44752582907676697\n",
      "Validation: Epoch [13], Batch [240/938], Loss: 0.3794228732585907\n",
      "Validation: Epoch [13], Batch [241/938], Loss: 0.5789055228233337\n",
      "Validation: Epoch [13], Batch [242/938], Loss: 0.30885565280914307\n",
      "Validation: Epoch [13], Batch [243/938], Loss: 0.7442399263381958\n",
      "Validation: Epoch [13], Batch [244/938], Loss: 0.844333291053772\n",
      "Validation: Epoch [13], Batch [245/938], Loss: 0.48315081000328064\n",
      "Validation: Epoch [13], Batch [246/938], Loss: 0.46034374833106995\n",
      "Validation: Epoch [13], Batch [247/938], Loss: 0.36156734824180603\n",
      "Validation: Epoch [13], Batch [248/938], Loss: 0.3770931363105774\n",
      "Validation: Epoch [13], Batch [249/938], Loss: 0.4235762655735016\n",
      "Validation: Epoch [13], Batch [250/938], Loss: 0.6639052629470825\n",
      "Validation: Epoch [13], Batch [251/938], Loss: 0.5845999717712402\n",
      "Validation: Epoch [13], Batch [252/938], Loss: 0.5104250907897949\n",
      "Validation: Epoch [13], Batch [253/938], Loss: 0.37038564682006836\n",
      "Validation: Epoch [13], Batch [254/938], Loss: 0.3637351989746094\n",
      "Validation: Epoch [13], Batch [255/938], Loss: 0.3658643066883087\n",
      "Validation: Epoch [13], Batch [256/938], Loss: 0.6718462109565735\n",
      "Validation: Epoch [13], Batch [257/938], Loss: 0.5961661338806152\n",
      "Validation: Epoch [13], Batch [258/938], Loss: 0.4467143416404724\n",
      "Validation: Epoch [13], Batch [259/938], Loss: 0.5638657808303833\n",
      "Validation: Epoch [13], Batch [260/938], Loss: 0.4227007031440735\n",
      "Validation: Epoch [13], Batch [261/938], Loss: 0.3540419340133667\n",
      "Validation: Epoch [13], Batch [262/938], Loss: 0.46402084827423096\n",
      "Validation: Epoch [13], Batch [263/938], Loss: 0.3055805563926697\n",
      "Validation: Epoch [13], Batch [264/938], Loss: 0.46881818771362305\n",
      "Validation: Epoch [13], Batch [265/938], Loss: 0.6038573980331421\n",
      "Validation: Epoch [13], Batch [266/938], Loss: 0.5121777057647705\n",
      "Validation: Epoch [13], Batch [267/938], Loss: 0.39089876413345337\n",
      "Validation: Epoch [13], Batch [268/938], Loss: 0.46882060170173645\n",
      "Validation: Epoch [13], Batch [269/938], Loss: 0.8032379746437073\n",
      "Validation: Epoch [13], Batch [270/938], Loss: 0.419974148273468\n",
      "Validation: Epoch [13], Batch [271/938], Loss: 0.49087777733802795\n",
      "Validation: Epoch [13], Batch [272/938], Loss: 0.45073235034942627\n",
      "Validation: Epoch [13], Batch [273/938], Loss: 0.4270424544811249\n",
      "Validation: Epoch [13], Batch [274/938], Loss: 0.6054131984710693\n",
      "Validation: Epoch [13], Batch [275/938], Loss: 0.4487132430076599\n",
      "Validation: Epoch [13], Batch [276/938], Loss: 0.41369861364364624\n",
      "Validation: Epoch [13], Batch [277/938], Loss: 0.4973837733268738\n",
      "Validation: Epoch [13], Batch [278/938], Loss: 0.46065986156463623\n",
      "Validation: Epoch [13], Batch [279/938], Loss: 0.6285742521286011\n",
      "Validation: Epoch [13], Batch [280/938], Loss: 0.45673036575317383\n",
      "Validation: Epoch [13], Batch [281/938], Loss: 0.4633650779724121\n",
      "Validation: Epoch [13], Batch [282/938], Loss: 0.39072924852371216\n",
      "Validation: Epoch [13], Batch [283/938], Loss: 0.43321043252944946\n",
      "Validation: Epoch [13], Batch [284/938], Loss: 0.36285632848739624\n",
      "Validation: Epoch [13], Batch [285/938], Loss: 0.5376571416854858\n",
      "Validation: Epoch [13], Batch [286/938], Loss: 0.6394298076629639\n",
      "Validation: Epoch [13], Batch [287/938], Loss: 0.450382262468338\n",
      "Validation: Epoch [13], Batch [288/938], Loss: 0.3582625091075897\n",
      "Validation: Epoch [13], Batch [289/938], Loss: 0.4413089156150818\n",
      "Validation: Epoch [13], Batch [290/938], Loss: 0.31307756900787354\n",
      "Validation: Epoch [13], Batch [291/938], Loss: 0.38627561926841736\n",
      "Validation: Epoch [13], Batch [292/938], Loss: 0.35199210047721863\n",
      "Validation: Epoch [13], Batch [293/938], Loss: 0.7838661670684814\n",
      "Validation: Epoch [13], Batch [294/938], Loss: 0.47125139832496643\n",
      "Validation: Epoch [13], Batch [295/938], Loss: 0.5221723914146423\n",
      "Validation: Epoch [13], Batch [296/938], Loss: 0.824816882610321\n",
      "Validation: Epoch [13], Batch [297/938], Loss: 0.32232534885406494\n",
      "Validation: Epoch [13], Batch [298/938], Loss: 0.5713391304016113\n",
      "Validation: Epoch [13], Batch [299/938], Loss: 0.5509723424911499\n",
      "Validation: Epoch [13], Batch [300/938], Loss: 0.2978343963623047\n",
      "Validation: Epoch [13], Batch [301/938], Loss: 0.4479432702064514\n",
      "Validation: Epoch [13], Batch [302/938], Loss: 0.3787115812301636\n",
      "Validation: Epoch [13], Batch [303/938], Loss: 0.4142810106277466\n",
      "Validation: Epoch [13], Batch [304/938], Loss: 0.5161093473434448\n",
      "Validation: Epoch [13], Batch [305/938], Loss: 0.4815811514854431\n",
      "Validation: Epoch [13], Batch [306/938], Loss: 0.4149063229560852\n",
      "Validation: Epoch [13], Batch [307/938], Loss: 0.5237020254135132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [308/938], Loss: 0.5180059671401978\n",
      "Validation: Epoch [13], Batch [309/938], Loss: 0.6063188910484314\n",
      "Validation: Epoch [13], Batch [310/938], Loss: 0.2855076193809509\n",
      "Validation: Epoch [13], Batch [311/938], Loss: 0.37211132049560547\n",
      "Validation: Epoch [13], Batch [312/938], Loss: 0.6768693923950195\n",
      "Validation: Epoch [13], Batch [313/938], Loss: 0.42684176564216614\n",
      "Validation: Epoch [13], Batch [314/938], Loss: 0.38997897505760193\n",
      "Validation: Epoch [13], Batch [315/938], Loss: 0.39896607398986816\n",
      "Validation: Epoch [13], Batch [316/938], Loss: 0.42711079120635986\n",
      "Validation: Epoch [13], Batch [317/938], Loss: 0.47453269362449646\n",
      "Validation: Epoch [13], Batch [318/938], Loss: 0.5200244784355164\n",
      "Validation: Epoch [13], Batch [319/938], Loss: 0.5459741353988647\n",
      "Validation: Epoch [13], Batch [320/938], Loss: 0.4705672264099121\n",
      "Validation: Epoch [13], Batch [321/938], Loss: 0.6272522211074829\n",
      "Validation: Epoch [13], Batch [322/938], Loss: 0.49032455682754517\n",
      "Validation: Epoch [13], Batch [323/938], Loss: 0.3425193428993225\n",
      "Validation: Epoch [13], Batch [324/938], Loss: 0.5045323371887207\n",
      "Validation: Epoch [13], Batch [325/938], Loss: 0.4695080518722534\n",
      "Validation: Epoch [13], Batch [326/938], Loss: 0.381258487701416\n",
      "Validation: Epoch [13], Batch [327/938], Loss: 0.2687091827392578\n",
      "Validation: Epoch [13], Batch [328/938], Loss: 0.6076132655143738\n",
      "Validation: Epoch [13], Batch [329/938], Loss: 0.3188624978065491\n",
      "Validation: Epoch [13], Batch [330/938], Loss: 0.5146629810333252\n",
      "Validation: Epoch [13], Batch [331/938], Loss: 0.42942720651626587\n",
      "Validation: Epoch [13], Batch [332/938], Loss: 0.5791970491409302\n",
      "Validation: Epoch [13], Batch [333/938], Loss: 0.3652474284172058\n",
      "Validation: Epoch [13], Batch [334/938], Loss: 0.5164061188697815\n",
      "Validation: Epoch [13], Batch [335/938], Loss: 0.38012272119522095\n",
      "Validation: Epoch [13], Batch [336/938], Loss: 0.4626876711845398\n",
      "Validation: Epoch [13], Batch [337/938], Loss: 0.43818405270576477\n",
      "Validation: Epoch [13], Batch [338/938], Loss: 0.25110331177711487\n",
      "Validation: Epoch [13], Batch [339/938], Loss: 0.621895432472229\n",
      "Validation: Epoch [13], Batch [340/938], Loss: 0.3619398772716522\n",
      "Validation: Epoch [13], Batch [341/938], Loss: 0.40035778284072876\n",
      "Validation: Epoch [13], Batch [342/938], Loss: 0.5745608806610107\n",
      "Validation: Epoch [13], Batch [343/938], Loss: 0.4400988221168518\n",
      "Validation: Epoch [13], Batch [344/938], Loss: 0.5040927529335022\n",
      "Validation: Epoch [13], Batch [345/938], Loss: 0.53351229429245\n",
      "Validation: Epoch [13], Batch [346/938], Loss: 0.4613193869590759\n",
      "Validation: Epoch [13], Batch [347/938], Loss: 0.5983346700668335\n",
      "Validation: Epoch [13], Batch [348/938], Loss: 0.7348464727401733\n",
      "Validation: Epoch [13], Batch [349/938], Loss: 0.7364493012428284\n",
      "Validation: Epoch [13], Batch [350/938], Loss: 0.38455986976623535\n",
      "Validation: Epoch [13], Batch [351/938], Loss: 0.5189090967178345\n",
      "Validation: Epoch [13], Batch [352/938], Loss: 0.4302460253238678\n",
      "Validation: Epoch [13], Batch [353/938], Loss: 0.4525796175003052\n",
      "Validation: Epoch [13], Batch [354/938], Loss: 0.5613992810249329\n",
      "Validation: Epoch [13], Batch [355/938], Loss: 0.4760913550853729\n",
      "Validation: Epoch [13], Batch [356/938], Loss: 0.33175233006477356\n",
      "Validation: Epoch [13], Batch [357/938], Loss: 0.4662846326828003\n",
      "Validation: Epoch [13], Batch [358/938], Loss: 0.3485163450241089\n",
      "Validation: Epoch [13], Batch [359/938], Loss: 0.5465002059936523\n",
      "Validation: Epoch [13], Batch [360/938], Loss: 0.514980673789978\n",
      "Validation: Epoch [13], Batch [361/938], Loss: 0.4010564088821411\n",
      "Validation: Epoch [13], Batch [362/938], Loss: 0.47570642828941345\n",
      "Validation: Epoch [13], Batch [363/938], Loss: 0.552093505859375\n",
      "Validation: Epoch [13], Batch [364/938], Loss: 0.3936368227005005\n",
      "Validation: Epoch [13], Batch [365/938], Loss: 0.5748188495635986\n",
      "Validation: Epoch [13], Batch [366/938], Loss: 0.6302262544631958\n",
      "Validation: Epoch [13], Batch [367/938], Loss: 0.4236348271369934\n",
      "Validation: Epoch [13], Batch [368/938], Loss: 0.496167927980423\n",
      "Validation: Epoch [13], Batch [369/938], Loss: 0.5746103525161743\n",
      "Validation: Epoch [13], Batch [370/938], Loss: 0.5344305634498596\n",
      "Validation: Epoch [13], Batch [371/938], Loss: 0.39334040880203247\n",
      "Validation: Epoch [13], Batch [372/938], Loss: 0.2700824737548828\n",
      "Validation: Epoch [13], Batch [373/938], Loss: 0.6592418551445007\n",
      "Validation: Epoch [13], Batch [374/938], Loss: 0.3826487064361572\n",
      "Validation: Epoch [13], Batch [375/938], Loss: 0.6295520067214966\n",
      "Validation: Epoch [13], Batch [376/938], Loss: 0.41981232166290283\n",
      "Validation: Epoch [13], Batch [377/938], Loss: 0.39956530928611755\n",
      "Validation: Epoch [13], Batch [378/938], Loss: 0.3278016746044159\n",
      "Validation: Epoch [13], Batch [379/938], Loss: 0.3966781497001648\n",
      "Validation: Epoch [13], Batch [380/938], Loss: 0.43635329604148865\n",
      "Validation: Epoch [13], Batch [381/938], Loss: 0.45143747329711914\n",
      "Validation: Epoch [13], Batch [382/938], Loss: 0.4101763069629669\n",
      "Validation: Epoch [13], Batch [383/938], Loss: 0.35010021924972534\n",
      "Validation: Epoch [13], Batch [384/938], Loss: 0.3045938313007355\n",
      "Validation: Epoch [13], Batch [385/938], Loss: 0.6372431516647339\n",
      "Validation: Epoch [13], Batch [386/938], Loss: 0.49109378457069397\n",
      "Validation: Epoch [13], Batch [387/938], Loss: 0.4890851676464081\n",
      "Validation: Epoch [13], Batch [388/938], Loss: 0.5062655806541443\n",
      "Validation: Epoch [13], Batch [389/938], Loss: 0.5927585959434509\n",
      "Validation: Epoch [13], Batch [390/938], Loss: 0.4722291827201843\n",
      "Validation: Epoch [13], Batch [391/938], Loss: 0.5745474100112915\n",
      "Validation: Epoch [13], Batch [392/938], Loss: 0.5930968523025513\n",
      "Validation: Epoch [13], Batch [393/938], Loss: 0.4285832941532135\n",
      "Validation: Epoch [13], Batch [394/938], Loss: 0.4126252233982086\n",
      "Validation: Epoch [13], Batch [395/938], Loss: 0.333799809217453\n",
      "Validation: Epoch [13], Batch [396/938], Loss: 0.4039444625377655\n",
      "Validation: Epoch [13], Batch [397/938], Loss: 0.5157179236412048\n",
      "Validation: Epoch [13], Batch [398/938], Loss: 0.6413462162017822\n",
      "Validation: Epoch [13], Batch [399/938], Loss: 0.5622280240058899\n",
      "Validation: Epoch [13], Batch [400/938], Loss: 0.48050498962402344\n",
      "Validation: Epoch [13], Batch [401/938], Loss: 0.6023015975952148\n",
      "Validation: Epoch [13], Batch [402/938], Loss: 0.5287398099899292\n",
      "Validation: Epoch [13], Batch [403/938], Loss: 0.6209585666656494\n",
      "Validation: Epoch [13], Batch [404/938], Loss: 0.4671715199947357\n",
      "Validation: Epoch [13], Batch [405/938], Loss: 0.3798185884952545\n",
      "Validation: Epoch [13], Batch [406/938], Loss: 0.5416111350059509\n",
      "Validation: Epoch [13], Batch [407/938], Loss: 0.3500775694847107\n",
      "Validation: Epoch [13], Batch [408/938], Loss: 0.4294787645339966\n",
      "Validation: Epoch [13], Batch [409/938], Loss: 0.6744310259819031\n",
      "Validation: Epoch [13], Batch [410/938], Loss: 0.337566077709198\n",
      "Validation: Epoch [13], Batch [411/938], Loss: 0.4631698429584503\n",
      "Validation: Epoch [13], Batch [412/938], Loss: 0.5398808717727661\n",
      "Validation: Epoch [13], Batch [413/938], Loss: 0.3628772497177124\n",
      "Validation: Epoch [13], Batch [414/938], Loss: 0.44373106956481934\n",
      "Validation: Epoch [13], Batch [415/938], Loss: 0.5233724117279053\n",
      "Validation: Epoch [13], Batch [416/938], Loss: 0.6033419370651245\n",
      "Validation: Epoch [13], Batch [417/938], Loss: 0.40895718336105347\n",
      "Validation: Epoch [13], Batch [418/938], Loss: 0.3709292411804199\n",
      "Validation: Epoch [13], Batch [419/938], Loss: 0.40667372941970825\n",
      "Validation: Epoch [13], Batch [420/938], Loss: 0.5790668725967407\n",
      "Validation: Epoch [13], Batch [421/938], Loss: 0.5097060203552246\n",
      "Validation: Epoch [13], Batch [422/938], Loss: 0.4337915778160095\n",
      "Validation: Epoch [13], Batch [423/938], Loss: 0.37840360403060913\n",
      "Validation: Epoch [13], Batch [424/938], Loss: 0.3756330907344818\n",
      "Validation: Epoch [13], Batch [425/938], Loss: 0.48415037989616394\n",
      "Validation: Epoch [13], Batch [426/938], Loss: 0.5257596373558044\n",
      "Validation: Epoch [13], Batch [427/938], Loss: 0.48659324645996094\n",
      "Validation: Epoch [13], Batch [428/938], Loss: 0.4547637104988098\n",
      "Validation: Epoch [13], Batch [429/938], Loss: 0.3762408196926117\n",
      "Validation: Epoch [13], Batch [430/938], Loss: 0.3233703374862671\n",
      "Validation: Epoch [13], Batch [431/938], Loss: 0.5723209977149963\n",
      "Validation: Epoch [13], Batch [432/938], Loss: 0.4950505495071411\n",
      "Validation: Epoch [13], Batch [433/938], Loss: 0.46832144260406494\n",
      "Validation: Epoch [13], Batch [434/938], Loss: 0.4640228748321533\n",
      "Validation: Epoch [13], Batch [435/938], Loss: 0.38653695583343506\n",
      "Validation: Epoch [13], Batch [436/938], Loss: 0.5050280094146729\n",
      "Validation: Epoch [13], Batch [437/938], Loss: 0.38153985142707825\n",
      "Validation: Epoch [13], Batch [438/938], Loss: 0.3591616153717041\n",
      "Validation: Epoch [13], Batch [439/938], Loss: 0.6217548251152039\n",
      "Validation: Epoch [13], Batch [440/938], Loss: 0.3403777480125427\n",
      "Validation: Epoch [13], Batch [441/938], Loss: 0.5159781575202942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [442/938], Loss: 0.44522374868392944\n",
      "Validation: Epoch [13], Batch [443/938], Loss: 0.5540671944618225\n",
      "Validation: Epoch [13], Batch [444/938], Loss: 0.5537188649177551\n",
      "Validation: Epoch [13], Batch [445/938], Loss: 0.4291471838951111\n",
      "Validation: Epoch [13], Batch [446/938], Loss: 0.6134568452835083\n",
      "Validation: Epoch [13], Batch [447/938], Loss: 0.4787559509277344\n",
      "Validation: Epoch [13], Batch [448/938], Loss: 0.4075164794921875\n",
      "Validation: Epoch [13], Batch [449/938], Loss: 0.4241558015346527\n",
      "Validation: Epoch [13], Batch [450/938], Loss: 0.44568347930908203\n",
      "Validation: Epoch [13], Batch [451/938], Loss: 0.5431312322616577\n",
      "Validation: Epoch [13], Batch [452/938], Loss: 0.5676888227462769\n",
      "Validation: Epoch [13], Batch [453/938], Loss: 0.430655837059021\n",
      "Validation: Epoch [13], Batch [454/938], Loss: 0.454254150390625\n",
      "Validation: Epoch [13], Batch [455/938], Loss: 0.3555372357368469\n",
      "Validation: Epoch [13], Batch [456/938], Loss: 0.438855916261673\n",
      "Validation: Epoch [13], Batch [457/938], Loss: 0.35271820425987244\n",
      "Validation: Epoch [13], Batch [458/938], Loss: 0.44530951976776123\n",
      "Validation: Epoch [13], Batch [459/938], Loss: 0.49537748098373413\n",
      "Validation: Epoch [13], Batch [460/938], Loss: 0.4877418875694275\n",
      "Validation: Epoch [13], Batch [461/938], Loss: 0.5379225611686707\n",
      "Validation: Epoch [13], Batch [462/938], Loss: 0.575703501701355\n",
      "Validation: Epoch [13], Batch [463/938], Loss: 0.3875972330570221\n",
      "Validation: Epoch [13], Batch [464/938], Loss: 0.4950382709503174\n",
      "Validation: Epoch [13], Batch [465/938], Loss: 0.4937930703163147\n",
      "Validation: Epoch [13], Batch [466/938], Loss: 0.41950440406799316\n",
      "Validation: Epoch [13], Batch [467/938], Loss: 0.4942326247692108\n",
      "Validation: Epoch [13], Batch [468/938], Loss: 0.535068690776825\n",
      "Validation: Epoch [13], Batch [469/938], Loss: 0.5288960337638855\n",
      "Validation: Epoch [13], Batch [470/938], Loss: 0.4162905514240265\n",
      "Validation: Epoch [13], Batch [471/938], Loss: 0.4864422082901001\n",
      "Validation: Epoch [13], Batch [472/938], Loss: 0.41692182421684265\n",
      "Validation: Epoch [13], Batch [473/938], Loss: 0.36676448583602905\n",
      "Validation: Epoch [13], Batch [474/938], Loss: 0.5504517555236816\n",
      "Validation: Epoch [13], Batch [475/938], Loss: 0.38973820209503174\n",
      "Validation: Epoch [13], Batch [476/938], Loss: 0.498005747795105\n",
      "Validation: Epoch [13], Batch [477/938], Loss: 0.47601625323295593\n",
      "Validation: Epoch [13], Batch [478/938], Loss: 0.4508515000343323\n",
      "Validation: Epoch [13], Batch [479/938], Loss: 0.47907987236976624\n",
      "Validation: Epoch [13], Batch [480/938], Loss: 0.3711870312690735\n",
      "Validation: Epoch [13], Batch [481/938], Loss: 0.6030042171478271\n",
      "Validation: Epoch [13], Batch [482/938], Loss: 0.4152045249938965\n",
      "Validation: Epoch [13], Batch [483/938], Loss: 0.4696043133735657\n",
      "Validation: Epoch [13], Batch [484/938], Loss: 0.4981588125228882\n",
      "Validation: Epoch [13], Batch [485/938], Loss: 0.5474535226821899\n",
      "Validation: Epoch [13], Batch [486/938], Loss: 0.4015134572982788\n",
      "Validation: Epoch [13], Batch [487/938], Loss: 0.3605208396911621\n",
      "Validation: Epoch [13], Batch [488/938], Loss: 0.4488620162010193\n",
      "Validation: Epoch [13], Batch [489/938], Loss: 0.4179571270942688\n",
      "Validation: Epoch [13], Batch [490/938], Loss: 0.40431058406829834\n",
      "Validation: Epoch [13], Batch [491/938], Loss: 0.3916413187980652\n",
      "Validation: Epoch [13], Batch [492/938], Loss: 0.6186589598655701\n",
      "Validation: Epoch [13], Batch [493/938], Loss: 0.7014034390449524\n",
      "Validation: Epoch [13], Batch [494/938], Loss: 0.44385701417922974\n",
      "Validation: Epoch [13], Batch [495/938], Loss: 0.3876190185546875\n",
      "Validation: Epoch [13], Batch [496/938], Loss: 0.4291393458843231\n",
      "Validation: Epoch [13], Batch [497/938], Loss: 0.44995906949043274\n",
      "Validation: Epoch [13], Batch [498/938], Loss: 0.3903111219406128\n",
      "Validation: Epoch [13], Batch [499/938], Loss: 0.41401922702789307\n",
      "Validation: Epoch [13], Batch [500/938], Loss: 0.3985055387020111\n",
      "Validation: Epoch [13], Batch [501/938], Loss: 0.5211969614028931\n",
      "Validation: Epoch [13], Batch [502/938], Loss: 0.48900288343429565\n",
      "Validation: Epoch [13], Batch [503/938], Loss: 0.5371216535568237\n",
      "Validation: Epoch [13], Batch [504/938], Loss: 0.46677571535110474\n",
      "Validation: Epoch [13], Batch [505/938], Loss: 0.3098534941673279\n",
      "Validation: Epoch [13], Batch [506/938], Loss: 0.5358708500862122\n",
      "Validation: Epoch [13], Batch [507/938], Loss: 0.4836072027683258\n",
      "Validation: Epoch [13], Batch [508/938], Loss: 0.317396879196167\n",
      "Validation: Epoch [13], Batch [509/938], Loss: 0.2522308826446533\n",
      "Validation: Epoch [13], Batch [510/938], Loss: 0.5309686064720154\n",
      "Validation: Epoch [13], Batch [511/938], Loss: 0.5342863202095032\n",
      "Validation: Epoch [13], Batch [512/938], Loss: 0.3277134895324707\n",
      "Validation: Epoch [13], Batch [513/938], Loss: 0.37488412857055664\n",
      "Validation: Epoch [13], Batch [514/938], Loss: 0.49830979108810425\n",
      "Validation: Epoch [13], Batch [515/938], Loss: 0.46079006791114807\n",
      "Validation: Epoch [13], Batch [516/938], Loss: 0.47999948263168335\n",
      "Validation: Epoch [13], Batch [517/938], Loss: 0.49980098009109497\n",
      "Validation: Epoch [13], Batch [518/938], Loss: 0.3504638671875\n",
      "Validation: Epoch [13], Batch [519/938], Loss: 0.41596218943595886\n",
      "Validation: Epoch [13], Batch [520/938], Loss: 0.5609593391418457\n",
      "Validation: Epoch [13], Batch [521/938], Loss: 0.436675488948822\n",
      "Validation: Epoch [13], Batch [522/938], Loss: 0.4975188672542572\n",
      "Validation: Epoch [13], Batch [523/938], Loss: 0.6101329326629639\n",
      "Validation: Epoch [13], Batch [524/938], Loss: 0.4244000315666199\n",
      "Validation: Epoch [13], Batch [525/938], Loss: 0.6048122644424438\n",
      "Validation: Epoch [13], Batch [526/938], Loss: 0.5217140316963196\n",
      "Validation: Epoch [13], Batch [527/938], Loss: 0.6289005875587463\n",
      "Validation: Epoch [13], Batch [528/938], Loss: 0.38358935713768005\n",
      "Validation: Epoch [13], Batch [529/938], Loss: 0.36936795711517334\n",
      "Validation: Epoch [13], Batch [530/938], Loss: 0.6028823256492615\n",
      "Validation: Epoch [13], Batch [531/938], Loss: 0.4865747392177582\n",
      "Validation: Epoch [13], Batch [532/938], Loss: 0.5314816236495972\n",
      "Validation: Epoch [13], Batch [533/938], Loss: 0.4448493421077728\n",
      "Validation: Epoch [13], Batch [534/938], Loss: 0.48768773674964905\n",
      "Validation: Epoch [13], Batch [535/938], Loss: 0.5926700830459595\n",
      "Validation: Epoch [13], Batch [536/938], Loss: 0.5569074749946594\n",
      "Validation: Epoch [13], Batch [537/938], Loss: 0.5736072063446045\n",
      "Validation: Epoch [13], Batch [538/938], Loss: 0.3881983160972595\n",
      "Validation: Epoch [13], Batch [539/938], Loss: 0.4590544104576111\n",
      "Validation: Epoch [13], Batch [540/938], Loss: 0.48638683557510376\n",
      "Validation: Epoch [13], Batch [541/938], Loss: 0.4438687562942505\n",
      "Validation: Epoch [13], Batch [542/938], Loss: 0.5011447072029114\n",
      "Validation: Epoch [13], Batch [543/938], Loss: 0.5345235466957092\n",
      "Validation: Epoch [13], Batch [544/938], Loss: 0.22604739665985107\n",
      "Validation: Epoch [13], Batch [545/938], Loss: 0.5835381150245667\n",
      "Validation: Epoch [13], Batch [546/938], Loss: 0.5290589332580566\n",
      "Validation: Epoch [13], Batch [547/938], Loss: 0.5583703517913818\n",
      "Validation: Epoch [13], Batch [548/938], Loss: 0.39069151878356934\n",
      "Validation: Epoch [13], Batch [549/938], Loss: 0.4455083906650543\n",
      "Validation: Epoch [13], Batch [550/938], Loss: 0.5724778175354004\n",
      "Validation: Epoch [13], Batch [551/938], Loss: 0.5285979509353638\n",
      "Validation: Epoch [13], Batch [552/938], Loss: 0.9466185569763184\n",
      "Validation: Epoch [13], Batch [553/938], Loss: 0.5507231950759888\n",
      "Validation: Epoch [13], Batch [554/938], Loss: 0.5818096399307251\n",
      "Validation: Epoch [13], Batch [555/938], Loss: 0.5132588148117065\n",
      "Validation: Epoch [13], Batch [556/938], Loss: 0.5166394114494324\n",
      "Validation: Epoch [13], Batch [557/938], Loss: 0.39219769835472107\n",
      "Validation: Epoch [13], Batch [558/938], Loss: 0.4419674277305603\n",
      "Validation: Epoch [13], Batch [559/938], Loss: 0.7225852012634277\n",
      "Validation: Epoch [13], Batch [560/938], Loss: 0.7147011756896973\n",
      "Validation: Epoch [13], Batch [561/938], Loss: 0.41690802574157715\n",
      "Validation: Epoch [13], Batch [562/938], Loss: 0.4759259819984436\n",
      "Validation: Epoch [13], Batch [563/938], Loss: 0.5830855369567871\n",
      "Validation: Epoch [13], Batch [564/938], Loss: 0.43003609776496887\n",
      "Validation: Epoch [13], Batch [565/938], Loss: 0.5444254875183105\n",
      "Validation: Epoch [13], Batch [566/938], Loss: 0.3885113000869751\n",
      "Validation: Epoch [13], Batch [567/938], Loss: 0.5109062790870667\n",
      "Validation: Epoch [13], Batch [568/938], Loss: 0.4362402558326721\n",
      "Validation: Epoch [13], Batch [569/938], Loss: 0.5911087989807129\n",
      "Validation: Epoch [13], Batch [570/938], Loss: 0.5135515332221985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [571/938], Loss: 0.4455161392688751\n",
      "Validation: Epoch [13], Batch [572/938], Loss: 0.40318697690963745\n",
      "Validation: Epoch [13], Batch [573/938], Loss: 0.4137342572212219\n",
      "Validation: Epoch [13], Batch [574/938], Loss: 0.3481912612915039\n",
      "Validation: Epoch [13], Batch [575/938], Loss: 0.43297433853149414\n",
      "Validation: Epoch [13], Batch [576/938], Loss: 0.5485354661941528\n",
      "Validation: Epoch [13], Batch [577/938], Loss: 0.498834490776062\n",
      "Validation: Epoch [13], Batch [578/938], Loss: 0.48037418723106384\n",
      "Validation: Epoch [13], Batch [579/938], Loss: 0.46364471316337585\n",
      "Validation: Epoch [13], Batch [580/938], Loss: 0.4906514585018158\n",
      "Validation: Epoch [13], Batch [581/938], Loss: 0.4294602572917938\n",
      "Validation: Epoch [13], Batch [582/938], Loss: 0.579998791217804\n",
      "Validation: Epoch [13], Batch [583/938], Loss: 0.42719578742980957\n",
      "Validation: Epoch [13], Batch [584/938], Loss: 0.5065701007843018\n",
      "Validation: Epoch [13], Batch [585/938], Loss: 0.3607732057571411\n",
      "Validation: Epoch [13], Batch [586/938], Loss: 0.5236006379127502\n",
      "Validation: Epoch [13], Batch [587/938], Loss: 0.4108037054538727\n",
      "Validation: Epoch [13], Batch [588/938], Loss: 0.47413328289985657\n",
      "Validation: Epoch [13], Batch [589/938], Loss: 0.5638099908828735\n",
      "Validation: Epoch [13], Batch [590/938], Loss: 0.5362740755081177\n",
      "Validation: Epoch [13], Batch [591/938], Loss: 0.5968848466873169\n",
      "Validation: Epoch [13], Batch [592/938], Loss: 0.44716188311576843\n",
      "Validation: Epoch [13], Batch [593/938], Loss: 0.6366519331932068\n",
      "Validation: Epoch [13], Batch [594/938], Loss: 0.45036667585372925\n",
      "Validation: Epoch [13], Batch [595/938], Loss: 0.4579976499080658\n",
      "Validation: Epoch [13], Batch [596/938], Loss: 0.390068918466568\n",
      "Validation: Epoch [13], Batch [597/938], Loss: 0.3574737310409546\n",
      "Validation: Epoch [13], Batch [598/938], Loss: 0.3796539604663849\n",
      "Validation: Epoch [13], Batch [599/938], Loss: 0.401775985956192\n",
      "Validation: Epoch [13], Batch [600/938], Loss: 0.46058574318885803\n",
      "Validation: Epoch [13], Batch [601/938], Loss: 0.3763004541397095\n",
      "Validation: Epoch [13], Batch [602/938], Loss: 0.49591130018234253\n",
      "Validation: Epoch [13], Batch [603/938], Loss: 0.48094266653060913\n",
      "Validation: Epoch [13], Batch [604/938], Loss: 0.4322076737880707\n",
      "Validation: Epoch [13], Batch [605/938], Loss: 0.33738189935684204\n",
      "Validation: Epoch [13], Batch [606/938], Loss: 0.4471542239189148\n",
      "Validation: Epoch [13], Batch [607/938], Loss: 0.4032038748264313\n",
      "Validation: Epoch [13], Batch [608/938], Loss: 0.40187233686447144\n",
      "Validation: Epoch [13], Batch [609/938], Loss: 0.548513650894165\n",
      "Validation: Epoch [13], Batch [610/938], Loss: 0.7786669731140137\n",
      "Validation: Epoch [13], Batch [611/938], Loss: 0.5190920829772949\n",
      "Validation: Epoch [13], Batch [612/938], Loss: 0.4664546847343445\n",
      "Validation: Epoch [13], Batch [613/938], Loss: 0.5037730932235718\n",
      "Validation: Epoch [13], Batch [614/938], Loss: 0.5077497959136963\n",
      "Validation: Epoch [13], Batch [615/938], Loss: 0.46390360593795776\n",
      "Validation: Epoch [13], Batch [616/938], Loss: 0.46874862909317017\n",
      "Validation: Epoch [13], Batch [617/938], Loss: 0.3079270124435425\n",
      "Validation: Epoch [13], Batch [618/938], Loss: 0.47462552785873413\n",
      "Validation: Epoch [13], Batch [619/938], Loss: 0.5366445779800415\n",
      "Validation: Epoch [13], Batch [620/938], Loss: 0.5307202935218811\n",
      "Validation: Epoch [13], Batch [621/938], Loss: 0.5582480430603027\n",
      "Validation: Epoch [13], Batch [622/938], Loss: 0.3891940414905548\n",
      "Validation: Epoch [13], Batch [623/938], Loss: 0.4407486617565155\n",
      "Validation: Epoch [13], Batch [624/938], Loss: 0.4442073106765747\n",
      "Validation: Epoch [13], Batch [625/938], Loss: 0.4110069274902344\n",
      "Validation: Epoch [13], Batch [626/938], Loss: 0.442038357257843\n",
      "Validation: Epoch [13], Batch [627/938], Loss: 0.4243723750114441\n",
      "Validation: Epoch [13], Batch [628/938], Loss: 0.40631037950515747\n",
      "Validation: Epoch [13], Batch [629/938], Loss: 0.520626425743103\n",
      "Validation: Epoch [13], Batch [630/938], Loss: 0.4372745752334595\n",
      "Validation: Epoch [13], Batch [631/938], Loss: 0.473276287317276\n",
      "Validation: Epoch [13], Batch [632/938], Loss: 0.6103311777114868\n",
      "Validation: Epoch [13], Batch [633/938], Loss: 0.2839215397834778\n",
      "Validation: Epoch [13], Batch [634/938], Loss: 0.5563840866088867\n",
      "Validation: Epoch [13], Batch [635/938], Loss: 0.46168655157089233\n",
      "Validation: Epoch [13], Batch [636/938], Loss: 0.5518603324890137\n",
      "Validation: Epoch [13], Batch [637/938], Loss: 0.6307138204574585\n",
      "Validation: Epoch [13], Batch [638/938], Loss: 0.28523993492126465\n",
      "Validation: Epoch [13], Batch [639/938], Loss: 0.6117544174194336\n",
      "Validation: Epoch [13], Batch [640/938], Loss: 0.5138863325119019\n",
      "Validation: Epoch [13], Batch [641/938], Loss: 0.5777420401573181\n",
      "Validation: Epoch [13], Batch [642/938], Loss: 0.4076306223869324\n",
      "Validation: Epoch [13], Batch [643/938], Loss: 0.30868661403656006\n",
      "Validation: Epoch [13], Batch [644/938], Loss: 0.5000914931297302\n",
      "Validation: Epoch [13], Batch [645/938], Loss: 0.5275335907936096\n",
      "Validation: Epoch [13], Batch [646/938], Loss: 0.4781460464000702\n",
      "Validation: Epoch [13], Batch [647/938], Loss: 0.5368478298187256\n",
      "Validation: Epoch [13], Batch [648/938], Loss: 0.3650583028793335\n",
      "Validation: Epoch [13], Batch [649/938], Loss: 0.41682466864585876\n",
      "Validation: Epoch [13], Batch [650/938], Loss: 0.3043334484100342\n",
      "Validation: Epoch [13], Batch [651/938], Loss: 0.41638606786727905\n",
      "Validation: Epoch [13], Batch [652/938], Loss: 0.5173208117485046\n",
      "Validation: Epoch [13], Batch [653/938], Loss: 0.7408958673477173\n",
      "Validation: Epoch [13], Batch [654/938], Loss: 0.45021867752075195\n",
      "Validation: Epoch [13], Batch [655/938], Loss: 0.6408450603485107\n",
      "Validation: Epoch [13], Batch [656/938], Loss: 0.36989516019821167\n",
      "Validation: Epoch [13], Batch [657/938], Loss: 0.413702130317688\n",
      "Validation: Epoch [13], Batch [658/938], Loss: 0.5139696598052979\n",
      "Validation: Epoch [13], Batch [659/938], Loss: 0.5424398183822632\n",
      "Validation: Epoch [13], Batch [660/938], Loss: 0.4682959020137787\n",
      "Validation: Epoch [13], Batch [661/938], Loss: 0.43588995933532715\n",
      "Validation: Epoch [13], Batch [662/938], Loss: 0.5784162878990173\n",
      "Validation: Epoch [13], Batch [663/938], Loss: 0.34688109159469604\n",
      "Validation: Epoch [13], Batch [664/938], Loss: 0.4279569387435913\n",
      "Validation: Epoch [13], Batch [665/938], Loss: 0.4511009454727173\n",
      "Validation: Epoch [13], Batch [666/938], Loss: 0.5674453973770142\n",
      "Validation: Epoch [13], Batch [667/938], Loss: 0.29395055770874023\n",
      "Validation: Epoch [13], Batch [668/938], Loss: 0.4325431287288666\n",
      "Validation: Epoch [13], Batch [669/938], Loss: 0.5157870650291443\n",
      "Validation: Epoch [13], Batch [670/938], Loss: 0.5683457851409912\n",
      "Validation: Epoch [13], Batch [671/938], Loss: 0.6816961169242859\n",
      "Validation: Epoch [13], Batch [672/938], Loss: 0.530872106552124\n",
      "Validation: Epoch [13], Batch [673/938], Loss: 0.43437057733535767\n",
      "Validation: Epoch [13], Batch [674/938], Loss: 0.520577073097229\n",
      "Validation: Epoch [13], Batch [675/938], Loss: 0.47775691747665405\n",
      "Validation: Epoch [13], Batch [676/938], Loss: 0.385853111743927\n",
      "Validation: Epoch [13], Batch [677/938], Loss: 0.42282748222351074\n",
      "Validation: Epoch [13], Batch [678/938], Loss: 0.31821176409721375\n",
      "Validation: Epoch [13], Batch [679/938], Loss: 0.6195787191390991\n",
      "Validation: Epoch [13], Batch [680/938], Loss: 0.43594634532928467\n",
      "Validation: Epoch [13], Batch [681/938], Loss: 0.7544570565223694\n",
      "Validation: Epoch [13], Batch [682/938], Loss: 0.5550494194030762\n",
      "Validation: Epoch [13], Batch [683/938], Loss: 0.5777392387390137\n",
      "Validation: Epoch [13], Batch [684/938], Loss: 0.46262457966804504\n",
      "Validation: Epoch [13], Batch [685/938], Loss: 0.4099279046058655\n",
      "Validation: Epoch [13], Batch [686/938], Loss: 0.43028688430786133\n",
      "Validation: Epoch [13], Batch [687/938], Loss: 0.4021289348602295\n",
      "Validation: Epoch [13], Batch [688/938], Loss: 0.44459399580955505\n",
      "Validation: Epoch [13], Batch [689/938], Loss: 0.5352736115455627\n",
      "Validation: Epoch [13], Batch [690/938], Loss: 0.6754721403121948\n",
      "Validation: Epoch [13], Batch [691/938], Loss: 0.3184616267681122\n",
      "Validation: Epoch [13], Batch [692/938], Loss: 0.4870379865169525\n",
      "Validation: Epoch [13], Batch [693/938], Loss: 0.4762791395187378\n",
      "Validation: Epoch [13], Batch [694/938], Loss: 0.4036325216293335\n",
      "Validation: Epoch [13], Batch [695/938], Loss: 0.44185519218444824\n",
      "Validation: Epoch [13], Batch [696/938], Loss: 0.453117698431015\n",
      "Validation: Epoch [13], Batch [697/938], Loss: 0.4687940776348114\n",
      "Validation: Epoch [13], Batch [698/938], Loss: 0.39718863368034363\n",
      "Validation: Epoch [13], Batch [699/938], Loss: 0.556861162185669\n",
      "Validation: Epoch [13], Batch [700/938], Loss: 0.6096659898757935\n",
      "Validation: Epoch [13], Batch [701/938], Loss: 0.28469374775886536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [702/938], Loss: 0.3207203447818756\n",
      "Validation: Epoch [13], Batch [703/938], Loss: 0.3917461037635803\n",
      "Validation: Epoch [13], Batch [704/938], Loss: 0.2637806534767151\n",
      "Validation: Epoch [13], Batch [705/938], Loss: 0.581171989440918\n",
      "Validation: Epoch [13], Batch [706/938], Loss: 0.5494390726089478\n",
      "Validation: Epoch [13], Batch [707/938], Loss: 0.5301213264465332\n",
      "Validation: Epoch [13], Batch [708/938], Loss: 0.41902050375938416\n",
      "Validation: Epoch [13], Batch [709/938], Loss: 0.6424530148506165\n",
      "Validation: Epoch [13], Batch [710/938], Loss: 0.5780138373374939\n",
      "Validation: Epoch [13], Batch [711/938], Loss: 0.4734076261520386\n",
      "Validation: Epoch [13], Batch [712/938], Loss: 0.5298159122467041\n",
      "Validation: Epoch [13], Batch [713/938], Loss: 0.5729238986968994\n",
      "Validation: Epoch [13], Batch [714/938], Loss: 0.6283360719680786\n",
      "Validation: Epoch [13], Batch [715/938], Loss: 0.43338683247566223\n",
      "Validation: Epoch [13], Batch [716/938], Loss: 0.3768863081932068\n",
      "Validation: Epoch [13], Batch [717/938], Loss: 0.5451847314834595\n",
      "Validation: Epoch [13], Batch [718/938], Loss: 0.6078732013702393\n",
      "Validation: Epoch [13], Batch [719/938], Loss: 0.39521148800849915\n",
      "Validation: Epoch [13], Batch [720/938], Loss: 0.47928714752197266\n",
      "Validation: Epoch [13], Batch [721/938], Loss: 0.5817397832870483\n",
      "Validation: Epoch [13], Batch [722/938], Loss: 0.6131333112716675\n",
      "Validation: Epoch [13], Batch [723/938], Loss: 0.7063325047492981\n",
      "Validation: Epoch [13], Batch [724/938], Loss: 0.35580456256866455\n",
      "Validation: Epoch [13], Batch [725/938], Loss: 0.35984688997268677\n",
      "Validation: Epoch [13], Batch [726/938], Loss: 0.4128689169883728\n",
      "Validation: Epoch [13], Batch [727/938], Loss: 0.3907020092010498\n",
      "Validation: Epoch [13], Batch [728/938], Loss: 0.48768317699432373\n",
      "Validation: Epoch [13], Batch [729/938], Loss: 0.4856530725955963\n",
      "Validation: Epoch [13], Batch [730/938], Loss: 0.24385833740234375\n",
      "Validation: Epoch [13], Batch [731/938], Loss: 0.3840655982494354\n",
      "Validation: Epoch [13], Batch [732/938], Loss: 0.6470571756362915\n",
      "Validation: Epoch [13], Batch [733/938], Loss: 0.43770650029182434\n",
      "Validation: Epoch [13], Batch [734/938], Loss: 0.5284940004348755\n",
      "Validation: Epoch [13], Batch [735/938], Loss: 0.42104625701904297\n",
      "Validation: Epoch [13], Batch [736/938], Loss: 0.6180282831192017\n",
      "Validation: Epoch [13], Batch [737/938], Loss: 0.43903374671936035\n",
      "Validation: Epoch [13], Batch [738/938], Loss: 0.45816344022750854\n",
      "Validation: Epoch [13], Batch [739/938], Loss: 0.5218024253845215\n",
      "Validation: Epoch [13], Batch [740/938], Loss: 0.5523114800453186\n",
      "Validation: Epoch [13], Batch [741/938], Loss: 0.48670661449432373\n",
      "Validation: Epoch [13], Batch [742/938], Loss: 0.37071144580841064\n",
      "Validation: Epoch [13], Batch [743/938], Loss: 0.5354276299476624\n",
      "Validation: Epoch [13], Batch [744/938], Loss: 0.43433037400245667\n",
      "Validation: Epoch [13], Batch [745/938], Loss: 0.5851383805274963\n",
      "Validation: Epoch [13], Batch [746/938], Loss: 0.4075220227241516\n",
      "Validation: Epoch [13], Batch [747/938], Loss: 0.46802186965942383\n",
      "Validation: Epoch [13], Batch [748/938], Loss: 0.4047814607620239\n",
      "Validation: Epoch [13], Batch [749/938], Loss: 0.5888763666152954\n",
      "Validation: Epoch [13], Batch [750/938], Loss: 0.6338235139846802\n",
      "Validation: Epoch [13], Batch [751/938], Loss: 0.49116992950439453\n",
      "Validation: Epoch [13], Batch [752/938], Loss: 0.38548052310943604\n",
      "Validation: Epoch [13], Batch [753/938], Loss: 0.3770397901535034\n",
      "Validation: Epoch [13], Batch [754/938], Loss: 0.32817643880844116\n",
      "Validation: Epoch [13], Batch [755/938], Loss: 0.47836539149284363\n",
      "Validation: Epoch [13], Batch [756/938], Loss: 0.446591854095459\n",
      "Validation: Epoch [13], Batch [757/938], Loss: 0.42522209882736206\n",
      "Validation: Epoch [13], Batch [758/938], Loss: 0.47889718413352966\n",
      "Validation: Epoch [13], Batch [759/938], Loss: 0.41101109981536865\n",
      "Validation: Epoch [13], Batch [760/938], Loss: 0.44949740171432495\n",
      "Validation: Epoch [13], Batch [761/938], Loss: 0.3093009889125824\n",
      "Validation: Epoch [13], Batch [762/938], Loss: 0.3643922209739685\n",
      "Validation: Epoch [13], Batch [763/938], Loss: 0.47960737347602844\n",
      "Validation: Epoch [13], Batch [764/938], Loss: 0.6732465028762817\n",
      "Validation: Epoch [13], Batch [765/938], Loss: 0.3670327067375183\n",
      "Validation: Epoch [13], Batch [766/938], Loss: 0.4012147784233093\n",
      "Validation: Epoch [13], Batch [767/938], Loss: 0.7093889713287354\n",
      "Validation: Epoch [13], Batch [768/938], Loss: 0.4300185441970825\n",
      "Validation: Epoch [13], Batch [769/938], Loss: 0.5175076723098755\n",
      "Validation: Epoch [13], Batch [770/938], Loss: 0.4346678853034973\n",
      "Validation: Epoch [13], Batch [771/938], Loss: 0.4087178707122803\n",
      "Validation: Epoch [13], Batch [772/938], Loss: 0.4266379475593567\n",
      "Validation: Epoch [13], Batch [773/938], Loss: 0.5308512449264526\n",
      "Validation: Epoch [13], Batch [774/938], Loss: 0.4674490690231323\n",
      "Validation: Epoch [13], Batch [775/938], Loss: 0.5342768430709839\n",
      "Validation: Epoch [13], Batch [776/938], Loss: 0.5452565550804138\n",
      "Validation: Epoch [13], Batch [777/938], Loss: 0.3892337381839752\n",
      "Validation: Epoch [13], Batch [778/938], Loss: 0.6090905666351318\n",
      "Validation: Epoch [13], Batch [779/938], Loss: 0.4737831652164459\n",
      "Validation: Epoch [13], Batch [780/938], Loss: 0.32910603284835815\n",
      "Validation: Epoch [13], Batch [781/938], Loss: 0.3399384319782257\n",
      "Validation: Epoch [13], Batch [782/938], Loss: 0.37192851305007935\n",
      "Validation: Epoch [13], Batch [783/938], Loss: 0.4583051800727844\n",
      "Validation: Epoch [13], Batch [784/938], Loss: 0.47616368532180786\n",
      "Validation: Epoch [13], Batch [785/938], Loss: 0.7272202968597412\n",
      "Validation: Epoch [13], Batch [786/938], Loss: 0.5197701454162598\n",
      "Validation: Epoch [13], Batch [787/938], Loss: 0.4757267236709595\n",
      "Validation: Epoch [13], Batch [788/938], Loss: 0.43001192808151245\n",
      "Validation: Epoch [13], Batch [789/938], Loss: 0.28701165318489075\n",
      "Validation: Epoch [13], Batch [790/938], Loss: 0.5711839199066162\n",
      "Validation: Epoch [13], Batch [791/938], Loss: 0.3061087429523468\n",
      "Validation: Epoch [13], Batch [792/938], Loss: 0.43687769770622253\n",
      "Validation: Epoch [13], Batch [793/938], Loss: 0.4095000922679901\n",
      "Validation: Epoch [13], Batch [794/938], Loss: 0.4194430112838745\n",
      "Validation: Epoch [13], Batch [795/938], Loss: 0.7013496160507202\n",
      "Validation: Epoch [13], Batch [796/938], Loss: 0.5234857797622681\n",
      "Validation: Epoch [13], Batch [797/938], Loss: 0.5047163963317871\n",
      "Validation: Epoch [13], Batch [798/938], Loss: 0.4683552384376526\n",
      "Validation: Epoch [13], Batch [799/938], Loss: 0.425781786441803\n",
      "Validation: Epoch [13], Batch [800/938], Loss: 0.3562047481536865\n",
      "Validation: Epoch [13], Batch [801/938], Loss: 0.3564775288105011\n",
      "Validation: Epoch [13], Batch [802/938], Loss: 0.6052428483963013\n",
      "Validation: Epoch [13], Batch [803/938], Loss: 0.604072630405426\n",
      "Validation: Epoch [13], Batch [804/938], Loss: 0.29496878385543823\n",
      "Validation: Epoch [13], Batch [805/938], Loss: 0.40481871366500854\n",
      "Validation: Epoch [13], Batch [806/938], Loss: 0.29759061336517334\n",
      "Validation: Epoch [13], Batch [807/938], Loss: 0.42196446657180786\n",
      "Validation: Epoch [13], Batch [808/938], Loss: 0.6511203050613403\n",
      "Validation: Epoch [13], Batch [809/938], Loss: 0.31884127855300903\n",
      "Validation: Epoch [13], Batch [810/938], Loss: 0.5223326683044434\n",
      "Validation: Epoch [13], Batch [811/938], Loss: 0.4627537131309509\n",
      "Validation: Epoch [13], Batch [812/938], Loss: 0.8057411313056946\n",
      "Validation: Epoch [13], Batch [813/938], Loss: 0.3907790780067444\n",
      "Validation: Epoch [13], Batch [814/938], Loss: 0.5060146450996399\n",
      "Validation: Epoch [13], Batch [815/938], Loss: 0.4594402313232422\n",
      "Validation: Epoch [13], Batch [816/938], Loss: 0.4369381070137024\n",
      "Validation: Epoch [13], Batch [817/938], Loss: 0.3610653281211853\n",
      "Validation: Epoch [13], Batch [818/938], Loss: 0.5361839532852173\n",
      "Validation: Epoch [13], Batch [819/938], Loss: 0.5377719402313232\n",
      "Validation: Epoch [13], Batch [820/938], Loss: 0.47951000928878784\n",
      "Validation: Epoch [13], Batch [821/938], Loss: 0.49310946464538574\n",
      "Validation: Epoch [13], Batch [822/938], Loss: 0.5141770839691162\n",
      "Validation: Epoch [13], Batch [823/938], Loss: 0.3271236717700958\n",
      "Validation: Epoch [13], Batch [824/938], Loss: 0.36912429332733154\n",
      "Validation: Epoch [13], Batch [825/938], Loss: 0.6636857986450195\n",
      "Validation: Epoch [13], Batch [826/938], Loss: 0.5914598703384399\n",
      "Validation: Epoch [13], Batch [827/938], Loss: 0.49704253673553467\n",
      "Validation: Epoch [13], Batch [828/938], Loss: 0.6707617044448853\n",
      "Validation: Epoch [13], Batch [829/938], Loss: 0.3280085325241089\n",
      "Validation: Epoch [13], Batch [830/938], Loss: 0.6798550486564636\n",
      "Validation: Epoch [13], Batch [831/938], Loss: 0.3761281967163086\n",
      "Validation: Epoch [13], Batch [832/938], Loss: 0.48897773027420044\n",
      "Validation: Epoch [13], Batch [833/938], Loss: 0.5391519665718079\n",
      "Validation: Epoch [13], Batch [834/938], Loss: 0.4716944992542267\n",
      "Validation: Epoch [13], Batch [835/938], Loss: 0.35708457231521606\n",
      "Validation: Epoch [13], Batch [836/938], Loss: 0.6121777296066284\n",
      "Validation: Epoch [13], Batch [837/938], Loss: 0.6036276817321777\n",
      "Validation: Epoch [13], Batch [838/938], Loss: 0.6198656558990479\n",
      "Validation: Epoch [13], Batch [839/938], Loss: 0.5983371734619141\n",
      "Validation: Epoch [13], Batch [840/938], Loss: 0.508002519607544\n",
      "Validation: Epoch [13], Batch [841/938], Loss: 0.4680212736129761\n",
      "Validation: Epoch [13], Batch [842/938], Loss: 0.5281486511230469\n",
      "Validation: Epoch [13], Batch [843/938], Loss: 0.3895234763622284\n",
      "Validation: Epoch [13], Batch [844/938], Loss: 0.503776490688324\n",
      "Validation: Epoch [13], Batch [845/938], Loss: 0.42447227239608765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [13], Batch [846/938], Loss: 0.3231690526008606\n",
      "Validation: Epoch [13], Batch [847/938], Loss: 0.4367942214012146\n",
      "Validation: Epoch [13], Batch [848/938], Loss: 0.4216618537902832\n",
      "Validation: Epoch [13], Batch [849/938], Loss: 0.3575237989425659\n",
      "Validation: Epoch [13], Batch [850/938], Loss: 0.6423435211181641\n",
      "Validation: Epoch [13], Batch [851/938], Loss: 0.4722473621368408\n",
      "Validation: Epoch [13], Batch [852/938], Loss: 0.4567958116531372\n",
      "Validation: Epoch [13], Batch [853/938], Loss: 0.4898945391178131\n",
      "Validation: Epoch [13], Batch [854/938], Loss: 0.5465025901794434\n",
      "Validation: Epoch [13], Batch [855/938], Loss: 0.37236571311950684\n",
      "Validation: Epoch [13], Batch [856/938], Loss: 0.43609100580215454\n",
      "Validation: Epoch [13], Batch [857/938], Loss: 0.5655184984207153\n",
      "Validation: Epoch [13], Batch [858/938], Loss: 0.3946027457714081\n",
      "Validation: Epoch [13], Batch [859/938], Loss: 0.48685386776924133\n",
      "Validation: Epoch [13], Batch [860/938], Loss: 0.5812429189682007\n",
      "Validation: Epoch [13], Batch [861/938], Loss: 0.46283528208732605\n",
      "Validation: Epoch [13], Batch [862/938], Loss: 0.2546180784702301\n",
      "Validation: Epoch [13], Batch [863/938], Loss: 0.47985950112342834\n",
      "Validation: Epoch [13], Batch [864/938], Loss: 0.5474916696548462\n",
      "Validation: Epoch [13], Batch [865/938], Loss: 0.6687941551208496\n",
      "Validation: Epoch [13], Batch [866/938], Loss: 0.41087448596954346\n",
      "Validation: Epoch [13], Batch [867/938], Loss: 0.46764349937438965\n",
      "Validation: Epoch [13], Batch [868/938], Loss: 0.47969162464141846\n",
      "Validation: Epoch [13], Batch [869/938], Loss: 0.41530942916870117\n",
      "Validation: Epoch [13], Batch [870/938], Loss: 0.4894299805164337\n",
      "Validation: Epoch [13], Batch [871/938], Loss: 0.3805985450744629\n",
      "Validation: Epoch [13], Batch [872/938], Loss: 0.5532795190811157\n",
      "Validation: Epoch [13], Batch [873/938], Loss: 0.5255532264709473\n",
      "Validation: Epoch [13], Batch [874/938], Loss: 0.40393275022506714\n",
      "Validation: Epoch [13], Batch [875/938], Loss: 0.32690343260765076\n",
      "Validation: Epoch [13], Batch [876/938], Loss: 0.5399723052978516\n",
      "Validation: Epoch [13], Batch [877/938], Loss: 0.41020089387893677\n",
      "Validation: Epoch [13], Batch [878/938], Loss: 0.41350024938583374\n",
      "Validation: Epoch [13], Batch [879/938], Loss: 0.3610714077949524\n",
      "Validation: Epoch [13], Batch [880/938], Loss: 0.4730670750141144\n",
      "Validation: Epoch [13], Batch [881/938], Loss: 0.42645901441574097\n",
      "Validation: Epoch [13], Batch [882/938], Loss: 0.4554947018623352\n",
      "Validation: Epoch [13], Batch [883/938], Loss: 0.41621899604797363\n",
      "Validation: Epoch [13], Batch [884/938], Loss: 0.3876999318599701\n",
      "Validation: Epoch [13], Batch [885/938], Loss: 0.3987955152988434\n",
      "Validation: Epoch [13], Batch [886/938], Loss: 0.3899487853050232\n",
      "Validation: Epoch [13], Batch [887/938], Loss: 0.4520633816719055\n",
      "Validation: Epoch [13], Batch [888/938], Loss: 0.482087105512619\n",
      "Validation: Epoch [13], Batch [889/938], Loss: 0.540083646774292\n",
      "Validation: Epoch [13], Batch [890/938], Loss: 0.46732476353645325\n",
      "Validation: Epoch [13], Batch [891/938], Loss: 0.6140062808990479\n",
      "Validation: Epoch [13], Batch [892/938], Loss: 0.37334588170051575\n",
      "Validation: Epoch [13], Batch [893/938], Loss: 0.42322278022766113\n",
      "Validation: Epoch [13], Batch [894/938], Loss: 0.652228832244873\n",
      "Validation: Epoch [13], Batch [895/938], Loss: 0.4761279821395874\n",
      "Validation: Epoch [13], Batch [896/938], Loss: 0.39262160658836365\n",
      "Validation: Epoch [13], Batch [897/938], Loss: 0.4562554657459259\n",
      "Validation: Epoch [13], Batch [898/938], Loss: 0.4911305904388428\n",
      "Validation: Epoch [13], Batch [899/938], Loss: 0.43822458386421204\n",
      "Validation: Epoch [13], Batch [900/938], Loss: 0.6259878873825073\n",
      "Validation: Epoch [13], Batch [901/938], Loss: 0.4445796608924866\n",
      "Validation: Epoch [13], Batch [902/938], Loss: 0.40583041310310364\n",
      "Validation: Epoch [13], Batch [903/938], Loss: 0.5130558609962463\n",
      "Validation: Epoch [13], Batch [904/938], Loss: 0.4772590398788452\n",
      "Validation: Epoch [13], Batch [905/938], Loss: 0.6019149422645569\n",
      "Validation: Epoch [13], Batch [906/938], Loss: 0.49927181005477905\n",
      "Validation: Epoch [13], Batch [907/938], Loss: 0.45762914419174194\n",
      "Validation: Epoch [13], Batch [908/938], Loss: 0.3688700497150421\n",
      "Validation: Epoch [13], Batch [909/938], Loss: 0.5388185381889343\n",
      "Validation: Epoch [13], Batch [910/938], Loss: 0.45086735486984253\n",
      "Validation: Epoch [13], Batch [911/938], Loss: 0.4235789179801941\n",
      "Validation: Epoch [13], Batch [912/938], Loss: 0.5642012357711792\n",
      "Validation: Epoch [13], Batch [913/938], Loss: 0.5616599321365356\n",
      "Validation: Epoch [13], Batch [914/938], Loss: 0.4131453037261963\n",
      "Validation: Epoch [13], Batch [915/938], Loss: 0.4085535407066345\n",
      "Validation: Epoch [13], Batch [916/938], Loss: 0.47896021604537964\n",
      "Validation: Epoch [13], Batch [917/938], Loss: 0.6540347337722778\n",
      "Validation: Epoch [13], Batch [918/938], Loss: 0.4842929542064667\n",
      "Validation: Epoch [13], Batch [919/938], Loss: 0.36317259073257446\n",
      "Validation: Epoch [13], Batch [920/938], Loss: 0.4445626735687256\n",
      "Validation: Epoch [13], Batch [921/938], Loss: 0.4387371242046356\n",
      "Validation: Epoch [13], Batch [922/938], Loss: 0.3832327127456665\n",
      "Validation: Epoch [13], Batch [923/938], Loss: 0.46472877264022827\n",
      "Validation: Epoch [13], Batch [924/938], Loss: 0.3410013020038605\n",
      "Validation: Epoch [13], Batch [925/938], Loss: 0.3919965624809265\n",
      "Validation: Epoch [13], Batch [926/938], Loss: 0.6344476938247681\n",
      "Validation: Epoch [13], Batch [927/938], Loss: 0.5114506483078003\n",
      "Validation: Epoch [13], Batch [928/938], Loss: 0.5488767027854919\n",
      "Validation: Epoch [13], Batch [929/938], Loss: 0.534344494342804\n",
      "Validation: Epoch [13], Batch [930/938], Loss: 0.48674723505973816\n",
      "Validation: Epoch [13], Batch [931/938], Loss: 0.5619279742240906\n",
      "Validation: Epoch [13], Batch [932/938], Loss: 0.43632400035858154\n",
      "Validation: Epoch [13], Batch [933/938], Loss: 0.6496908664703369\n",
      "Validation: Epoch [13], Batch [934/938], Loss: 0.536673367023468\n",
      "Validation: Epoch [13], Batch [935/938], Loss: 0.35506555438041687\n",
      "Validation: Epoch [13], Batch [936/938], Loss: 0.5243600606918335\n",
      "Validation: Epoch [13], Batch [937/938], Loss: 0.43834859132766724\n",
      "Validation: Epoch [13], Batch [938/938], Loss: 0.5187422633171082\n",
      "Accuracy of test set: 0.8311\n",
      "Train: Epoch [14], Batch [1/938], Loss: 0.3620457351207733\n",
      "Train: Epoch [14], Batch [2/938], Loss: 0.350796103477478\n",
      "Train: Epoch [14], Batch [3/938], Loss: 0.36195647716522217\n",
      "Train: Epoch [14], Batch [4/938], Loss: 0.7731004953384399\n",
      "Train: Epoch [14], Batch [5/938], Loss: 0.5987851619720459\n",
      "Train: Epoch [14], Batch [6/938], Loss: 0.47867465019226074\n",
      "Train: Epoch [14], Batch [7/938], Loss: 0.42948418855667114\n",
      "Train: Epoch [14], Batch [8/938], Loss: 0.47334012389183044\n",
      "Train: Epoch [14], Batch [9/938], Loss: 0.48419108986854553\n",
      "Train: Epoch [14], Batch [10/938], Loss: 0.5618787407875061\n",
      "Train: Epoch [14], Batch [11/938], Loss: 0.6090244054794312\n",
      "Train: Epoch [14], Batch [12/938], Loss: 0.47873055934906006\n",
      "Train: Epoch [14], Batch [13/938], Loss: 0.5207957029342651\n",
      "Train: Epoch [14], Batch [14/938], Loss: 0.28412389755249023\n",
      "Train: Epoch [14], Batch [15/938], Loss: 0.4068872928619385\n",
      "Train: Epoch [14], Batch [16/938], Loss: 0.49368250370025635\n",
      "Train: Epoch [14], Batch [17/938], Loss: 0.6497601270675659\n",
      "Train: Epoch [14], Batch [18/938], Loss: 0.5718445181846619\n",
      "Train: Epoch [14], Batch [19/938], Loss: 0.6000986099243164\n",
      "Train: Epoch [14], Batch [20/938], Loss: 0.4519495666027069\n",
      "Train: Epoch [14], Batch [21/938], Loss: 0.3659078776836395\n",
      "Train: Epoch [14], Batch [22/938], Loss: 0.42078304290771484\n",
      "Train: Epoch [14], Batch [23/938], Loss: 0.4690910577774048\n",
      "Train: Epoch [14], Batch [24/938], Loss: 0.46941065788269043\n",
      "Train: Epoch [14], Batch [25/938], Loss: 0.4470584988594055\n",
      "Train: Epoch [14], Batch [26/938], Loss: 0.4298093914985657\n",
      "Train: Epoch [14], Batch [27/938], Loss: 0.5774160623550415\n",
      "Train: Epoch [14], Batch [28/938], Loss: 0.5256131291389465\n",
      "Train: Epoch [14], Batch [29/938], Loss: 0.46335020661354065\n",
      "Train: Epoch [14], Batch [30/938], Loss: 0.47680172324180603\n",
      "Train: Epoch [14], Batch [31/938], Loss: 0.48800164461135864\n",
      "Train: Epoch [14], Batch [32/938], Loss: 0.35189974308013916\n",
      "Train: Epoch [14], Batch [33/938], Loss: 0.3802693784236908\n",
      "Train: Epoch [14], Batch [34/938], Loss: 0.4681786298751831\n",
      "Train: Epoch [14], Batch [35/938], Loss: 0.484666645526886\n",
      "Train: Epoch [14], Batch [36/938], Loss: 0.39028751850128174\n",
      "Train: Epoch [14], Batch [37/938], Loss: 0.38924551010131836\n",
      "Train: Epoch [14], Batch [38/938], Loss: 0.4477916955947876\n",
      "Train: Epoch [14], Batch [39/938], Loss: 0.38368695974349976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [40/938], Loss: 0.4239116609096527\n",
      "Train: Epoch [14], Batch [41/938], Loss: 0.23022958636283875\n",
      "Train: Epoch [14], Batch [42/938], Loss: 0.35188382863998413\n",
      "Train: Epoch [14], Batch [43/938], Loss: 0.447267085313797\n",
      "Train: Epoch [14], Batch [44/938], Loss: 0.3657844066619873\n",
      "Train: Epoch [14], Batch [45/938], Loss: 0.4570847749710083\n",
      "Train: Epoch [14], Batch [46/938], Loss: 0.5347626209259033\n",
      "Train: Epoch [14], Batch [47/938], Loss: 0.5319139957427979\n",
      "Train: Epoch [14], Batch [48/938], Loss: 0.5445123910903931\n",
      "Train: Epoch [14], Batch [49/938], Loss: 0.7759115099906921\n",
      "Train: Epoch [14], Batch [50/938], Loss: 0.5241235494613647\n",
      "Train: Epoch [14], Batch [51/938], Loss: 0.4687100648880005\n",
      "Train: Epoch [14], Batch [52/938], Loss: 0.6471458077430725\n",
      "Train: Epoch [14], Batch [53/938], Loss: 0.3760228157043457\n",
      "Train: Epoch [14], Batch [54/938], Loss: 0.7433676719665527\n",
      "Train: Epoch [14], Batch [55/938], Loss: 0.48619768023490906\n",
      "Train: Epoch [14], Batch [56/938], Loss: 0.4428291618824005\n",
      "Train: Epoch [14], Batch [57/938], Loss: 0.3969937562942505\n",
      "Train: Epoch [14], Batch [58/938], Loss: 0.34987401962280273\n",
      "Train: Epoch [14], Batch [59/938], Loss: 0.2939615845680237\n",
      "Train: Epoch [14], Batch [60/938], Loss: 0.44849783182144165\n",
      "Train: Epoch [14], Batch [61/938], Loss: 0.4645419716835022\n",
      "Train: Epoch [14], Batch [62/938], Loss: 0.40171870589256287\n",
      "Train: Epoch [14], Batch [63/938], Loss: 0.4595211148262024\n",
      "Train: Epoch [14], Batch [64/938], Loss: 0.7060920000076294\n",
      "Train: Epoch [14], Batch [65/938], Loss: 0.34727275371551514\n",
      "Train: Epoch [14], Batch [66/938], Loss: 0.5863938331604004\n",
      "Train: Epoch [14], Batch [67/938], Loss: 0.39401692152023315\n",
      "Train: Epoch [14], Batch [68/938], Loss: 0.3779367506504059\n",
      "Train: Epoch [14], Batch [69/938], Loss: 0.4453471899032593\n",
      "Train: Epoch [14], Batch [70/938], Loss: 0.4688511788845062\n",
      "Train: Epoch [14], Batch [71/938], Loss: 0.38251441717147827\n",
      "Train: Epoch [14], Batch [72/938], Loss: 0.5009250044822693\n",
      "Train: Epoch [14], Batch [73/938], Loss: 0.34142374992370605\n",
      "Train: Epoch [14], Batch [74/938], Loss: 0.6832098960876465\n",
      "Train: Epoch [14], Batch [75/938], Loss: 0.3165817856788635\n",
      "Train: Epoch [14], Batch [76/938], Loss: 0.46101114153862\n",
      "Train: Epoch [14], Batch [77/938], Loss: 0.41183656454086304\n",
      "Train: Epoch [14], Batch [78/938], Loss: 0.3353997766971588\n",
      "Train: Epoch [14], Batch [79/938], Loss: 0.5270413160324097\n",
      "Train: Epoch [14], Batch [80/938], Loss: 0.3730963468551636\n",
      "Train: Epoch [14], Batch [81/938], Loss: 0.6660022735595703\n",
      "Train: Epoch [14], Batch [82/938], Loss: 0.691923201084137\n",
      "Train: Epoch [14], Batch [83/938], Loss: 0.5679113864898682\n",
      "Train: Epoch [14], Batch [84/938], Loss: 0.7220605611801147\n",
      "Train: Epoch [14], Batch [85/938], Loss: 0.6416797637939453\n",
      "Train: Epoch [14], Batch [86/938], Loss: 0.3666378855705261\n",
      "Train: Epoch [14], Batch [87/938], Loss: 0.3720826208591461\n",
      "Train: Epoch [14], Batch [88/938], Loss: 0.5132785439491272\n",
      "Train: Epoch [14], Batch [89/938], Loss: 0.421112596988678\n",
      "Train: Epoch [14], Batch [90/938], Loss: 0.4979976713657379\n",
      "Train: Epoch [14], Batch [91/938], Loss: 0.6111389398574829\n",
      "Train: Epoch [14], Batch [92/938], Loss: 0.6603261232376099\n",
      "Train: Epoch [14], Batch [93/938], Loss: 0.3726387619972229\n",
      "Train: Epoch [14], Batch [94/938], Loss: 0.5380737781524658\n",
      "Train: Epoch [14], Batch [95/938], Loss: 0.3542221486568451\n",
      "Train: Epoch [14], Batch [96/938], Loss: 0.5342450141906738\n",
      "Train: Epoch [14], Batch [97/938], Loss: 0.43468526005744934\n",
      "Train: Epoch [14], Batch [98/938], Loss: 0.5602031350135803\n",
      "Train: Epoch [14], Batch [99/938], Loss: 0.43763798475265503\n",
      "Train: Epoch [14], Batch [100/938], Loss: 0.47048231959342957\n",
      "Train: Epoch [14], Batch [101/938], Loss: 0.4015594720840454\n",
      "Train: Epoch [14], Batch [102/938], Loss: 0.23355907201766968\n",
      "Train: Epoch [14], Batch [103/938], Loss: 0.4394645094871521\n",
      "Train: Epoch [14], Batch [104/938], Loss: 0.6351675987243652\n",
      "Train: Epoch [14], Batch [105/938], Loss: 0.5400696992874146\n",
      "Train: Epoch [14], Batch [106/938], Loss: 0.5097314119338989\n",
      "Train: Epoch [14], Batch [107/938], Loss: 0.7223532199859619\n",
      "Train: Epoch [14], Batch [108/938], Loss: 0.5613082647323608\n",
      "Train: Epoch [14], Batch [109/938], Loss: 0.6379693746566772\n",
      "Train: Epoch [14], Batch [110/938], Loss: 0.44189807772636414\n",
      "Train: Epoch [14], Batch [111/938], Loss: 0.44568967819213867\n",
      "Train: Epoch [14], Batch [112/938], Loss: 0.4695076644420624\n",
      "Train: Epoch [14], Batch [113/938], Loss: 0.5579937100410461\n",
      "Train: Epoch [14], Batch [114/938], Loss: 0.5861028432846069\n",
      "Train: Epoch [14], Batch [115/938], Loss: 0.43990427255630493\n",
      "Train: Epoch [14], Batch [116/938], Loss: 0.5289208889007568\n",
      "Train: Epoch [14], Batch [117/938], Loss: 0.5907143354415894\n",
      "Train: Epoch [14], Batch [118/938], Loss: 0.47720852494239807\n",
      "Train: Epoch [14], Batch [119/938], Loss: 0.4819066524505615\n",
      "Train: Epoch [14], Batch [120/938], Loss: 0.5371376872062683\n",
      "Train: Epoch [14], Batch [121/938], Loss: 0.3522922694683075\n",
      "Train: Epoch [14], Batch [122/938], Loss: 0.43967103958129883\n",
      "Train: Epoch [14], Batch [123/938], Loss: 0.31508713960647583\n",
      "Train: Epoch [14], Batch [124/938], Loss: 0.43244558572769165\n",
      "Train: Epoch [14], Batch [125/938], Loss: 0.6671149730682373\n",
      "Train: Epoch [14], Batch [126/938], Loss: 0.4105529487133026\n",
      "Train: Epoch [14], Batch [127/938], Loss: 0.39363712072372437\n",
      "Train: Epoch [14], Batch [128/938], Loss: 0.35712045431137085\n",
      "Train: Epoch [14], Batch [129/938], Loss: 0.372114896774292\n",
      "Train: Epoch [14], Batch [130/938], Loss: 0.39974522590637207\n",
      "Train: Epoch [14], Batch [131/938], Loss: 0.38090425729751587\n",
      "Train: Epoch [14], Batch [132/938], Loss: 0.37888985872268677\n",
      "Train: Epoch [14], Batch [133/938], Loss: 0.4507519006729126\n",
      "Train: Epoch [14], Batch [134/938], Loss: 0.4045883119106293\n",
      "Train: Epoch [14], Batch [135/938], Loss: 0.5188658237457275\n",
      "Train: Epoch [14], Batch [136/938], Loss: 0.4338570535182953\n",
      "Train: Epoch [14], Batch [137/938], Loss: 0.43581750988960266\n",
      "Train: Epoch [14], Batch [138/938], Loss: 0.5624571442604065\n",
      "Train: Epoch [14], Batch [139/938], Loss: 0.5206920504570007\n",
      "Train: Epoch [14], Batch [140/938], Loss: 0.63160640001297\n",
      "Train: Epoch [14], Batch [141/938], Loss: 0.464167058467865\n",
      "Train: Epoch [14], Batch [142/938], Loss: 0.41940587759017944\n",
      "Train: Epoch [14], Batch [143/938], Loss: 0.55086350440979\n",
      "Train: Epoch [14], Batch [144/938], Loss: 0.5366172790527344\n",
      "Train: Epoch [14], Batch [145/938], Loss: 0.37926018238067627\n",
      "Train: Epoch [14], Batch [146/938], Loss: 0.5969194173812866\n",
      "Train: Epoch [14], Batch [147/938], Loss: 0.5226461887359619\n",
      "Train: Epoch [14], Batch [148/938], Loss: 0.570142388343811\n",
      "Train: Epoch [14], Batch [149/938], Loss: 0.4302513599395752\n",
      "Train: Epoch [14], Batch [150/938], Loss: 0.48208099603652954\n",
      "Train: Epoch [14], Batch [151/938], Loss: 0.6158996820449829\n",
      "Train: Epoch [14], Batch [152/938], Loss: 0.30838674306869507\n",
      "Train: Epoch [14], Batch [153/938], Loss: 0.4140460193157196\n",
      "Train: Epoch [14], Batch [154/938], Loss: 0.47964972257614136\n",
      "Train: Epoch [14], Batch [155/938], Loss: 0.6092253923416138\n",
      "Train: Epoch [14], Batch [156/938], Loss: 0.4651849865913391\n",
      "Train: Epoch [14], Batch [157/938], Loss: 0.3883552551269531\n",
      "Train: Epoch [14], Batch [158/938], Loss: 0.3880807161331177\n",
      "Train: Epoch [14], Batch [159/938], Loss: 0.4983397424221039\n",
      "Train: Epoch [14], Batch [160/938], Loss: 0.5211619138717651\n",
      "Train: Epoch [14], Batch [161/938], Loss: 0.6242606043815613\n",
      "Train: Epoch [14], Batch [162/938], Loss: 0.5145068764686584\n",
      "Train: Epoch [14], Batch [163/938], Loss: 0.4586476981639862\n",
      "Train: Epoch [14], Batch [164/938], Loss: 0.6507757902145386\n",
      "Train: Epoch [14], Batch [165/938], Loss: 0.67097008228302\n",
      "Train: Epoch [14], Batch [166/938], Loss: 0.7235207557678223\n",
      "Train: Epoch [14], Batch [167/938], Loss: 0.422162801027298\n",
      "Train: Epoch [14], Batch [168/938], Loss: 0.5559781193733215\n",
      "Train: Epoch [14], Batch [169/938], Loss: 0.4763687551021576\n",
      "Train: Epoch [14], Batch [170/938], Loss: 0.5291827917098999\n",
      "Train: Epoch [14], Batch [171/938], Loss: 0.5115301609039307\n",
      "Train: Epoch [14], Batch [172/938], Loss: 0.5082715749740601\n",
      "Train: Epoch [14], Batch [173/938], Loss: 0.3746695816516876\n",
      "Train: Epoch [14], Batch [174/938], Loss: 0.7390948534011841\n",
      "Train: Epoch [14], Batch [175/938], Loss: 0.6342588067054749\n",
      "Train: Epoch [14], Batch [176/938], Loss: 0.5288724899291992\n",
      "Train: Epoch [14], Batch [177/938], Loss: 0.485960453748703\n",
      "Train: Epoch [14], Batch [178/938], Loss: 0.5273462533950806\n",
      "Train: Epoch [14], Batch [179/938], Loss: 0.39337727427482605\n",
      "Train: Epoch [14], Batch [180/938], Loss: 0.6036015748977661\n",
      "Train: Epoch [14], Batch [181/938], Loss: 0.3397105932235718\n",
      "Train: Epoch [14], Batch [182/938], Loss: 0.5357243418693542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [183/938], Loss: 0.5706732273101807\n",
      "Train: Epoch [14], Batch [184/938], Loss: 0.6308329105377197\n",
      "Train: Epoch [14], Batch [185/938], Loss: 0.40523067116737366\n",
      "Train: Epoch [14], Batch [186/938], Loss: 0.6087950468063354\n",
      "Train: Epoch [14], Batch [187/938], Loss: 0.4574216306209564\n",
      "Train: Epoch [14], Batch [188/938], Loss: 0.49746251106262207\n",
      "Train: Epoch [14], Batch [189/938], Loss: 0.6306555271148682\n",
      "Train: Epoch [14], Batch [190/938], Loss: 0.4871445298194885\n",
      "Train: Epoch [14], Batch [191/938], Loss: 0.4792575240135193\n",
      "Train: Epoch [14], Batch [192/938], Loss: 0.35373085737228394\n",
      "Train: Epoch [14], Batch [193/938], Loss: 0.2800510823726654\n",
      "Train: Epoch [14], Batch [194/938], Loss: 0.44439154863357544\n",
      "Train: Epoch [14], Batch [195/938], Loss: 0.4763566255569458\n",
      "Train: Epoch [14], Batch [196/938], Loss: 0.38516509532928467\n",
      "Train: Epoch [14], Batch [197/938], Loss: 0.5381019115447998\n",
      "Train: Epoch [14], Batch [198/938], Loss: 0.3893439471721649\n",
      "Train: Epoch [14], Batch [199/938], Loss: 0.6531305909156799\n",
      "Train: Epoch [14], Batch [200/938], Loss: 0.39477217197418213\n",
      "Train: Epoch [14], Batch [201/938], Loss: 0.34102314710617065\n",
      "Train: Epoch [14], Batch [202/938], Loss: 0.4155086278915405\n",
      "Train: Epoch [14], Batch [203/938], Loss: 0.38334566354751587\n",
      "Train: Epoch [14], Batch [204/938], Loss: 0.5845786333084106\n",
      "Train: Epoch [14], Batch [205/938], Loss: 0.48401498794555664\n",
      "Train: Epoch [14], Batch [206/938], Loss: 0.6168363690376282\n",
      "Train: Epoch [14], Batch [207/938], Loss: 0.31029757857322693\n",
      "Train: Epoch [14], Batch [208/938], Loss: 0.5284730195999146\n",
      "Train: Epoch [14], Batch [209/938], Loss: 0.44080018997192383\n",
      "Train: Epoch [14], Batch [210/938], Loss: 0.4980385899543762\n",
      "Train: Epoch [14], Batch [211/938], Loss: 0.6618800163269043\n",
      "Train: Epoch [14], Batch [212/938], Loss: 0.6751667261123657\n",
      "Train: Epoch [14], Batch [213/938], Loss: 0.42029160261154175\n",
      "Train: Epoch [14], Batch [214/938], Loss: 0.38108134269714355\n",
      "Train: Epoch [14], Batch [215/938], Loss: 0.5214856863021851\n",
      "Train: Epoch [14], Batch [216/938], Loss: 0.4175400137901306\n",
      "Train: Epoch [14], Batch [217/938], Loss: 0.42759573459625244\n",
      "Train: Epoch [14], Batch [218/938], Loss: 0.4718319773674011\n",
      "Train: Epoch [14], Batch [219/938], Loss: 0.3668678402900696\n",
      "Train: Epoch [14], Batch [220/938], Loss: 0.5068713426589966\n",
      "Train: Epoch [14], Batch [221/938], Loss: 0.5415802001953125\n",
      "Train: Epoch [14], Batch [222/938], Loss: 0.4389457106590271\n",
      "Train: Epoch [14], Batch [223/938], Loss: 0.423191100358963\n",
      "Train: Epoch [14], Batch [224/938], Loss: 0.4021087884902954\n",
      "Train: Epoch [14], Batch [225/938], Loss: 0.4377618432044983\n",
      "Train: Epoch [14], Batch [226/938], Loss: 0.4763423800468445\n",
      "Train: Epoch [14], Batch [227/938], Loss: 0.35532838106155396\n",
      "Train: Epoch [14], Batch [228/938], Loss: 0.34596043825149536\n",
      "Train: Epoch [14], Batch [229/938], Loss: 0.5523035526275635\n",
      "Train: Epoch [14], Batch [230/938], Loss: 0.5790491104125977\n",
      "Train: Epoch [14], Batch [231/938], Loss: 0.48343268036842346\n",
      "Train: Epoch [14], Batch [232/938], Loss: 0.5737645030021667\n",
      "Train: Epoch [14], Batch [233/938], Loss: 0.5807454586029053\n",
      "Train: Epoch [14], Batch [234/938], Loss: 0.46419158577919006\n",
      "Train: Epoch [14], Batch [235/938], Loss: 0.43464258313179016\n",
      "Train: Epoch [14], Batch [236/938], Loss: 0.5982475876808167\n",
      "Train: Epoch [14], Batch [237/938], Loss: 0.6914876699447632\n",
      "Train: Epoch [14], Batch [238/938], Loss: 0.5717654228210449\n",
      "Train: Epoch [14], Batch [239/938], Loss: 0.4923391044139862\n",
      "Train: Epoch [14], Batch [240/938], Loss: 0.507178544998169\n",
      "Train: Epoch [14], Batch [241/938], Loss: 0.2383125275373459\n",
      "Train: Epoch [14], Batch [242/938], Loss: 0.4714975357055664\n",
      "Train: Epoch [14], Batch [243/938], Loss: 0.3811471462249756\n",
      "Train: Epoch [14], Batch [244/938], Loss: 0.3554948568344116\n",
      "Train: Epoch [14], Batch [245/938], Loss: 0.4591570496559143\n",
      "Train: Epoch [14], Batch [246/938], Loss: 0.5728946328163147\n",
      "Train: Epoch [14], Batch [247/938], Loss: 0.3719864785671234\n",
      "Train: Epoch [14], Batch [248/938], Loss: 0.2945267856121063\n",
      "Train: Epoch [14], Batch [249/938], Loss: 0.431393027305603\n",
      "Train: Epoch [14], Batch [250/938], Loss: 0.3630533218383789\n",
      "Train: Epoch [14], Batch [251/938], Loss: 0.42087265849113464\n",
      "Train: Epoch [14], Batch [252/938], Loss: 0.38071662187576294\n",
      "Train: Epoch [14], Batch [253/938], Loss: 0.5423627495765686\n",
      "Train: Epoch [14], Batch [254/938], Loss: 0.5747641921043396\n",
      "Train: Epoch [14], Batch [255/938], Loss: 0.4631360173225403\n",
      "Train: Epoch [14], Batch [256/938], Loss: 0.2688182592391968\n",
      "Train: Epoch [14], Batch [257/938], Loss: 0.41617128252983093\n",
      "Train: Epoch [14], Batch [258/938], Loss: 0.5886304378509521\n",
      "Train: Epoch [14], Batch [259/938], Loss: 0.45633724331855774\n",
      "Train: Epoch [14], Batch [260/938], Loss: 0.6923758387565613\n",
      "Train: Epoch [14], Batch [261/938], Loss: 0.5964780449867249\n",
      "Train: Epoch [14], Batch [262/938], Loss: 0.38472428917884827\n",
      "Train: Epoch [14], Batch [263/938], Loss: 0.3623896539211273\n",
      "Train: Epoch [14], Batch [264/938], Loss: 0.3255399763584137\n",
      "Train: Epoch [14], Batch [265/938], Loss: 0.5620609521865845\n",
      "Train: Epoch [14], Batch [266/938], Loss: 0.35272693634033203\n",
      "Train: Epoch [14], Batch [267/938], Loss: 0.6960631608963013\n",
      "Train: Epoch [14], Batch [268/938], Loss: 0.4916157126426697\n",
      "Train: Epoch [14], Batch [269/938], Loss: 0.43544018268585205\n",
      "Train: Epoch [14], Batch [270/938], Loss: 0.3421712815761566\n",
      "Train: Epoch [14], Batch [271/938], Loss: 0.46517693996429443\n",
      "Train: Epoch [14], Batch [272/938], Loss: 0.4595232307910919\n",
      "Train: Epoch [14], Batch [273/938], Loss: 0.39819011092185974\n",
      "Train: Epoch [14], Batch [274/938], Loss: 0.5806684494018555\n",
      "Train: Epoch [14], Batch [275/938], Loss: 0.5176371932029724\n",
      "Train: Epoch [14], Batch [276/938], Loss: 0.43721818923950195\n",
      "Train: Epoch [14], Batch [277/938], Loss: 0.32938235998153687\n",
      "Train: Epoch [14], Batch [278/938], Loss: 0.4521985650062561\n",
      "Train: Epoch [14], Batch [279/938], Loss: 0.5511429309844971\n",
      "Train: Epoch [14], Batch [280/938], Loss: 0.5720764398574829\n",
      "Train: Epoch [14], Batch [281/938], Loss: 0.4782097637653351\n",
      "Train: Epoch [14], Batch [282/938], Loss: 0.5513485670089722\n",
      "Train: Epoch [14], Batch [283/938], Loss: 0.32854172587394714\n",
      "Train: Epoch [14], Batch [284/938], Loss: 0.36406052112579346\n",
      "Train: Epoch [14], Batch [285/938], Loss: 0.40745967626571655\n",
      "Train: Epoch [14], Batch [286/938], Loss: 0.3389677107334137\n",
      "Train: Epoch [14], Batch [287/938], Loss: 0.3824513554573059\n",
      "Train: Epoch [14], Batch [288/938], Loss: 0.36671826243400574\n",
      "Train: Epoch [14], Batch [289/938], Loss: 0.5912691950798035\n",
      "Train: Epoch [14], Batch [290/938], Loss: 0.3370758891105652\n",
      "Train: Epoch [14], Batch [291/938], Loss: 0.49429285526275635\n",
      "Train: Epoch [14], Batch [292/938], Loss: 0.5350269079208374\n",
      "Train: Epoch [14], Batch [293/938], Loss: 0.5249142050743103\n",
      "Train: Epoch [14], Batch [294/938], Loss: 0.44034892320632935\n",
      "Train: Epoch [14], Batch [295/938], Loss: 0.3419899344444275\n",
      "Train: Epoch [14], Batch [296/938], Loss: 0.5461075901985168\n",
      "Train: Epoch [14], Batch [297/938], Loss: 0.35927069187164307\n",
      "Train: Epoch [14], Batch [298/938], Loss: 0.446406751871109\n",
      "Train: Epoch [14], Batch [299/938], Loss: 0.38989901542663574\n",
      "Train: Epoch [14], Batch [300/938], Loss: 0.6235756278038025\n",
      "Train: Epoch [14], Batch [301/938], Loss: 0.5014758110046387\n",
      "Train: Epoch [14], Batch [302/938], Loss: 0.7099084258079529\n",
      "Train: Epoch [14], Batch [303/938], Loss: 0.6889635920524597\n",
      "Train: Epoch [14], Batch [304/938], Loss: 0.5521938800811768\n",
      "Train: Epoch [14], Batch [305/938], Loss: 0.6775320172309875\n",
      "Train: Epoch [14], Batch [306/938], Loss: 0.6736839413642883\n",
      "Train: Epoch [14], Batch [307/938], Loss: 0.3155444860458374\n",
      "Train: Epoch [14], Batch [308/938], Loss: 0.47170183062553406\n",
      "Train: Epoch [14], Batch [309/938], Loss: 0.4906091094017029\n",
      "Train: Epoch [14], Batch [310/938], Loss: 0.38960880041122437\n",
      "Train: Epoch [14], Batch [311/938], Loss: 0.5006869435310364\n",
      "Train: Epoch [14], Batch [312/938], Loss: 0.44090762734413147\n",
      "Train: Epoch [14], Batch [313/938], Loss: 0.42706188559532166\n",
      "Train: Epoch [14], Batch [314/938], Loss: 0.682004451751709\n",
      "Train: Epoch [14], Batch [315/938], Loss: 0.47151583433151245\n",
      "Train: Epoch [14], Batch [316/938], Loss: 0.4479978382587433\n",
      "Train: Epoch [14], Batch [317/938], Loss: 0.38860028982162476\n",
      "Train: Epoch [14], Batch [318/938], Loss: 0.6130639314651489\n",
      "Train: Epoch [14], Batch [319/938], Loss: 0.46435970067977905\n",
      "Train: Epoch [14], Batch [320/938], Loss: 0.4999385178089142\n",
      "Train: Epoch [14], Batch [321/938], Loss: 0.46141982078552246\n",
      "Train: Epoch [14], Batch [322/938], Loss: 0.4137055277824402\n",
      "Train: Epoch [14], Batch [323/938], Loss: 0.693917989730835\n",
      "Train: Epoch [14], Batch [324/938], Loss: 0.37759169936180115\n",
      "Train: Epoch [14], Batch [325/938], Loss: 0.6153256893157959\n",
      "Train: Epoch [14], Batch [326/938], Loss: 0.3966313302516937\n",
      "Train: Epoch [14], Batch [327/938], Loss: 0.38939720392227173\n",
      "Train: Epoch [14], Batch [328/938], Loss: 0.4414175748825073\n",
      "Train: Epoch [14], Batch [329/938], Loss: 0.35788050293922424\n",
      "Train: Epoch [14], Batch [330/938], Loss: 0.29517796635627747\n",
      "Train: Epoch [14], Batch [331/938], Loss: 0.6465549468994141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [332/938], Loss: 0.5485314130783081\n",
      "Train: Epoch [14], Batch [333/938], Loss: 0.5308173894882202\n",
      "Train: Epoch [14], Batch [334/938], Loss: 0.386289119720459\n",
      "Train: Epoch [14], Batch [335/938], Loss: 0.3775258958339691\n",
      "Train: Epoch [14], Batch [336/938], Loss: 0.30378276109695435\n",
      "Train: Epoch [14], Batch [337/938], Loss: 0.5155229568481445\n",
      "Train: Epoch [14], Batch [338/938], Loss: 0.428667813539505\n",
      "Train: Epoch [14], Batch [339/938], Loss: 0.4477041959762573\n",
      "Train: Epoch [14], Batch [340/938], Loss: 0.3228932023048401\n",
      "Train: Epoch [14], Batch [341/938], Loss: 0.7564026117324829\n",
      "Train: Epoch [14], Batch [342/938], Loss: 0.44344162940979004\n",
      "Train: Epoch [14], Batch [343/938], Loss: 0.4722193777561188\n",
      "Train: Epoch [14], Batch [344/938], Loss: 0.6410483717918396\n",
      "Train: Epoch [14], Batch [345/938], Loss: 0.43293246626853943\n",
      "Train: Epoch [14], Batch [346/938], Loss: 0.40569359064102173\n",
      "Train: Epoch [14], Batch [347/938], Loss: 0.6951704025268555\n",
      "Train: Epoch [14], Batch [348/938], Loss: 0.5355758666992188\n",
      "Train: Epoch [14], Batch [349/938], Loss: 0.5205647945404053\n",
      "Train: Epoch [14], Batch [350/938], Loss: 0.45361077785491943\n",
      "Train: Epoch [14], Batch [351/938], Loss: 0.4589834213256836\n",
      "Train: Epoch [14], Batch [352/938], Loss: 0.5159977078437805\n",
      "Train: Epoch [14], Batch [353/938], Loss: 0.43567898869514465\n",
      "Train: Epoch [14], Batch [354/938], Loss: 0.5504636764526367\n",
      "Train: Epoch [14], Batch [355/938], Loss: 0.5530809760093689\n",
      "Train: Epoch [14], Batch [356/938], Loss: 0.4996911883354187\n",
      "Train: Epoch [14], Batch [357/938], Loss: 0.4307025671005249\n",
      "Train: Epoch [14], Batch [358/938], Loss: 0.4076826274394989\n",
      "Train: Epoch [14], Batch [359/938], Loss: 0.41922527551651\n",
      "Train: Epoch [14], Batch [360/938], Loss: 0.5952112078666687\n",
      "Train: Epoch [14], Batch [361/938], Loss: 0.27865779399871826\n",
      "Train: Epoch [14], Batch [362/938], Loss: 0.296301007270813\n",
      "Train: Epoch [14], Batch [363/938], Loss: 0.4913114309310913\n",
      "Train: Epoch [14], Batch [364/938], Loss: 0.630743145942688\n",
      "Train: Epoch [14], Batch [365/938], Loss: 0.44715726375579834\n",
      "Train: Epoch [14], Batch [366/938], Loss: 0.46982550621032715\n",
      "Train: Epoch [14], Batch [367/938], Loss: 0.560532808303833\n",
      "Train: Epoch [14], Batch [368/938], Loss: 0.4242999255657196\n",
      "Train: Epoch [14], Batch [369/938], Loss: 0.44991958141326904\n",
      "Train: Epoch [14], Batch [370/938], Loss: 0.6983749270439148\n",
      "Train: Epoch [14], Batch [371/938], Loss: 0.4532778263092041\n",
      "Train: Epoch [14], Batch [372/938], Loss: 0.36316075921058655\n",
      "Train: Epoch [14], Batch [373/938], Loss: 0.5715821981430054\n",
      "Train: Epoch [14], Batch [374/938], Loss: 0.40723007917404175\n",
      "Train: Epoch [14], Batch [375/938], Loss: 0.38596683740615845\n",
      "Train: Epoch [14], Batch [376/938], Loss: 0.6241247653961182\n",
      "Train: Epoch [14], Batch [377/938], Loss: 0.5539312362670898\n",
      "Train: Epoch [14], Batch [378/938], Loss: 0.4841672480106354\n",
      "Train: Epoch [14], Batch [379/938], Loss: 0.4483286738395691\n",
      "Train: Epoch [14], Batch [380/938], Loss: 0.48908403515815735\n",
      "Train: Epoch [14], Batch [381/938], Loss: 0.351391077041626\n",
      "Train: Epoch [14], Batch [382/938], Loss: 0.5880556702613831\n",
      "Train: Epoch [14], Batch [383/938], Loss: 0.28188881278038025\n",
      "Train: Epoch [14], Batch [384/938], Loss: 0.47630125284194946\n",
      "Train: Epoch [14], Batch [385/938], Loss: 0.4720386266708374\n",
      "Train: Epoch [14], Batch [386/938], Loss: 0.4118838906288147\n",
      "Train: Epoch [14], Batch [387/938], Loss: 0.5763205289840698\n",
      "Train: Epoch [14], Batch [388/938], Loss: 0.5343517065048218\n",
      "Train: Epoch [14], Batch [389/938], Loss: 0.3148823082447052\n",
      "Train: Epoch [14], Batch [390/938], Loss: 0.33369386196136475\n",
      "Train: Epoch [14], Batch [391/938], Loss: 0.41810110211372375\n",
      "Train: Epoch [14], Batch [392/938], Loss: 0.42199575901031494\n",
      "Train: Epoch [14], Batch [393/938], Loss: 0.49007901549339294\n",
      "Train: Epoch [14], Batch [394/938], Loss: 0.6388977766036987\n",
      "Train: Epoch [14], Batch [395/938], Loss: 0.44615447521209717\n",
      "Train: Epoch [14], Batch [396/938], Loss: 0.44377803802490234\n",
      "Train: Epoch [14], Batch [397/938], Loss: 0.5500295162200928\n",
      "Train: Epoch [14], Batch [398/938], Loss: 0.46875667572021484\n",
      "Train: Epoch [14], Batch [399/938], Loss: 0.6942974925041199\n",
      "Train: Epoch [14], Batch [400/938], Loss: 0.43450096249580383\n",
      "Train: Epoch [14], Batch [401/938], Loss: 0.40367162227630615\n",
      "Train: Epoch [14], Batch [402/938], Loss: 0.513940691947937\n",
      "Train: Epoch [14], Batch [403/938], Loss: 0.46986615657806396\n",
      "Train: Epoch [14], Batch [404/938], Loss: 0.5354536771774292\n",
      "Train: Epoch [14], Batch [405/938], Loss: 0.5548006296157837\n",
      "Train: Epoch [14], Batch [406/938], Loss: 0.4790499210357666\n",
      "Train: Epoch [14], Batch [407/938], Loss: 0.5385277271270752\n",
      "Train: Epoch [14], Batch [408/938], Loss: 0.7606099843978882\n",
      "Train: Epoch [14], Batch [409/938], Loss: 0.36760300397872925\n",
      "Train: Epoch [14], Batch [410/938], Loss: 0.675121545791626\n",
      "Train: Epoch [14], Batch [411/938], Loss: 0.6819958686828613\n",
      "Train: Epoch [14], Batch [412/938], Loss: 0.7227027416229248\n",
      "Train: Epoch [14], Batch [413/938], Loss: 0.5676074028015137\n",
      "Train: Epoch [14], Batch [414/938], Loss: 0.5277825593948364\n",
      "Train: Epoch [14], Batch [415/938], Loss: 0.38316601514816284\n",
      "Train: Epoch [14], Batch [416/938], Loss: 0.5073291063308716\n",
      "Train: Epoch [14], Batch [417/938], Loss: 0.559935986995697\n",
      "Train: Epoch [14], Batch [418/938], Loss: 0.45198947191238403\n",
      "Train: Epoch [14], Batch [419/938], Loss: 0.5613111257553101\n",
      "Train: Epoch [14], Batch [420/938], Loss: 0.3984795808792114\n",
      "Train: Epoch [14], Batch [421/938], Loss: 0.5622767210006714\n",
      "Train: Epoch [14], Batch [422/938], Loss: 0.4518638849258423\n",
      "Train: Epoch [14], Batch [423/938], Loss: 0.4588584303855896\n",
      "Train: Epoch [14], Batch [424/938], Loss: 0.5044625401496887\n",
      "Train: Epoch [14], Batch [425/938], Loss: 0.3957972824573517\n",
      "Train: Epoch [14], Batch [426/938], Loss: 0.2902001142501831\n",
      "Train: Epoch [14], Batch [427/938], Loss: 0.5343309640884399\n",
      "Train: Epoch [14], Batch [428/938], Loss: 0.5715196132659912\n",
      "Train: Epoch [14], Batch [429/938], Loss: 0.3316902816295624\n",
      "Train: Epoch [14], Batch [430/938], Loss: 0.38125741481781006\n",
      "Train: Epoch [14], Batch [431/938], Loss: 0.3410935699939728\n",
      "Train: Epoch [14], Batch [432/938], Loss: 0.3369729816913605\n",
      "Train: Epoch [14], Batch [433/938], Loss: 0.4051305949687958\n",
      "Train: Epoch [14], Batch [434/938], Loss: 0.38567349314689636\n",
      "Train: Epoch [14], Batch [435/938], Loss: 0.7281650304794312\n",
      "Train: Epoch [14], Batch [436/938], Loss: 0.4576265811920166\n",
      "Train: Epoch [14], Batch [437/938], Loss: 0.48298612236976624\n",
      "Train: Epoch [14], Batch [438/938], Loss: 0.4979780912399292\n",
      "Train: Epoch [14], Batch [439/938], Loss: 0.406110018491745\n",
      "Train: Epoch [14], Batch [440/938], Loss: 0.5783258080482483\n",
      "Train: Epoch [14], Batch [441/938], Loss: 0.7191746234893799\n",
      "Train: Epoch [14], Batch [442/938], Loss: 0.4232655167579651\n",
      "Train: Epoch [14], Batch [443/938], Loss: 0.3362177610397339\n",
      "Train: Epoch [14], Batch [444/938], Loss: 0.347364217042923\n",
      "Train: Epoch [14], Batch [445/938], Loss: 0.6029350757598877\n",
      "Train: Epoch [14], Batch [446/938], Loss: 0.736467182636261\n",
      "Train: Epoch [14], Batch [447/938], Loss: 0.3846537470817566\n",
      "Train: Epoch [14], Batch [448/938], Loss: 0.4978015720844269\n",
      "Train: Epoch [14], Batch [449/938], Loss: 0.5356691479682922\n",
      "Train: Epoch [14], Batch [450/938], Loss: 0.2518562376499176\n",
      "Train: Epoch [14], Batch [451/938], Loss: 0.6912345886230469\n",
      "Train: Epoch [14], Batch [452/938], Loss: 0.4149702191352844\n",
      "Train: Epoch [14], Batch [453/938], Loss: 0.545229434967041\n",
      "Train: Epoch [14], Batch [454/938], Loss: 0.37068045139312744\n",
      "Train: Epoch [14], Batch [455/938], Loss: 0.5005403757095337\n",
      "Train: Epoch [14], Batch [456/938], Loss: 0.2593075633049011\n",
      "Train: Epoch [14], Batch [457/938], Loss: 0.4201251268386841\n",
      "Train: Epoch [14], Batch [458/938], Loss: 0.35721325874328613\n",
      "Train: Epoch [14], Batch [459/938], Loss: 0.45870211720466614\n",
      "Train: Epoch [14], Batch [460/938], Loss: 0.5490232706069946\n",
      "Train: Epoch [14], Batch [461/938], Loss: 0.6665986776351929\n",
      "Train: Epoch [14], Batch [462/938], Loss: 0.4573062062263489\n",
      "Train: Epoch [14], Batch [463/938], Loss: 0.41256147623062134\n",
      "Train: Epoch [14], Batch [464/938], Loss: 0.45975226163864136\n",
      "Train: Epoch [14], Batch [465/938], Loss: 0.44490671157836914\n",
      "Train: Epoch [14], Batch [466/938], Loss: 0.3940213918685913\n",
      "Train: Epoch [14], Batch [467/938], Loss: 0.5914103984832764\n",
      "Train: Epoch [14], Batch [468/938], Loss: 0.3049026131629944\n",
      "Train: Epoch [14], Batch [469/938], Loss: 0.5551151037216187\n",
      "Train: Epoch [14], Batch [470/938], Loss: 0.47330033779144287\n",
      "Train: Epoch [14], Batch [471/938], Loss: 0.3255698084831238\n",
      "Train: Epoch [14], Batch [472/938], Loss: 0.4360349774360657\n",
      "Train: Epoch [14], Batch [473/938], Loss: 0.4768684208393097\n",
      "Train: Epoch [14], Batch [474/938], Loss: 0.3387800455093384\n",
      "Train: Epoch [14], Batch [475/938], Loss: 0.3568617105484009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [476/938], Loss: 0.26547670364379883\n",
      "Train: Epoch [14], Batch [477/938], Loss: 0.43642762303352356\n",
      "Train: Epoch [14], Batch [478/938], Loss: 0.4462449550628662\n",
      "Train: Epoch [14], Batch [479/938], Loss: 0.39794132113456726\n",
      "Train: Epoch [14], Batch [480/938], Loss: 0.42526477575302124\n",
      "Train: Epoch [14], Batch [481/938], Loss: 0.43267351388931274\n",
      "Train: Epoch [14], Batch [482/938], Loss: 0.49943676590919495\n",
      "Train: Epoch [14], Batch [483/938], Loss: 0.3947969973087311\n",
      "Train: Epoch [14], Batch [484/938], Loss: 0.6021807193756104\n",
      "Train: Epoch [14], Batch [485/938], Loss: 0.5157341957092285\n",
      "Train: Epoch [14], Batch [486/938], Loss: 0.6082501411437988\n",
      "Train: Epoch [14], Batch [487/938], Loss: 0.49727755784988403\n",
      "Train: Epoch [14], Batch [488/938], Loss: 0.49862760305404663\n",
      "Train: Epoch [14], Batch [489/938], Loss: 0.5700287818908691\n",
      "Train: Epoch [14], Batch [490/938], Loss: 0.3119801878929138\n",
      "Train: Epoch [14], Batch [491/938], Loss: 0.43147027492523193\n",
      "Train: Epoch [14], Batch [492/938], Loss: 0.3375203013420105\n",
      "Train: Epoch [14], Batch [493/938], Loss: 0.5804450511932373\n",
      "Train: Epoch [14], Batch [494/938], Loss: 0.40745458006858826\n",
      "Train: Epoch [14], Batch [495/938], Loss: 0.5921252965927124\n",
      "Train: Epoch [14], Batch [496/938], Loss: 0.4247018098831177\n",
      "Train: Epoch [14], Batch [497/938], Loss: 0.31995898485183716\n",
      "Train: Epoch [14], Batch [498/938], Loss: 0.5126118659973145\n",
      "Train: Epoch [14], Batch [499/938], Loss: 0.44617509841918945\n",
      "Train: Epoch [14], Batch [500/938], Loss: 0.5695725083351135\n",
      "Train: Epoch [14], Batch [501/938], Loss: 0.40400218963623047\n",
      "Train: Epoch [14], Batch [502/938], Loss: 0.4704360067844391\n",
      "Train: Epoch [14], Batch [503/938], Loss: 0.4368298649787903\n",
      "Train: Epoch [14], Batch [504/938], Loss: 0.15712574124336243\n",
      "Train: Epoch [14], Batch [505/938], Loss: 0.35214415192604065\n",
      "Train: Epoch [14], Batch [506/938], Loss: 0.45445534586906433\n",
      "Train: Epoch [14], Batch [507/938], Loss: 0.5420308709144592\n",
      "Train: Epoch [14], Batch [508/938], Loss: 0.4607216417789459\n",
      "Train: Epoch [14], Batch [509/938], Loss: 0.3648885488510132\n",
      "Train: Epoch [14], Batch [510/938], Loss: 0.43918681144714355\n",
      "Train: Epoch [14], Batch [511/938], Loss: 0.6791232824325562\n",
      "Train: Epoch [14], Batch [512/938], Loss: 0.4978061318397522\n",
      "Train: Epoch [14], Batch [513/938], Loss: 0.5901851058006287\n",
      "Train: Epoch [14], Batch [514/938], Loss: 0.5398426055908203\n",
      "Train: Epoch [14], Batch [515/938], Loss: 0.37472203373908997\n",
      "Train: Epoch [14], Batch [516/938], Loss: 0.2512795031070709\n",
      "Train: Epoch [14], Batch [517/938], Loss: 0.4817582368850708\n",
      "Train: Epoch [14], Batch [518/938], Loss: 0.5585873126983643\n",
      "Train: Epoch [14], Batch [519/938], Loss: 0.4503278136253357\n",
      "Train: Epoch [14], Batch [520/938], Loss: 0.42136386036872864\n",
      "Train: Epoch [14], Batch [521/938], Loss: 0.4676198363304138\n",
      "Train: Epoch [14], Batch [522/938], Loss: 0.4938128590583801\n",
      "Train: Epoch [14], Batch [523/938], Loss: 0.3660392761230469\n",
      "Train: Epoch [14], Batch [524/938], Loss: 0.538108229637146\n",
      "Train: Epoch [14], Batch [525/938], Loss: 0.3802691102027893\n",
      "Train: Epoch [14], Batch [526/938], Loss: 0.44821837544441223\n",
      "Train: Epoch [14], Batch [527/938], Loss: 0.3393629491329193\n",
      "Train: Epoch [14], Batch [528/938], Loss: 0.4455341398715973\n",
      "Train: Epoch [14], Batch [529/938], Loss: 0.5431025624275208\n",
      "Train: Epoch [14], Batch [530/938], Loss: 0.4119049608707428\n",
      "Train: Epoch [14], Batch [531/938], Loss: 0.4513684809207916\n",
      "Train: Epoch [14], Batch [532/938], Loss: 0.538391649723053\n",
      "Train: Epoch [14], Batch [533/938], Loss: 0.5502803325653076\n",
      "Train: Epoch [14], Batch [534/938], Loss: 0.456262469291687\n",
      "Train: Epoch [14], Batch [535/938], Loss: 0.4729844331741333\n",
      "Train: Epoch [14], Batch [536/938], Loss: 0.3473036289215088\n",
      "Train: Epoch [14], Batch [537/938], Loss: 0.5619125366210938\n",
      "Train: Epoch [14], Batch [538/938], Loss: 0.5032265186309814\n",
      "Train: Epoch [14], Batch [539/938], Loss: 0.49687469005584717\n",
      "Train: Epoch [14], Batch [540/938], Loss: 0.4240036606788635\n",
      "Train: Epoch [14], Batch [541/938], Loss: 0.5479428768157959\n",
      "Train: Epoch [14], Batch [542/938], Loss: 0.6730478405952454\n",
      "Train: Epoch [14], Batch [543/938], Loss: 0.5717853307723999\n",
      "Train: Epoch [14], Batch [544/938], Loss: 0.44191253185272217\n",
      "Train: Epoch [14], Batch [545/938], Loss: 0.5643116235733032\n",
      "Train: Epoch [14], Batch [546/938], Loss: 0.5176475644111633\n",
      "Train: Epoch [14], Batch [547/938], Loss: 0.5251643657684326\n",
      "Train: Epoch [14], Batch [548/938], Loss: 0.5663418769836426\n",
      "Train: Epoch [14], Batch [549/938], Loss: 0.7588467001914978\n",
      "Train: Epoch [14], Batch [550/938], Loss: 0.43928220868110657\n",
      "Train: Epoch [14], Batch [551/938], Loss: 0.4365253746509552\n",
      "Train: Epoch [14], Batch [552/938], Loss: 0.5549361705780029\n",
      "Train: Epoch [14], Batch [553/938], Loss: 0.27613523602485657\n",
      "Train: Epoch [14], Batch [554/938], Loss: 0.49024268984794617\n",
      "Train: Epoch [14], Batch [555/938], Loss: 0.38726717233657837\n",
      "Train: Epoch [14], Batch [556/938], Loss: 0.5342099070549011\n",
      "Train: Epoch [14], Batch [557/938], Loss: 0.41784393787384033\n",
      "Train: Epoch [14], Batch [558/938], Loss: 0.41986051201820374\n",
      "Train: Epoch [14], Batch [559/938], Loss: 0.43146008253097534\n",
      "Train: Epoch [14], Batch [560/938], Loss: 0.30138489603996277\n",
      "Train: Epoch [14], Batch [561/938], Loss: 0.31519952416419983\n",
      "Train: Epoch [14], Batch [562/938], Loss: 0.673128604888916\n",
      "Train: Epoch [14], Batch [563/938], Loss: 0.3667803108692169\n",
      "Train: Epoch [14], Batch [564/938], Loss: 0.6167635321617126\n",
      "Train: Epoch [14], Batch [565/938], Loss: 0.44551441073417664\n",
      "Train: Epoch [14], Batch [566/938], Loss: 0.5234396457672119\n",
      "Train: Epoch [14], Batch [567/938], Loss: 0.5109256505966187\n",
      "Train: Epoch [14], Batch [568/938], Loss: 0.42079633474349976\n",
      "Train: Epoch [14], Batch [569/938], Loss: 0.427051842212677\n",
      "Train: Epoch [14], Batch [570/938], Loss: 0.4115082025527954\n",
      "Train: Epoch [14], Batch [571/938], Loss: 0.2687222361564636\n",
      "Train: Epoch [14], Batch [572/938], Loss: 0.7705734968185425\n",
      "Train: Epoch [14], Batch [573/938], Loss: 0.5512259006500244\n",
      "Train: Epoch [14], Batch [574/938], Loss: 0.6594537496566772\n",
      "Train: Epoch [14], Batch [575/938], Loss: 0.54795241355896\n",
      "Train: Epoch [14], Batch [576/938], Loss: 0.4417440891265869\n",
      "Train: Epoch [14], Batch [577/938], Loss: 0.4717975854873657\n",
      "Train: Epoch [14], Batch [578/938], Loss: 0.3090061545372009\n",
      "Train: Epoch [14], Batch [579/938], Loss: 0.4130454659461975\n",
      "Train: Epoch [14], Batch [580/938], Loss: 0.5212967991828918\n",
      "Train: Epoch [14], Batch [581/938], Loss: 0.4795973300933838\n",
      "Train: Epoch [14], Batch [582/938], Loss: 0.3742790222167969\n",
      "Train: Epoch [14], Batch [583/938], Loss: 0.6459177732467651\n",
      "Train: Epoch [14], Batch [584/938], Loss: 0.4509630501270294\n",
      "Train: Epoch [14], Batch [585/938], Loss: 0.4168052673339844\n",
      "Train: Epoch [14], Batch [586/938], Loss: 0.43219974637031555\n",
      "Train: Epoch [14], Batch [587/938], Loss: 0.5757635831832886\n",
      "Train: Epoch [14], Batch [588/938], Loss: 0.41036850214004517\n",
      "Train: Epoch [14], Batch [589/938], Loss: 0.3583731949329376\n",
      "Train: Epoch [14], Batch [590/938], Loss: 0.37979787588119507\n",
      "Train: Epoch [14], Batch [591/938], Loss: 0.5809668302536011\n",
      "Train: Epoch [14], Batch [592/938], Loss: 0.5578716993331909\n",
      "Train: Epoch [14], Batch [593/938], Loss: 0.4535759389400482\n",
      "Train: Epoch [14], Batch [594/938], Loss: 0.37757837772369385\n",
      "Train: Epoch [14], Batch [595/938], Loss: 0.3619276285171509\n",
      "Train: Epoch [14], Batch [596/938], Loss: 0.5369044542312622\n",
      "Train: Epoch [14], Batch [597/938], Loss: 0.5001844167709351\n",
      "Train: Epoch [14], Batch [598/938], Loss: 0.3873119354248047\n",
      "Train: Epoch [14], Batch [599/938], Loss: 0.6626676917076111\n",
      "Train: Epoch [14], Batch [600/938], Loss: 0.3683777451515198\n",
      "Train: Epoch [14], Batch [601/938], Loss: 0.45065319538116455\n",
      "Train: Epoch [14], Batch [602/938], Loss: 0.6218874454498291\n",
      "Train: Epoch [14], Batch [603/938], Loss: 0.7696132063865662\n",
      "Train: Epoch [14], Batch [604/938], Loss: 0.44313353300094604\n",
      "Train: Epoch [14], Batch [605/938], Loss: 0.7445867657661438\n",
      "Train: Epoch [14], Batch [606/938], Loss: 0.5407793521881104\n",
      "Train: Epoch [14], Batch [607/938], Loss: 0.5086054801940918\n",
      "Train: Epoch [14], Batch [608/938], Loss: 0.29319435358047485\n",
      "Train: Epoch [14], Batch [609/938], Loss: 0.4456835389137268\n",
      "Train: Epoch [14], Batch [610/938], Loss: 0.43715739250183105\n",
      "Train: Epoch [14], Batch [611/938], Loss: 0.47832101583480835\n",
      "Train: Epoch [14], Batch [612/938], Loss: 0.5756405591964722\n",
      "Train: Epoch [14], Batch [613/938], Loss: 0.45353490114212036\n",
      "Train: Epoch [14], Batch [614/938], Loss: 0.40537840127944946\n",
      "Train: Epoch [14], Batch [615/938], Loss: 0.5287448167800903\n",
      "Train: Epoch [14], Batch [616/938], Loss: 0.6480766534805298\n",
      "Train: Epoch [14], Batch [617/938], Loss: 0.4506905972957611\n",
      "Train: Epoch [14], Batch [618/938], Loss: 0.5376447439193726\n",
      "Train: Epoch [14], Batch [619/938], Loss: 0.5276031494140625\n",
      "Train: Epoch [14], Batch [620/938], Loss: 0.5632295608520508\n",
      "Train: Epoch [14], Batch [621/938], Loss: 0.5516217947006226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [622/938], Loss: 0.43407008051872253\n",
      "Train: Epoch [14], Batch [623/938], Loss: 0.44052761793136597\n",
      "Train: Epoch [14], Batch [624/938], Loss: 0.3826766014099121\n",
      "Train: Epoch [14], Batch [625/938], Loss: 0.7303119897842407\n",
      "Train: Epoch [14], Batch [626/938], Loss: 0.5976331233978271\n",
      "Train: Epoch [14], Batch [627/938], Loss: 0.3990177512168884\n",
      "Train: Epoch [14], Batch [628/938], Loss: 0.26124873757362366\n",
      "Train: Epoch [14], Batch [629/938], Loss: 0.39584648609161377\n",
      "Train: Epoch [14], Batch [630/938], Loss: 0.3952379822731018\n",
      "Train: Epoch [14], Batch [631/938], Loss: 0.5612847805023193\n",
      "Train: Epoch [14], Batch [632/938], Loss: 0.37493059039115906\n",
      "Train: Epoch [14], Batch [633/938], Loss: 0.326151579618454\n",
      "Train: Epoch [14], Batch [634/938], Loss: 0.5196973085403442\n",
      "Train: Epoch [14], Batch [635/938], Loss: 0.5155153274536133\n",
      "Train: Epoch [14], Batch [636/938], Loss: 0.5173478722572327\n",
      "Train: Epoch [14], Batch [637/938], Loss: 0.5894396305084229\n",
      "Train: Epoch [14], Batch [638/938], Loss: 0.4354286193847656\n",
      "Train: Epoch [14], Batch [639/938], Loss: 0.5573773384094238\n",
      "Train: Epoch [14], Batch [640/938], Loss: 0.5346455574035645\n",
      "Train: Epoch [14], Batch [641/938], Loss: 0.3770856261253357\n",
      "Train: Epoch [14], Batch [642/938], Loss: 0.4030880630016327\n",
      "Train: Epoch [14], Batch [643/938], Loss: 0.5643442869186401\n",
      "Train: Epoch [14], Batch [644/938], Loss: 0.6144793033599854\n",
      "Train: Epoch [14], Batch [645/938], Loss: 0.26380759477615356\n",
      "Train: Epoch [14], Batch [646/938], Loss: 0.4418644905090332\n",
      "Train: Epoch [14], Batch [647/938], Loss: 0.42881089448928833\n",
      "Train: Epoch [14], Batch [648/938], Loss: 0.3267757296562195\n",
      "Train: Epoch [14], Batch [649/938], Loss: 0.44339776039123535\n",
      "Train: Epoch [14], Batch [650/938], Loss: 0.32193315029144287\n",
      "Train: Epoch [14], Batch [651/938], Loss: 0.34501850605010986\n",
      "Train: Epoch [14], Batch [652/938], Loss: 0.785523533821106\n",
      "Train: Epoch [14], Batch [653/938], Loss: 0.34277284145355225\n",
      "Train: Epoch [14], Batch [654/938], Loss: 0.5047358870506287\n",
      "Train: Epoch [14], Batch [655/938], Loss: 0.3453167676925659\n",
      "Train: Epoch [14], Batch [656/938], Loss: 0.4546268582344055\n",
      "Train: Epoch [14], Batch [657/938], Loss: 0.5554653406143188\n",
      "Train: Epoch [14], Batch [658/938], Loss: 0.5541082620620728\n",
      "Train: Epoch [14], Batch [659/938], Loss: 0.572525143623352\n",
      "Train: Epoch [14], Batch [660/938], Loss: 0.6401256322860718\n",
      "Train: Epoch [14], Batch [661/938], Loss: 0.41851499676704407\n",
      "Train: Epoch [14], Batch [662/938], Loss: 0.5352804064750671\n",
      "Train: Epoch [14], Batch [663/938], Loss: 0.26408523321151733\n",
      "Train: Epoch [14], Batch [664/938], Loss: 0.5063400268554688\n",
      "Train: Epoch [14], Batch [665/938], Loss: 0.3698784112930298\n",
      "Train: Epoch [14], Batch [666/938], Loss: 0.4750298261642456\n",
      "Train: Epoch [14], Batch [667/938], Loss: 0.28522056341171265\n",
      "Train: Epoch [14], Batch [668/938], Loss: 0.6053271889686584\n",
      "Train: Epoch [14], Batch [669/938], Loss: 0.5292816758155823\n",
      "Train: Epoch [14], Batch [670/938], Loss: 0.47702836990356445\n",
      "Train: Epoch [14], Batch [671/938], Loss: 0.4329078793525696\n",
      "Train: Epoch [14], Batch [672/938], Loss: 0.2787412405014038\n",
      "Train: Epoch [14], Batch [673/938], Loss: 0.6037307977676392\n",
      "Train: Epoch [14], Batch [674/938], Loss: 0.647895872592926\n",
      "Train: Epoch [14], Batch [675/938], Loss: 0.38155749440193176\n",
      "Train: Epoch [14], Batch [676/938], Loss: 0.3431457281112671\n",
      "Train: Epoch [14], Batch [677/938], Loss: 0.4389652609825134\n",
      "Train: Epoch [14], Batch [678/938], Loss: 0.5669829845428467\n",
      "Train: Epoch [14], Batch [679/938], Loss: 0.4572060704231262\n",
      "Train: Epoch [14], Batch [680/938], Loss: 0.4391293525695801\n",
      "Train: Epoch [14], Batch [681/938], Loss: 0.45454633235931396\n",
      "Train: Epoch [14], Batch [682/938], Loss: 0.29959556460380554\n",
      "Train: Epoch [14], Batch [683/938], Loss: 0.40485668182373047\n",
      "Train: Epoch [14], Batch [684/938], Loss: 0.4224793016910553\n",
      "Train: Epoch [14], Batch [685/938], Loss: 0.6367608308792114\n",
      "Train: Epoch [14], Batch [686/938], Loss: 0.3073238730430603\n",
      "Train: Epoch [14], Batch [687/938], Loss: 0.3477472960948944\n",
      "Train: Epoch [14], Batch [688/938], Loss: 0.5708683729171753\n",
      "Train: Epoch [14], Batch [689/938], Loss: 0.34434962272644043\n",
      "Train: Epoch [14], Batch [690/938], Loss: 0.4235273003578186\n",
      "Train: Epoch [14], Batch [691/938], Loss: 0.33338916301727295\n",
      "Train: Epoch [14], Batch [692/938], Loss: 0.5693390369415283\n",
      "Train: Epoch [14], Batch [693/938], Loss: 0.48513075709342957\n",
      "Train: Epoch [14], Batch [694/938], Loss: 0.4087713360786438\n",
      "Train: Epoch [14], Batch [695/938], Loss: 0.45650649070739746\n",
      "Train: Epoch [14], Batch [696/938], Loss: 0.4768661856651306\n",
      "Train: Epoch [14], Batch [697/938], Loss: 0.4626280963420868\n",
      "Train: Epoch [14], Batch [698/938], Loss: 0.33558711409568787\n",
      "Train: Epoch [14], Batch [699/938], Loss: 0.3494024872779846\n",
      "Train: Epoch [14], Batch [700/938], Loss: 0.4365554749965668\n",
      "Train: Epoch [14], Batch [701/938], Loss: 0.5401800274848938\n",
      "Train: Epoch [14], Batch [702/938], Loss: 0.33013680577278137\n",
      "Train: Epoch [14], Batch [703/938], Loss: 0.536124587059021\n",
      "Train: Epoch [14], Batch [704/938], Loss: 0.49831467866897583\n",
      "Train: Epoch [14], Batch [705/938], Loss: 0.5827581286430359\n",
      "Train: Epoch [14], Batch [706/938], Loss: 0.39051657915115356\n",
      "Train: Epoch [14], Batch [707/938], Loss: 0.30874553322792053\n",
      "Train: Epoch [14], Batch [708/938], Loss: 0.4940475523471832\n",
      "Train: Epoch [14], Batch [709/938], Loss: 0.40995657444000244\n",
      "Train: Epoch [14], Batch [710/938], Loss: 0.5522888898849487\n",
      "Train: Epoch [14], Batch [711/938], Loss: 0.4315820336341858\n",
      "Train: Epoch [14], Batch [712/938], Loss: 0.39305171370506287\n",
      "Train: Epoch [14], Batch [713/938], Loss: 0.4989035427570343\n",
      "Train: Epoch [14], Batch [714/938], Loss: 0.4619438648223877\n",
      "Train: Epoch [14], Batch [715/938], Loss: 0.46651771664619446\n",
      "Train: Epoch [14], Batch [716/938], Loss: 0.4130752384662628\n",
      "Train: Epoch [14], Batch [717/938], Loss: 0.45632487535476685\n",
      "Train: Epoch [14], Batch [718/938], Loss: 0.28717631101608276\n",
      "Train: Epoch [14], Batch [719/938], Loss: 0.5406394004821777\n",
      "Train: Epoch [14], Batch [720/938], Loss: 0.5544060468673706\n",
      "Train: Epoch [14], Batch [721/938], Loss: 0.27249792218208313\n",
      "Train: Epoch [14], Batch [722/938], Loss: 0.6785956025123596\n",
      "Train: Epoch [14], Batch [723/938], Loss: 0.45514026284217834\n",
      "Train: Epoch [14], Batch [724/938], Loss: 0.36865153908729553\n",
      "Train: Epoch [14], Batch [725/938], Loss: 0.31764328479766846\n",
      "Train: Epoch [14], Batch [726/938], Loss: 0.48226308822631836\n",
      "Train: Epoch [14], Batch [727/938], Loss: 0.32758671045303345\n",
      "Train: Epoch [14], Batch [728/938], Loss: 0.4913941025733948\n",
      "Train: Epoch [14], Batch [729/938], Loss: 0.3736504316329956\n",
      "Train: Epoch [14], Batch [730/938], Loss: 0.4276494085788727\n",
      "Train: Epoch [14], Batch [731/938], Loss: 0.7218884229660034\n",
      "Train: Epoch [14], Batch [732/938], Loss: 0.607977032661438\n",
      "Train: Epoch [14], Batch [733/938], Loss: 0.4004710614681244\n",
      "Train: Epoch [14], Batch [734/938], Loss: 0.44659706950187683\n",
      "Train: Epoch [14], Batch [735/938], Loss: 0.39567282795906067\n",
      "Train: Epoch [14], Batch [736/938], Loss: 0.48073357343673706\n",
      "Train: Epoch [14], Batch [737/938], Loss: 0.4869457483291626\n",
      "Train: Epoch [14], Batch [738/938], Loss: 0.522309422492981\n",
      "Train: Epoch [14], Batch [739/938], Loss: 0.47893065214157104\n",
      "Train: Epoch [14], Batch [740/938], Loss: 0.49389219284057617\n",
      "Train: Epoch [14], Batch [741/938], Loss: 0.7077376842498779\n",
      "Train: Epoch [14], Batch [742/938], Loss: 0.46901220083236694\n",
      "Train: Epoch [14], Batch [743/938], Loss: 0.5248260498046875\n",
      "Train: Epoch [14], Batch [744/938], Loss: 0.5330258011817932\n",
      "Train: Epoch [14], Batch [745/938], Loss: 0.5981144309043884\n",
      "Train: Epoch [14], Batch [746/938], Loss: 0.3839467167854309\n",
      "Train: Epoch [14], Batch [747/938], Loss: 0.48172762989997864\n",
      "Train: Epoch [14], Batch [748/938], Loss: 0.39764487743377686\n",
      "Train: Epoch [14], Batch [749/938], Loss: 0.47383344173431396\n",
      "Train: Epoch [14], Batch [750/938], Loss: 0.4348130226135254\n",
      "Train: Epoch [14], Batch [751/938], Loss: 0.28361523151397705\n",
      "Train: Epoch [14], Batch [752/938], Loss: 0.47546273469924927\n",
      "Train: Epoch [14], Batch [753/938], Loss: 0.42467230558395386\n",
      "Train: Epoch [14], Batch [754/938], Loss: 0.6490174531936646\n",
      "Train: Epoch [14], Batch [755/938], Loss: 0.4849517047405243\n",
      "Train: Epoch [14], Batch [756/938], Loss: 0.5208017826080322\n",
      "Train: Epoch [14], Batch [757/938], Loss: 0.4845816493034363\n",
      "Train: Epoch [14], Batch [758/938], Loss: 0.475902795791626\n",
      "Train: Epoch [14], Batch [759/938], Loss: 0.4114809036254883\n",
      "Train: Epoch [14], Batch [760/938], Loss: 0.5105909109115601\n",
      "Train: Epoch [14], Batch [761/938], Loss: 0.4480212926864624\n",
      "Train: Epoch [14], Batch [762/938], Loss: 0.4466917812824249\n",
      "Train: Epoch [14], Batch [763/938], Loss: 0.47385185956954956\n",
      "Train: Epoch [14], Batch [764/938], Loss: 0.3318670094013214\n",
      "Train: Epoch [14], Batch [765/938], Loss: 0.3296828866004944\n",
      "Train: Epoch [14], Batch [766/938], Loss: 0.4150524139404297\n",
      "Train: Epoch [14], Batch [767/938], Loss: 0.3192991614341736\n",
      "Train: Epoch [14], Batch [768/938], Loss: 0.3776346743106842\n",
      "Train: Epoch [14], Batch [769/938], Loss: 0.7427698373794556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [770/938], Loss: 0.44358474016189575\n",
      "Train: Epoch [14], Batch [771/938], Loss: 0.609840989112854\n",
      "Train: Epoch [14], Batch [772/938], Loss: 0.2940746247768402\n",
      "Train: Epoch [14], Batch [773/938], Loss: 0.4003472328186035\n",
      "Train: Epoch [14], Batch [774/938], Loss: 0.35806718468666077\n",
      "Train: Epoch [14], Batch [775/938], Loss: 0.34932026267051697\n",
      "Train: Epoch [14], Batch [776/938], Loss: 0.44514116644859314\n",
      "Train: Epoch [14], Batch [777/938], Loss: 0.3966289460659027\n",
      "Train: Epoch [14], Batch [778/938], Loss: 0.42385321855545044\n",
      "Train: Epoch [14], Batch [779/938], Loss: 0.4420732855796814\n",
      "Train: Epoch [14], Batch [780/938], Loss: 0.47012513875961304\n",
      "Train: Epoch [14], Batch [781/938], Loss: 0.46673810482025146\n",
      "Train: Epoch [14], Batch [782/938], Loss: 0.5036122798919678\n",
      "Train: Epoch [14], Batch [783/938], Loss: 0.39166516065597534\n",
      "Train: Epoch [14], Batch [784/938], Loss: 0.5199319124221802\n",
      "Train: Epoch [14], Batch [785/938], Loss: 0.37484312057495117\n",
      "Train: Epoch [14], Batch [786/938], Loss: 0.29217952489852905\n",
      "Train: Epoch [14], Batch [787/938], Loss: 0.33139440417289734\n",
      "Train: Epoch [14], Batch [788/938], Loss: 0.46150413155555725\n",
      "Train: Epoch [14], Batch [789/938], Loss: 0.3571009039878845\n",
      "Train: Epoch [14], Batch [790/938], Loss: 0.585120439529419\n",
      "Train: Epoch [14], Batch [791/938], Loss: 0.608945906162262\n",
      "Train: Epoch [14], Batch [792/938], Loss: 0.44149380922317505\n",
      "Train: Epoch [14], Batch [793/938], Loss: 0.3811618387699127\n",
      "Train: Epoch [14], Batch [794/938], Loss: 0.5634797215461731\n",
      "Train: Epoch [14], Batch [795/938], Loss: 0.3447512984275818\n",
      "Train: Epoch [14], Batch [796/938], Loss: 0.37569403648376465\n",
      "Train: Epoch [14], Batch [797/938], Loss: 0.5261821150779724\n",
      "Train: Epoch [14], Batch [798/938], Loss: 0.6174183487892151\n",
      "Train: Epoch [14], Batch [799/938], Loss: 0.430026113986969\n",
      "Train: Epoch [14], Batch [800/938], Loss: 0.6729738712310791\n",
      "Train: Epoch [14], Batch [801/938], Loss: 0.4889566898345947\n",
      "Train: Epoch [14], Batch [802/938], Loss: 0.5493021011352539\n",
      "Train: Epoch [14], Batch [803/938], Loss: 0.4192671775817871\n",
      "Train: Epoch [14], Batch [804/938], Loss: 0.3266355097293854\n",
      "Train: Epoch [14], Batch [805/938], Loss: 0.5152536630630493\n",
      "Train: Epoch [14], Batch [806/938], Loss: 0.47690349817276\n",
      "Train: Epoch [14], Batch [807/938], Loss: 0.5640722513198853\n",
      "Train: Epoch [14], Batch [808/938], Loss: 0.5815538763999939\n",
      "Train: Epoch [14], Batch [809/938], Loss: 0.6154041290283203\n",
      "Train: Epoch [14], Batch [810/938], Loss: 0.8182168006896973\n",
      "Train: Epoch [14], Batch [811/938], Loss: 0.5118885040283203\n",
      "Train: Epoch [14], Batch [812/938], Loss: 0.2573246955871582\n",
      "Train: Epoch [14], Batch [813/938], Loss: 0.6035754680633545\n",
      "Train: Epoch [14], Batch [814/938], Loss: 0.27057066559791565\n",
      "Train: Epoch [14], Batch [815/938], Loss: 0.47552254796028137\n",
      "Train: Epoch [14], Batch [816/938], Loss: 0.6021827459335327\n",
      "Train: Epoch [14], Batch [817/938], Loss: 0.6290722489356995\n",
      "Train: Epoch [14], Batch [818/938], Loss: 0.3151003122329712\n",
      "Train: Epoch [14], Batch [819/938], Loss: 0.4839168190956116\n",
      "Train: Epoch [14], Batch [820/938], Loss: 0.5385202169418335\n",
      "Train: Epoch [14], Batch [821/938], Loss: 0.3471035063266754\n",
      "Train: Epoch [14], Batch [822/938], Loss: 0.46281805634498596\n",
      "Train: Epoch [14], Batch [823/938], Loss: 0.42616891860961914\n",
      "Train: Epoch [14], Batch [824/938], Loss: 0.2718215882778168\n",
      "Train: Epoch [14], Batch [825/938], Loss: 0.4787599742412567\n",
      "Train: Epoch [14], Batch [826/938], Loss: 0.46726909279823303\n",
      "Train: Epoch [14], Batch [827/938], Loss: 0.5158454775810242\n",
      "Train: Epoch [14], Batch [828/938], Loss: 0.34746283292770386\n",
      "Train: Epoch [14], Batch [829/938], Loss: 0.6017467975616455\n",
      "Train: Epoch [14], Batch [830/938], Loss: 0.3929193615913391\n",
      "Train: Epoch [14], Batch [831/938], Loss: 0.4224278926849365\n",
      "Train: Epoch [14], Batch [832/938], Loss: 0.4658023416996002\n",
      "Train: Epoch [14], Batch [833/938], Loss: 0.3363320827484131\n",
      "Train: Epoch [14], Batch [834/938], Loss: 0.4679142236709595\n",
      "Train: Epoch [14], Batch [835/938], Loss: 0.2953473925590515\n",
      "Train: Epoch [14], Batch [836/938], Loss: 0.42241713404655457\n",
      "Train: Epoch [14], Batch [837/938], Loss: 0.5894807577133179\n",
      "Train: Epoch [14], Batch [838/938], Loss: 0.5320456027984619\n",
      "Train: Epoch [14], Batch [839/938], Loss: 0.6509863138198853\n",
      "Train: Epoch [14], Batch [840/938], Loss: 0.35469377040863037\n",
      "Train: Epoch [14], Batch [841/938], Loss: 0.3755459189414978\n",
      "Train: Epoch [14], Batch [842/938], Loss: 0.3500882387161255\n",
      "Train: Epoch [14], Batch [843/938], Loss: 0.39487582445144653\n",
      "Train: Epoch [14], Batch [844/938], Loss: 0.6201390027999878\n",
      "Train: Epoch [14], Batch [845/938], Loss: 0.6195324063301086\n",
      "Train: Epoch [14], Batch [846/938], Loss: 0.34840500354766846\n",
      "Train: Epoch [14], Batch [847/938], Loss: 0.5341109037399292\n",
      "Train: Epoch [14], Batch [848/938], Loss: 0.5033634901046753\n",
      "Train: Epoch [14], Batch [849/938], Loss: 0.4179533123970032\n",
      "Train: Epoch [14], Batch [850/938], Loss: 0.3575940728187561\n",
      "Train: Epoch [14], Batch [851/938], Loss: 0.4353632628917694\n",
      "Train: Epoch [14], Batch [852/938], Loss: 0.5895992517471313\n",
      "Train: Epoch [14], Batch [853/938], Loss: 0.5472822785377502\n",
      "Train: Epoch [14], Batch [854/938], Loss: 0.6464493274688721\n",
      "Train: Epoch [14], Batch [855/938], Loss: 0.449626624584198\n",
      "Train: Epoch [14], Batch [856/938], Loss: 0.3369694650173187\n",
      "Train: Epoch [14], Batch [857/938], Loss: 0.41948819160461426\n",
      "Train: Epoch [14], Batch [858/938], Loss: 0.5267136096954346\n",
      "Train: Epoch [14], Batch [859/938], Loss: 0.4348546862602234\n",
      "Train: Epoch [14], Batch [860/938], Loss: 0.48406705260276794\n",
      "Train: Epoch [14], Batch [861/938], Loss: 0.34666115045547485\n",
      "Train: Epoch [14], Batch [862/938], Loss: 0.32931971549987793\n",
      "Train: Epoch [14], Batch [863/938], Loss: 0.47849011421203613\n",
      "Train: Epoch [14], Batch [864/938], Loss: 0.53759765625\n",
      "Train: Epoch [14], Batch [865/938], Loss: 0.45956951379776\n",
      "Train: Epoch [14], Batch [866/938], Loss: 0.44358277320861816\n",
      "Train: Epoch [14], Batch [867/938], Loss: 0.45868900418281555\n",
      "Train: Epoch [14], Batch [868/938], Loss: 0.4510670602321625\n",
      "Train: Epoch [14], Batch [869/938], Loss: 0.5381646156311035\n",
      "Train: Epoch [14], Batch [870/938], Loss: 0.6188951730728149\n",
      "Train: Epoch [14], Batch [871/938], Loss: 0.8480135202407837\n",
      "Train: Epoch [14], Batch [872/938], Loss: 0.5510421991348267\n",
      "Train: Epoch [14], Batch [873/938], Loss: 0.7175631523132324\n",
      "Train: Epoch [14], Batch [874/938], Loss: 0.4220862090587616\n",
      "Train: Epoch [14], Batch [875/938], Loss: 0.4361404776573181\n",
      "Train: Epoch [14], Batch [876/938], Loss: 0.6303793787956238\n",
      "Train: Epoch [14], Batch [877/938], Loss: 0.49386900663375854\n",
      "Train: Epoch [14], Batch [878/938], Loss: 0.6575517654418945\n",
      "Train: Epoch [14], Batch [879/938], Loss: 0.47693368792533875\n",
      "Train: Epoch [14], Batch [880/938], Loss: 0.42753279209136963\n",
      "Train: Epoch [14], Batch [881/938], Loss: 0.4608832001686096\n",
      "Train: Epoch [14], Batch [882/938], Loss: 0.3106068968772888\n",
      "Train: Epoch [14], Batch [883/938], Loss: 0.5723662376403809\n",
      "Train: Epoch [14], Batch [884/938], Loss: 0.4611964523792267\n",
      "Train: Epoch [14], Batch [885/938], Loss: 0.4738670587539673\n",
      "Train: Epoch [14], Batch [886/938], Loss: 0.28653663396835327\n",
      "Train: Epoch [14], Batch [887/938], Loss: 0.5027867555618286\n",
      "Train: Epoch [14], Batch [888/938], Loss: 0.44277846813201904\n",
      "Train: Epoch [14], Batch [889/938], Loss: 0.7444577813148499\n",
      "Train: Epoch [14], Batch [890/938], Loss: 0.23482859134674072\n",
      "Train: Epoch [14], Batch [891/938], Loss: 0.5533351898193359\n",
      "Train: Epoch [14], Batch [892/938], Loss: 0.6433775424957275\n",
      "Train: Epoch [14], Batch [893/938], Loss: 0.4470464587211609\n",
      "Train: Epoch [14], Batch [894/938], Loss: 0.5496377944946289\n",
      "Train: Epoch [14], Batch [895/938], Loss: 0.5712695121765137\n",
      "Train: Epoch [14], Batch [896/938], Loss: 0.588291347026825\n",
      "Train: Epoch [14], Batch [897/938], Loss: 0.5546047687530518\n",
      "Train: Epoch [14], Batch [898/938], Loss: 0.44442057609558105\n",
      "Train: Epoch [14], Batch [899/938], Loss: 0.6212275624275208\n",
      "Train: Epoch [14], Batch [900/938], Loss: 0.4556274116039276\n",
      "Train: Epoch [14], Batch [901/938], Loss: 0.5028742551803589\n",
      "Train: Epoch [14], Batch [902/938], Loss: 0.3461250364780426\n",
      "Train: Epoch [14], Batch [903/938], Loss: 0.7644723653793335\n",
      "Train: Epoch [14], Batch [904/938], Loss: 0.6466772556304932\n",
      "Train: Epoch [14], Batch [905/938], Loss: 0.4993666112422943\n",
      "Train: Epoch [14], Batch [906/938], Loss: 0.4753478169441223\n",
      "Train: Epoch [14], Batch [907/938], Loss: 0.43120867013931274\n",
      "Train: Epoch [14], Batch [908/938], Loss: 0.3804681897163391\n",
      "Train: Epoch [14], Batch [909/938], Loss: 0.6782511472702026\n",
      "Train: Epoch [14], Batch [910/938], Loss: 0.41848835349082947\n",
      "Train: Epoch [14], Batch [911/938], Loss: 0.5941896438598633\n",
      "Train: Epoch [14], Batch [912/938], Loss: 0.6157490015029907\n",
      "Train: Epoch [14], Batch [913/938], Loss: 0.3494981825351715\n",
      "Train: Epoch [14], Batch [914/938], Loss: 0.5015692710876465\n",
      "Train: Epoch [14], Batch [915/938], Loss: 0.5219390988349915\n",
      "Train: Epoch [14], Batch [916/938], Loss: 0.5296483039855957\n",
      "Train: Epoch [14], Batch [917/938], Loss: 0.42798641324043274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [14], Batch [918/938], Loss: 0.36022236943244934\n",
      "Train: Epoch [14], Batch [919/938], Loss: 0.34588244557380676\n",
      "Train: Epoch [14], Batch [920/938], Loss: 0.5114635229110718\n",
      "Train: Epoch [14], Batch [921/938], Loss: 0.4994995594024658\n",
      "Train: Epoch [14], Batch [922/938], Loss: 0.4344109296798706\n",
      "Train: Epoch [14], Batch [923/938], Loss: 0.32178789377212524\n",
      "Train: Epoch [14], Batch [924/938], Loss: 0.40632307529449463\n",
      "Train: Epoch [14], Batch [925/938], Loss: 0.5745702981948853\n",
      "Train: Epoch [14], Batch [926/938], Loss: 0.391027569770813\n",
      "Train: Epoch [14], Batch [927/938], Loss: 0.5290243625640869\n",
      "Train: Epoch [14], Batch [928/938], Loss: 0.5859143733978271\n",
      "Train: Epoch [14], Batch [929/938], Loss: 0.39452797174453735\n",
      "Train: Epoch [14], Batch [930/938], Loss: 0.39708685874938965\n",
      "Train: Epoch [14], Batch [931/938], Loss: 0.35610440373420715\n",
      "Train: Epoch [14], Batch [932/938], Loss: 0.527770459651947\n",
      "Train: Epoch [14], Batch [933/938], Loss: 0.4025542736053467\n",
      "Train: Epoch [14], Batch [934/938], Loss: 0.40756016969680786\n",
      "Train: Epoch [14], Batch [935/938], Loss: 0.5398375988006592\n",
      "Train: Epoch [14], Batch [936/938], Loss: 0.50831538438797\n",
      "Train: Epoch [14], Batch [937/938], Loss: 0.5257493257522583\n",
      "Train: Epoch [14], Batch [938/938], Loss: 0.34998542070388794\n",
      "Accuracy of train set: 0.8344666666666667\n",
      "Validation: Epoch [14], Batch [1/938], Loss: 0.37384241819381714\n",
      "Validation: Epoch [14], Batch [2/938], Loss: 0.5146006345748901\n",
      "Validation: Epoch [14], Batch [3/938], Loss: 0.655088484287262\n",
      "Validation: Epoch [14], Batch [4/938], Loss: 0.4702431559562683\n",
      "Validation: Epoch [14], Batch [5/938], Loss: 0.40133771300315857\n",
      "Validation: Epoch [14], Batch [6/938], Loss: 0.33901840448379517\n",
      "Validation: Epoch [14], Batch [7/938], Loss: 0.49322545528411865\n",
      "Validation: Epoch [14], Batch [8/938], Loss: 0.6698869466781616\n",
      "Validation: Epoch [14], Batch [9/938], Loss: 0.3828314542770386\n",
      "Validation: Epoch [14], Batch [10/938], Loss: 0.7086689472198486\n",
      "Validation: Epoch [14], Batch [11/938], Loss: 0.5720075368881226\n",
      "Validation: Epoch [14], Batch [12/938], Loss: 0.717081606388092\n",
      "Validation: Epoch [14], Batch [13/938], Loss: 0.3361568748950958\n",
      "Validation: Epoch [14], Batch [14/938], Loss: 0.5007114410400391\n",
      "Validation: Epoch [14], Batch [15/938], Loss: 0.3537601828575134\n",
      "Validation: Epoch [14], Batch [16/938], Loss: 0.43538880348205566\n",
      "Validation: Epoch [14], Batch [17/938], Loss: 0.333096444606781\n",
      "Validation: Epoch [14], Batch [18/938], Loss: 0.4430573582649231\n",
      "Validation: Epoch [14], Batch [19/938], Loss: 0.6158562898635864\n",
      "Validation: Epoch [14], Batch [20/938], Loss: 0.36895227432250977\n",
      "Validation: Epoch [14], Batch [21/938], Loss: 0.5232905149459839\n",
      "Validation: Epoch [14], Batch [22/938], Loss: 0.4127516746520996\n",
      "Validation: Epoch [14], Batch [23/938], Loss: 0.5018160343170166\n",
      "Validation: Epoch [14], Batch [24/938], Loss: 0.3403591811656952\n",
      "Validation: Epoch [14], Batch [25/938], Loss: 0.6916819214820862\n",
      "Validation: Epoch [14], Batch [26/938], Loss: 0.5560635328292847\n",
      "Validation: Epoch [14], Batch [27/938], Loss: 0.5339928269386292\n",
      "Validation: Epoch [14], Batch [28/938], Loss: 0.3990330696105957\n",
      "Validation: Epoch [14], Batch [29/938], Loss: 0.5419464707374573\n",
      "Validation: Epoch [14], Batch [30/938], Loss: 0.6352049112319946\n",
      "Validation: Epoch [14], Batch [31/938], Loss: 0.4577092230319977\n",
      "Validation: Epoch [14], Batch [32/938], Loss: 0.42468735575675964\n",
      "Validation: Epoch [14], Batch [33/938], Loss: 0.482013076543808\n",
      "Validation: Epoch [14], Batch [34/938], Loss: 0.512964129447937\n",
      "Validation: Epoch [14], Batch [35/938], Loss: 0.3144832253456116\n",
      "Validation: Epoch [14], Batch [36/938], Loss: 0.4655323028564453\n",
      "Validation: Epoch [14], Batch [37/938], Loss: 0.43818220496177673\n",
      "Validation: Epoch [14], Batch [38/938], Loss: 0.691737174987793\n",
      "Validation: Epoch [14], Batch [39/938], Loss: 0.6404234170913696\n",
      "Validation: Epoch [14], Batch [40/938], Loss: 0.7215467691421509\n",
      "Validation: Epoch [14], Batch [41/938], Loss: 0.2733106315135956\n",
      "Validation: Epoch [14], Batch [42/938], Loss: 0.5171487927436829\n",
      "Validation: Epoch [14], Batch [43/938], Loss: 0.5189839005470276\n",
      "Validation: Epoch [14], Batch [44/938], Loss: 0.41062626242637634\n",
      "Validation: Epoch [14], Batch [45/938], Loss: 0.6215616464614868\n",
      "Validation: Epoch [14], Batch [46/938], Loss: 0.4924764037132263\n",
      "Validation: Epoch [14], Batch [47/938], Loss: 0.3516179323196411\n",
      "Validation: Epoch [14], Batch [48/938], Loss: 0.4169405400753021\n",
      "Validation: Epoch [14], Batch [49/938], Loss: 0.3664548695087433\n",
      "Validation: Epoch [14], Batch [50/938], Loss: 0.5236621499061584\n",
      "Validation: Epoch [14], Batch [51/938], Loss: 0.4044090211391449\n",
      "Validation: Epoch [14], Batch [52/938], Loss: 0.5624197721481323\n",
      "Validation: Epoch [14], Batch [53/938], Loss: 0.3495612144470215\n",
      "Validation: Epoch [14], Batch [54/938], Loss: 0.5473122596740723\n",
      "Validation: Epoch [14], Batch [55/938], Loss: 0.4681239128112793\n",
      "Validation: Epoch [14], Batch [56/938], Loss: 0.452009916305542\n",
      "Validation: Epoch [14], Batch [57/938], Loss: 0.44642722606658936\n",
      "Validation: Epoch [14], Batch [58/938], Loss: 0.44734814763069153\n",
      "Validation: Epoch [14], Batch [59/938], Loss: 0.5691387057304382\n",
      "Validation: Epoch [14], Batch [60/938], Loss: 0.4130032956600189\n",
      "Validation: Epoch [14], Batch [61/938], Loss: 0.39165809750556946\n",
      "Validation: Epoch [14], Batch [62/938], Loss: 0.46445757150650024\n",
      "Validation: Epoch [14], Batch [63/938], Loss: 0.24925559759140015\n",
      "Validation: Epoch [14], Batch [64/938], Loss: 0.37659206986427307\n",
      "Validation: Epoch [14], Batch [65/938], Loss: 0.33271652460098267\n",
      "Validation: Epoch [14], Batch [66/938], Loss: 0.44248926639556885\n",
      "Validation: Epoch [14], Batch [67/938], Loss: 0.3471458852291107\n",
      "Validation: Epoch [14], Batch [68/938], Loss: 0.5102449655532837\n",
      "Validation: Epoch [14], Batch [69/938], Loss: 0.4880579113960266\n",
      "Validation: Epoch [14], Batch [70/938], Loss: 0.28696975111961365\n",
      "Validation: Epoch [14], Batch [71/938], Loss: 0.4814281165599823\n",
      "Validation: Epoch [14], Batch [72/938], Loss: 0.596167802810669\n",
      "Validation: Epoch [14], Batch [73/938], Loss: 0.442884624004364\n",
      "Validation: Epoch [14], Batch [74/938], Loss: 0.4568950831890106\n",
      "Validation: Epoch [14], Batch [75/938], Loss: 0.4690204858779907\n",
      "Validation: Epoch [14], Batch [76/938], Loss: 0.5725542306900024\n",
      "Validation: Epoch [14], Batch [77/938], Loss: 0.49198195338249207\n",
      "Validation: Epoch [14], Batch [78/938], Loss: 0.5746270418167114\n",
      "Validation: Epoch [14], Batch [79/938], Loss: 0.29115283489227295\n",
      "Validation: Epoch [14], Batch [80/938], Loss: 0.46710479259490967\n",
      "Validation: Epoch [14], Batch [81/938], Loss: 0.5660500526428223\n",
      "Validation: Epoch [14], Batch [82/938], Loss: 0.46764206886291504\n",
      "Validation: Epoch [14], Batch [83/938], Loss: 0.454254686832428\n",
      "Validation: Epoch [14], Batch [84/938], Loss: 0.6509419679641724\n",
      "Validation: Epoch [14], Batch [85/938], Loss: 0.4304230809211731\n",
      "Validation: Epoch [14], Batch [86/938], Loss: 0.43056344985961914\n",
      "Validation: Epoch [14], Batch [87/938], Loss: 0.40153148770332336\n",
      "Validation: Epoch [14], Batch [88/938], Loss: 0.4027547836303711\n",
      "Validation: Epoch [14], Batch [89/938], Loss: 0.5228350758552551\n",
      "Validation: Epoch [14], Batch [90/938], Loss: 0.3612641394138336\n",
      "Validation: Epoch [14], Batch [91/938], Loss: 0.5371321439743042\n",
      "Validation: Epoch [14], Batch [92/938], Loss: 0.4683999717235565\n",
      "Validation: Epoch [14], Batch [93/938], Loss: 0.5325048565864563\n",
      "Validation: Epoch [14], Batch [94/938], Loss: 0.45550158619880676\n",
      "Validation: Epoch [14], Batch [95/938], Loss: 0.8203957080841064\n",
      "Validation: Epoch [14], Batch [96/938], Loss: 0.23886774480342865\n",
      "Validation: Epoch [14], Batch [97/938], Loss: 0.4151571989059448\n",
      "Validation: Epoch [14], Batch [98/938], Loss: 0.45174816250801086\n",
      "Validation: Epoch [14], Batch [99/938], Loss: 0.5818743109703064\n",
      "Validation: Epoch [14], Batch [100/938], Loss: 0.4839304983615875\n",
      "Validation: Epoch [14], Batch [101/938], Loss: 0.516345739364624\n",
      "Validation: Epoch [14], Batch [102/938], Loss: 0.4033140242099762\n",
      "Validation: Epoch [14], Batch [103/938], Loss: 0.27217331528663635\n",
      "Validation: Epoch [14], Batch [104/938], Loss: 0.49496710300445557\n",
      "Validation: Epoch [14], Batch [105/938], Loss: 0.4451141357421875\n",
      "Validation: Epoch [14], Batch [106/938], Loss: 0.7075438499450684\n",
      "Validation: Epoch [14], Batch [107/938], Loss: 0.5257464647293091\n",
      "Validation: Epoch [14], Batch [108/938], Loss: 0.4069799482822418\n",
      "Validation: Epoch [14], Batch [109/938], Loss: 0.28052079677581787\n",
      "Validation: Epoch [14], Batch [110/938], Loss: 0.41167497634887695\n",
      "Validation: Epoch [14], Batch [111/938], Loss: 0.30960750579833984\n",
      "Validation: Epoch [14], Batch [112/938], Loss: 0.5084744691848755\n",
      "Validation: Epoch [14], Batch [113/938], Loss: 0.513008713722229\n",
      "Validation: Epoch [14], Batch [114/938], Loss: 0.5831355452537537\n",
      "Validation: Epoch [14], Batch [115/938], Loss: 0.5855097770690918\n",
      "Validation: Epoch [14], Batch [116/938], Loss: 0.5251302719116211\n",
      "Validation: Epoch [14], Batch [117/938], Loss: 0.5840725302696228\n",
      "Validation: Epoch [14], Batch [118/938], Loss: 0.40831708908081055\n",
      "Validation: Epoch [14], Batch [119/938], Loss: 0.685466468334198\n",
      "Validation: Epoch [14], Batch [120/938], Loss: 0.5241450071334839\n",
      "Validation: Epoch [14], Batch [121/938], Loss: 0.4164181649684906\n",
      "Validation: Epoch [14], Batch [122/938], Loss: 0.34713613986968994\n",
      "Validation: Epoch [14], Batch [123/938], Loss: 0.5138987302780151\n",
      "Validation: Epoch [14], Batch [124/938], Loss: 0.2752705216407776\n",
      "Validation: Epoch [14], Batch [125/938], Loss: 0.5687083601951599\n",
      "Validation: Epoch [14], Batch [126/938], Loss: 0.38981640338897705\n",
      "Validation: Epoch [14], Batch [127/938], Loss: 0.6341700553894043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [128/938], Loss: 0.8117033839225769\n",
      "Validation: Epoch [14], Batch [129/938], Loss: 0.5132469534873962\n",
      "Validation: Epoch [14], Batch [130/938], Loss: 0.4547913074493408\n",
      "Validation: Epoch [14], Batch [131/938], Loss: 0.3739228844642639\n",
      "Validation: Epoch [14], Batch [132/938], Loss: 0.48108482360839844\n",
      "Validation: Epoch [14], Batch [133/938], Loss: 0.4970051050186157\n",
      "Validation: Epoch [14], Batch [134/938], Loss: 0.3388603925704956\n",
      "Validation: Epoch [14], Batch [135/938], Loss: 0.6022030711174011\n",
      "Validation: Epoch [14], Batch [136/938], Loss: 0.31845545768737793\n",
      "Validation: Epoch [14], Batch [137/938], Loss: 0.43219760060310364\n",
      "Validation: Epoch [14], Batch [138/938], Loss: 0.5245869755744934\n",
      "Validation: Epoch [14], Batch [139/938], Loss: 0.6335469484329224\n",
      "Validation: Epoch [14], Batch [140/938], Loss: 0.5237802267074585\n",
      "Validation: Epoch [14], Batch [141/938], Loss: 0.706810712814331\n",
      "Validation: Epoch [14], Batch [142/938], Loss: 0.3446054458618164\n",
      "Validation: Epoch [14], Batch [143/938], Loss: 0.43439218401908875\n",
      "Validation: Epoch [14], Batch [144/938], Loss: 0.40636610984802246\n",
      "Validation: Epoch [14], Batch [145/938], Loss: 0.4788093566894531\n",
      "Validation: Epoch [14], Batch [146/938], Loss: 0.39867010712623596\n",
      "Validation: Epoch [14], Batch [147/938], Loss: 0.5300818681716919\n",
      "Validation: Epoch [14], Batch [148/938], Loss: 0.5031324028968811\n",
      "Validation: Epoch [14], Batch [149/938], Loss: 0.3826453685760498\n",
      "Validation: Epoch [14], Batch [150/938], Loss: 0.4678046703338623\n",
      "Validation: Epoch [14], Batch [151/938], Loss: 0.5624868869781494\n",
      "Validation: Epoch [14], Batch [152/938], Loss: 0.5413687229156494\n",
      "Validation: Epoch [14], Batch [153/938], Loss: 0.33535706996917725\n",
      "Validation: Epoch [14], Batch [154/938], Loss: 0.48573848605155945\n",
      "Validation: Epoch [14], Batch [155/938], Loss: 0.4690333604812622\n",
      "Validation: Epoch [14], Batch [156/938], Loss: 0.4724271297454834\n",
      "Validation: Epoch [14], Batch [157/938], Loss: 0.544442892074585\n",
      "Validation: Epoch [14], Batch [158/938], Loss: 0.46011364459991455\n",
      "Validation: Epoch [14], Batch [159/938], Loss: 0.5111348032951355\n",
      "Validation: Epoch [14], Batch [160/938], Loss: 0.35349464416503906\n",
      "Validation: Epoch [14], Batch [161/938], Loss: 0.4817578196525574\n",
      "Validation: Epoch [14], Batch [162/938], Loss: 0.46588924527168274\n",
      "Validation: Epoch [14], Batch [163/938], Loss: 0.34481632709503174\n",
      "Validation: Epoch [14], Batch [164/938], Loss: 0.5562323927879333\n",
      "Validation: Epoch [14], Batch [165/938], Loss: 0.47880837321281433\n",
      "Validation: Epoch [14], Batch [166/938], Loss: 0.5319309234619141\n",
      "Validation: Epoch [14], Batch [167/938], Loss: 0.544221818447113\n",
      "Validation: Epoch [14], Batch [168/938], Loss: 0.409900426864624\n",
      "Validation: Epoch [14], Batch [169/938], Loss: 0.6761443018913269\n",
      "Validation: Epoch [14], Batch [170/938], Loss: 0.5373732447624207\n",
      "Validation: Epoch [14], Batch [171/938], Loss: 0.4905904233455658\n",
      "Validation: Epoch [14], Batch [172/938], Loss: 0.43896812200546265\n",
      "Validation: Epoch [14], Batch [173/938], Loss: 0.4597139358520508\n",
      "Validation: Epoch [14], Batch [174/938], Loss: 0.33739030361175537\n",
      "Validation: Epoch [14], Batch [175/938], Loss: 0.5006521940231323\n",
      "Validation: Epoch [14], Batch [176/938], Loss: 0.4212658703327179\n",
      "Validation: Epoch [14], Batch [177/938], Loss: 0.33113324642181396\n",
      "Validation: Epoch [14], Batch [178/938], Loss: 0.3908836841583252\n",
      "Validation: Epoch [14], Batch [179/938], Loss: 0.4732740521430969\n",
      "Validation: Epoch [14], Batch [180/938], Loss: 0.46679455041885376\n",
      "Validation: Epoch [14], Batch [181/938], Loss: 0.32507461309432983\n",
      "Validation: Epoch [14], Batch [182/938], Loss: 0.48392120003700256\n",
      "Validation: Epoch [14], Batch [183/938], Loss: 0.4742093086242676\n",
      "Validation: Epoch [14], Batch [184/938], Loss: 0.34960633516311646\n",
      "Validation: Epoch [14], Batch [185/938], Loss: 0.3811674416065216\n",
      "Validation: Epoch [14], Batch [186/938], Loss: 0.31060364842414856\n",
      "Validation: Epoch [14], Batch [187/938], Loss: 0.717909574508667\n",
      "Validation: Epoch [14], Batch [188/938], Loss: 0.4895543158054352\n",
      "Validation: Epoch [14], Batch [189/938], Loss: 0.41574540734291077\n",
      "Validation: Epoch [14], Batch [190/938], Loss: 0.6145986914634705\n",
      "Validation: Epoch [14], Batch [191/938], Loss: 0.4550183117389679\n",
      "Validation: Epoch [14], Batch [192/938], Loss: 0.5301452279090881\n",
      "Validation: Epoch [14], Batch [193/938], Loss: 0.6892818808555603\n",
      "Validation: Epoch [14], Batch [194/938], Loss: 0.3931994140148163\n",
      "Validation: Epoch [14], Batch [195/938], Loss: 0.47594642639160156\n",
      "Validation: Epoch [14], Batch [196/938], Loss: 0.6194947957992554\n",
      "Validation: Epoch [14], Batch [197/938], Loss: 0.417402446269989\n",
      "Validation: Epoch [14], Batch [198/938], Loss: 0.631109356880188\n",
      "Validation: Epoch [14], Batch [199/938], Loss: 0.5466373562812805\n",
      "Validation: Epoch [14], Batch [200/938], Loss: 0.48586925864219666\n",
      "Validation: Epoch [14], Batch [201/938], Loss: 0.5184478759765625\n",
      "Validation: Epoch [14], Batch [202/938], Loss: 0.4424648880958557\n",
      "Validation: Epoch [14], Batch [203/938], Loss: 0.42384207248687744\n",
      "Validation: Epoch [14], Batch [204/938], Loss: 0.4724527597427368\n",
      "Validation: Epoch [14], Batch [205/938], Loss: 0.4243272840976715\n",
      "Validation: Epoch [14], Batch [206/938], Loss: 0.31449979543685913\n",
      "Validation: Epoch [14], Batch [207/938], Loss: 0.4303584694862366\n",
      "Validation: Epoch [14], Batch [208/938], Loss: 0.6002439856529236\n",
      "Validation: Epoch [14], Batch [209/938], Loss: 0.2810012102127075\n",
      "Validation: Epoch [14], Batch [210/938], Loss: 0.5508332252502441\n",
      "Validation: Epoch [14], Batch [211/938], Loss: 0.40698373317718506\n",
      "Validation: Epoch [14], Batch [212/938], Loss: 0.4571008086204529\n",
      "Validation: Epoch [14], Batch [213/938], Loss: 0.4235524535179138\n",
      "Validation: Epoch [14], Batch [214/938], Loss: 0.448783814907074\n",
      "Validation: Epoch [14], Batch [215/938], Loss: 0.47635141015052795\n",
      "Validation: Epoch [14], Batch [216/938], Loss: 0.4526713490486145\n",
      "Validation: Epoch [14], Batch [217/938], Loss: 0.4156479239463806\n",
      "Validation: Epoch [14], Batch [218/938], Loss: 0.530493974685669\n",
      "Validation: Epoch [14], Batch [219/938], Loss: 0.43219074606895447\n",
      "Validation: Epoch [14], Batch [220/938], Loss: 0.342060387134552\n",
      "Validation: Epoch [14], Batch [221/938], Loss: 0.5066853761672974\n",
      "Validation: Epoch [14], Batch [222/938], Loss: 0.5419743657112122\n",
      "Validation: Epoch [14], Batch [223/938], Loss: 0.440254271030426\n",
      "Validation: Epoch [14], Batch [224/938], Loss: 0.6815443634986877\n",
      "Validation: Epoch [14], Batch [225/938], Loss: 0.4020169973373413\n",
      "Validation: Epoch [14], Batch [226/938], Loss: 0.42723017930984497\n",
      "Validation: Epoch [14], Batch [227/938], Loss: 0.3771275579929352\n",
      "Validation: Epoch [14], Batch [228/938], Loss: 0.4187294542789459\n",
      "Validation: Epoch [14], Batch [229/938], Loss: 0.5989638566970825\n",
      "Validation: Epoch [14], Batch [230/938], Loss: 0.48538392782211304\n",
      "Validation: Epoch [14], Batch [231/938], Loss: 0.382061630487442\n",
      "Validation: Epoch [14], Batch [232/938], Loss: 0.48359429836273193\n",
      "Validation: Epoch [14], Batch [233/938], Loss: 0.47097718715667725\n",
      "Validation: Epoch [14], Batch [234/938], Loss: 0.5674792528152466\n",
      "Validation: Epoch [14], Batch [235/938], Loss: 0.4678969979286194\n",
      "Validation: Epoch [14], Batch [236/938], Loss: 0.43661120533943176\n",
      "Validation: Epoch [14], Batch [237/938], Loss: 0.42967215180397034\n",
      "Validation: Epoch [14], Batch [238/938], Loss: 0.39963120222091675\n",
      "Validation: Epoch [14], Batch [239/938], Loss: 0.4695807099342346\n",
      "Validation: Epoch [14], Batch [240/938], Loss: 0.3283972144126892\n",
      "Validation: Epoch [14], Batch [241/938], Loss: 0.630240797996521\n",
      "Validation: Epoch [14], Batch [242/938], Loss: 0.4835551083087921\n",
      "Validation: Epoch [14], Batch [243/938], Loss: 0.577057421207428\n",
      "Validation: Epoch [14], Batch [244/938], Loss: 0.23799826204776764\n",
      "Validation: Epoch [14], Batch [245/938], Loss: 0.4698883891105652\n",
      "Validation: Epoch [14], Batch [246/938], Loss: 0.4857670068740845\n",
      "Validation: Epoch [14], Batch [247/938], Loss: 0.303373783826828\n",
      "Validation: Epoch [14], Batch [248/938], Loss: 0.5978201031684875\n",
      "Validation: Epoch [14], Batch [249/938], Loss: 0.6631152629852295\n",
      "Validation: Epoch [14], Batch [250/938], Loss: 0.3980945944786072\n",
      "Validation: Epoch [14], Batch [251/938], Loss: 0.38493812084198\n",
      "Validation: Epoch [14], Batch [252/938], Loss: 0.36100563406944275\n",
      "Validation: Epoch [14], Batch [253/938], Loss: 0.6259506344795227\n",
      "Validation: Epoch [14], Batch [254/938], Loss: 0.39600589871406555\n",
      "Validation: Epoch [14], Batch [255/938], Loss: 0.5818681120872498\n",
      "Validation: Epoch [14], Batch [256/938], Loss: 0.4742651879787445\n",
      "Validation: Epoch [14], Batch [257/938], Loss: 0.5418350696563721\n",
      "Validation: Epoch [14], Batch [258/938], Loss: 0.4756036400794983\n",
      "Validation: Epoch [14], Batch [259/938], Loss: 0.534893810749054\n",
      "Validation: Epoch [14], Batch [260/938], Loss: 0.4342327117919922\n",
      "Validation: Epoch [14], Batch [261/938], Loss: 0.40483978390693665\n",
      "Validation: Epoch [14], Batch [262/938], Loss: 0.4097480773925781\n",
      "Validation: Epoch [14], Batch [263/938], Loss: 0.3844039738178253\n",
      "Validation: Epoch [14], Batch [264/938], Loss: 0.49309927225112915\n",
      "Validation: Epoch [14], Batch [265/938], Loss: 0.22880081832408905\n",
      "Validation: Epoch [14], Batch [266/938], Loss: 0.5900720357894897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [267/938], Loss: 0.568755030632019\n",
      "Validation: Epoch [14], Batch [268/938], Loss: 0.6048330068588257\n",
      "Validation: Epoch [14], Batch [269/938], Loss: 0.569743275642395\n",
      "Validation: Epoch [14], Batch [270/938], Loss: 0.4840520918369293\n",
      "Validation: Epoch [14], Batch [271/938], Loss: 0.6013959646224976\n",
      "Validation: Epoch [14], Batch [272/938], Loss: 0.365648090839386\n",
      "Validation: Epoch [14], Batch [273/938], Loss: 0.4113985002040863\n",
      "Validation: Epoch [14], Batch [274/938], Loss: 0.2717347741127014\n",
      "Validation: Epoch [14], Batch [275/938], Loss: 0.6865576505661011\n",
      "Validation: Epoch [14], Batch [276/938], Loss: 0.4641636610031128\n",
      "Validation: Epoch [14], Batch [277/938], Loss: 0.43921878933906555\n",
      "Validation: Epoch [14], Batch [278/938], Loss: 0.3102599084377289\n",
      "Validation: Epoch [14], Batch [279/938], Loss: 0.42914220690727234\n",
      "Validation: Epoch [14], Batch [280/938], Loss: 0.5208602547645569\n",
      "Validation: Epoch [14], Batch [281/938], Loss: 0.38733118772506714\n",
      "Validation: Epoch [14], Batch [282/938], Loss: 0.34014391899108887\n",
      "Validation: Epoch [14], Batch [283/938], Loss: 0.474135160446167\n",
      "Validation: Epoch [14], Batch [284/938], Loss: 0.4713599383831024\n",
      "Validation: Epoch [14], Batch [285/938], Loss: 0.27565571665763855\n",
      "Validation: Epoch [14], Batch [286/938], Loss: 0.573005199432373\n",
      "Validation: Epoch [14], Batch [287/938], Loss: 0.2641703486442566\n",
      "Validation: Epoch [14], Batch [288/938], Loss: 0.3256089985370636\n",
      "Validation: Epoch [14], Batch [289/938], Loss: 0.6360597610473633\n",
      "Validation: Epoch [14], Batch [290/938], Loss: 0.5488875508308411\n",
      "Validation: Epoch [14], Batch [291/938], Loss: 0.4532991051673889\n",
      "Validation: Epoch [14], Batch [292/938], Loss: 0.2899775803089142\n",
      "Validation: Epoch [14], Batch [293/938], Loss: 0.2844628393650055\n",
      "Validation: Epoch [14], Batch [294/938], Loss: 0.5230295658111572\n",
      "Validation: Epoch [14], Batch [295/938], Loss: 0.37825173139572144\n",
      "Validation: Epoch [14], Batch [296/938], Loss: 0.45119747519493103\n",
      "Validation: Epoch [14], Batch [297/938], Loss: 0.5135164260864258\n",
      "Validation: Epoch [14], Batch [298/938], Loss: 0.33982160687446594\n",
      "Validation: Epoch [14], Batch [299/938], Loss: 0.3518518805503845\n",
      "Validation: Epoch [14], Batch [300/938], Loss: 0.4615010917186737\n",
      "Validation: Epoch [14], Batch [301/938], Loss: 0.4069352447986603\n",
      "Validation: Epoch [14], Batch [302/938], Loss: 0.4771619141101837\n",
      "Validation: Epoch [14], Batch [303/938], Loss: 0.4573756456375122\n",
      "Validation: Epoch [14], Batch [304/938], Loss: 0.5789202451705933\n",
      "Validation: Epoch [14], Batch [305/938], Loss: 0.5522541999816895\n",
      "Validation: Epoch [14], Batch [306/938], Loss: 0.4039841890335083\n",
      "Validation: Epoch [14], Batch [307/938], Loss: 0.6061047315597534\n",
      "Validation: Epoch [14], Batch [308/938], Loss: 0.45450520515441895\n",
      "Validation: Epoch [14], Batch [309/938], Loss: 0.41258978843688965\n",
      "Validation: Epoch [14], Batch [310/938], Loss: 0.4415469765663147\n",
      "Validation: Epoch [14], Batch [311/938], Loss: 0.5352917909622192\n",
      "Validation: Epoch [14], Batch [312/938], Loss: 0.48318982124328613\n",
      "Validation: Epoch [14], Batch [313/938], Loss: 0.6135554909706116\n",
      "Validation: Epoch [14], Batch [314/938], Loss: 0.4190100431442261\n",
      "Validation: Epoch [14], Batch [315/938], Loss: 0.44684213399887085\n",
      "Validation: Epoch [14], Batch [316/938], Loss: 0.2702621817588806\n",
      "Validation: Epoch [14], Batch [317/938], Loss: 0.40726834535598755\n",
      "Validation: Epoch [14], Batch [318/938], Loss: 0.4432772696018219\n",
      "Validation: Epoch [14], Batch [319/938], Loss: 0.5676062107086182\n",
      "Validation: Epoch [14], Batch [320/938], Loss: 0.35935842990875244\n",
      "Validation: Epoch [14], Batch [321/938], Loss: 0.3973233699798584\n",
      "Validation: Epoch [14], Batch [322/938], Loss: 0.39375051856040955\n",
      "Validation: Epoch [14], Batch [323/938], Loss: 0.4618004560470581\n",
      "Validation: Epoch [14], Batch [324/938], Loss: 0.4820159673690796\n",
      "Validation: Epoch [14], Batch [325/938], Loss: 0.38062751293182373\n",
      "Validation: Epoch [14], Batch [326/938], Loss: 0.49301743507385254\n",
      "Validation: Epoch [14], Batch [327/938], Loss: 0.47968289256095886\n",
      "Validation: Epoch [14], Batch [328/938], Loss: 0.6062649488449097\n",
      "Validation: Epoch [14], Batch [329/938], Loss: 0.5819001197814941\n",
      "Validation: Epoch [14], Batch [330/938], Loss: 0.3100510239601135\n",
      "Validation: Epoch [14], Batch [331/938], Loss: 0.38913673162460327\n",
      "Validation: Epoch [14], Batch [332/938], Loss: 0.5556297302246094\n",
      "Validation: Epoch [14], Batch [333/938], Loss: 0.4552762508392334\n",
      "Validation: Epoch [14], Batch [334/938], Loss: 0.43326622247695923\n",
      "Validation: Epoch [14], Batch [335/938], Loss: 0.5463153123855591\n",
      "Validation: Epoch [14], Batch [336/938], Loss: 0.36768829822540283\n",
      "Validation: Epoch [14], Batch [337/938], Loss: 0.48451513051986694\n",
      "Validation: Epoch [14], Batch [338/938], Loss: 0.5381565093994141\n",
      "Validation: Epoch [14], Batch [339/938], Loss: 0.3499789834022522\n",
      "Validation: Epoch [14], Batch [340/938], Loss: 0.46261218190193176\n",
      "Validation: Epoch [14], Batch [341/938], Loss: 0.5672889947891235\n",
      "Validation: Epoch [14], Batch [342/938], Loss: 0.5316975116729736\n",
      "Validation: Epoch [14], Batch [343/938], Loss: 0.4918513000011444\n",
      "Validation: Epoch [14], Batch [344/938], Loss: 0.4458158016204834\n",
      "Validation: Epoch [14], Batch [345/938], Loss: 0.5030019879341125\n",
      "Validation: Epoch [14], Batch [346/938], Loss: 0.682853639125824\n",
      "Validation: Epoch [14], Batch [347/938], Loss: 0.8765297532081604\n",
      "Validation: Epoch [14], Batch [348/938], Loss: 0.4629260301589966\n",
      "Validation: Epoch [14], Batch [349/938], Loss: 0.3871356248855591\n",
      "Validation: Epoch [14], Batch [350/938], Loss: 0.4262421131134033\n",
      "Validation: Epoch [14], Batch [351/938], Loss: 0.4979846477508545\n",
      "Validation: Epoch [14], Batch [352/938], Loss: 0.41655731201171875\n",
      "Validation: Epoch [14], Batch [353/938], Loss: 0.4817003607749939\n",
      "Validation: Epoch [14], Batch [354/938], Loss: 0.4909743666648865\n",
      "Validation: Epoch [14], Batch [355/938], Loss: 0.49848228693008423\n",
      "Validation: Epoch [14], Batch [356/938], Loss: 0.5586352944374084\n",
      "Validation: Epoch [14], Batch [357/938], Loss: 0.4865565598011017\n",
      "Validation: Epoch [14], Batch [358/938], Loss: 0.629681408405304\n",
      "Validation: Epoch [14], Batch [359/938], Loss: 0.46507641673088074\n",
      "Validation: Epoch [14], Batch [360/938], Loss: 0.4452272057533264\n",
      "Validation: Epoch [14], Batch [361/938], Loss: 0.2233683317899704\n",
      "Validation: Epoch [14], Batch [362/938], Loss: 0.527640700340271\n",
      "Validation: Epoch [14], Batch [363/938], Loss: 0.5288742184638977\n",
      "Validation: Epoch [14], Batch [364/938], Loss: 0.39864593744277954\n",
      "Validation: Epoch [14], Batch [365/938], Loss: 0.3262617290019989\n",
      "Validation: Epoch [14], Batch [366/938], Loss: 0.49098700284957886\n",
      "Validation: Epoch [14], Batch [367/938], Loss: 0.36257535219192505\n",
      "Validation: Epoch [14], Batch [368/938], Loss: 0.5801019072532654\n",
      "Validation: Epoch [14], Batch [369/938], Loss: 0.5009172558784485\n",
      "Validation: Epoch [14], Batch [370/938], Loss: 0.4171774983406067\n",
      "Validation: Epoch [14], Batch [371/938], Loss: 0.4370701014995575\n",
      "Validation: Epoch [14], Batch [372/938], Loss: 0.5834174156188965\n",
      "Validation: Epoch [14], Batch [373/938], Loss: 0.43957817554473877\n",
      "Validation: Epoch [14], Batch [374/938], Loss: 0.38676124811172485\n",
      "Validation: Epoch [14], Batch [375/938], Loss: 0.3933396637439728\n",
      "Validation: Epoch [14], Batch [376/938], Loss: 0.3953908085823059\n",
      "Validation: Epoch [14], Batch [377/938], Loss: 0.47459614276885986\n",
      "Validation: Epoch [14], Batch [378/938], Loss: 0.42116302251815796\n",
      "Validation: Epoch [14], Batch [379/938], Loss: 0.561151921749115\n",
      "Validation: Epoch [14], Batch [380/938], Loss: 0.47319409251213074\n",
      "Validation: Epoch [14], Batch [381/938], Loss: 0.4414908289909363\n",
      "Validation: Epoch [14], Batch [382/938], Loss: 0.5239322185516357\n",
      "Validation: Epoch [14], Batch [383/938], Loss: 0.3608032464981079\n",
      "Validation: Epoch [14], Batch [384/938], Loss: 0.35534030199050903\n",
      "Validation: Epoch [14], Batch [385/938], Loss: 0.48044124245643616\n",
      "Validation: Epoch [14], Batch [386/938], Loss: 0.38196974992752075\n",
      "Validation: Epoch [14], Batch [387/938], Loss: 0.6565979719161987\n",
      "Validation: Epoch [14], Batch [388/938], Loss: 0.324589341878891\n",
      "Validation: Epoch [14], Batch [389/938], Loss: 0.4607290029525757\n",
      "Validation: Epoch [14], Batch [390/938], Loss: 0.4564749002456665\n",
      "Validation: Epoch [14], Batch [391/938], Loss: 0.4225348234176636\n",
      "Validation: Epoch [14], Batch [392/938], Loss: 0.4572140574455261\n",
      "Validation: Epoch [14], Batch [393/938], Loss: 0.42981353402137756\n",
      "Validation: Epoch [14], Batch [394/938], Loss: 0.5958601236343384\n",
      "Validation: Epoch [14], Batch [395/938], Loss: 0.36722052097320557\n",
      "Validation: Epoch [14], Batch [396/938], Loss: 0.5556446313858032\n",
      "Validation: Epoch [14], Batch [397/938], Loss: 0.5312798023223877\n",
      "Validation: Epoch [14], Batch [398/938], Loss: 0.5859268307685852\n",
      "Validation: Epoch [14], Batch [399/938], Loss: 0.4344702661037445\n",
      "Validation: Epoch [14], Batch [400/938], Loss: 0.2832251489162445\n",
      "Validation: Epoch [14], Batch [401/938], Loss: 0.3813498914241791\n",
      "Validation: Epoch [14], Batch [402/938], Loss: 0.40989789366722107\n",
      "Validation: Epoch [14], Batch [403/938], Loss: 0.7338555455207825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [404/938], Loss: 0.583892822265625\n",
      "Validation: Epoch [14], Batch [405/938], Loss: 0.4811175763607025\n",
      "Validation: Epoch [14], Batch [406/938], Loss: 0.4746173620223999\n",
      "Validation: Epoch [14], Batch [407/938], Loss: 0.49949878454208374\n",
      "Validation: Epoch [14], Batch [408/938], Loss: 0.4215533137321472\n",
      "Validation: Epoch [14], Batch [409/938], Loss: 0.46508312225341797\n",
      "Validation: Epoch [14], Batch [410/938], Loss: 0.5233265161514282\n",
      "Validation: Epoch [14], Batch [411/938], Loss: 0.5053879022598267\n",
      "Validation: Epoch [14], Batch [412/938], Loss: 0.29162272810935974\n",
      "Validation: Epoch [14], Batch [413/938], Loss: 0.7034401893615723\n",
      "Validation: Epoch [14], Batch [414/938], Loss: 0.48296672105789185\n",
      "Validation: Epoch [14], Batch [415/938], Loss: 0.4249378442764282\n",
      "Validation: Epoch [14], Batch [416/938], Loss: 0.651138424873352\n",
      "Validation: Epoch [14], Batch [417/938], Loss: 0.5518273115158081\n",
      "Validation: Epoch [14], Batch [418/938], Loss: 0.5591681599617004\n",
      "Validation: Epoch [14], Batch [419/938], Loss: 0.4093511700630188\n",
      "Validation: Epoch [14], Batch [420/938], Loss: 0.3300640881061554\n",
      "Validation: Epoch [14], Batch [421/938], Loss: 0.38628053665161133\n",
      "Validation: Epoch [14], Batch [422/938], Loss: 0.6948983669281006\n",
      "Validation: Epoch [14], Batch [423/938], Loss: 0.7478759288787842\n",
      "Validation: Epoch [14], Batch [424/938], Loss: 0.37520337104797363\n",
      "Validation: Epoch [14], Batch [425/938], Loss: 0.5120284557342529\n",
      "Validation: Epoch [14], Batch [426/938], Loss: 0.543494462966919\n",
      "Validation: Epoch [14], Batch [427/938], Loss: 0.4107068181037903\n",
      "Validation: Epoch [14], Batch [428/938], Loss: 0.5563109517097473\n",
      "Validation: Epoch [14], Batch [429/938], Loss: 0.4493958652019501\n",
      "Validation: Epoch [14], Batch [430/938], Loss: 0.3554912209510803\n",
      "Validation: Epoch [14], Batch [431/938], Loss: 0.4213787615299225\n",
      "Validation: Epoch [14], Batch [432/938], Loss: 0.5543037056922913\n",
      "Validation: Epoch [14], Batch [433/938], Loss: 0.7018665671348572\n",
      "Validation: Epoch [14], Batch [434/938], Loss: 0.4331698715686798\n",
      "Validation: Epoch [14], Batch [435/938], Loss: 0.5042216777801514\n",
      "Validation: Epoch [14], Batch [436/938], Loss: 0.5738932490348816\n",
      "Validation: Epoch [14], Batch [437/938], Loss: 0.481391966342926\n",
      "Validation: Epoch [14], Batch [438/938], Loss: 0.4306276738643646\n",
      "Validation: Epoch [14], Batch [439/938], Loss: 0.8059494495391846\n",
      "Validation: Epoch [14], Batch [440/938], Loss: 0.6766924262046814\n",
      "Validation: Epoch [14], Batch [441/938], Loss: 0.45340901613235474\n",
      "Validation: Epoch [14], Batch [442/938], Loss: 0.28564682602882385\n",
      "Validation: Epoch [14], Batch [443/938], Loss: 0.7348723411560059\n",
      "Validation: Epoch [14], Batch [444/938], Loss: 0.3710750341415405\n",
      "Validation: Epoch [14], Batch [445/938], Loss: 0.4585859775543213\n",
      "Validation: Epoch [14], Batch [446/938], Loss: 0.41494739055633545\n",
      "Validation: Epoch [14], Batch [447/938], Loss: 0.5548794269561768\n",
      "Validation: Epoch [14], Batch [448/938], Loss: 0.35767799615859985\n",
      "Validation: Epoch [14], Batch [449/938], Loss: 0.6252747774124146\n",
      "Validation: Epoch [14], Batch [450/938], Loss: 0.44878628849983215\n",
      "Validation: Epoch [14], Batch [451/938], Loss: 0.2917754054069519\n",
      "Validation: Epoch [14], Batch [452/938], Loss: 0.36610931158065796\n",
      "Validation: Epoch [14], Batch [453/938], Loss: 0.5133533477783203\n",
      "Validation: Epoch [14], Batch [454/938], Loss: 0.4486635625362396\n",
      "Validation: Epoch [14], Batch [455/938], Loss: 0.48869913816452026\n",
      "Validation: Epoch [14], Batch [456/938], Loss: 0.3587243854999542\n",
      "Validation: Epoch [14], Batch [457/938], Loss: 0.3946310877799988\n",
      "Validation: Epoch [14], Batch [458/938], Loss: 0.5530592203140259\n",
      "Validation: Epoch [14], Batch [459/938], Loss: 0.4082113206386566\n",
      "Validation: Epoch [14], Batch [460/938], Loss: 0.47877076268196106\n",
      "Validation: Epoch [14], Batch [461/938], Loss: 0.37924227118492126\n",
      "Validation: Epoch [14], Batch [462/938], Loss: 0.4750871956348419\n",
      "Validation: Epoch [14], Batch [463/938], Loss: 0.39534837007522583\n",
      "Validation: Epoch [14], Batch [464/938], Loss: 0.5542356371879578\n",
      "Validation: Epoch [14], Batch [465/938], Loss: 0.6532099843025208\n",
      "Validation: Epoch [14], Batch [466/938], Loss: 0.2505767345428467\n",
      "Validation: Epoch [14], Batch [467/938], Loss: 0.48455533385276794\n",
      "Validation: Epoch [14], Batch [468/938], Loss: 0.5873605012893677\n",
      "Validation: Epoch [14], Batch [469/938], Loss: 0.49037614464759827\n",
      "Validation: Epoch [14], Batch [470/938], Loss: 0.4830288887023926\n",
      "Validation: Epoch [14], Batch [471/938], Loss: 0.40961015224456787\n",
      "Validation: Epoch [14], Batch [472/938], Loss: 0.28360041975975037\n",
      "Validation: Epoch [14], Batch [473/938], Loss: 0.3931180238723755\n",
      "Validation: Epoch [14], Batch [474/938], Loss: 0.40099257230758667\n",
      "Validation: Epoch [14], Batch [475/938], Loss: 0.3580513894557953\n",
      "Validation: Epoch [14], Batch [476/938], Loss: 0.45566025376319885\n",
      "Validation: Epoch [14], Batch [477/938], Loss: 0.5594120025634766\n",
      "Validation: Epoch [14], Batch [478/938], Loss: 0.3198796510696411\n",
      "Validation: Epoch [14], Batch [479/938], Loss: 0.5498692989349365\n",
      "Validation: Epoch [14], Batch [480/938], Loss: 0.3280488848686218\n",
      "Validation: Epoch [14], Batch [481/938], Loss: 0.6867092847824097\n",
      "Validation: Epoch [14], Batch [482/938], Loss: 0.49753350019454956\n",
      "Validation: Epoch [14], Batch [483/938], Loss: 0.4130263328552246\n",
      "Validation: Epoch [14], Batch [484/938], Loss: 0.4433678984642029\n",
      "Validation: Epoch [14], Batch [485/938], Loss: 0.6973081231117249\n",
      "Validation: Epoch [14], Batch [486/938], Loss: 0.473417192697525\n",
      "Validation: Epoch [14], Batch [487/938], Loss: 0.5025237202644348\n",
      "Validation: Epoch [14], Batch [488/938], Loss: 0.4269104301929474\n",
      "Validation: Epoch [14], Batch [489/938], Loss: 0.3666348457336426\n",
      "Validation: Epoch [14], Batch [490/938], Loss: 0.46966552734375\n",
      "Validation: Epoch [14], Batch [491/938], Loss: 0.5806890726089478\n",
      "Validation: Epoch [14], Batch [492/938], Loss: 0.4630338251590729\n",
      "Validation: Epoch [14], Batch [493/938], Loss: 0.3338139057159424\n",
      "Validation: Epoch [14], Batch [494/938], Loss: 0.5226953029632568\n",
      "Validation: Epoch [14], Batch [495/938], Loss: 0.5175519585609436\n",
      "Validation: Epoch [14], Batch [496/938], Loss: 0.3568655550479889\n",
      "Validation: Epoch [14], Batch [497/938], Loss: 0.3887445330619812\n",
      "Validation: Epoch [14], Batch [498/938], Loss: 0.43699702620506287\n",
      "Validation: Epoch [14], Batch [499/938], Loss: 0.4443020224571228\n",
      "Validation: Epoch [14], Batch [500/938], Loss: 0.353532075881958\n",
      "Validation: Epoch [14], Batch [501/938], Loss: 0.5628591775894165\n",
      "Validation: Epoch [14], Batch [502/938], Loss: 0.6386351585388184\n",
      "Validation: Epoch [14], Batch [503/938], Loss: 0.4931250512599945\n",
      "Validation: Epoch [14], Batch [504/938], Loss: 0.48571109771728516\n",
      "Validation: Epoch [14], Batch [505/938], Loss: 0.508186936378479\n",
      "Validation: Epoch [14], Batch [506/938], Loss: 0.6430846452713013\n",
      "Validation: Epoch [14], Batch [507/938], Loss: 0.511620044708252\n",
      "Validation: Epoch [14], Batch [508/938], Loss: 0.6079339981079102\n",
      "Validation: Epoch [14], Batch [509/938], Loss: 0.3652297258377075\n",
      "Validation: Epoch [14], Batch [510/938], Loss: 0.4685620665550232\n",
      "Validation: Epoch [14], Batch [511/938], Loss: 0.4167829155921936\n",
      "Validation: Epoch [14], Batch [512/938], Loss: 0.4108988046646118\n",
      "Validation: Epoch [14], Batch [513/938], Loss: 0.4406779706478119\n",
      "Validation: Epoch [14], Batch [514/938], Loss: 0.27732500433921814\n",
      "Validation: Epoch [14], Batch [515/938], Loss: 0.3892195522785187\n",
      "Validation: Epoch [14], Batch [516/938], Loss: 0.39457035064697266\n",
      "Validation: Epoch [14], Batch [517/938], Loss: 0.42650094628334045\n",
      "Validation: Epoch [14], Batch [518/938], Loss: 0.4641442894935608\n",
      "Validation: Epoch [14], Batch [519/938], Loss: 0.3619253933429718\n",
      "Validation: Epoch [14], Batch [520/938], Loss: 0.31320029497146606\n",
      "Validation: Epoch [14], Batch [521/938], Loss: 0.43830662965774536\n",
      "Validation: Epoch [14], Batch [522/938], Loss: 0.48657384514808655\n",
      "Validation: Epoch [14], Batch [523/938], Loss: 0.37250375747680664\n",
      "Validation: Epoch [14], Batch [524/938], Loss: 0.41948384046554565\n",
      "Validation: Epoch [14], Batch [525/938], Loss: 0.7332746982574463\n",
      "Validation: Epoch [14], Batch [526/938], Loss: 0.5848990082740784\n",
      "Validation: Epoch [14], Batch [527/938], Loss: 0.2875635623931885\n",
      "Validation: Epoch [14], Batch [528/938], Loss: 0.41356244683265686\n",
      "Validation: Epoch [14], Batch [529/938], Loss: 0.4324447512626648\n",
      "Validation: Epoch [14], Batch [530/938], Loss: 0.4673176407814026\n",
      "Validation: Epoch [14], Batch [531/938], Loss: 0.5354954600334167\n",
      "Validation: Epoch [14], Batch [532/938], Loss: 0.5367778539657593\n",
      "Validation: Epoch [14], Batch [533/938], Loss: 0.40244442224502563\n",
      "Validation: Epoch [14], Batch [534/938], Loss: 0.366970956325531\n",
      "Validation: Epoch [14], Batch [535/938], Loss: 0.4716142416000366\n",
      "Validation: Epoch [14], Batch [536/938], Loss: 0.4533318281173706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [537/938], Loss: 0.5871908068656921\n",
      "Validation: Epoch [14], Batch [538/938], Loss: 0.28166624903678894\n",
      "Validation: Epoch [14], Batch [539/938], Loss: 0.43862390518188477\n",
      "Validation: Epoch [14], Batch [540/938], Loss: 0.43504598736763\n",
      "Validation: Epoch [14], Batch [541/938], Loss: 0.42916035652160645\n",
      "Validation: Epoch [14], Batch [542/938], Loss: 0.4404066205024719\n",
      "Validation: Epoch [14], Batch [543/938], Loss: 0.4938916563987732\n",
      "Validation: Epoch [14], Batch [544/938], Loss: 0.4773876368999481\n",
      "Validation: Epoch [14], Batch [545/938], Loss: 0.4688867926597595\n",
      "Validation: Epoch [14], Batch [546/938], Loss: 0.5078486204147339\n",
      "Validation: Epoch [14], Batch [547/938], Loss: 0.4509757161140442\n",
      "Validation: Epoch [14], Batch [548/938], Loss: 0.4273034334182739\n",
      "Validation: Epoch [14], Batch [549/938], Loss: 0.5823343992233276\n",
      "Validation: Epoch [14], Batch [550/938], Loss: 0.5080792903900146\n",
      "Validation: Epoch [14], Batch [551/938], Loss: 0.38833779096603394\n",
      "Validation: Epoch [14], Batch [552/938], Loss: 0.4021676778793335\n",
      "Validation: Epoch [14], Batch [553/938], Loss: 0.47165030241012573\n",
      "Validation: Epoch [14], Batch [554/938], Loss: 0.5032587647438049\n",
      "Validation: Epoch [14], Batch [555/938], Loss: 0.5215193629264832\n",
      "Validation: Epoch [14], Batch [556/938], Loss: 0.595201313495636\n",
      "Validation: Epoch [14], Batch [557/938], Loss: 0.476056307554245\n",
      "Validation: Epoch [14], Batch [558/938], Loss: 0.3341498076915741\n",
      "Validation: Epoch [14], Batch [559/938], Loss: 0.5962749719619751\n",
      "Validation: Epoch [14], Batch [560/938], Loss: 0.4792450964450836\n",
      "Validation: Epoch [14], Batch [561/938], Loss: 0.4542854428291321\n",
      "Validation: Epoch [14], Batch [562/938], Loss: 0.6029137372970581\n",
      "Validation: Epoch [14], Batch [563/938], Loss: 0.3805926740169525\n",
      "Validation: Epoch [14], Batch [564/938], Loss: 0.44831565022468567\n",
      "Validation: Epoch [14], Batch [565/938], Loss: 0.2892609238624573\n",
      "Validation: Epoch [14], Batch [566/938], Loss: 0.4880845546722412\n",
      "Validation: Epoch [14], Batch [567/938], Loss: 0.44191205501556396\n",
      "Validation: Epoch [14], Batch [568/938], Loss: 0.4226055145263672\n",
      "Validation: Epoch [14], Batch [569/938], Loss: 0.4669419229030609\n",
      "Validation: Epoch [14], Batch [570/938], Loss: 0.5435308218002319\n",
      "Validation: Epoch [14], Batch [571/938], Loss: 0.3909335434436798\n",
      "Validation: Epoch [14], Batch [572/938], Loss: 0.6133294105529785\n",
      "Validation: Epoch [14], Batch [573/938], Loss: 0.36190491914749146\n",
      "Validation: Epoch [14], Batch [574/938], Loss: 0.5190565586090088\n",
      "Validation: Epoch [14], Batch [575/938], Loss: 0.4547438621520996\n",
      "Validation: Epoch [14], Batch [576/938], Loss: 0.4263913035392761\n",
      "Validation: Epoch [14], Batch [577/938], Loss: 0.6245776414871216\n",
      "Validation: Epoch [14], Batch [578/938], Loss: 0.36109867691993713\n",
      "Validation: Epoch [14], Batch [579/938], Loss: 0.5563890337944031\n",
      "Validation: Epoch [14], Batch [580/938], Loss: 0.5826631188392639\n",
      "Validation: Epoch [14], Batch [581/938], Loss: 0.39371156692504883\n",
      "Validation: Epoch [14], Batch [582/938], Loss: 0.4051715135574341\n",
      "Validation: Epoch [14], Batch [583/938], Loss: 0.5088093876838684\n",
      "Validation: Epoch [14], Batch [584/938], Loss: 0.44954773783683777\n",
      "Validation: Epoch [14], Batch [585/938], Loss: 0.3264732360839844\n",
      "Validation: Epoch [14], Batch [586/938], Loss: 0.3873213529586792\n",
      "Validation: Epoch [14], Batch [587/938], Loss: 0.6628901362419128\n",
      "Validation: Epoch [14], Batch [588/938], Loss: 0.38169538974761963\n",
      "Validation: Epoch [14], Batch [589/938], Loss: 0.7197693586349487\n",
      "Validation: Epoch [14], Batch [590/938], Loss: 0.3850646913051605\n",
      "Validation: Epoch [14], Batch [591/938], Loss: 0.39672955870628357\n",
      "Validation: Epoch [14], Batch [592/938], Loss: 0.2533254623413086\n",
      "Validation: Epoch [14], Batch [593/938], Loss: 0.29292386770248413\n",
      "Validation: Epoch [14], Batch [594/938], Loss: 0.3616763949394226\n",
      "Validation: Epoch [14], Batch [595/938], Loss: 0.45072323083877563\n",
      "Validation: Epoch [14], Batch [596/938], Loss: 0.4318530261516571\n",
      "Validation: Epoch [14], Batch [597/938], Loss: 0.4113788604736328\n",
      "Validation: Epoch [14], Batch [598/938], Loss: 0.2971869111061096\n",
      "Validation: Epoch [14], Batch [599/938], Loss: 0.3409995436668396\n",
      "Validation: Epoch [14], Batch [600/938], Loss: 0.5490429401397705\n",
      "Validation: Epoch [14], Batch [601/938], Loss: 0.27209722995758057\n",
      "Validation: Epoch [14], Batch [602/938], Loss: 0.3787880539894104\n",
      "Validation: Epoch [14], Batch [603/938], Loss: 0.523673415184021\n",
      "Validation: Epoch [14], Batch [604/938], Loss: 0.30082079768180847\n",
      "Validation: Epoch [14], Batch [605/938], Loss: 0.5843971967697144\n",
      "Validation: Epoch [14], Batch [606/938], Loss: 0.45134419202804565\n",
      "Validation: Epoch [14], Batch [607/938], Loss: 0.7195649147033691\n",
      "Validation: Epoch [14], Batch [608/938], Loss: 0.4380640685558319\n",
      "Validation: Epoch [14], Batch [609/938], Loss: 0.7657327055931091\n",
      "Validation: Epoch [14], Batch [610/938], Loss: 0.37078166007995605\n",
      "Validation: Epoch [14], Batch [611/938], Loss: 0.5895756483078003\n",
      "Validation: Epoch [14], Batch [612/938], Loss: 0.37089356780052185\n",
      "Validation: Epoch [14], Batch [613/938], Loss: 0.8439468145370483\n",
      "Validation: Epoch [14], Batch [614/938], Loss: 0.6455339789390564\n",
      "Validation: Epoch [14], Batch [615/938], Loss: 0.40309271216392517\n",
      "Validation: Epoch [14], Batch [616/938], Loss: 0.38267752528190613\n",
      "Validation: Epoch [14], Batch [617/938], Loss: 0.5258573293685913\n",
      "Validation: Epoch [14], Batch [618/938], Loss: 0.27500200271606445\n",
      "Validation: Epoch [14], Batch [619/938], Loss: 0.49641168117523193\n",
      "Validation: Epoch [14], Batch [620/938], Loss: 0.566464900970459\n",
      "Validation: Epoch [14], Batch [621/938], Loss: 0.4492332935333252\n",
      "Validation: Epoch [14], Batch [622/938], Loss: 0.4780397415161133\n",
      "Validation: Epoch [14], Batch [623/938], Loss: 0.4190301299095154\n",
      "Validation: Epoch [14], Batch [624/938], Loss: 0.4638024568557739\n",
      "Validation: Epoch [14], Batch [625/938], Loss: 0.42481347918510437\n",
      "Validation: Epoch [14], Batch [626/938], Loss: 0.388863742351532\n",
      "Validation: Epoch [14], Batch [627/938], Loss: 0.42342859506607056\n",
      "Validation: Epoch [14], Batch [628/938], Loss: 0.4704456329345703\n",
      "Validation: Epoch [14], Batch [629/938], Loss: 0.5961629152297974\n",
      "Validation: Epoch [14], Batch [630/938], Loss: 0.4092809557914734\n",
      "Validation: Epoch [14], Batch [631/938], Loss: 0.6319898962974548\n",
      "Validation: Epoch [14], Batch [632/938], Loss: 0.44534802436828613\n",
      "Validation: Epoch [14], Batch [633/938], Loss: 0.3253750801086426\n",
      "Validation: Epoch [14], Batch [634/938], Loss: 0.5278149843215942\n",
      "Validation: Epoch [14], Batch [635/938], Loss: 0.3968411982059479\n",
      "Validation: Epoch [14], Batch [636/938], Loss: 0.7447875738143921\n",
      "Validation: Epoch [14], Batch [637/938], Loss: 0.2882097661495209\n",
      "Validation: Epoch [14], Batch [638/938], Loss: 0.480074405670166\n",
      "Validation: Epoch [14], Batch [639/938], Loss: 0.41819825768470764\n",
      "Validation: Epoch [14], Batch [640/938], Loss: 0.3664419949054718\n",
      "Validation: Epoch [14], Batch [641/938], Loss: 0.616534948348999\n",
      "Validation: Epoch [14], Batch [642/938], Loss: 0.3373754620552063\n",
      "Validation: Epoch [14], Batch [643/938], Loss: 0.41652342677116394\n",
      "Validation: Epoch [14], Batch [644/938], Loss: 0.5174672603607178\n",
      "Validation: Epoch [14], Batch [645/938], Loss: 0.5037310719490051\n",
      "Validation: Epoch [14], Batch [646/938], Loss: 0.3994414806365967\n",
      "Validation: Epoch [14], Batch [647/938], Loss: 0.4824729263782501\n",
      "Validation: Epoch [14], Batch [648/938], Loss: 0.45111554861068726\n",
      "Validation: Epoch [14], Batch [649/938], Loss: 0.44320493936538696\n",
      "Validation: Epoch [14], Batch [650/938], Loss: 0.425712525844574\n",
      "Validation: Epoch [14], Batch [651/938], Loss: 0.43029075860977173\n",
      "Validation: Epoch [14], Batch [652/938], Loss: 0.5686169266700745\n",
      "Validation: Epoch [14], Batch [653/938], Loss: 0.36038875579833984\n",
      "Validation: Epoch [14], Batch [654/938], Loss: 0.5579745769500732\n",
      "Validation: Epoch [14], Batch [655/938], Loss: 0.5567017197608948\n",
      "Validation: Epoch [14], Batch [656/938], Loss: 0.4731709659099579\n",
      "Validation: Epoch [14], Batch [657/938], Loss: 0.4123249650001526\n",
      "Validation: Epoch [14], Batch [658/938], Loss: 0.557042121887207\n",
      "Validation: Epoch [14], Batch [659/938], Loss: 0.4872986078262329\n",
      "Validation: Epoch [14], Batch [660/938], Loss: 0.6893831491470337\n",
      "Validation: Epoch [14], Batch [661/938], Loss: 0.406122624874115\n",
      "Validation: Epoch [14], Batch [662/938], Loss: 0.3617044985294342\n",
      "Validation: Epoch [14], Batch [663/938], Loss: 0.40698474645614624\n",
      "Validation: Epoch [14], Batch [664/938], Loss: 0.45641759037971497\n",
      "Validation: Epoch [14], Batch [665/938], Loss: 0.5334082841873169\n",
      "Validation: Epoch [14], Batch [666/938], Loss: 0.3808203935623169\n",
      "Validation: Epoch [14], Batch [667/938], Loss: 0.23753657937049866\n",
      "Validation: Epoch [14], Batch [668/938], Loss: 0.5828812122344971\n",
      "Validation: Epoch [14], Batch [669/938], Loss: 0.43768423795700073\n",
      "Validation: Epoch [14], Batch [670/938], Loss: 0.7013339400291443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [671/938], Loss: 0.3649469316005707\n",
      "Validation: Epoch [14], Batch [672/938], Loss: 0.593026876449585\n",
      "Validation: Epoch [14], Batch [673/938], Loss: 0.4251123368740082\n",
      "Validation: Epoch [14], Batch [674/938], Loss: 0.32548630237579346\n",
      "Validation: Epoch [14], Batch [675/938], Loss: 0.4692167639732361\n",
      "Validation: Epoch [14], Batch [676/938], Loss: 0.4590490460395813\n",
      "Validation: Epoch [14], Batch [677/938], Loss: 0.6159089803695679\n",
      "Validation: Epoch [14], Batch [678/938], Loss: 0.6228735446929932\n",
      "Validation: Epoch [14], Batch [679/938], Loss: 0.7783522009849548\n",
      "Validation: Epoch [14], Batch [680/938], Loss: 0.364109605550766\n",
      "Validation: Epoch [14], Batch [681/938], Loss: 0.3357256054878235\n",
      "Validation: Epoch [14], Batch [682/938], Loss: 0.37432587146759033\n",
      "Validation: Epoch [14], Batch [683/938], Loss: 0.3949366509914398\n",
      "Validation: Epoch [14], Batch [684/938], Loss: 0.660559892654419\n",
      "Validation: Epoch [14], Batch [685/938], Loss: 0.3786078095436096\n",
      "Validation: Epoch [14], Batch [686/938], Loss: 0.43427905440330505\n",
      "Validation: Epoch [14], Batch [687/938], Loss: 0.4181861877441406\n",
      "Validation: Epoch [14], Batch [688/938], Loss: 0.5399554371833801\n",
      "Validation: Epoch [14], Batch [689/938], Loss: 0.42446112632751465\n",
      "Validation: Epoch [14], Batch [690/938], Loss: 0.4842126965522766\n",
      "Validation: Epoch [14], Batch [691/938], Loss: 0.36416223645210266\n",
      "Validation: Epoch [14], Batch [692/938], Loss: 0.6493438482284546\n",
      "Validation: Epoch [14], Batch [693/938], Loss: 0.44750598073005676\n",
      "Validation: Epoch [14], Batch [694/938], Loss: 0.363445520401001\n",
      "Validation: Epoch [14], Batch [695/938], Loss: 0.4086948335170746\n",
      "Validation: Epoch [14], Batch [696/938], Loss: 0.5083469748497009\n",
      "Validation: Epoch [14], Batch [697/938], Loss: 0.41976696252822876\n",
      "Validation: Epoch [14], Batch [698/938], Loss: 0.2965471148490906\n",
      "Validation: Epoch [14], Batch [699/938], Loss: 0.34350889921188354\n",
      "Validation: Epoch [14], Batch [700/938], Loss: 0.5790866613388062\n",
      "Validation: Epoch [14], Batch [701/938], Loss: 0.5829755663871765\n",
      "Validation: Epoch [14], Batch [702/938], Loss: 0.5087670087814331\n",
      "Validation: Epoch [14], Batch [703/938], Loss: 0.44147172570228577\n",
      "Validation: Epoch [14], Batch [704/938], Loss: 0.47284966707229614\n",
      "Validation: Epoch [14], Batch [705/938], Loss: 0.4203972816467285\n",
      "Validation: Epoch [14], Batch [706/938], Loss: 0.3936444818973541\n",
      "Validation: Epoch [14], Batch [707/938], Loss: 0.4464517831802368\n",
      "Validation: Epoch [14], Batch [708/938], Loss: 0.3714551031589508\n",
      "Validation: Epoch [14], Batch [709/938], Loss: 0.47449344396591187\n",
      "Validation: Epoch [14], Batch [710/938], Loss: 0.413663774728775\n",
      "Validation: Epoch [14], Batch [711/938], Loss: 0.41932055354118347\n",
      "Validation: Epoch [14], Batch [712/938], Loss: 0.3768244981765747\n",
      "Validation: Epoch [14], Batch [713/938], Loss: 0.4550364017486572\n",
      "Validation: Epoch [14], Batch [714/938], Loss: 0.3453129231929779\n",
      "Validation: Epoch [14], Batch [715/938], Loss: 0.31938087940216064\n",
      "Validation: Epoch [14], Batch [716/938], Loss: 0.5003917813301086\n",
      "Validation: Epoch [14], Batch [717/938], Loss: 0.5404793620109558\n",
      "Validation: Epoch [14], Batch [718/938], Loss: 0.43847936391830444\n",
      "Validation: Epoch [14], Batch [719/938], Loss: 0.42949748039245605\n",
      "Validation: Epoch [14], Batch [720/938], Loss: 0.5140457153320312\n",
      "Validation: Epoch [14], Batch [721/938], Loss: 0.3485409617424011\n",
      "Validation: Epoch [14], Batch [722/938], Loss: 0.6175903081893921\n",
      "Validation: Epoch [14], Batch [723/938], Loss: 0.5087511539459229\n",
      "Validation: Epoch [14], Batch [724/938], Loss: 0.5287268161773682\n",
      "Validation: Epoch [14], Batch [725/938], Loss: 0.4879387617111206\n",
      "Validation: Epoch [14], Batch [726/938], Loss: 0.4014013707637787\n",
      "Validation: Epoch [14], Batch [727/938], Loss: 0.3391881585121155\n",
      "Validation: Epoch [14], Batch [728/938], Loss: 0.7305870056152344\n",
      "Validation: Epoch [14], Batch [729/938], Loss: 0.49062126874923706\n",
      "Validation: Epoch [14], Batch [730/938], Loss: 0.3679311275482178\n",
      "Validation: Epoch [14], Batch [731/938], Loss: 0.3557615578174591\n",
      "Validation: Epoch [14], Batch [732/938], Loss: 0.496553897857666\n",
      "Validation: Epoch [14], Batch [733/938], Loss: 0.3409115672111511\n",
      "Validation: Epoch [14], Batch [734/938], Loss: 0.3501961827278137\n",
      "Validation: Epoch [14], Batch [735/938], Loss: 0.3711700439453125\n",
      "Validation: Epoch [14], Batch [736/938], Loss: 0.35751253366470337\n",
      "Validation: Epoch [14], Batch [737/938], Loss: 0.4411885142326355\n",
      "Validation: Epoch [14], Batch [738/938], Loss: 0.7040611505508423\n",
      "Validation: Epoch [14], Batch [739/938], Loss: 0.2729293704032898\n",
      "Validation: Epoch [14], Batch [740/938], Loss: 0.30432119965553284\n",
      "Validation: Epoch [14], Batch [741/938], Loss: 0.3332555890083313\n",
      "Validation: Epoch [14], Batch [742/938], Loss: 0.5143808126449585\n",
      "Validation: Epoch [14], Batch [743/938], Loss: 0.5669100880622864\n",
      "Validation: Epoch [14], Batch [744/938], Loss: 0.6925187110900879\n",
      "Validation: Epoch [14], Batch [745/938], Loss: 0.4360489249229431\n",
      "Validation: Epoch [14], Batch [746/938], Loss: 0.5336025953292847\n",
      "Validation: Epoch [14], Batch [747/938], Loss: 0.28281062841415405\n",
      "Validation: Epoch [14], Batch [748/938], Loss: 0.46757206320762634\n",
      "Validation: Epoch [14], Batch [749/938], Loss: 0.49959316849708557\n",
      "Validation: Epoch [14], Batch [750/938], Loss: 0.5451695919036865\n",
      "Validation: Epoch [14], Batch [751/938], Loss: 0.502856969833374\n",
      "Validation: Epoch [14], Batch [752/938], Loss: 0.3265625238418579\n",
      "Validation: Epoch [14], Batch [753/938], Loss: 0.5145667791366577\n",
      "Validation: Epoch [14], Batch [754/938], Loss: 0.3628317713737488\n",
      "Validation: Epoch [14], Batch [755/938], Loss: 0.5046281218528748\n",
      "Validation: Epoch [14], Batch [756/938], Loss: 0.47140517830848694\n",
      "Validation: Epoch [14], Batch [757/938], Loss: 0.4572267532348633\n",
      "Validation: Epoch [14], Batch [758/938], Loss: 0.5483375787734985\n",
      "Validation: Epoch [14], Batch [759/938], Loss: 0.29187244176864624\n",
      "Validation: Epoch [14], Batch [760/938], Loss: 0.3485181927680969\n",
      "Validation: Epoch [14], Batch [761/938], Loss: 0.5767459869384766\n",
      "Validation: Epoch [14], Batch [762/938], Loss: 0.4649161100387573\n",
      "Validation: Epoch [14], Batch [763/938], Loss: 0.597478449344635\n",
      "Validation: Epoch [14], Batch [764/938], Loss: 0.42318785190582275\n",
      "Validation: Epoch [14], Batch [765/938], Loss: 0.387162446975708\n",
      "Validation: Epoch [14], Batch [766/938], Loss: 0.4799013137817383\n",
      "Validation: Epoch [14], Batch [767/938], Loss: 0.4988873600959778\n",
      "Validation: Epoch [14], Batch [768/938], Loss: 0.3787844479084015\n",
      "Validation: Epoch [14], Batch [769/938], Loss: 0.49816930294036865\n",
      "Validation: Epoch [14], Batch [770/938], Loss: 0.4376002252101898\n",
      "Validation: Epoch [14], Batch [771/938], Loss: 0.4937305748462677\n",
      "Validation: Epoch [14], Batch [772/938], Loss: 0.4021179974079132\n",
      "Validation: Epoch [14], Batch [773/938], Loss: 0.3407169580459595\n",
      "Validation: Epoch [14], Batch [774/938], Loss: 0.4057009518146515\n",
      "Validation: Epoch [14], Batch [775/938], Loss: 0.4427946209907532\n",
      "Validation: Epoch [14], Batch [776/938], Loss: 0.5485563278198242\n",
      "Validation: Epoch [14], Batch [777/938], Loss: 0.42475569248199463\n",
      "Validation: Epoch [14], Batch [778/938], Loss: 0.43258926272392273\n",
      "Validation: Epoch [14], Batch [779/938], Loss: 0.45207658410072327\n",
      "Validation: Epoch [14], Batch [780/938], Loss: 0.4741026759147644\n",
      "Validation: Epoch [14], Batch [781/938], Loss: 0.331429660320282\n",
      "Validation: Epoch [14], Batch [782/938], Loss: 0.50494384765625\n",
      "Validation: Epoch [14], Batch [783/938], Loss: 0.4997141659259796\n",
      "Validation: Epoch [14], Batch [784/938], Loss: 0.40570735931396484\n",
      "Validation: Epoch [14], Batch [785/938], Loss: 0.39578670263290405\n",
      "Validation: Epoch [14], Batch [786/938], Loss: 0.3857918977737427\n",
      "Validation: Epoch [14], Batch [787/938], Loss: 0.47210246324539185\n",
      "Validation: Epoch [14], Batch [788/938], Loss: 0.3208010792732239\n",
      "Validation: Epoch [14], Batch [789/938], Loss: 0.4010886549949646\n",
      "Validation: Epoch [14], Batch [790/938], Loss: 0.31839436292648315\n",
      "Validation: Epoch [14], Batch [791/938], Loss: 0.34205684065818787\n",
      "Validation: Epoch [14], Batch [792/938], Loss: 0.5595118403434753\n",
      "Validation: Epoch [14], Batch [793/938], Loss: 0.39291900396347046\n",
      "Validation: Epoch [14], Batch [794/938], Loss: 0.5996214151382446\n",
      "Validation: Epoch [14], Batch [795/938], Loss: 0.3463316857814789\n",
      "Validation: Epoch [14], Batch [796/938], Loss: 0.4969920217990875\n",
      "Validation: Epoch [14], Batch [797/938], Loss: 0.42887914180755615\n",
      "Validation: Epoch [14], Batch [798/938], Loss: 0.8997515439987183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [799/938], Loss: 0.47248613834381104\n",
      "Validation: Epoch [14], Batch [800/938], Loss: 0.5477648377418518\n",
      "Validation: Epoch [14], Batch [801/938], Loss: 0.2940899431705475\n",
      "Validation: Epoch [14], Batch [802/938], Loss: 0.5669865608215332\n",
      "Validation: Epoch [14], Batch [803/938], Loss: 0.3081133961677551\n",
      "Validation: Epoch [14], Batch [804/938], Loss: 0.581039309501648\n",
      "Validation: Epoch [14], Batch [805/938], Loss: 0.68135666847229\n",
      "Validation: Epoch [14], Batch [806/938], Loss: 0.4548117518424988\n",
      "Validation: Epoch [14], Batch [807/938], Loss: 0.4988711178302765\n",
      "Validation: Epoch [14], Batch [808/938], Loss: 0.3291122317314148\n",
      "Validation: Epoch [14], Batch [809/938], Loss: 0.5561586618423462\n",
      "Validation: Epoch [14], Batch [810/938], Loss: 0.3951760530471802\n",
      "Validation: Epoch [14], Batch [811/938], Loss: 0.4212532639503479\n",
      "Validation: Epoch [14], Batch [812/938], Loss: 0.377819687128067\n",
      "Validation: Epoch [14], Batch [813/938], Loss: 0.37111061811447144\n",
      "Validation: Epoch [14], Batch [814/938], Loss: 0.42543748021125793\n",
      "Validation: Epoch [14], Batch [815/938], Loss: 0.4796796441078186\n",
      "Validation: Epoch [14], Batch [816/938], Loss: 0.5731921195983887\n",
      "Validation: Epoch [14], Batch [817/938], Loss: 0.6978529691696167\n",
      "Validation: Epoch [14], Batch [818/938], Loss: 0.4867677390575409\n",
      "Validation: Epoch [14], Batch [819/938], Loss: 0.5713801383972168\n",
      "Validation: Epoch [14], Batch [820/938], Loss: 0.4913976788520813\n",
      "Validation: Epoch [14], Batch [821/938], Loss: 0.40438538789749146\n",
      "Validation: Epoch [14], Batch [822/938], Loss: 0.4464576840400696\n",
      "Validation: Epoch [14], Batch [823/938], Loss: 0.40777772665023804\n",
      "Validation: Epoch [14], Batch [824/938], Loss: 0.4578389823436737\n",
      "Validation: Epoch [14], Batch [825/938], Loss: 0.347756952047348\n",
      "Validation: Epoch [14], Batch [826/938], Loss: 0.4183378219604492\n",
      "Validation: Epoch [14], Batch [827/938], Loss: 0.2820338308811188\n",
      "Validation: Epoch [14], Batch [828/938], Loss: 0.26386758685112\n",
      "Validation: Epoch [14], Batch [829/938], Loss: 0.5762071013450623\n",
      "Validation: Epoch [14], Batch [830/938], Loss: 0.4550526738166809\n",
      "Validation: Epoch [14], Batch [831/938], Loss: 0.43949317932128906\n",
      "Validation: Epoch [14], Batch [832/938], Loss: 0.28552690148353577\n",
      "Validation: Epoch [14], Batch [833/938], Loss: 0.5729627013206482\n",
      "Validation: Epoch [14], Batch [834/938], Loss: 0.42562487721443176\n",
      "Validation: Epoch [14], Batch [835/938], Loss: 0.2917924225330353\n",
      "Validation: Epoch [14], Batch [836/938], Loss: 0.6808512806892395\n",
      "Validation: Epoch [14], Batch [837/938], Loss: 0.47828570008277893\n",
      "Validation: Epoch [14], Batch [838/938], Loss: 0.5255725979804993\n",
      "Validation: Epoch [14], Batch [839/938], Loss: 0.3684585988521576\n",
      "Validation: Epoch [14], Batch [840/938], Loss: 0.5583484172821045\n",
      "Validation: Epoch [14], Batch [841/938], Loss: 0.3242104649543762\n",
      "Validation: Epoch [14], Batch [842/938], Loss: 0.5319478511810303\n",
      "Validation: Epoch [14], Batch [843/938], Loss: 0.4022391736507416\n",
      "Validation: Epoch [14], Batch [844/938], Loss: 0.5369139313697815\n",
      "Validation: Epoch [14], Batch [845/938], Loss: 0.6058130264282227\n",
      "Validation: Epoch [14], Batch [846/938], Loss: 0.4352124333381653\n",
      "Validation: Epoch [14], Batch [847/938], Loss: 0.3308032155036926\n",
      "Validation: Epoch [14], Batch [848/938], Loss: 0.483055055141449\n",
      "Validation: Epoch [14], Batch [849/938], Loss: 0.42338305711746216\n",
      "Validation: Epoch [14], Batch [850/938], Loss: 0.5419538021087646\n",
      "Validation: Epoch [14], Batch [851/938], Loss: 0.5808427929878235\n",
      "Validation: Epoch [14], Batch [852/938], Loss: 0.2668592035770416\n",
      "Validation: Epoch [14], Batch [853/938], Loss: 0.32237905263900757\n",
      "Validation: Epoch [14], Batch [854/938], Loss: 0.3354119062423706\n",
      "Validation: Epoch [14], Batch [855/938], Loss: 0.4723472595214844\n",
      "Validation: Epoch [14], Batch [856/938], Loss: 0.47266584634780884\n",
      "Validation: Epoch [14], Batch [857/938], Loss: 0.4339161813259125\n",
      "Validation: Epoch [14], Batch [858/938], Loss: 0.5553317070007324\n",
      "Validation: Epoch [14], Batch [859/938], Loss: 0.2567315101623535\n",
      "Validation: Epoch [14], Batch [860/938], Loss: 0.3736805319786072\n",
      "Validation: Epoch [14], Batch [861/938], Loss: 0.6340417265892029\n",
      "Validation: Epoch [14], Batch [862/938], Loss: 0.3767426311969757\n",
      "Validation: Epoch [14], Batch [863/938], Loss: 0.32772067189216614\n",
      "Validation: Epoch [14], Batch [864/938], Loss: 0.41132915019989014\n",
      "Validation: Epoch [14], Batch [865/938], Loss: 0.5632575154304504\n",
      "Validation: Epoch [14], Batch [866/938], Loss: 0.34686219692230225\n",
      "Validation: Epoch [14], Batch [867/938], Loss: 0.3517388701438904\n",
      "Validation: Epoch [14], Batch [868/938], Loss: 0.5758482217788696\n",
      "Validation: Epoch [14], Batch [869/938], Loss: 0.355120450258255\n",
      "Validation: Epoch [14], Batch [870/938], Loss: 0.37468522787094116\n",
      "Validation: Epoch [14], Batch [871/938], Loss: 0.4596085548400879\n",
      "Validation: Epoch [14], Batch [872/938], Loss: 0.4493550658226013\n",
      "Validation: Epoch [14], Batch [873/938], Loss: 0.33527857065200806\n",
      "Validation: Epoch [14], Batch [874/938], Loss: 0.435050368309021\n",
      "Validation: Epoch [14], Batch [875/938], Loss: 0.5078252553939819\n",
      "Validation: Epoch [14], Batch [876/938], Loss: 0.5331157445907593\n",
      "Validation: Epoch [14], Batch [877/938], Loss: 0.4498136639595032\n",
      "Validation: Epoch [14], Batch [878/938], Loss: 0.46290069818496704\n",
      "Validation: Epoch [14], Batch [879/938], Loss: 0.4422510266304016\n",
      "Validation: Epoch [14], Batch [880/938], Loss: 0.6457234025001526\n",
      "Validation: Epoch [14], Batch [881/938], Loss: 0.35750895738601685\n",
      "Validation: Epoch [14], Batch [882/938], Loss: 0.453652024269104\n",
      "Validation: Epoch [14], Batch [883/938], Loss: 0.6729094386100769\n",
      "Validation: Epoch [14], Batch [884/938], Loss: 0.5680891275405884\n",
      "Validation: Epoch [14], Batch [885/938], Loss: 0.4244508445262909\n",
      "Validation: Epoch [14], Batch [886/938], Loss: 0.570212185382843\n",
      "Validation: Epoch [14], Batch [887/938], Loss: 0.46800822019577026\n",
      "Validation: Epoch [14], Batch [888/938], Loss: 0.5743308067321777\n",
      "Validation: Epoch [14], Batch [889/938], Loss: 0.48391470313072205\n",
      "Validation: Epoch [14], Batch [890/938], Loss: 0.48930805921554565\n",
      "Validation: Epoch [14], Batch [891/938], Loss: 0.7506816983222961\n",
      "Validation: Epoch [14], Batch [892/938], Loss: 0.31732726097106934\n",
      "Validation: Epoch [14], Batch [893/938], Loss: 0.32299739122390747\n",
      "Validation: Epoch [14], Batch [894/938], Loss: 0.45771628618240356\n",
      "Validation: Epoch [14], Batch [895/938], Loss: 0.4645422399044037\n",
      "Validation: Epoch [14], Batch [896/938], Loss: 0.5946747660636902\n",
      "Validation: Epoch [14], Batch [897/938], Loss: 0.4641987681388855\n",
      "Validation: Epoch [14], Batch [898/938], Loss: 0.31821227073669434\n",
      "Validation: Epoch [14], Batch [899/938], Loss: 0.5493595004081726\n",
      "Validation: Epoch [14], Batch [900/938], Loss: 0.6615587472915649\n",
      "Validation: Epoch [14], Batch [901/938], Loss: 0.5950526595115662\n",
      "Validation: Epoch [14], Batch [902/938], Loss: 0.4835129678249359\n",
      "Validation: Epoch [14], Batch [903/938], Loss: 0.6611708402633667\n",
      "Validation: Epoch [14], Batch [904/938], Loss: 0.38644036650657654\n",
      "Validation: Epoch [14], Batch [905/938], Loss: 0.38296204805374146\n",
      "Validation: Epoch [14], Batch [906/938], Loss: 0.49113109707832336\n",
      "Validation: Epoch [14], Batch [907/938], Loss: 0.5844547152519226\n",
      "Validation: Epoch [14], Batch [908/938], Loss: 0.5962800979614258\n",
      "Validation: Epoch [14], Batch [909/938], Loss: 0.7117337584495544\n",
      "Validation: Epoch [14], Batch [910/938], Loss: 0.4836646616458893\n",
      "Validation: Epoch [14], Batch [911/938], Loss: 0.42384350299835205\n",
      "Validation: Epoch [14], Batch [912/938], Loss: 0.4138994812965393\n",
      "Validation: Epoch [14], Batch [913/938], Loss: 0.5827556848526001\n",
      "Validation: Epoch [14], Batch [914/938], Loss: 0.6102723479270935\n",
      "Validation: Epoch [14], Batch [915/938], Loss: 0.427599161863327\n",
      "Validation: Epoch [14], Batch [916/938], Loss: 0.33230113983154297\n",
      "Validation: Epoch [14], Batch [917/938], Loss: 0.40498626232147217\n",
      "Validation: Epoch [14], Batch [918/938], Loss: 0.4526267647743225\n",
      "Validation: Epoch [14], Batch [919/938], Loss: 0.6455092430114746\n",
      "Validation: Epoch [14], Batch [920/938], Loss: 0.6709449887275696\n",
      "Validation: Epoch [14], Batch [921/938], Loss: 0.6334078311920166\n",
      "Validation: Epoch [14], Batch [922/938], Loss: 0.36614537239074707\n",
      "Validation: Epoch [14], Batch [923/938], Loss: 0.4480481445789337\n",
      "Validation: Epoch [14], Batch [924/938], Loss: 0.6799181699752808\n",
      "Validation: Epoch [14], Batch [925/938], Loss: 0.39348846673965454\n",
      "Validation: Epoch [14], Batch [926/938], Loss: 0.5833878517150879\n",
      "Validation: Epoch [14], Batch [927/938], Loss: 0.3782482147216797\n",
      "Validation: Epoch [14], Batch [928/938], Loss: 0.5865321159362793\n",
      "Validation: Epoch [14], Batch [929/938], Loss: 0.3419801592826843\n",
      "Validation: Epoch [14], Batch [930/938], Loss: 0.5019087791442871\n",
      "Validation: Epoch [14], Batch [931/938], Loss: 0.35752421617507935\n",
      "Validation: Epoch [14], Batch [932/938], Loss: 0.4019927382469177\n",
      "Validation: Epoch [14], Batch [933/938], Loss: 0.3864017724990845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [14], Batch [934/938], Loss: 0.47490257024765015\n",
      "Validation: Epoch [14], Batch [935/938], Loss: 0.4358680248260498\n",
      "Validation: Epoch [14], Batch [936/938], Loss: 0.4630832076072693\n",
      "Validation: Epoch [14], Batch [937/938], Loss: 0.38849714398384094\n",
      "Validation: Epoch [14], Batch [938/938], Loss: 0.337840735912323\n",
      "Accuracy of test set: 0.8370333333333333\n",
      "Train: Epoch [15], Batch [1/938], Loss: 0.42297664284706116\n",
      "Train: Epoch [15], Batch [2/938], Loss: 0.2785463333129883\n",
      "Train: Epoch [15], Batch [3/938], Loss: 0.45994484424591064\n",
      "Train: Epoch [15], Batch [4/938], Loss: 0.2931910753250122\n",
      "Train: Epoch [15], Batch [5/938], Loss: 0.5650773048400879\n",
      "Train: Epoch [15], Batch [6/938], Loss: 0.4998602867126465\n",
      "Train: Epoch [15], Batch [7/938], Loss: 0.49327394366264343\n",
      "Train: Epoch [15], Batch [8/938], Loss: 0.33279934525489807\n",
      "Train: Epoch [15], Batch [9/938], Loss: 0.4159090220928192\n",
      "Train: Epoch [15], Batch [10/938], Loss: 0.34139832854270935\n",
      "Train: Epoch [15], Batch [11/938], Loss: 0.3920787274837494\n",
      "Train: Epoch [15], Batch [12/938], Loss: 0.29332512617111206\n",
      "Train: Epoch [15], Batch [13/938], Loss: 0.34480586647987366\n",
      "Train: Epoch [15], Batch [14/938], Loss: 0.37523138523101807\n",
      "Train: Epoch [15], Batch [15/938], Loss: 0.6470156908035278\n",
      "Train: Epoch [15], Batch [16/938], Loss: 0.5995998382568359\n",
      "Train: Epoch [15], Batch [17/938], Loss: 0.36615198850631714\n",
      "Train: Epoch [15], Batch [18/938], Loss: 0.5555514693260193\n",
      "Train: Epoch [15], Batch [19/938], Loss: 0.45595964789390564\n",
      "Train: Epoch [15], Batch [20/938], Loss: 0.41145628690719604\n",
      "Train: Epoch [15], Batch [21/938], Loss: 0.3357769846916199\n",
      "Train: Epoch [15], Batch [22/938], Loss: 0.3370606601238251\n",
      "Train: Epoch [15], Batch [23/938], Loss: 0.6028432846069336\n",
      "Train: Epoch [15], Batch [24/938], Loss: 0.5733451843261719\n",
      "Train: Epoch [15], Batch [25/938], Loss: 0.48897016048431396\n",
      "Train: Epoch [15], Batch [26/938], Loss: 0.29537898302078247\n",
      "Train: Epoch [15], Batch [27/938], Loss: 0.49847885966300964\n",
      "Train: Epoch [15], Batch [28/938], Loss: 0.44145989418029785\n",
      "Train: Epoch [15], Batch [29/938], Loss: 0.4575675129890442\n",
      "Train: Epoch [15], Batch [30/938], Loss: 0.6293169260025024\n",
      "Train: Epoch [15], Batch [31/938], Loss: 0.4313898980617523\n",
      "Train: Epoch [15], Batch [32/938], Loss: 0.4165938198566437\n",
      "Train: Epoch [15], Batch [33/938], Loss: 0.4454417824745178\n",
      "Train: Epoch [15], Batch [34/938], Loss: 0.48452529311180115\n",
      "Train: Epoch [15], Batch [35/938], Loss: 0.6311671733856201\n",
      "Train: Epoch [15], Batch [36/938], Loss: 0.46802031993865967\n",
      "Train: Epoch [15], Batch [37/938], Loss: 0.4584038555622101\n",
      "Train: Epoch [15], Batch [38/938], Loss: 0.43562984466552734\n",
      "Train: Epoch [15], Batch [39/938], Loss: 0.5203117728233337\n",
      "Train: Epoch [15], Batch [40/938], Loss: 0.5562073588371277\n",
      "Train: Epoch [15], Batch [41/938], Loss: 0.4464631676673889\n",
      "Train: Epoch [15], Batch [42/938], Loss: 0.2868853807449341\n",
      "Train: Epoch [15], Batch [43/938], Loss: 0.7620533108711243\n",
      "Train: Epoch [15], Batch [44/938], Loss: 0.4582635760307312\n",
      "Train: Epoch [15], Batch [45/938], Loss: 0.3845197558403015\n",
      "Train: Epoch [15], Batch [46/938], Loss: 0.62911057472229\n",
      "Train: Epoch [15], Batch [47/938], Loss: 0.43865200877189636\n",
      "Train: Epoch [15], Batch [48/938], Loss: 0.46200066804885864\n",
      "Train: Epoch [15], Batch [49/938], Loss: 0.4264013171195984\n",
      "Train: Epoch [15], Batch [50/938], Loss: 0.7128793001174927\n",
      "Train: Epoch [15], Batch [51/938], Loss: 0.38983017206192017\n",
      "Train: Epoch [15], Batch [52/938], Loss: 0.592534065246582\n",
      "Train: Epoch [15], Batch [53/938], Loss: 0.3643954396247864\n",
      "Train: Epoch [15], Batch [54/938], Loss: 0.2700883746147156\n",
      "Train: Epoch [15], Batch [55/938], Loss: 0.583814799785614\n",
      "Train: Epoch [15], Batch [56/938], Loss: 0.44668009877204895\n",
      "Train: Epoch [15], Batch [57/938], Loss: 0.5617344975471497\n",
      "Train: Epoch [15], Batch [58/938], Loss: 0.36963480710983276\n",
      "Train: Epoch [15], Batch [59/938], Loss: 0.3742454946041107\n",
      "Train: Epoch [15], Batch [60/938], Loss: 0.23029103875160217\n",
      "Train: Epoch [15], Batch [61/938], Loss: 0.5837483406066895\n",
      "Train: Epoch [15], Batch [62/938], Loss: 0.4138461649417877\n",
      "Train: Epoch [15], Batch [63/938], Loss: 0.39496520161628723\n",
      "Train: Epoch [15], Batch [64/938], Loss: 0.5176025629043579\n",
      "Train: Epoch [15], Batch [65/938], Loss: 0.4098222255706787\n",
      "Train: Epoch [15], Batch [66/938], Loss: 0.4897382855415344\n",
      "Train: Epoch [15], Batch [67/938], Loss: 0.5082355737686157\n",
      "Train: Epoch [15], Batch [68/938], Loss: 0.33800795674324036\n",
      "Train: Epoch [15], Batch [69/938], Loss: 0.4187571108341217\n",
      "Train: Epoch [15], Batch [70/938], Loss: 0.5438211560249329\n",
      "Train: Epoch [15], Batch [71/938], Loss: 0.5439779758453369\n",
      "Train: Epoch [15], Batch [72/938], Loss: 0.5185226202011108\n",
      "Train: Epoch [15], Batch [73/938], Loss: 0.7470784187316895\n",
      "Train: Epoch [15], Batch [74/938], Loss: 0.39811864495277405\n",
      "Train: Epoch [15], Batch [75/938], Loss: 0.4656085968017578\n",
      "Train: Epoch [15], Batch [76/938], Loss: 0.4066928029060364\n",
      "Train: Epoch [15], Batch [77/938], Loss: 0.5940955281257629\n",
      "Train: Epoch [15], Batch [78/938], Loss: 0.43315643072128296\n",
      "Train: Epoch [15], Batch [79/938], Loss: 0.3736848533153534\n",
      "Train: Epoch [15], Batch [80/938], Loss: 0.5343496203422546\n",
      "Train: Epoch [15], Batch [81/938], Loss: 0.6126408576965332\n",
      "Train: Epoch [15], Batch [82/938], Loss: 0.5621130466461182\n",
      "Train: Epoch [15], Batch [83/938], Loss: 0.6062480211257935\n",
      "Train: Epoch [15], Batch [84/938], Loss: 0.36363697052001953\n",
      "Train: Epoch [15], Batch [85/938], Loss: 0.5333166122436523\n",
      "Train: Epoch [15], Batch [86/938], Loss: 0.5256410837173462\n",
      "Train: Epoch [15], Batch [87/938], Loss: 0.47187572717666626\n",
      "Train: Epoch [15], Batch [88/938], Loss: 0.4105512499809265\n",
      "Train: Epoch [15], Batch [89/938], Loss: 0.3353691101074219\n",
      "Train: Epoch [15], Batch [90/938], Loss: 0.49125564098358154\n",
      "Train: Epoch [15], Batch [91/938], Loss: 0.494322270154953\n",
      "Train: Epoch [15], Batch [92/938], Loss: 0.5823399424552917\n",
      "Train: Epoch [15], Batch [93/938], Loss: 0.5294963717460632\n",
      "Train: Epoch [15], Batch [94/938], Loss: 0.5012240409851074\n",
      "Train: Epoch [15], Batch [95/938], Loss: 0.5746755599975586\n",
      "Train: Epoch [15], Batch [96/938], Loss: 0.4301530122756958\n",
      "Train: Epoch [15], Batch [97/938], Loss: 0.5095690488815308\n",
      "Train: Epoch [15], Batch [98/938], Loss: 0.5615634918212891\n",
      "Train: Epoch [15], Batch [99/938], Loss: 0.3966163992881775\n",
      "Train: Epoch [15], Batch [100/938], Loss: 0.43178147077560425\n",
      "Train: Epoch [15], Batch [101/938], Loss: 0.4905637502670288\n",
      "Train: Epoch [15], Batch [102/938], Loss: 0.5116549730300903\n",
      "Train: Epoch [15], Batch [103/938], Loss: 0.34494924545288086\n",
      "Train: Epoch [15], Batch [104/938], Loss: 0.4414469599723816\n",
      "Train: Epoch [15], Batch [105/938], Loss: 0.39134398102760315\n",
      "Train: Epoch [15], Batch [106/938], Loss: 0.4785057604312897\n",
      "Train: Epoch [15], Batch [107/938], Loss: 0.527509331703186\n",
      "Train: Epoch [15], Batch [108/938], Loss: 0.4772566556930542\n",
      "Train: Epoch [15], Batch [109/938], Loss: 0.6084488034248352\n",
      "Train: Epoch [15], Batch [110/938], Loss: 0.5250927209854126\n",
      "Train: Epoch [15], Batch [111/938], Loss: 0.5680760145187378\n",
      "Train: Epoch [15], Batch [112/938], Loss: 0.5312411785125732\n",
      "Train: Epoch [15], Batch [113/938], Loss: 0.504773736000061\n",
      "Train: Epoch [15], Batch [114/938], Loss: 0.5335551500320435\n",
      "Train: Epoch [15], Batch [115/938], Loss: 0.5685264468193054\n",
      "Train: Epoch [15], Batch [116/938], Loss: 0.538236677646637\n",
      "Train: Epoch [15], Batch [117/938], Loss: 0.3782880902290344\n",
      "Train: Epoch [15], Batch [118/938], Loss: 0.5910937786102295\n",
      "Train: Epoch [15], Batch [119/938], Loss: 0.3512120246887207\n",
      "Train: Epoch [15], Batch [120/938], Loss: 0.44170475006103516\n",
      "Train: Epoch [15], Batch [121/938], Loss: 0.4891812205314636\n",
      "Train: Epoch [15], Batch [122/938], Loss: 0.4598512649536133\n",
      "Train: Epoch [15], Batch [123/938], Loss: 0.47393134236335754\n",
      "Train: Epoch [15], Batch [124/938], Loss: 0.47314703464508057\n",
      "Train: Epoch [15], Batch [125/938], Loss: 0.3045722246170044\n",
      "Train: Epoch [15], Batch [126/938], Loss: 0.4481654167175293\n",
      "Train: Epoch [15], Batch [127/938], Loss: 0.3522545099258423\n",
      "Train: Epoch [15], Batch [128/938], Loss: 0.46668073534965515\n",
      "Train: Epoch [15], Batch [129/938], Loss: 0.49174124002456665\n",
      "Train: Epoch [15], Batch [130/938], Loss: 0.3298635482788086\n",
      "Train: Epoch [15], Batch [131/938], Loss: 0.44324877858161926\n",
      "Train: Epoch [15], Batch [132/938], Loss: 0.36675697565078735\n",
      "Train: Epoch [15], Batch [133/938], Loss: 0.4671168327331543\n",
      "Train: Epoch [15], Batch [134/938], Loss: 0.44608747959136963\n",
      "Train: Epoch [15], Batch [135/938], Loss: 0.446073055267334\n",
      "Train: Epoch [15], Batch [136/938], Loss: 0.4919774532318115\n",
      "Train: Epoch [15], Batch [137/938], Loss: 0.5801674127578735\n",
      "Train: Epoch [15], Batch [138/938], Loss: 0.33884960412979126\n",
      "Train: Epoch [15], Batch [139/938], Loss: 0.46568673849105835\n",
      "Train: Epoch [15], Batch [140/938], Loss: 0.47733309864997864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [15], Batch [141/938], Loss: 0.45372626185417175\n",
      "Train: Epoch [15], Batch [142/938], Loss: 0.4763658046722412\n",
      "Train: Epoch [15], Batch [143/938], Loss: 0.7015763521194458\n",
      "Train: Epoch [15], Batch [144/938], Loss: 0.32419028878211975\n",
      "Train: Epoch [15], Batch [145/938], Loss: 0.37643447518348694\n",
      "Train: Epoch [15], Batch [146/938], Loss: 0.6121060848236084\n",
      "Train: Epoch [15], Batch [147/938], Loss: 0.49479246139526367\n",
      "Train: Epoch [15], Batch [148/938], Loss: 0.587752103805542\n",
      "Train: Epoch [15], Batch [149/938], Loss: 0.518824577331543\n",
      "Train: Epoch [15], Batch [150/938], Loss: 0.419460654258728\n",
      "Train: Epoch [15], Batch [151/938], Loss: 0.3308790326118469\n",
      "Train: Epoch [15], Batch [152/938], Loss: 0.3517756164073944\n",
      "Train: Epoch [15], Batch [153/938], Loss: 0.46158885955810547\n",
      "Train: Epoch [15], Batch [154/938], Loss: 0.5182081460952759\n",
      "Train: Epoch [15], Batch [155/938], Loss: 0.6190983653068542\n",
      "Train: Epoch [15], Batch [156/938], Loss: 0.48317015171051025\n",
      "Train: Epoch [15], Batch [157/938], Loss: 0.5102648735046387\n",
      "Train: Epoch [15], Batch [158/938], Loss: 0.3272892236709595\n",
      "Train: Epoch [15], Batch [159/938], Loss: 0.4381024241447449\n",
      "Train: Epoch [15], Batch [160/938], Loss: 0.5035732388496399\n",
      "Train: Epoch [15], Batch [161/938], Loss: 0.6782768964767456\n",
      "Train: Epoch [15], Batch [162/938], Loss: 0.6969956159591675\n",
      "Train: Epoch [15], Batch [163/938], Loss: 0.5269084572792053\n",
      "Train: Epoch [15], Batch [164/938], Loss: 0.6175599098205566\n",
      "Train: Epoch [15], Batch [165/938], Loss: 0.5107841491699219\n",
      "Train: Epoch [15], Batch [166/938], Loss: 0.2880497872829437\n",
      "Train: Epoch [15], Batch [167/938], Loss: 0.38815945386886597\n",
      "Train: Epoch [15], Batch [168/938], Loss: 0.2820366322994232\n",
      "Train: Epoch [15], Batch [169/938], Loss: 0.513427734375\n",
      "Train: Epoch [15], Batch [170/938], Loss: 0.35423368215560913\n",
      "Train: Epoch [15], Batch [171/938], Loss: 0.4504662752151489\n",
      "Train: Epoch [15], Batch [172/938], Loss: 0.43145859241485596\n",
      "Train: Epoch [15], Batch [173/938], Loss: 0.4285176694393158\n",
      "Train: Epoch [15], Batch [174/938], Loss: 0.4568261504173279\n",
      "Train: Epoch [15], Batch [175/938], Loss: 0.32223382592201233\n",
      "Train: Epoch [15], Batch [176/938], Loss: 0.5124903917312622\n",
      "Train: Epoch [15], Batch [177/938], Loss: 0.49820348620414734\n",
      "Train: Epoch [15], Batch [178/938], Loss: 0.5802260637283325\n",
      "Train: Epoch [15], Batch [179/938], Loss: 0.47878924012184143\n",
      "Train: Epoch [15], Batch [180/938], Loss: 0.49561241269111633\n",
      "Train: Epoch [15], Batch [181/938], Loss: 0.3447721302509308\n",
      "Train: Epoch [15], Batch [182/938], Loss: 0.3877180516719818\n",
      "Train: Epoch [15], Batch [183/938], Loss: 0.4385879337787628\n",
      "Train: Epoch [15], Batch [184/938], Loss: 0.6710091829299927\n",
      "Train: Epoch [15], Batch [185/938], Loss: 0.4452735185623169\n",
      "Train: Epoch [15], Batch [186/938], Loss: 0.5468074679374695\n",
      "Train: Epoch [15], Batch [187/938], Loss: 0.5273366570472717\n",
      "Train: Epoch [15], Batch [188/938], Loss: 0.3882353901863098\n",
      "Train: Epoch [15], Batch [189/938], Loss: 0.3934920132160187\n",
      "Train: Epoch [15], Batch [190/938], Loss: 0.32154175639152527\n",
      "Train: Epoch [15], Batch [191/938], Loss: 0.2648472785949707\n",
      "Train: Epoch [15], Batch [192/938], Loss: 0.40402883291244507\n",
      "Train: Epoch [15], Batch [193/938], Loss: 0.2992442846298218\n",
      "Train: Epoch [15], Batch [194/938], Loss: 0.47764483094215393\n",
      "Train: Epoch [15], Batch [195/938], Loss: 0.44147491455078125\n",
      "Train: Epoch [15], Batch [196/938], Loss: 0.5737596154212952\n",
      "Train: Epoch [15], Batch [197/938], Loss: 0.4536619782447815\n",
      "Train: Epoch [15], Batch [198/938], Loss: 0.37060418725013733\n",
      "Train: Epoch [15], Batch [199/938], Loss: 0.5149096250534058\n",
      "Train: Epoch [15], Batch [200/938], Loss: 0.40894848108291626\n",
      "Train: Epoch [15], Batch [201/938], Loss: 0.4467620253562927\n",
      "Train: Epoch [15], Batch [202/938], Loss: 0.5668473243713379\n",
      "Train: Epoch [15], Batch [203/938], Loss: 0.4002988934516907\n",
      "Train: Epoch [15], Batch [204/938], Loss: 0.366255521774292\n",
      "Train: Epoch [15], Batch [205/938], Loss: 0.6379634737968445\n",
      "Train: Epoch [15], Batch [206/938], Loss: 0.45296424627304077\n",
      "Train: Epoch [15], Batch [207/938], Loss: 0.39550256729125977\n",
      "Train: Epoch [15], Batch [208/938], Loss: 0.38542768359184265\n",
      "Train: Epoch [15], Batch [209/938], Loss: 0.40935057401657104\n",
      "Train: Epoch [15], Batch [210/938], Loss: 0.4550646245479584\n",
      "Train: Epoch [15], Batch [211/938], Loss: 0.3230641484260559\n",
      "Train: Epoch [15], Batch [212/938], Loss: 0.4087423086166382\n",
      "Train: Epoch [15], Batch [213/938], Loss: 0.49324920773506165\n",
      "Train: Epoch [15], Batch [214/938], Loss: 0.42605060338974\n",
      "Train: Epoch [15], Batch [215/938], Loss: 0.3215840756893158\n",
      "Train: Epoch [15], Batch [216/938], Loss: 0.5153781771659851\n",
      "Train: Epoch [15], Batch [217/938], Loss: 0.5569884777069092\n",
      "Train: Epoch [15], Batch [218/938], Loss: 0.5714887380599976\n",
      "Train: Epoch [15], Batch [219/938], Loss: 0.3292088806629181\n",
      "Train: Epoch [15], Batch [220/938], Loss: 0.6487953662872314\n",
      "Train: Epoch [15], Batch [221/938], Loss: 0.42128512263298035\n",
      "Train: Epoch [15], Batch [222/938], Loss: 0.2095526158809662\n",
      "Train: Epoch [15], Batch [223/938], Loss: 0.7485955357551575\n",
      "Train: Epoch [15], Batch [224/938], Loss: 0.35737746953964233\n",
      "Train: Epoch [15], Batch [225/938], Loss: 0.431273877620697\n",
      "Train: Epoch [15], Batch [226/938], Loss: 0.43264198303222656\n",
      "Train: Epoch [15], Batch [227/938], Loss: 0.5099619030952454\n",
      "Train: Epoch [15], Batch [228/938], Loss: 0.5880016088485718\n",
      "Train: Epoch [15], Batch [229/938], Loss: 0.504843533039093\n",
      "Train: Epoch [15], Batch [230/938], Loss: 0.6144154071807861\n",
      "Train: Epoch [15], Batch [231/938], Loss: 0.44805872440338135\n",
      "Train: Epoch [15], Batch [232/938], Loss: 0.6345441341400146\n",
      "Train: Epoch [15], Batch [233/938], Loss: 0.44700995087623596\n",
      "Train: Epoch [15], Batch [234/938], Loss: 0.4364032745361328\n",
      "Train: Epoch [15], Batch [235/938], Loss: 0.31705230474472046\n",
      "Train: Epoch [15], Batch [236/938], Loss: 0.5058695077896118\n",
      "Train: Epoch [15], Batch [237/938], Loss: 0.42933303117752075\n",
      "Train: Epoch [15], Batch [238/938], Loss: 0.5339550971984863\n",
      "Train: Epoch [15], Batch [239/938], Loss: 0.49606525897979736\n",
      "Train: Epoch [15], Batch [240/938], Loss: 0.4799611270427704\n",
      "Train: Epoch [15], Batch [241/938], Loss: 0.4959123432636261\n",
      "Train: Epoch [15], Batch [242/938], Loss: 0.4975666403770447\n",
      "Train: Epoch [15], Batch [243/938], Loss: 0.5032240748405457\n",
      "Train: Epoch [15], Batch [244/938], Loss: 0.34210529923439026\n",
      "Train: Epoch [15], Batch [245/938], Loss: 0.5146403908729553\n",
      "Train: Epoch [15], Batch [246/938], Loss: 0.48969653248786926\n",
      "Train: Epoch [15], Batch [247/938], Loss: 0.5466043949127197\n",
      "Train: Epoch [15], Batch [248/938], Loss: 0.44932571053504944\n",
      "Train: Epoch [15], Batch [249/938], Loss: 0.5468307137489319\n",
      "Train: Epoch [15], Batch [250/938], Loss: 0.48408597707748413\n",
      "Train: Epoch [15], Batch [251/938], Loss: 0.43645331263542175\n",
      "Train: Epoch [15], Batch [252/938], Loss: 0.40620213747024536\n",
      "Train: Epoch [15], Batch [253/938], Loss: 0.4445533752441406\n",
      "Train: Epoch [15], Batch [254/938], Loss: 0.32890281081199646\n",
      "Train: Epoch [15], Batch [255/938], Loss: 0.5237261652946472\n",
      "Train: Epoch [15], Batch [256/938], Loss: 0.3112655282020569\n",
      "Train: Epoch [15], Batch [257/938], Loss: 0.34033435583114624\n",
      "Train: Epoch [15], Batch [258/938], Loss: 0.2988477349281311\n",
      "Train: Epoch [15], Batch [259/938], Loss: 0.8892776966094971\n",
      "Train: Epoch [15], Batch [260/938], Loss: 0.30586016178131104\n",
      "Train: Epoch [15], Batch [261/938], Loss: 0.35141587257385254\n",
      "Train: Epoch [15], Batch [262/938], Loss: 0.5527462959289551\n",
      "Train: Epoch [15], Batch [263/938], Loss: 0.28387969732284546\n",
      "Train: Epoch [15], Batch [264/938], Loss: 0.5631598234176636\n",
      "Train: Epoch [15], Batch [265/938], Loss: 0.46998050808906555\n",
      "Train: Epoch [15], Batch [266/938], Loss: 0.3868609368801117\n",
      "Train: Epoch [15], Batch [267/938], Loss: 0.36384105682373047\n",
      "Train: Epoch [15], Batch [268/938], Loss: 0.4605351686477661\n",
      "Train: Epoch [15], Batch [269/938], Loss: 0.5795966386795044\n",
      "Train: Epoch [15], Batch [270/938], Loss: 0.3178393244743347\n",
      "Train: Epoch [15], Batch [271/938], Loss: 0.33891749382019043\n",
      "Train: Epoch [15], Batch [272/938], Loss: 0.480540931224823\n",
      "Train: Epoch [15], Batch [273/938], Loss: 0.4448152780532837\n",
      "Train: Epoch [15], Batch [274/938], Loss: 0.47322359681129456\n",
      "Train: Epoch [15], Batch [275/938], Loss: 0.36138612031936646\n",
      "Train: Epoch [15], Batch [276/938], Loss: 0.6063865423202515\n",
      "Train: Epoch [15], Batch [277/938], Loss: 0.5065299272537231\n",
      "Train: Epoch [15], Batch [278/938], Loss: 0.4373335838317871\n",
      "Train: Epoch [15], Batch [279/938], Loss: 0.3369101881980896\n",
      "Train: Epoch [15], Batch [280/938], Loss: 0.5025517344474792\n",
      "Train: Epoch [15], Batch [281/938], Loss: 0.5532582998275757\n",
      "Train: Epoch [15], Batch [282/938], Loss: 0.4193454086780548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [15], Batch [283/938], Loss: 0.5605102777481079\n",
      "Train: Epoch [15], Batch [284/938], Loss: 0.5896389484405518\n",
      "Train: Epoch [15], Batch [285/938], Loss: 0.36201173067092896\n",
      "Train: Epoch [15], Batch [286/938], Loss: 0.2742270231246948\n",
      "Train: Epoch [15], Batch [287/938], Loss: 0.4194546937942505\n",
      "Train: Epoch [15], Batch [288/938], Loss: 0.38946887850761414\n",
      "Train: Epoch [15], Batch [289/938], Loss: 0.39191317558288574\n",
      "Train: Epoch [15], Batch [290/938], Loss: 0.5451205968856812\n",
      "Train: Epoch [15], Batch [291/938], Loss: 0.35587865114212036\n",
      "Train: Epoch [15], Batch [292/938], Loss: 0.29105323553085327\n",
      "Train: Epoch [15], Batch [293/938], Loss: 0.633873701095581\n",
      "Train: Epoch [15], Batch [294/938], Loss: 0.3778403401374817\n",
      "Train: Epoch [15], Batch [295/938], Loss: 0.5211001634597778\n",
      "Train: Epoch [15], Batch [296/938], Loss: 0.33499473333358765\n",
      "Train: Epoch [15], Batch [297/938], Loss: 0.3969189524650574\n",
      "Train: Epoch [15], Batch [298/938], Loss: 0.4973403513431549\n",
      "Train: Epoch [15], Batch [299/938], Loss: 0.43422338366508484\n",
      "Train: Epoch [15], Batch [300/938], Loss: 0.4704650640487671\n",
      "Train: Epoch [15], Batch [301/938], Loss: 0.39855843782424927\n",
      "Train: Epoch [15], Batch [302/938], Loss: 0.6231573820114136\n",
      "Train: Epoch [15], Batch [303/938], Loss: 0.5284160375595093\n",
      "Train: Epoch [15], Batch [304/938], Loss: 0.5154962539672852\n",
      "Train: Epoch [15], Batch [305/938], Loss: 0.5993534326553345\n",
      "Train: Epoch [15], Batch [306/938], Loss: 0.40023842453956604\n",
      "Train: Epoch [15], Batch [307/938], Loss: 0.5011167526245117\n",
      "Train: Epoch [15], Batch [308/938], Loss: 0.4169139564037323\n",
      "Train: Epoch [15], Batch [309/938], Loss: 0.5067873001098633\n",
      "Train: Epoch [15], Batch [310/938], Loss: 0.37369969487190247\n",
      "Train: Epoch [15], Batch [311/938], Loss: 0.38606196641921997\n",
      "Train: Epoch [15], Batch [312/938], Loss: 0.5455933809280396\n",
      "Train: Epoch [15], Batch [313/938], Loss: 0.3531127870082855\n",
      "Train: Epoch [15], Batch [314/938], Loss: 0.6284552216529846\n",
      "Train: Epoch [15], Batch [315/938], Loss: 0.5198879241943359\n",
      "Train: Epoch [15], Batch [316/938], Loss: 0.6625064015388489\n",
      "Train: Epoch [15], Batch [317/938], Loss: 0.4514743983745575\n",
      "Train: Epoch [15], Batch [318/938], Loss: 0.6652761101722717\n",
      "Train: Epoch [15], Batch [319/938], Loss: 0.4051859974861145\n",
      "Train: Epoch [15], Batch [320/938], Loss: 0.650941014289856\n",
      "Train: Epoch [15], Batch [321/938], Loss: 0.4188724160194397\n",
      "Train: Epoch [15], Batch [322/938], Loss: 0.56312096118927\n",
      "Train: Epoch [15], Batch [323/938], Loss: 0.49587181210517883\n",
      "Train: Epoch [15], Batch [324/938], Loss: 0.3480435907840729\n",
      "Train: Epoch [15], Batch [325/938], Loss: 0.4309486746788025\n",
      "Train: Epoch [15], Batch [326/938], Loss: 0.43927955627441406\n",
      "Train: Epoch [15], Batch [327/938], Loss: 0.5137408971786499\n",
      "Train: Epoch [15], Batch [328/938], Loss: 0.5676208734512329\n",
      "Train: Epoch [15], Batch [329/938], Loss: 0.4261085093021393\n",
      "Train: Epoch [15], Batch [330/938], Loss: 0.5935388803482056\n",
      "Train: Epoch [15], Batch [331/938], Loss: 0.42571505904197693\n",
      "Train: Epoch [15], Batch [332/938], Loss: 0.5980463624000549\n",
      "Train: Epoch [15], Batch [333/938], Loss: 0.4182310700416565\n",
      "Train: Epoch [15], Batch [334/938], Loss: 0.5626767873764038\n",
      "Train: Epoch [15], Batch [335/938], Loss: 0.4858331084251404\n",
      "Train: Epoch [15], Batch [336/938], Loss: 0.37151673436164856\n",
      "Train: Epoch [15], Batch [337/938], Loss: 0.4697253406047821\n",
      "Train: Epoch [15], Batch [338/938], Loss: 0.4458579421043396\n",
      "Train: Epoch [15], Batch [339/938], Loss: 0.4391334652900696\n",
      "Train: Epoch [15], Batch [340/938], Loss: 0.673348605632782\n",
      "Train: Epoch [15], Batch [341/938], Loss: 0.32328903675079346\n",
      "Train: Epoch [15], Batch [342/938], Loss: 0.4563508927822113\n",
      "Train: Epoch [15], Batch [343/938], Loss: 0.6149640083312988\n",
      "Train: Epoch [15], Batch [344/938], Loss: 0.33267658948898315\n",
      "Train: Epoch [15], Batch [345/938], Loss: 0.5056818127632141\n",
      "Train: Epoch [15], Batch [346/938], Loss: 0.5813744068145752\n",
      "Train: Epoch [15], Batch [347/938], Loss: 0.3107766807079315\n",
      "Train: Epoch [15], Batch [348/938], Loss: 0.4342148005962372\n",
      "Train: Epoch [15], Batch [349/938], Loss: 0.6013663411140442\n",
      "Train: Epoch [15], Batch [350/938], Loss: 0.6183862686157227\n",
      "Train: Epoch [15], Batch [351/938], Loss: 0.3303309679031372\n",
      "Train: Epoch [15], Batch [352/938], Loss: 0.23463132977485657\n",
      "Train: Epoch [15], Batch [353/938], Loss: 0.3976624608039856\n",
      "Train: Epoch [15], Batch [354/938], Loss: 0.3209383487701416\n",
      "Train: Epoch [15], Batch [355/938], Loss: 0.5985247492790222\n",
      "Train: Epoch [15], Batch [356/938], Loss: 0.4498695135116577\n",
      "Train: Epoch [15], Batch [357/938], Loss: 0.5338915586471558\n",
      "Train: Epoch [15], Batch [358/938], Loss: 0.3467105031013489\n",
      "Train: Epoch [15], Batch [359/938], Loss: 0.3809437155723572\n",
      "Train: Epoch [15], Batch [360/938], Loss: 0.5163182020187378\n",
      "Train: Epoch [15], Batch [361/938], Loss: 0.5027415752410889\n",
      "Train: Epoch [15], Batch [362/938], Loss: 0.4150519371032715\n",
      "Train: Epoch [15], Batch [363/938], Loss: 0.37953442335128784\n",
      "Train: Epoch [15], Batch [364/938], Loss: 0.3424176573753357\n",
      "Train: Epoch [15], Batch [365/938], Loss: 0.616378903388977\n",
      "Train: Epoch [15], Batch [366/938], Loss: 0.5597596168518066\n",
      "Train: Epoch [15], Batch [367/938], Loss: 0.291555255651474\n",
      "Train: Epoch [15], Batch [368/938], Loss: 0.5328702926635742\n",
      "Train: Epoch [15], Batch [369/938], Loss: 0.4133373498916626\n",
      "Train: Epoch [15], Batch [370/938], Loss: 0.37482500076293945\n",
      "Train: Epoch [15], Batch [371/938], Loss: 0.517228364944458\n",
      "Train: Epoch [15], Batch [372/938], Loss: 0.3012852668762207\n",
      "Train: Epoch [15], Batch [373/938], Loss: 0.4717850685119629\n",
      "Train: Epoch [15], Batch [374/938], Loss: 0.5184795260429382\n",
      "Train: Epoch [15], Batch [375/938], Loss: 0.4690643548965454\n",
      "Train: Epoch [15], Batch [376/938], Loss: 0.4539690911769867\n",
      "Train: Epoch [15], Batch [377/938], Loss: 0.5218948721885681\n",
      "Train: Epoch [15], Batch [378/938], Loss: 0.45074698328971863\n",
      "Train: Epoch [15], Batch [379/938], Loss: 0.4067060053348541\n",
      "Train: Epoch [15], Batch [380/938], Loss: 0.5228899717330933\n",
      "Train: Epoch [15], Batch [381/938], Loss: 0.385542094707489\n",
      "Train: Epoch [15], Batch [382/938], Loss: 0.5079808235168457\n",
      "Train: Epoch [15], Batch [383/938], Loss: 0.5138454437255859\n",
      "Train: Epoch [15], Batch [384/938], Loss: 0.34897497296333313\n",
      "Train: Epoch [15], Batch [385/938], Loss: 0.5352067351341248\n",
      "Train: Epoch [15], Batch [386/938], Loss: 0.5224450826644897\n",
      "Train: Epoch [15], Batch [387/938], Loss: 0.334868848323822\n",
      "Train: Epoch [15], Batch [388/938], Loss: 0.3946459889411926\n",
      "Train: Epoch [15], Batch [389/938], Loss: 0.4518251121044159\n",
      "Train: Epoch [15], Batch [390/938], Loss: 0.36627522110939026\n",
      "Train: Epoch [15], Batch [391/938], Loss: 0.9237174391746521\n",
      "Train: Epoch [15], Batch [392/938], Loss: 0.41078245639801025\n",
      "Train: Epoch [15], Batch [393/938], Loss: 0.5182160139083862\n",
      "Train: Epoch [15], Batch [394/938], Loss: 0.3484097719192505\n",
      "Train: Epoch [15], Batch [395/938], Loss: 0.6408331394195557\n",
      "Train: Epoch [15], Batch [396/938], Loss: 0.4591892957687378\n",
      "Train: Epoch [15], Batch [397/938], Loss: 0.3290785253047943\n",
      "Train: Epoch [15], Batch [398/938], Loss: 0.511648952960968\n",
      "Train: Epoch [15], Batch [399/938], Loss: 0.45545467734336853\n",
      "Train: Epoch [15], Batch [400/938], Loss: 0.5998901724815369\n",
      "Train: Epoch [15], Batch [401/938], Loss: 0.34001535177230835\n",
      "Train: Epoch [15], Batch [402/938], Loss: 0.44025975465774536\n",
      "Train: Epoch [15], Batch [403/938], Loss: 0.460565984249115\n",
      "Train: Epoch [15], Batch [404/938], Loss: 0.4394933879375458\n",
      "Train: Epoch [15], Batch [405/938], Loss: 0.3704061210155487\n",
      "Train: Epoch [15], Batch [406/938], Loss: 0.47134929895401\n",
      "Train: Epoch [15], Batch [407/938], Loss: 0.29206007719039917\n",
      "Train: Epoch [15], Batch [408/938], Loss: 0.35317370295524597\n",
      "Train: Epoch [15], Batch [409/938], Loss: 0.5513919591903687\n",
      "Train: Epoch [15], Batch [410/938], Loss: 0.3125065863132477\n",
      "Train: Epoch [15], Batch [411/938], Loss: 0.2953915596008301\n",
      "Train: Epoch [15], Batch [412/938], Loss: 0.5159937143325806\n",
      "Train: Epoch [15], Batch [413/938], Loss: 0.3959132134914398\n",
      "Train: Epoch [15], Batch [414/938], Loss: 0.4887983202934265\n",
      "Train: Epoch [15], Batch [415/938], Loss: 0.4776584506034851\n",
      "Train: Epoch [15], Batch [416/938], Loss: 0.4172769784927368\n",
      "Train: Epoch [15], Batch [417/938], Loss: 0.6978187561035156\n",
      "Train: Epoch [15], Batch [418/938], Loss: 0.6033296585083008\n",
      "Train: Epoch [15], Batch [419/938], Loss: 0.22031456232070923\n",
      "Train: Epoch [15], Batch [420/938], Loss: 0.635866641998291\n",
      "Train: Epoch [15], Batch [421/938], Loss: 0.6373698711395264\n",
      "Train: Epoch [15], Batch [422/938], Loss: 0.5338327288627625\n",
      "Train: Epoch [15], Batch [423/938], Loss: 0.5531195402145386\n",
      "Train: Epoch [15], Batch [424/938], Loss: 0.35343265533447266\n",
      "Train: Epoch [15], Batch [425/938], Loss: 0.5273470878601074\n",
      "Train: Epoch [15], Batch [426/938], Loss: 0.363633930683136\n",
      "Train: Epoch [15], Batch [427/938], Loss: 0.44518497586250305\n",
      "Train: Epoch [15], Batch [428/938], Loss: 0.5185927748680115\n",
      "Train: Epoch [15], Batch [429/938], Loss: 0.49745991826057434\n",
      "Train: Epoch [15], Batch [430/938], Loss: 0.5184115171432495\n",
      "Train: Epoch [15], Batch [431/938], Loss: 0.3363226056098938\n",
      "Train: Epoch [15], Batch [432/938], Loss: 0.3061377704143524\n",
      "Train: Epoch [15], Batch [433/938], Loss: 0.43812596797943115\n",
      "Train: Epoch [15], Batch [434/938], Loss: 0.40387624502182007\n",
      "Train: Epoch [15], Batch [435/938], Loss: 0.4796786308288574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [15], Batch [436/938], Loss: 0.3004971146583557\n",
      "Train: Epoch [15], Batch [437/938], Loss: 0.3511698246002197\n",
      "Train: Epoch [15], Batch [438/938], Loss: 0.5371164679527283\n",
      "Train: Epoch [15], Batch [439/938], Loss: 0.4419189691543579\n",
      "Train: Epoch [15], Batch [440/938], Loss: 0.4751720428466797\n",
      "Train: Epoch [15], Batch [441/938], Loss: 0.25929319858551025\n",
      "Train: Epoch [15], Batch [442/938], Loss: 0.43740710616111755\n",
      "Train: Epoch [15], Batch [443/938], Loss: 0.4064484238624573\n",
      "Train: Epoch [15], Batch [444/938], Loss: 0.48508667945861816\n",
      "Train: Epoch [15], Batch [445/938], Loss: 0.6014834642410278\n",
      "Train: Epoch [15], Batch [446/938], Loss: 0.4560454785823822\n",
      "Train: Epoch [15], Batch [447/938], Loss: 0.4308834969997406\n",
      "Train: Epoch [15], Batch [448/938], Loss: 0.5920230746269226\n",
      "Train: Epoch [15], Batch [449/938], Loss: 0.45325952768325806\n",
      "Train: Epoch [15], Batch [450/938], Loss: 0.4170377254486084\n",
      "Train: Epoch [15], Batch [451/938], Loss: 0.6495850086212158\n",
      "Train: Epoch [15], Batch [452/938], Loss: 0.41018110513687134\n",
      "Train: Epoch [15], Batch [453/938], Loss: 0.42702043056488037\n",
      "Train: Epoch [15], Batch [454/938], Loss: 0.3079034388065338\n",
      "Train: Epoch [15], Batch [455/938], Loss: 0.4969017803668976\n",
      "Train: Epoch [15], Batch [456/938], Loss: 0.549083948135376\n",
      "Train: Epoch [15], Batch [457/938], Loss: 0.413929283618927\n",
      "Train: Epoch [15], Batch [458/938], Loss: 0.4342963695526123\n",
      "Train: Epoch [15], Batch [459/938], Loss: 0.42289361357688904\n",
      "Train: Epoch [15], Batch [460/938], Loss: 0.6071287393569946\n",
      "Train: Epoch [15], Batch [461/938], Loss: 0.5685640573501587\n",
      "Train: Epoch [15], Batch [462/938], Loss: 0.572177529335022\n",
      "Train: Epoch [15], Batch [463/938], Loss: 0.41684627532958984\n",
      "Train: Epoch [15], Batch [464/938], Loss: 0.7096800804138184\n",
      "Train: Epoch [15], Batch [465/938], Loss: 0.5040439367294312\n",
      "Train: Epoch [15], Batch [466/938], Loss: 0.5001379251480103\n",
      "Train: Epoch [15], Batch [467/938], Loss: 0.41330820322036743\n",
      "Train: Epoch [15], Batch [468/938], Loss: 0.33917945623397827\n",
      "Train: Epoch [15], Batch [469/938], Loss: 0.3684854507446289\n",
      "Train: Epoch [15], Batch [470/938], Loss: 0.40113088488578796\n",
      "Train: Epoch [15], Batch [471/938], Loss: 0.46701234579086304\n",
      "Train: Epoch [15], Batch [472/938], Loss: 0.45387840270996094\n",
      "Train: Epoch [15], Batch [473/938], Loss: 0.47053903341293335\n",
      "Train: Epoch [15], Batch [474/938], Loss: 0.3656410574913025\n",
      "Train: Epoch [15], Batch [475/938], Loss: 0.49241194128990173\n",
      "Train: Epoch [15], Batch [476/938], Loss: 0.43738722801208496\n",
      "Train: Epoch [15], Batch [477/938], Loss: 0.3623315393924713\n",
      "Train: Epoch [15], Batch [478/938], Loss: 0.3673540949821472\n",
      "Train: Epoch [15], Batch [479/938], Loss: 0.6164565086364746\n",
      "Train: Epoch [15], Batch [480/938], Loss: 0.2915848195552826\n",
      "Train: Epoch [15], Batch [481/938], Loss: 0.42514386773109436\n",
      "Train: Epoch [15], Batch [482/938], Loss: 0.3729685842990875\n",
      "Train: Epoch [15], Batch [483/938], Loss: 0.5552401542663574\n",
      "Train: Epoch [15], Batch [484/938], Loss: 0.361240953207016\n",
      "Train: Epoch [15], Batch [485/938], Loss: 0.4212147891521454\n",
      "Train: Epoch [15], Batch [486/938], Loss: 0.43864718079566956\n",
      "Train: Epoch [15], Batch [487/938], Loss: 0.6043865084648132\n",
      "Train: Epoch [15], Batch [488/938], Loss: 0.5346163511276245\n",
      "Train: Epoch [15], Batch [489/938], Loss: 0.457217812538147\n",
      "Train: Epoch [15], Batch [490/938], Loss: 0.41231441497802734\n",
      "Train: Epoch [15], Batch [491/938], Loss: 0.37061774730682373\n",
      "Train: Epoch [15], Batch [492/938], Loss: 0.373555064201355\n",
      "Train: Epoch [15], Batch [493/938], Loss: 0.3109080195426941\n",
      "Train: Epoch [15], Batch [494/938], Loss: 0.47446200251579285\n",
      "Train: Epoch [15], Batch [495/938], Loss: 0.3941153287887573\n",
      "Train: Epoch [15], Batch [496/938], Loss: 0.3248227536678314\n",
      "Train: Epoch [15], Batch [497/938], Loss: 0.47344428300857544\n",
      "Train: Epoch [15], Batch [498/938], Loss: 0.43779394030570984\n",
      "Train: Epoch [15], Batch [499/938], Loss: 0.35351091623306274\n",
      "Train: Epoch [15], Batch [500/938], Loss: 0.33973807096481323\n",
      "Train: Epoch [15], Batch [501/938], Loss: 0.3969441056251526\n",
      "Train: Epoch [15], Batch [502/938], Loss: 0.437065452337265\n",
      "Train: Epoch [15], Batch [503/938], Loss: 0.6931846737861633\n",
      "Train: Epoch [15], Batch [504/938], Loss: 0.4439520537853241\n",
      "Train: Epoch [15], Batch [505/938], Loss: 0.2992191016674042\n",
      "Train: Epoch [15], Batch [506/938], Loss: 0.6408511996269226\n",
      "Train: Epoch [15], Batch [507/938], Loss: 0.656506359577179\n",
      "Train: Epoch [15], Batch [508/938], Loss: 0.44122564792633057\n",
      "Train: Epoch [15], Batch [509/938], Loss: 0.391194611787796\n",
      "Train: Epoch [15], Batch [510/938], Loss: 0.42054641246795654\n",
      "Train: Epoch [15], Batch [511/938], Loss: 0.3682347238063812\n",
      "Train: Epoch [15], Batch [512/938], Loss: 0.6063762903213501\n",
      "Train: Epoch [15], Batch [513/938], Loss: 0.44517549872398376\n",
      "Train: Epoch [15], Batch [514/938], Loss: 0.3630762994289398\n",
      "Train: Epoch [15], Batch [515/938], Loss: 0.41469353437423706\n",
      "Train: Epoch [15], Batch [516/938], Loss: 0.38798460364341736\n",
      "Train: Epoch [15], Batch [517/938], Loss: 0.4214538335800171\n",
      "Train: Epoch [15], Batch [518/938], Loss: 0.40604424476623535\n",
      "Train: Epoch [15], Batch [519/938], Loss: 0.5050015449523926\n",
      "Train: Epoch [15], Batch [520/938], Loss: 0.4745359420776367\n",
      "Train: Epoch [15], Batch [521/938], Loss: 0.2824498414993286\n",
      "Train: Epoch [15], Batch [522/938], Loss: 0.3388488292694092\n",
      "Train: Epoch [15], Batch [523/938], Loss: 0.503038763999939\n",
      "Train: Epoch [15], Batch [524/938], Loss: 0.4250788688659668\n",
      "Train: Epoch [15], Batch [525/938], Loss: 0.2066861391067505\n",
      "Train: Epoch [15], Batch [526/938], Loss: 0.5678796172142029\n",
      "Train: Epoch [15], Batch [527/938], Loss: 0.4291667342185974\n",
      "Train: Epoch [15], Batch [528/938], Loss: 0.4371158480644226\n",
      "Train: Epoch [15], Batch [529/938], Loss: 0.31681856513023376\n",
      "Train: Epoch [15], Batch [530/938], Loss: 0.4750831127166748\n",
      "Train: Epoch [15], Batch [531/938], Loss: 0.5634266138076782\n",
      "Train: Epoch [15], Batch [532/938], Loss: 0.597976803779602\n",
      "Train: Epoch [15], Batch [533/938], Loss: 0.4573887288570404\n",
      "Train: Epoch [15], Batch [534/938], Loss: 0.6746851801872253\n",
      "Train: Epoch [15], Batch [535/938], Loss: 0.6390204429626465\n",
      "Train: Epoch [15], Batch [536/938], Loss: 0.5055161118507385\n",
      "Train: Epoch [15], Batch [537/938], Loss: 0.38193443417549133\n",
      "Train: Epoch [15], Batch [538/938], Loss: 0.392275869846344\n",
      "Train: Epoch [15], Batch [539/938], Loss: 0.6391314268112183\n",
      "Train: Epoch [15], Batch [540/938], Loss: 0.3514660596847534\n",
      "Train: Epoch [15], Batch [541/938], Loss: 0.43534034490585327\n",
      "Train: Epoch [15], Batch [542/938], Loss: 0.7582497596740723\n",
      "Train: Epoch [15], Batch [543/938], Loss: 0.36807042360305786\n",
      "Train: Epoch [15], Batch [544/938], Loss: 0.4619141221046448\n",
      "Train: Epoch [15], Batch [545/938], Loss: 0.5717015266418457\n",
      "Train: Epoch [15], Batch [546/938], Loss: 0.38484326004981995\n",
      "Train: Epoch [15], Batch [547/938], Loss: 0.5941851139068604\n",
      "Train: Epoch [15], Batch [548/938], Loss: 0.6227543354034424\n",
      "Train: Epoch [15], Batch [549/938], Loss: 0.5539835691452026\n",
      "Train: Epoch [15], Batch [550/938], Loss: 0.5754811763763428\n",
      "Train: Epoch [15], Batch [551/938], Loss: 0.43998804688453674\n",
      "Train: Epoch [15], Batch [552/938], Loss: 0.6400420665740967\n",
      "Train: Epoch [15], Batch [553/938], Loss: 0.4183451533317566\n",
      "Train: Epoch [15], Batch [554/938], Loss: 0.5160224437713623\n",
      "Train: Epoch [15], Batch [555/938], Loss: 0.4675474464893341\n",
      "Train: Epoch [15], Batch [556/938], Loss: 0.3286590278148651\n",
      "Train: Epoch [15], Batch [557/938], Loss: 0.6519452333450317\n",
      "Train: Epoch [15], Batch [558/938], Loss: 0.4179469645023346\n",
      "Train: Epoch [15], Batch [559/938], Loss: 0.29835426807403564\n",
      "Train: Epoch [15], Batch [560/938], Loss: 0.6086481809616089\n",
      "Train: Epoch [15], Batch [561/938], Loss: 0.5083918571472168\n",
      "Train: Epoch [15], Batch [562/938], Loss: 0.6946665644645691\n",
      "Train: Epoch [15], Batch [563/938], Loss: 0.3651887774467468\n",
      "Train: Epoch [15], Batch [564/938], Loss: 0.38950085639953613\n",
      "Train: Epoch [15], Batch [565/938], Loss: 0.4414995312690735\n",
      "Train: Epoch [15], Batch [566/938], Loss: 0.47720763087272644\n",
      "Train: Epoch [15], Batch [567/938], Loss: 0.39287135004997253\n",
      "Train: Epoch [15], Batch [568/938], Loss: 0.3850672245025635\n",
      "Train: Epoch [15], Batch [569/938], Loss: 0.5015212297439575\n",
      "Train: Epoch [15], Batch [570/938], Loss: 0.465310662984848\n",
      "Train: Epoch [15], Batch [571/938], Loss: 0.5882025957107544\n",
      "Train: Epoch [15], Batch [572/938], Loss: 0.41249150037765503\n",
      "Train: Epoch [15], Batch [573/938], Loss: 0.31977617740631104\n",
      "Train: Epoch [15], Batch [574/938], Loss: 0.32151204347610474\n",
      "Train: Epoch [15], Batch [575/938], Loss: 0.34722018241882324\n",
      "Train: Epoch [15], Batch [576/938], Loss: 0.3895016312599182\n",
      "Train: Epoch [15], Batch [577/938], Loss: 0.2666955292224884\n",
      "Train: Epoch [15], Batch [578/938], Loss: 0.272594690322876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [15], Batch [579/938], Loss: 0.7043697834014893\n",
      "Train: Epoch [15], Batch [580/938], Loss: 0.3597087562084198\n",
      "Train: Epoch [15], Batch [581/938], Loss: 0.4011384844779968\n",
      "Train: Epoch [15], Batch [582/938], Loss: 0.5563369989395142\n",
      "Train: Epoch [15], Batch [583/938], Loss: 0.5771840810775757\n",
      "Train: Epoch [15], Batch [584/938], Loss: 0.5080462098121643\n",
      "Train: Epoch [15], Batch [585/938], Loss: 0.36996546387672424\n",
      "Train: Epoch [15], Batch [586/938], Loss: 0.6153481006622314\n",
      "Train: Epoch [15], Batch [587/938], Loss: 0.6184526681900024\n",
      "Train: Epoch [15], Batch [588/938], Loss: 0.33462703227996826\n",
      "Train: Epoch [15], Batch [589/938], Loss: 0.45662951469421387\n",
      "Train: Epoch [15], Batch [590/938], Loss: 0.34605348110198975\n",
      "Train: Epoch [15], Batch [591/938], Loss: 0.3829956650733948\n",
      "Train: Epoch [15], Batch [592/938], Loss: 0.3724247217178345\n",
      "Train: Epoch [15], Batch [593/938], Loss: 0.4883940815925598\n",
      "Train: Epoch [15], Batch [594/938], Loss: 0.47487351298332214\n",
      "Train: Epoch [15], Batch [595/938], Loss: 0.4025348424911499\n",
      "Train: Epoch [15], Batch [596/938], Loss: 0.389080286026001\n",
      "Train: Epoch [15], Batch [597/938], Loss: 0.3576476573944092\n",
      "Train: Epoch [15], Batch [598/938], Loss: 0.44059643149375916\n",
      "Train: Epoch [15], Batch [599/938], Loss: 0.5603660345077515\n",
      "Train: Epoch [15], Batch [600/938], Loss: 0.47329774498939514\n",
      "Train: Epoch [15], Batch [601/938], Loss: 0.597244143486023\n",
      "Train: Epoch [15], Batch [602/938], Loss: 0.6751206517219543\n",
      "Train: Epoch [15], Batch [603/938], Loss: 0.41045576333999634\n",
      "Train: Epoch [15], Batch [604/938], Loss: 0.7928822040557861\n",
      "Train: Epoch [15], Batch [605/938], Loss: 0.5089375972747803\n",
      "Train: Epoch [15], Batch [606/938], Loss: 0.4214649200439453\n",
      "Train: Epoch [15], Batch [607/938], Loss: 0.27699315547943115\n",
      "Train: Epoch [15], Batch [608/938], Loss: 0.32059207558631897\n",
      "Train: Epoch [15], Batch [609/938], Loss: 0.37869107723236084\n",
      "Train: Epoch [15], Batch [610/938], Loss: 0.7271482944488525\n",
      "Train: Epoch [15], Batch [611/938], Loss: 0.4379195272922516\n",
      "Train: Epoch [15], Batch [612/938], Loss: 0.3729705214500427\n",
      "Train: Epoch [15], Batch [613/938], Loss: 0.4733874499797821\n",
      "Train: Epoch [15], Batch [614/938], Loss: 0.30956536531448364\n",
      "Train: Epoch [15], Batch [615/938], Loss: 0.5994985699653625\n",
      "Train: Epoch [15], Batch [616/938], Loss: 0.39767512679100037\n",
      "Train: Epoch [15], Batch [617/938], Loss: 0.547067403793335\n",
      "Train: Epoch [15], Batch [618/938], Loss: 0.3569541573524475\n",
      "Train: Epoch [15], Batch [619/938], Loss: 0.6422230005264282\n",
      "Train: Epoch [15], Batch [620/938], Loss: 0.3573889434337616\n",
      "Train: Epoch [15], Batch [621/938], Loss: 0.48378506302833557\n",
      "Train: Epoch [15], Batch [622/938], Loss: 0.24677860736846924\n",
      "Train: Epoch [15], Batch [623/938], Loss: 0.32668912410736084\n",
      "Train: Epoch [15], Batch [624/938], Loss: 0.5145623683929443\n",
      "Train: Epoch [15], Batch [625/938], Loss: 0.2337220162153244\n",
      "Train: Epoch [15], Batch [626/938], Loss: 0.5822534561157227\n",
      "Train: Epoch [15], Batch [627/938], Loss: 0.5251246094703674\n",
      "Train: Epoch [15], Batch [628/938], Loss: 0.6169620752334595\n",
      "Train: Epoch [15], Batch [629/938], Loss: 0.4696493446826935\n",
      "Train: Epoch [15], Batch [630/938], Loss: 0.4557727575302124\n",
      "Train: Epoch [15], Batch [631/938], Loss: 0.3693166673183441\n",
      "Train: Epoch [15], Batch [632/938], Loss: 0.5795446634292603\n",
      "Train: Epoch [15], Batch [633/938], Loss: 0.5060062408447266\n",
      "Train: Epoch [15], Batch [634/938], Loss: 0.5023365616798401\n",
      "Train: Epoch [15], Batch [635/938], Loss: 0.3790724277496338\n",
      "Train: Epoch [15], Batch [636/938], Loss: 0.4397149980068207\n",
      "Train: Epoch [15], Batch [637/938], Loss: 0.3885147273540497\n",
      "Train: Epoch [15], Batch [638/938], Loss: 0.4598427414894104\n",
      "Train: Epoch [15], Batch [639/938], Loss: 0.450842022895813\n",
      "Train: Epoch [15], Batch [640/938], Loss: 0.44031742215156555\n",
      "Train: Epoch [15], Batch [641/938], Loss: 0.35365521907806396\n",
      "Train: Epoch [15], Batch [642/938], Loss: 0.5131194591522217\n",
      "Train: Epoch [15], Batch [643/938], Loss: 0.3171875774860382\n",
      "Train: Epoch [15], Batch [644/938], Loss: 0.5157878398895264\n",
      "Train: Epoch [15], Batch [645/938], Loss: 0.5913265943527222\n",
      "Train: Epoch [15], Batch [646/938], Loss: 0.5157923698425293\n",
      "Train: Epoch [15], Batch [647/938], Loss: 0.3640337586402893\n",
      "Train: Epoch [15], Batch [648/938], Loss: 0.3762705326080322\n",
      "Train: Epoch [15], Batch [649/938], Loss: 0.3794931173324585\n",
      "Train: Epoch [15], Batch [650/938], Loss: 0.41945621371269226\n",
      "Train: Epoch [15], Batch [651/938], Loss: 0.46145936846733093\n",
      "Train: Epoch [15], Batch [652/938], Loss: 0.5972424745559692\n",
      "Train: Epoch [15], Batch [653/938], Loss: 0.431013822555542\n",
      "Train: Epoch [15], Batch [654/938], Loss: 0.2948608100414276\n",
      "Train: Epoch [15], Batch [655/938], Loss: 0.35300636291503906\n",
      "Train: Epoch [15], Batch [656/938], Loss: 0.38076525926589966\n",
      "Train: Epoch [15], Batch [657/938], Loss: 0.5199535489082336\n",
      "Train: Epoch [15], Batch [658/938], Loss: 0.5030840635299683\n",
      "Train: Epoch [15], Batch [659/938], Loss: 0.2608017921447754\n",
      "Train: Epoch [15], Batch [660/938], Loss: 0.38901931047439575\n",
      "Train: Epoch [15], Batch [661/938], Loss: 0.5681113600730896\n",
      "Train: Epoch [15], Batch [662/938], Loss: 0.7669461369514465\n",
      "Train: Epoch [15], Batch [663/938], Loss: 0.4927888512611389\n",
      "Train: Epoch [15], Batch [664/938], Loss: 0.22914206981658936\n",
      "Train: Epoch [15], Batch [665/938], Loss: 0.40416717529296875\n",
      "Train: Epoch [15], Batch [666/938], Loss: 0.5876072645187378\n",
      "Train: Epoch [15], Batch [667/938], Loss: 0.7271565794944763\n",
      "Train: Epoch [15], Batch [668/938], Loss: 0.4427737891674042\n",
      "Train: Epoch [15], Batch [669/938], Loss: 0.3353610634803772\n",
      "Train: Epoch [15], Batch [670/938], Loss: 0.37081214785575867\n",
      "Train: Epoch [15], Batch [671/938], Loss: 0.7306346297264099\n",
      "Train: Epoch [15], Batch [672/938], Loss: 0.46484607458114624\n",
      "Train: Epoch [15], Batch [673/938], Loss: 0.3491820693016052\n",
      "Train: Epoch [15], Batch [674/938], Loss: 0.35842499136924744\n",
      "Train: Epoch [15], Batch [675/938], Loss: 0.45274096727371216\n",
      "Train: Epoch [15], Batch [676/938], Loss: 0.5937495231628418\n",
      "Train: Epoch [15], Batch [677/938], Loss: 0.44547078013420105\n",
      "Train: Epoch [15], Batch [678/938], Loss: 0.42717790603637695\n",
      "Train: Epoch [15], Batch [679/938], Loss: 0.40618452429771423\n",
      "Train: Epoch [15], Batch [680/938], Loss: 0.5428212881088257\n",
      "Train: Epoch [15], Batch [681/938], Loss: 0.5567896366119385\n",
      "Train: Epoch [15], Batch [682/938], Loss: 0.4592965543270111\n",
      "Train: Epoch [15], Batch [683/938], Loss: 0.40195703506469727\n",
      "Train: Epoch [15], Batch [684/938], Loss: 0.6178944110870361\n",
      "Train: Epoch [15], Batch [685/938], Loss: 0.41880929470062256\n",
      "Train: Epoch [15], Batch [686/938], Loss: 0.5110296010971069\n",
      "Train: Epoch [15], Batch [687/938], Loss: 0.43773728609085083\n",
      "Train: Epoch [15], Batch [688/938], Loss: 0.3895089626312256\n",
      "Train: Epoch [15], Batch [689/938], Loss: 0.621440052986145\n",
      "Train: Epoch [15], Batch [690/938], Loss: 0.4781762361526489\n",
      "Train: Epoch [15], Batch [691/938], Loss: 0.33490753173828125\n",
      "Train: Epoch [15], Batch [692/938], Loss: 0.6170675754547119\n",
      "Train: Epoch [15], Batch [693/938], Loss: 0.5673775672912598\n",
      "Train: Epoch [15], Batch [694/938], Loss: 0.48761865496635437\n",
      "Train: Epoch [15], Batch [695/938], Loss: 0.3953310251235962\n",
      "Train: Epoch [15], Batch [696/938], Loss: 0.4904680550098419\n",
      "Train: Epoch [15], Batch [697/938], Loss: 0.63218092918396\n",
      "Train: Epoch [15], Batch [698/938], Loss: 0.35672223567962646\n",
      "Train: Epoch [15], Batch [699/938], Loss: 0.310167521238327\n",
      "Train: Epoch [15], Batch [700/938], Loss: 0.3514730930328369\n",
      "Train: Epoch [15], Batch [701/938], Loss: 0.32091134786605835\n",
      "Train: Epoch [15], Batch [702/938], Loss: 0.4814488887786865\n",
      "Train: Epoch [15], Batch [703/938], Loss: 0.5758098363876343\n",
      "Train: Epoch [15], Batch [704/938], Loss: 0.5104199051856995\n",
      "Train: Epoch [15], Batch [705/938], Loss: 0.5548957586288452\n",
      "Train: Epoch [15], Batch [706/938], Loss: 0.3922888934612274\n",
      "Train: Epoch [15], Batch [707/938], Loss: 0.4004257321357727\n",
      "Train: Epoch [15], Batch [708/938], Loss: 0.5448987483978271\n",
      "Train: Epoch [15], Batch [709/938], Loss: 0.3910548686981201\n",
      "Train: Epoch [15], Batch [710/938], Loss: 0.5007071495056152\n",
      "Train: Epoch [15], Batch [711/938], Loss: 0.6305361986160278\n",
      "Train: Epoch [15], Batch [712/938], Loss: 0.43232977390289307\n",
      "Train: Epoch [15], Batch [713/938], Loss: 0.4051803946495056\n",
      "Train: Epoch [15], Batch [714/938], Loss: 0.2989197075366974\n",
      "Train: Epoch [15], Batch [715/938], Loss: 0.44637906551361084\n",
      "Train: Epoch [15], Batch [716/938], Loss: 0.46537917852401733\n",
      "Train: Epoch [15], Batch [717/938], Loss: 0.5730243921279907\n",
      "Train: Epoch [15], Batch [718/938], Loss: 0.33841291069984436\n",
      "Train: Epoch [15], Batch [719/938], Loss: 0.5470045804977417\n",
      "Train: Epoch [15], Batch [720/938], Loss: 0.49139681458473206\n",
      "Train: Epoch [15], Batch [721/938], Loss: 0.5340304970741272\n",
      "Train: Epoch [15], Batch [722/938], Loss: 0.5214353799819946\n",
      "Train: Epoch [15], Batch [723/938], Loss: 0.40041595697402954\n",
      "Train: Epoch [15], Batch [724/938], Loss: 0.4620824456214905\n",
      "Train: Epoch [15], Batch [725/938], Loss: 0.42627614736557007\n",
      "Train: Epoch [15], Batch [726/938], Loss: 0.42229676246643066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [15], Batch [727/938], Loss: 0.5018590688705444\n",
      "Train: Epoch [15], Batch [728/938], Loss: 0.42844486236572266\n",
      "Train: Epoch [15], Batch [729/938], Loss: 0.6517250537872314\n",
      "Train: Epoch [15], Batch [730/938], Loss: 0.36125504970550537\n",
      "Train: Epoch [15], Batch [731/938], Loss: 0.4460059106349945\n",
      "Train: Epoch [15], Batch [732/938], Loss: 0.40941911935806274\n",
      "Train: Epoch [15], Batch [733/938], Loss: 0.5324263572692871\n",
      "Train: Epoch [15], Batch [734/938], Loss: 0.5087127685546875\n",
      "Train: Epoch [15], Batch [735/938], Loss: 0.7185333967208862\n",
      "Train: Epoch [15], Batch [736/938], Loss: 0.40982818603515625\n",
      "Train: Epoch [15], Batch [737/938], Loss: 0.41004642844200134\n",
      "Train: Epoch [15], Batch [738/938], Loss: 0.4201786518096924\n",
      "Train: Epoch [15], Batch [739/938], Loss: 0.3435312509536743\n",
      "Train: Epoch [15], Batch [740/938], Loss: 0.43272656202316284\n",
      "Train: Epoch [15], Batch [741/938], Loss: 0.4519973397254944\n",
      "Train: Epoch [15], Batch [742/938], Loss: 0.5875982642173767\n",
      "Train: Epoch [15], Batch [743/938], Loss: 0.47943681478500366\n",
      "Train: Epoch [15], Batch [744/938], Loss: 0.4114319682121277\n",
      "Train: Epoch [15], Batch [745/938], Loss: 0.5150313377380371\n",
      "Train: Epoch [15], Batch [746/938], Loss: 0.3798801004886627\n",
      "Train: Epoch [15], Batch [747/938], Loss: 0.6467937231063843\n",
      "Train: Epoch [15], Batch [748/938], Loss: 0.5752331614494324\n",
      "Train: Epoch [15], Batch [749/938], Loss: 0.43333202600479126\n",
      "Train: Epoch [15], Batch [750/938], Loss: 0.562741219997406\n",
      "Train: Epoch [15], Batch [751/938], Loss: 0.5662953853607178\n",
      "Train: Epoch [15], Batch [752/938], Loss: 0.4383780360221863\n",
      "Train: Epoch [15], Batch [753/938], Loss: 0.41884341835975647\n",
      "Train: Epoch [15], Batch [754/938], Loss: 0.42215070128440857\n",
      "Train: Epoch [15], Batch [755/938], Loss: 0.42867332696914673\n",
      "Train: Epoch [15], Batch [756/938], Loss: 0.6354202032089233\n",
      "Train: Epoch [15], Batch [757/938], Loss: 0.48369377851486206\n",
      "Train: Epoch [15], Batch [758/938], Loss: 0.46748584508895874\n",
      "Train: Epoch [15], Batch [759/938], Loss: 0.4276393949985504\n",
      "Train: Epoch [15], Batch [760/938], Loss: 0.4098036587238312\n",
      "Train: Epoch [15], Batch [761/938], Loss: 0.3773479461669922\n",
      "Train: Epoch [15], Batch [762/938], Loss: 0.5576845407485962\n",
      "Train: Epoch [15], Batch [763/938], Loss: 0.28856387734413147\n",
      "Train: Epoch [15], Batch [764/938], Loss: 0.3262539505958557\n",
      "Train: Epoch [15], Batch [765/938], Loss: 0.4232454299926758\n",
      "Train: Epoch [15], Batch [766/938], Loss: 0.514267086982727\n",
      "Train: Epoch [15], Batch [767/938], Loss: 0.41098248958587646\n",
      "Train: Epoch [15], Batch [768/938], Loss: 0.3017500638961792\n",
      "Train: Epoch [15], Batch [769/938], Loss: 0.40363380312919617\n",
      "Train: Epoch [15], Batch [770/938], Loss: 0.459622859954834\n",
      "Train: Epoch [15], Batch [771/938], Loss: 0.48546332120895386\n",
      "Train: Epoch [15], Batch [772/938], Loss: 0.4998117685317993\n",
      "Train: Epoch [15], Batch [773/938], Loss: 0.4289975166320801\n",
      "Train: Epoch [15], Batch [774/938], Loss: 0.6511884331703186\n",
      "Train: Epoch [15], Batch [775/938], Loss: 0.4895424544811249\n",
      "Train: Epoch [15], Batch [776/938], Loss: 0.3006591200828552\n",
      "Train: Epoch [15], Batch [777/938], Loss: 0.4729066491127014\n",
      "Train: Epoch [15], Batch [778/938], Loss: 0.4753873944282532\n",
      "Train: Epoch [15], Batch [779/938], Loss: 0.6506295204162598\n",
      "Train: Epoch [15], Batch [780/938], Loss: 0.5003399848937988\n",
      "Train: Epoch [15], Batch [781/938], Loss: 0.4241090714931488\n",
      "Train: Epoch [15], Batch [782/938], Loss: 0.376007616519928\n",
      "Train: Epoch [15], Batch [783/938], Loss: 0.3753293454647064\n",
      "Train: Epoch [15], Batch [784/938], Loss: 0.40030747652053833\n",
      "Train: Epoch [15], Batch [785/938], Loss: 0.4324052333831787\n",
      "Train: Epoch [15], Batch [786/938], Loss: 0.6153785586357117\n",
      "Train: Epoch [15], Batch [787/938], Loss: 0.4753003716468811\n",
      "Train: Epoch [15], Batch [788/938], Loss: 0.3625347316265106\n",
      "Train: Epoch [15], Batch [789/938], Loss: 0.3917866349220276\n",
      "Train: Epoch [15], Batch [790/938], Loss: 0.42873984575271606\n",
      "Train: Epoch [15], Batch [791/938], Loss: 0.3282235264778137\n",
      "Train: Epoch [15], Batch [792/938], Loss: 0.5913068056106567\n",
      "Train: Epoch [15], Batch [793/938], Loss: 0.4882519841194153\n",
      "Train: Epoch [15], Batch [794/938], Loss: 0.6690843105316162\n",
      "Train: Epoch [15], Batch [795/938], Loss: 0.5306485891342163\n",
      "Train: Epoch [15], Batch [796/938], Loss: 0.6847048401832581\n",
      "Train: Epoch [15], Batch [797/938], Loss: 0.3230380713939667\n",
      "Train: Epoch [15], Batch [798/938], Loss: 0.5678560733795166\n",
      "Train: Epoch [15], Batch [799/938], Loss: 0.6718928813934326\n",
      "Train: Epoch [15], Batch [800/938], Loss: 0.32992851734161377\n",
      "Train: Epoch [15], Batch [801/938], Loss: 0.3739326596260071\n",
      "Train: Epoch [15], Batch [802/938], Loss: 0.460293710231781\n",
      "Train: Epoch [15], Batch [803/938], Loss: 0.38759908080101013\n",
      "Train: Epoch [15], Batch [804/938], Loss: 0.37703245878219604\n",
      "Train: Epoch [15], Batch [805/938], Loss: 0.38376539945602417\n",
      "Train: Epoch [15], Batch [806/938], Loss: 0.4769776463508606\n",
      "Train: Epoch [15], Batch [807/938], Loss: 0.2979559898376465\n",
      "Train: Epoch [15], Batch [808/938], Loss: 0.4576740860939026\n",
      "Train: Epoch [15], Batch [809/938], Loss: 0.5150509476661682\n",
      "Train: Epoch [15], Batch [810/938], Loss: 0.4586614668369293\n",
      "Train: Epoch [15], Batch [811/938], Loss: 0.28124257922172546\n",
      "Train: Epoch [15], Batch [812/938], Loss: 0.5205411911010742\n",
      "Train: Epoch [15], Batch [813/938], Loss: 0.742255687713623\n",
      "Train: Epoch [15], Batch [814/938], Loss: 0.8509498834609985\n",
      "Train: Epoch [15], Batch [815/938], Loss: 0.44943171739578247\n",
      "Train: Epoch [15], Batch [816/938], Loss: 0.4124947190284729\n",
      "Train: Epoch [15], Batch [817/938], Loss: 0.5679440498352051\n",
      "Train: Epoch [15], Batch [818/938], Loss: 0.5572925806045532\n",
      "Train: Epoch [15], Batch [819/938], Loss: 0.42569684982299805\n",
      "Train: Epoch [15], Batch [820/938], Loss: 0.37322741746902466\n",
      "Train: Epoch [15], Batch [821/938], Loss: 0.6138565540313721\n",
      "Train: Epoch [15], Batch [822/938], Loss: 0.336061954498291\n",
      "Train: Epoch [15], Batch [823/938], Loss: 0.47704753279685974\n",
      "Train: Epoch [15], Batch [824/938], Loss: 0.5600041151046753\n",
      "Train: Epoch [15], Batch [825/938], Loss: 0.5114694237709045\n",
      "Train: Epoch [15], Batch [826/938], Loss: 0.41455820202827454\n",
      "Train: Epoch [15], Batch [827/938], Loss: 0.42476019263267517\n",
      "Train: Epoch [15], Batch [828/938], Loss: 0.4600200653076172\n",
      "Train: Epoch [15], Batch [829/938], Loss: 0.4389418661594391\n",
      "Train: Epoch [15], Batch [830/938], Loss: 0.32885485887527466\n",
      "Train: Epoch [15], Batch [831/938], Loss: 0.7254370450973511\n",
      "Train: Epoch [15], Batch [832/938], Loss: 0.45644450187683105\n",
      "Train: Epoch [15], Batch [833/938], Loss: 0.3832618296146393\n",
      "Train: Epoch [15], Batch [834/938], Loss: 0.43745866417884827\n",
      "Train: Epoch [15], Batch [835/938], Loss: 0.518140435218811\n",
      "Train: Epoch [15], Batch [836/938], Loss: 0.4390909969806671\n",
      "Train: Epoch [15], Batch [837/938], Loss: 0.5252131223678589\n",
      "Train: Epoch [15], Batch [838/938], Loss: 0.33581116795539856\n",
      "Train: Epoch [15], Batch [839/938], Loss: 0.4014112055301666\n",
      "Train: Epoch [15], Batch [840/938], Loss: 0.4956629276275635\n",
      "Train: Epoch [15], Batch [841/938], Loss: 0.6051963567733765\n",
      "Train: Epoch [15], Batch [842/938], Loss: 0.3886220157146454\n",
      "Train: Epoch [15], Batch [843/938], Loss: 0.4319869875907898\n",
      "Train: Epoch [15], Batch [844/938], Loss: 0.46398797631263733\n",
      "Train: Epoch [15], Batch [845/938], Loss: 0.652534544467926\n",
      "Train: Epoch [15], Batch [846/938], Loss: 0.4810057580471039\n",
      "Train: Epoch [15], Batch [847/938], Loss: 0.350289523601532\n",
      "Train: Epoch [15], Batch [848/938], Loss: 0.6134309768676758\n",
      "Train: Epoch [15], Batch [849/938], Loss: 0.5266028642654419\n",
      "Train: Epoch [15], Batch [850/938], Loss: 0.44018688797950745\n",
      "Train: Epoch [15], Batch [851/938], Loss: 0.4988858997821808\n",
      "Train: Epoch [15], Batch [852/938], Loss: 0.3066929876804352\n",
      "Train: Epoch [15], Batch [853/938], Loss: 0.4371858835220337\n",
      "Train: Epoch [15], Batch [854/938], Loss: 0.4121510982513428\n",
      "Train: Epoch [15], Batch [855/938], Loss: 0.43277958035469055\n",
      "Train: Epoch [15], Batch [856/938], Loss: 0.735741376876831\n",
      "Train: Epoch [15], Batch [857/938], Loss: 0.5832918882369995\n",
      "Train: Epoch [15], Batch [858/938], Loss: 0.482201486825943\n",
      "Train: Epoch [15], Batch [859/938], Loss: 0.1897788643836975\n",
      "Train: Epoch [15], Batch [860/938], Loss: 0.41886547207832336\n",
      "Train: Epoch [15], Batch [861/938], Loss: 0.759555459022522\n",
      "Train: Epoch [15], Batch [862/938], Loss: 0.6313617825508118\n",
      "Train: Epoch [15], Batch [863/938], Loss: 0.5038930177688599\n",
      "Train: Epoch [15], Batch [864/938], Loss: 0.4698012173175812\n",
      "Train: Epoch [15], Batch [865/938], Loss: 0.5817233920097351\n",
      "Train: Epoch [15], Batch [866/938], Loss: 0.5750870704650879\n",
      "Train: Epoch [15], Batch [867/938], Loss: 0.38514444231987\n",
      "Train: Epoch [15], Batch [868/938], Loss: 0.2926596999168396\n",
      "Train: Epoch [15], Batch [869/938], Loss: 0.2126922607421875\n",
      "Train: Epoch [15], Batch [870/938], Loss: 0.4881417155265808\n",
      "Train: Epoch [15], Batch [871/938], Loss: 0.5227371454238892\n",
      "Train: Epoch [15], Batch [872/938], Loss: 0.5659536719322205\n",
      "Train: Epoch [15], Batch [873/938], Loss: 0.4661870002746582\n",
      "Train: Epoch [15], Batch [874/938], Loss: 0.5701465606689453\n",
      "Train: Epoch [15], Batch [875/938], Loss: 0.43402746319770813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [15], Batch [876/938], Loss: 0.5208933353424072\n",
      "Train: Epoch [15], Batch [877/938], Loss: 0.38392484188079834\n",
      "Train: Epoch [15], Batch [878/938], Loss: 0.558881402015686\n",
      "Train: Epoch [15], Batch [879/938], Loss: 0.3934468924999237\n",
      "Train: Epoch [15], Batch [880/938], Loss: 0.4970940947532654\n",
      "Train: Epoch [15], Batch [881/938], Loss: 0.7490373253822327\n",
      "Train: Epoch [15], Batch [882/938], Loss: 0.5382941961288452\n",
      "Train: Epoch [15], Batch [883/938], Loss: 0.5810845494270325\n",
      "Train: Epoch [15], Batch [884/938], Loss: 0.6538118124008179\n",
      "Train: Epoch [15], Batch [885/938], Loss: 0.31791192293167114\n",
      "Train: Epoch [15], Batch [886/938], Loss: 0.4952455759048462\n",
      "Train: Epoch [15], Batch [887/938], Loss: 0.6014831066131592\n",
      "Train: Epoch [15], Batch [888/938], Loss: 0.43204429745674133\n",
      "Train: Epoch [15], Batch [889/938], Loss: 0.2762252390384674\n",
      "Train: Epoch [15], Batch [890/938], Loss: 0.6514300107955933\n",
      "Train: Epoch [15], Batch [891/938], Loss: 0.3647056818008423\n",
      "Train: Epoch [15], Batch [892/938], Loss: 0.3645033538341522\n",
      "Train: Epoch [15], Batch [893/938], Loss: 0.3376285135746002\n",
      "Train: Epoch [15], Batch [894/938], Loss: 0.5296086072921753\n",
      "Train: Epoch [15], Batch [895/938], Loss: 0.22339607775211334\n",
      "Train: Epoch [15], Batch [896/938], Loss: 0.5174840092658997\n",
      "Train: Epoch [15], Batch [897/938], Loss: 0.30652567744255066\n",
      "Train: Epoch [15], Batch [898/938], Loss: 0.34338438510894775\n",
      "Train: Epoch [15], Batch [899/938], Loss: 0.56180739402771\n",
      "Train: Epoch [15], Batch [900/938], Loss: 0.6130020618438721\n",
      "Train: Epoch [15], Batch [901/938], Loss: 0.38097870349884033\n",
      "Train: Epoch [15], Batch [902/938], Loss: 0.5298514366149902\n",
      "Train: Epoch [15], Batch [903/938], Loss: 0.3772386908531189\n",
      "Train: Epoch [15], Batch [904/938], Loss: 0.472299724817276\n",
      "Train: Epoch [15], Batch [905/938], Loss: 0.4009343385696411\n",
      "Train: Epoch [15], Batch [906/938], Loss: 0.5928642153739929\n",
      "Train: Epoch [15], Batch [907/938], Loss: 0.5376300811767578\n",
      "Train: Epoch [15], Batch [908/938], Loss: 0.7620105147361755\n",
      "Train: Epoch [15], Batch [909/938], Loss: 0.5518404841423035\n",
      "Train: Epoch [15], Batch [910/938], Loss: 0.3983962833881378\n",
      "Train: Epoch [15], Batch [911/938], Loss: 0.6425434947013855\n",
      "Train: Epoch [15], Batch [912/938], Loss: 0.536600649356842\n",
      "Train: Epoch [15], Batch [913/938], Loss: 0.3077951669692993\n",
      "Train: Epoch [15], Batch [914/938], Loss: 0.5214896202087402\n",
      "Train: Epoch [15], Batch [915/938], Loss: 0.29057222604751587\n",
      "Train: Epoch [15], Batch [916/938], Loss: 1.222480058670044\n",
      "Train: Epoch [15], Batch [917/938], Loss: 0.35194912552833557\n",
      "Train: Epoch [15], Batch [918/938], Loss: 0.3413856029510498\n",
      "Train: Epoch [15], Batch [919/938], Loss: 0.8416743278503418\n",
      "Train: Epoch [15], Batch [920/938], Loss: 0.3479578197002411\n",
      "Train: Epoch [15], Batch [921/938], Loss: 0.5973633527755737\n",
      "Train: Epoch [15], Batch [922/938], Loss: 0.4234507381916046\n",
      "Train: Epoch [15], Batch [923/938], Loss: 0.39553195238113403\n",
      "Train: Epoch [15], Batch [924/938], Loss: 0.36512961983680725\n",
      "Train: Epoch [15], Batch [925/938], Loss: 0.4610900282859802\n",
      "Train: Epoch [15], Batch [926/938], Loss: 0.44075465202331543\n",
      "Train: Epoch [15], Batch [927/938], Loss: 0.35388755798339844\n",
      "Train: Epoch [15], Batch [928/938], Loss: 0.34210342168807983\n",
      "Train: Epoch [15], Batch [929/938], Loss: 0.4619691073894501\n",
      "Train: Epoch [15], Batch [930/938], Loss: 0.5310198068618774\n",
      "Train: Epoch [15], Batch [931/938], Loss: 0.3632664680480957\n",
      "Train: Epoch [15], Batch [932/938], Loss: 0.3869321346282959\n",
      "Train: Epoch [15], Batch [933/938], Loss: 0.21531647443771362\n",
      "Train: Epoch [15], Batch [934/938], Loss: 0.3936152160167694\n",
      "Train: Epoch [15], Batch [935/938], Loss: 0.4056272506713867\n",
      "Train: Epoch [15], Batch [936/938], Loss: 0.365544855594635\n",
      "Train: Epoch [15], Batch [937/938], Loss: 0.5373237133026123\n",
      "Train: Epoch [15], Batch [938/938], Loss: 0.5258899927139282\n",
      "Accuracy of train set: 0.83695\n",
      "Validation: Epoch [15], Batch [1/938], Loss: 0.5293847322463989\n",
      "Validation: Epoch [15], Batch [2/938], Loss: 0.46409937739372253\n",
      "Validation: Epoch [15], Batch [3/938], Loss: 0.46739503741264343\n",
      "Validation: Epoch [15], Batch [4/938], Loss: 0.34988003969192505\n",
      "Validation: Epoch [15], Batch [5/938], Loss: 0.4543353021144867\n",
      "Validation: Epoch [15], Batch [6/938], Loss: 0.5970913171768188\n",
      "Validation: Epoch [15], Batch [7/938], Loss: 0.6356631517410278\n",
      "Validation: Epoch [15], Batch [8/938], Loss: 0.760941743850708\n",
      "Validation: Epoch [15], Batch [9/938], Loss: 0.6361654996871948\n",
      "Validation: Epoch [15], Batch [10/938], Loss: 0.5140522122383118\n",
      "Validation: Epoch [15], Batch [11/938], Loss: 0.4015137553215027\n",
      "Validation: Epoch [15], Batch [12/938], Loss: 0.534649133682251\n",
      "Validation: Epoch [15], Batch [13/938], Loss: 0.45969489216804504\n",
      "Validation: Epoch [15], Batch [14/938], Loss: 0.47411903738975525\n",
      "Validation: Epoch [15], Batch [15/938], Loss: 0.3832740783691406\n",
      "Validation: Epoch [15], Batch [16/938], Loss: 0.43027442693710327\n",
      "Validation: Epoch [15], Batch [17/938], Loss: 0.3663941025733948\n",
      "Validation: Epoch [15], Batch [18/938], Loss: 0.6117715835571289\n",
      "Validation: Epoch [15], Batch [19/938], Loss: 0.36223524808883667\n",
      "Validation: Epoch [15], Batch [20/938], Loss: 0.34539276361465454\n",
      "Validation: Epoch [15], Batch [21/938], Loss: 0.5100231766700745\n",
      "Validation: Epoch [15], Batch [22/938], Loss: 0.438770592212677\n",
      "Validation: Epoch [15], Batch [23/938], Loss: 0.41022253036499023\n",
      "Validation: Epoch [15], Batch [24/938], Loss: 0.6971902847290039\n",
      "Validation: Epoch [15], Batch [25/938], Loss: 0.3465122878551483\n",
      "Validation: Epoch [15], Batch [26/938], Loss: 0.4056780934333801\n",
      "Validation: Epoch [15], Batch [27/938], Loss: 0.48408159613609314\n",
      "Validation: Epoch [15], Batch [28/938], Loss: 0.2764468789100647\n",
      "Validation: Epoch [15], Batch [29/938], Loss: 0.5573509931564331\n",
      "Validation: Epoch [15], Batch [30/938], Loss: 0.331484317779541\n",
      "Validation: Epoch [15], Batch [31/938], Loss: 0.6315146684646606\n",
      "Validation: Epoch [15], Batch [32/938], Loss: 0.672561764717102\n",
      "Validation: Epoch [15], Batch [33/938], Loss: 0.5607454776763916\n",
      "Validation: Epoch [15], Batch [34/938], Loss: 0.3851543068885803\n",
      "Validation: Epoch [15], Batch [35/938], Loss: 0.38001829385757446\n",
      "Validation: Epoch [15], Batch [36/938], Loss: 0.396699994802475\n",
      "Validation: Epoch [15], Batch [37/938], Loss: 0.32042962312698364\n",
      "Validation: Epoch [15], Batch [38/938], Loss: 0.5268552899360657\n",
      "Validation: Epoch [15], Batch [39/938], Loss: 0.4660130441188812\n",
      "Validation: Epoch [15], Batch [40/938], Loss: 0.4017629027366638\n",
      "Validation: Epoch [15], Batch [41/938], Loss: 0.4858874976634979\n",
      "Validation: Epoch [15], Batch [42/938], Loss: 0.3296968936920166\n",
      "Validation: Epoch [15], Batch [43/938], Loss: 0.28746485710144043\n",
      "Validation: Epoch [15], Batch [44/938], Loss: 0.5363526344299316\n",
      "Validation: Epoch [15], Batch [45/938], Loss: 0.35509154200553894\n",
      "Validation: Epoch [15], Batch [46/938], Loss: 0.4596216082572937\n",
      "Validation: Epoch [15], Batch [47/938], Loss: 0.5233219861984253\n",
      "Validation: Epoch [15], Batch [48/938], Loss: 0.45335012674331665\n",
      "Validation: Epoch [15], Batch [49/938], Loss: 0.4652791917324066\n",
      "Validation: Epoch [15], Batch [50/938], Loss: 0.40860646963119507\n",
      "Validation: Epoch [15], Batch [51/938], Loss: 0.3132293224334717\n",
      "Validation: Epoch [15], Batch [52/938], Loss: 0.38545626401901245\n",
      "Validation: Epoch [15], Batch [53/938], Loss: 0.5038338899612427\n",
      "Validation: Epoch [15], Batch [54/938], Loss: 0.29274481534957886\n",
      "Validation: Epoch [15], Batch [55/938], Loss: 0.37026429176330566\n",
      "Validation: Epoch [15], Batch [56/938], Loss: 0.3623417019844055\n",
      "Validation: Epoch [15], Batch [57/938], Loss: 0.44573837518692017\n",
      "Validation: Epoch [15], Batch [58/938], Loss: 0.5552173852920532\n",
      "Validation: Epoch [15], Batch [59/938], Loss: 0.40179121494293213\n",
      "Validation: Epoch [15], Batch [60/938], Loss: 0.6583107709884644\n",
      "Validation: Epoch [15], Batch [61/938], Loss: 0.4912048578262329\n",
      "Validation: Epoch [15], Batch [62/938], Loss: 0.4697398543357849\n",
      "Validation: Epoch [15], Batch [63/938], Loss: 0.37569981813430786\n",
      "Validation: Epoch [15], Batch [64/938], Loss: 0.39444807171821594\n",
      "Validation: Epoch [15], Batch [65/938], Loss: 0.5088344216346741\n",
      "Validation: Epoch [15], Batch [66/938], Loss: 0.30035144090652466\n",
      "Validation: Epoch [15], Batch [67/938], Loss: 0.4527879059314728\n",
      "Validation: Epoch [15], Batch [68/938], Loss: 0.49232301115989685\n",
      "Validation: Epoch [15], Batch [69/938], Loss: 0.5169420838356018\n",
      "Validation: Epoch [15], Batch [70/938], Loss: 0.5641906261444092\n",
      "Validation: Epoch [15], Batch [71/938], Loss: 0.5008420944213867\n",
      "Validation: Epoch [15], Batch [72/938], Loss: 0.3817327618598938\n",
      "Validation: Epoch [15], Batch [73/938], Loss: 0.2908805012702942\n",
      "Validation: Epoch [15], Batch [74/938], Loss: 0.25441503524780273\n",
      "Validation: Epoch [15], Batch [75/938], Loss: 0.40006223320961\n",
      "Validation: Epoch [15], Batch [76/938], Loss: 0.4152964949607849\n",
      "Validation: Epoch [15], Batch [77/938], Loss: 0.3463364243507385\n",
      "Validation: Epoch [15], Batch [78/938], Loss: 0.30441996455192566\n",
      "Validation: Epoch [15], Batch [79/938], Loss: 0.30679506063461304\n",
      "Validation: Epoch [15], Batch [80/938], Loss: 0.32133904099464417\n",
      "Validation: Epoch [15], Batch [81/938], Loss: 0.5802986025810242\n",
      "Validation: Epoch [15], Batch [82/938], Loss: 0.46068328619003296\n",
      "Validation: Epoch [15], Batch [83/938], Loss: 0.4073556065559387\n",
      "Validation: Epoch [15], Batch [84/938], Loss: 0.34714147448539734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [85/938], Loss: 0.6653344631195068\n",
      "Validation: Epoch [15], Batch [86/938], Loss: 0.44698190689086914\n",
      "Validation: Epoch [15], Batch [87/938], Loss: 0.38672977685928345\n",
      "Validation: Epoch [15], Batch [88/938], Loss: 0.8639171123504639\n",
      "Validation: Epoch [15], Batch [89/938], Loss: 0.49431300163269043\n",
      "Validation: Epoch [15], Batch [90/938], Loss: 0.47492367029190063\n",
      "Validation: Epoch [15], Batch [91/938], Loss: 0.32350653409957886\n",
      "Validation: Epoch [15], Batch [92/938], Loss: 0.5555737018585205\n",
      "Validation: Epoch [15], Batch [93/938], Loss: 0.42447102069854736\n",
      "Validation: Epoch [15], Batch [94/938], Loss: 0.2916486859321594\n",
      "Validation: Epoch [15], Batch [95/938], Loss: 0.3309677243232727\n",
      "Validation: Epoch [15], Batch [96/938], Loss: 0.3640563488006592\n",
      "Validation: Epoch [15], Batch [97/938], Loss: 0.3800960183143616\n",
      "Validation: Epoch [15], Batch [98/938], Loss: 0.25857672095298767\n",
      "Validation: Epoch [15], Batch [99/938], Loss: 0.3891516327857971\n",
      "Validation: Epoch [15], Batch [100/938], Loss: 0.502060055732727\n",
      "Validation: Epoch [15], Batch [101/938], Loss: 0.4246266484260559\n",
      "Validation: Epoch [15], Batch [102/938], Loss: 0.34994128346443176\n",
      "Validation: Epoch [15], Batch [103/938], Loss: 0.3908666968345642\n",
      "Validation: Epoch [15], Batch [104/938], Loss: 0.35705721378326416\n",
      "Validation: Epoch [15], Batch [105/938], Loss: 0.4254333972930908\n",
      "Validation: Epoch [15], Batch [106/938], Loss: 0.37884727120399475\n",
      "Validation: Epoch [15], Batch [107/938], Loss: 0.36684778332710266\n",
      "Validation: Epoch [15], Batch [108/938], Loss: 0.4064897298812866\n",
      "Validation: Epoch [15], Batch [109/938], Loss: 0.5729191303253174\n",
      "Validation: Epoch [15], Batch [110/938], Loss: 0.21851342916488647\n",
      "Validation: Epoch [15], Batch [111/938], Loss: 0.37838947772979736\n",
      "Validation: Epoch [15], Batch [112/938], Loss: 0.34998854994773865\n",
      "Validation: Epoch [15], Batch [113/938], Loss: 0.6210439205169678\n",
      "Validation: Epoch [15], Batch [114/938], Loss: 0.26318812370300293\n",
      "Validation: Epoch [15], Batch [115/938], Loss: 0.411314994096756\n",
      "Validation: Epoch [15], Batch [116/938], Loss: 0.4518764615058899\n",
      "Validation: Epoch [15], Batch [117/938], Loss: 0.33138272166252136\n",
      "Validation: Epoch [15], Batch [118/938], Loss: 0.42799314856529236\n",
      "Validation: Epoch [15], Batch [119/938], Loss: 0.3510017693042755\n",
      "Validation: Epoch [15], Batch [120/938], Loss: 0.5843167304992676\n",
      "Validation: Epoch [15], Batch [121/938], Loss: 0.33074408769607544\n",
      "Validation: Epoch [15], Batch [122/938], Loss: 0.42010220885276794\n",
      "Validation: Epoch [15], Batch [123/938], Loss: 0.5166319608688354\n",
      "Validation: Epoch [15], Batch [124/938], Loss: 0.2732805609703064\n",
      "Validation: Epoch [15], Batch [125/938], Loss: 0.39121556282043457\n",
      "Validation: Epoch [15], Batch [126/938], Loss: 0.4038519263267517\n",
      "Validation: Epoch [15], Batch [127/938], Loss: 0.4415428042411804\n",
      "Validation: Epoch [15], Batch [128/938], Loss: 0.488832026720047\n",
      "Validation: Epoch [15], Batch [129/938], Loss: 0.39206403493881226\n",
      "Validation: Epoch [15], Batch [130/938], Loss: 0.36555197834968567\n",
      "Validation: Epoch [15], Batch [131/938], Loss: 0.506973385810852\n",
      "Validation: Epoch [15], Batch [132/938], Loss: 0.4470415413379669\n",
      "Validation: Epoch [15], Batch [133/938], Loss: 0.5697661638259888\n",
      "Validation: Epoch [15], Batch [134/938], Loss: 0.6501370668411255\n",
      "Validation: Epoch [15], Batch [135/938], Loss: 0.49555131793022156\n",
      "Validation: Epoch [15], Batch [136/938], Loss: 0.4918533265590668\n",
      "Validation: Epoch [15], Batch [137/938], Loss: 0.48655804991722107\n",
      "Validation: Epoch [15], Batch [138/938], Loss: 0.3399089574813843\n",
      "Validation: Epoch [15], Batch [139/938], Loss: 0.4046175181865692\n",
      "Validation: Epoch [15], Batch [140/938], Loss: 0.5190861225128174\n",
      "Validation: Epoch [15], Batch [141/938], Loss: 0.37193429470062256\n",
      "Validation: Epoch [15], Batch [142/938], Loss: 0.4272134304046631\n",
      "Validation: Epoch [15], Batch [143/938], Loss: 0.28299808502197266\n",
      "Validation: Epoch [15], Batch [144/938], Loss: 0.37708455324172974\n",
      "Validation: Epoch [15], Batch [145/938], Loss: 0.6061782836914062\n",
      "Validation: Epoch [15], Batch [146/938], Loss: 0.39212965965270996\n",
      "Validation: Epoch [15], Batch [147/938], Loss: 0.38970744609832764\n",
      "Validation: Epoch [15], Batch [148/938], Loss: 0.40623560547828674\n",
      "Validation: Epoch [15], Batch [149/938], Loss: 0.5799108743667603\n",
      "Validation: Epoch [15], Batch [150/938], Loss: 0.34569820761680603\n",
      "Validation: Epoch [15], Batch [151/938], Loss: 0.4852421283721924\n",
      "Validation: Epoch [15], Batch [152/938], Loss: 0.4213399887084961\n",
      "Validation: Epoch [15], Batch [153/938], Loss: 0.5172141194343567\n",
      "Validation: Epoch [15], Batch [154/938], Loss: 0.3522941470146179\n",
      "Validation: Epoch [15], Batch [155/938], Loss: 0.6093628406524658\n",
      "Validation: Epoch [15], Batch [156/938], Loss: 0.41251230239868164\n",
      "Validation: Epoch [15], Batch [157/938], Loss: 0.4515075087547302\n",
      "Validation: Epoch [15], Batch [158/938], Loss: 0.510360062122345\n",
      "Validation: Epoch [15], Batch [159/938], Loss: 0.4619642496109009\n",
      "Validation: Epoch [15], Batch [160/938], Loss: 0.6361379027366638\n",
      "Validation: Epoch [15], Batch [161/938], Loss: 0.624946117401123\n",
      "Validation: Epoch [15], Batch [162/938], Loss: 0.4444843828678131\n",
      "Validation: Epoch [15], Batch [163/938], Loss: 0.37965574860572815\n",
      "Validation: Epoch [15], Batch [164/938], Loss: 0.4026893079280853\n",
      "Validation: Epoch [15], Batch [165/938], Loss: 0.35653048753738403\n",
      "Validation: Epoch [15], Batch [166/938], Loss: 0.49990034103393555\n",
      "Validation: Epoch [15], Batch [167/938], Loss: 0.4728007912635803\n",
      "Validation: Epoch [15], Batch [168/938], Loss: 0.5879058837890625\n",
      "Validation: Epoch [15], Batch [169/938], Loss: 0.3899717926979065\n",
      "Validation: Epoch [15], Batch [170/938], Loss: 0.5601539015769958\n",
      "Validation: Epoch [15], Batch [171/938], Loss: 0.4555111527442932\n",
      "Validation: Epoch [15], Batch [172/938], Loss: 0.4358561336994171\n",
      "Validation: Epoch [15], Batch [173/938], Loss: 0.617174506187439\n",
      "Validation: Epoch [15], Batch [174/938], Loss: 0.3047785460948944\n",
      "Validation: Epoch [15], Batch [175/938], Loss: 0.5222924947738647\n",
      "Validation: Epoch [15], Batch [176/938], Loss: 0.4720192849636078\n",
      "Validation: Epoch [15], Batch [177/938], Loss: 0.45463529229164124\n",
      "Validation: Epoch [15], Batch [178/938], Loss: 0.6111236214637756\n",
      "Validation: Epoch [15], Batch [179/938], Loss: 0.41399237513542175\n",
      "Validation: Epoch [15], Batch [180/938], Loss: 0.33436596393585205\n",
      "Validation: Epoch [15], Batch [181/938], Loss: 0.5677387714385986\n",
      "Validation: Epoch [15], Batch [182/938], Loss: 0.40959039330482483\n",
      "Validation: Epoch [15], Batch [183/938], Loss: 0.5202390551567078\n",
      "Validation: Epoch [15], Batch [184/938], Loss: 0.5367038249969482\n",
      "Validation: Epoch [15], Batch [185/938], Loss: 0.4211777448654175\n",
      "Validation: Epoch [15], Batch [186/938], Loss: 0.3133571445941925\n",
      "Validation: Epoch [15], Batch [187/938], Loss: 0.5340628623962402\n",
      "Validation: Epoch [15], Batch [188/938], Loss: 0.523317813873291\n",
      "Validation: Epoch [15], Batch [189/938], Loss: 0.3947937488555908\n",
      "Validation: Epoch [15], Batch [190/938], Loss: 0.6694192290306091\n",
      "Validation: Epoch [15], Batch [191/938], Loss: 0.36106061935424805\n",
      "Validation: Epoch [15], Batch [192/938], Loss: 0.5395159721374512\n",
      "Validation: Epoch [15], Batch [193/938], Loss: 0.412917822599411\n",
      "Validation: Epoch [15], Batch [194/938], Loss: 0.4680793285369873\n",
      "Validation: Epoch [15], Batch [195/938], Loss: 0.4965062439441681\n",
      "Validation: Epoch [15], Batch [196/938], Loss: 0.4744194746017456\n",
      "Validation: Epoch [15], Batch [197/938], Loss: 0.40929898619651794\n",
      "Validation: Epoch [15], Batch [198/938], Loss: 0.49628376960754395\n",
      "Validation: Epoch [15], Batch [199/938], Loss: 0.3677513301372528\n",
      "Validation: Epoch [15], Batch [200/938], Loss: 0.4608522057533264\n",
      "Validation: Epoch [15], Batch [201/938], Loss: 0.43623727560043335\n",
      "Validation: Epoch [15], Batch [202/938], Loss: 0.37686121463775635\n",
      "Validation: Epoch [15], Batch [203/938], Loss: 0.47135859727859497\n",
      "Validation: Epoch [15], Batch [204/938], Loss: 0.40837615728378296\n",
      "Validation: Epoch [15], Batch [205/938], Loss: 0.3798724412918091\n",
      "Validation: Epoch [15], Batch [206/938], Loss: 0.41825634241104126\n",
      "Validation: Epoch [15], Batch [207/938], Loss: 0.5377333164215088\n",
      "Validation: Epoch [15], Batch [208/938], Loss: 0.37527352571487427\n",
      "Validation: Epoch [15], Batch [209/938], Loss: 0.47025761008262634\n",
      "Validation: Epoch [15], Batch [210/938], Loss: 0.35099369287490845\n",
      "Validation: Epoch [15], Batch [211/938], Loss: 0.6050832271575928\n",
      "Validation: Epoch [15], Batch [212/938], Loss: 0.4215666353702545\n",
      "Validation: Epoch [15], Batch [213/938], Loss: 0.6261399984359741\n",
      "Validation: Epoch [15], Batch [214/938], Loss: 0.28249043226242065\n",
      "Validation: Epoch [15], Batch [215/938], Loss: 0.5564090609550476\n",
      "Validation: Epoch [15], Batch [216/938], Loss: 0.3279178738594055\n",
      "Validation: Epoch [15], Batch [217/938], Loss: 0.4570413827896118\n",
      "Validation: Epoch [15], Batch [218/938], Loss: 0.4531058669090271\n",
      "Validation: Epoch [15], Batch [219/938], Loss: 0.4275899827480316\n",
      "Validation: Epoch [15], Batch [220/938], Loss: 0.6007915735244751\n",
      "Validation: Epoch [15], Batch [221/938], Loss: 0.5049412846565247\n",
      "Validation: Epoch [15], Batch [222/938], Loss: 0.32145947217941284\n",
      "Validation: Epoch [15], Batch [223/938], Loss: 0.407554030418396\n",
      "Validation: Epoch [15], Batch [224/938], Loss: 0.39318907260894775\n",
      "Validation: Epoch [15], Batch [225/938], Loss: 0.3357827365398407\n",
      "Validation: Epoch [15], Batch [226/938], Loss: 0.4003390073776245\n",
      "Validation: Epoch [15], Batch [227/938], Loss: 0.393502801656723\n",
      "Validation: Epoch [15], Batch [228/938], Loss: 0.5270909070968628\n",
      "Validation: Epoch [15], Batch [229/938], Loss: 0.48696279525756836\n",
      "Validation: Epoch [15], Batch [230/938], Loss: 0.5493249893188477\n",
      "Validation: Epoch [15], Batch [231/938], Loss: 0.3877178430557251\n",
      "Validation: Epoch [15], Batch [232/938], Loss: 0.4354643225669861\n",
      "Validation: Epoch [15], Batch [233/938], Loss: 0.3642612099647522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [234/938], Loss: 0.44356340169906616\n",
      "Validation: Epoch [15], Batch [235/938], Loss: 0.4826342463493347\n",
      "Validation: Epoch [15], Batch [236/938], Loss: 0.549721360206604\n",
      "Validation: Epoch [15], Batch [237/938], Loss: 0.629095196723938\n",
      "Validation: Epoch [15], Batch [238/938], Loss: 0.48292970657348633\n",
      "Validation: Epoch [15], Batch [239/938], Loss: 0.481044203042984\n",
      "Validation: Epoch [15], Batch [240/938], Loss: 0.37609216570854187\n",
      "Validation: Epoch [15], Batch [241/938], Loss: 0.4425460696220398\n",
      "Validation: Epoch [15], Batch [242/938], Loss: 0.674342691898346\n",
      "Validation: Epoch [15], Batch [243/938], Loss: 0.40834277868270874\n",
      "Validation: Epoch [15], Batch [244/938], Loss: 0.43957024812698364\n",
      "Validation: Epoch [15], Batch [245/938], Loss: 0.32720696926116943\n",
      "Validation: Epoch [15], Batch [246/938], Loss: 0.4812682569026947\n",
      "Validation: Epoch [15], Batch [247/938], Loss: 0.45450329780578613\n",
      "Validation: Epoch [15], Batch [248/938], Loss: 0.4473942816257477\n",
      "Validation: Epoch [15], Batch [249/938], Loss: 0.4512141942977905\n",
      "Validation: Epoch [15], Batch [250/938], Loss: 0.4189820885658264\n",
      "Validation: Epoch [15], Batch [251/938], Loss: 0.47084614634513855\n",
      "Validation: Epoch [15], Batch [252/938], Loss: 0.3378477394580841\n",
      "Validation: Epoch [15], Batch [253/938], Loss: 0.5385549068450928\n",
      "Validation: Epoch [15], Batch [254/938], Loss: 0.5299246907234192\n",
      "Validation: Epoch [15], Batch [255/938], Loss: 0.34286776185035706\n",
      "Validation: Epoch [15], Batch [256/938], Loss: 0.4065636992454529\n",
      "Validation: Epoch [15], Batch [257/938], Loss: 0.3217241168022156\n",
      "Validation: Epoch [15], Batch [258/938], Loss: 0.44930368661880493\n",
      "Validation: Epoch [15], Batch [259/938], Loss: 0.31328222155570984\n",
      "Validation: Epoch [15], Batch [260/938], Loss: 0.6211006045341492\n",
      "Validation: Epoch [15], Batch [261/938], Loss: 0.4019889831542969\n",
      "Validation: Epoch [15], Batch [262/938], Loss: 0.3863281011581421\n",
      "Validation: Epoch [15], Batch [263/938], Loss: 0.5745720863342285\n",
      "Validation: Epoch [15], Batch [264/938], Loss: 0.5442451238632202\n",
      "Validation: Epoch [15], Batch [265/938], Loss: 0.4114947021007538\n",
      "Validation: Epoch [15], Batch [266/938], Loss: 0.5082158446311951\n",
      "Validation: Epoch [15], Batch [267/938], Loss: 0.7303769588470459\n",
      "Validation: Epoch [15], Batch [268/938], Loss: 0.44811755418777466\n",
      "Validation: Epoch [15], Batch [269/938], Loss: 0.39155906438827515\n",
      "Validation: Epoch [15], Batch [270/938], Loss: 0.4257130026817322\n",
      "Validation: Epoch [15], Batch [271/938], Loss: 0.41739827394485474\n",
      "Validation: Epoch [15], Batch [272/938], Loss: 0.44522643089294434\n",
      "Validation: Epoch [15], Batch [273/938], Loss: 0.3900975286960602\n",
      "Validation: Epoch [15], Batch [274/938], Loss: 0.4189831018447876\n",
      "Validation: Epoch [15], Batch [275/938], Loss: 0.4430308938026428\n",
      "Validation: Epoch [15], Batch [276/938], Loss: 0.2363385111093521\n",
      "Validation: Epoch [15], Batch [277/938], Loss: 0.51993727684021\n",
      "Validation: Epoch [15], Batch [278/938], Loss: 0.25453484058380127\n",
      "Validation: Epoch [15], Batch [279/938], Loss: 0.37376394867897034\n",
      "Validation: Epoch [15], Batch [280/938], Loss: 0.44661417603492737\n",
      "Validation: Epoch [15], Batch [281/938], Loss: 0.4179479479789734\n",
      "Validation: Epoch [15], Batch [282/938], Loss: 0.41699761152267456\n",
      "Validation: Epoch [15], Batch [283/938], Loss: 0.3615362048149109\n",
      "Validation: Epoch [15], Batch [284/938], Loss: 0.3933820128440857\n",
      "Validation: Epoch [15], Batch [285/938], Loss: 0.5135918259620667\n",
      "Validation: Epoch [15], Batch [286/938], Loss: 0.40545955300331116\n",
      "Validation: Epoch [15], Batch [287/938], Loss: 0.37823837995529175\n",
      "Validation: Epoch [15], Batch [288/938], Loss: 0.5897977352142334\n",
      "Validation: Epoch [15], Batch [289/938], Loss: 0.4132140874862671\n",
      "Validation: Epoch [15], Batch [290/938], Loss: 0.6445862054824829\n",
      "Validation: Epoch [15], Batch [291/938], Loss: 0.34588420391082764\n",
      "Validation: Epoch [15], Batch [292/938], Loss: 0.4288620352745056\n",
      "Validation: Epoch [15], Batch [293/938], Loss: 0.4427514672279358\n",
      "Validation: Epoch [15], Batch [294/938], Loss: 0.3286095857620239\n",
      "Validation: Epoch [15], Batch [295/938], Loss: 0.45822274684906006\n",
      "Validation: Epoch [15], Batch [296/938], Loss: 0.39566779136657715\n",
      "Validation: Epoch [15], Batch [297/938], Loss: 0.4791664481163025\n",
      "Validation: Epoch [15], Batch [298/938], Loss: 0.5545964241027832\n",
      "Validation: Epoch [15], Batch [299/938], Loss: 0.3424699604511261\n",
      "Validation: Epoch [15], Batch [300/938], Loss: 0.34059226512908936\n",
      "Validation: Epoch [15], Batch [301/938], Loss: 0.3689832091331482\n",
      "Validation: Epoch [15], Batch [302/938], Loss: 0.5174075365066528\n",
      "Validation: Epoch [15], Batch [303/938], Loss: 0.6681500673294067\n",
      "Validation: Epoch [15], Batch [304/938], Loss: 0.5935518741607666\n",
      "Validation: Epoch [15], Batch [305/938], Loss: 0.41233208775520325\n",
      "Validation: Epoch [15], Batch [306/938], Loss: 0.4481200575828552\n",
      "Validation: Epoch [15], Batch [307/938], Loss: 0.38018107414245605\n",
      "Validation: Epoch [15], Batch [308/938], Loss: 0.5285139083862305\n",
      "Validation: Epoch [15], Batch [309/938], Loss: 0.5840327143669128\n",
      "Validation: Epoch [15], Batch [310/938], Loss: 0.5557311773300171\n",
      "Validation: Epoch [15], Batch [311/938], Loss: 0.2842761278152466\n",
      "Validation: Epoch [15], Batch [312/938], Loss: 0.38322216272354126\n",
      "Validation: Epoch [15], Batch [313/938], Loss: 0.2634848654270172\n",
      "Validation: Epoch [15], Batch [314/938], Loss: 0.38481074571609497\n",
      "Validation: Epoch [15], Batch [315/938], Loss: 0.5030639171600342\n",
      "Validation: Epoch [15], Batch [316/938], Loss: 0.3712123930454254\n",
      "Validation: Epoch [15], Batch [317/938], Loss: 0.5614601373672485\n",
      "Validation: Epoch [15], Batch [318/938], Loss: 0.37080878019332886\n",
      "Validation: Epoch [15], Batch [319/938], Loss: 0.37578463554382324\n",
      "Validation: Epoch [15], Batch [320/938], Loss: 0.4619837999343872\n",
      "Validation: Epoch [15], Batch [321/938], Loss: 0.38314908742904663\n",
      "Validation: Epoch [15], Batch [322/938], Loss: 0.6546260118484497\n",
      "Validation: Epoch [15], Batch [323/938], Loss: 0.5889976024627686\n",
      "Validation: Epoch [15], Batch [324/938], Loss: 0.7630521655082703\n",
      "Validation: Epoch [15], Batch [325/938], Loss: 0.5579044818878174\n",
      "Validation: Epoch [15], Batch [326/938], Loss: 0.35036158561706543\n",
      "Validation: Epoch [15], Batch [327/938], Loss: 0.2977038621902466\n",
      "Validation: Epoch [15], Batch [328/938], Loss: 0.33131030201911926\n",
      "Validation: Epoch [15], Batch [329/938], Loss: 0.5433200597763062\n",
      "Validation: Epoch [15], Batch [330/938], Loss: 0.3673834800720215\n",
      "Validation: Epoch [15], Batch [331/938], Loss: 0.4330398738384247\n",
      "Validation: Epoch [15], Batch [332/938], Loss: 0.48043444752693176\n",
      "Validation: Epoch [15], Batch [333/938], Loss: 0.32057318091392517\n",
      "Validation: Epoch [15], Batch [334/938], Loss: 0.3555924892425537\n",
      "Validation: Epoch [15], Batch [335/938], Loss: 0.5010981559753418\n",
      "Validation: Epoch [15], Batch [336/938], Loss: 0.3126812279224396\n",
      "Validation: Epoch [15], Batch [337/938], Loss: 0.5773181319236755\n",
      "Validation: Epoch [15], Batch [338/938], Loss: 0.41609376668930054\n",
      "Validation: Epoch [15], Batch [339/938], Loss: 0.4237632751464844\n",
      "Validation: Epoch [15], Batch [340/938], Loss: 0.44508832693099976\n",
      "Validation: Epoch [15], Batch [341/938], Loss: 0.6011902689933777\n",
      "Validation: Epoch [15], Batch [342/938], Loss: 0.3306199610233307\n",
      "Validation: Epoch [15], Batch [343/938], Loss: 0.577953577041626\n",
      "Validation: Epoch [15], Batch [344/938], Loss: 0.5286034941673279\n",
      "Validation: Epoch [15], Batch [345/938], Loss: 0.37591612339019775\n",
      "Validation: Epoch [15], Batch [346/938], Loss: 0.4429951310157776\n",
      "Validation: Epoch [15], Batch [347/938], Loss: 0.3635076880455017\n",
      "Validation: Epoch [15], Batch [348/938], Loss: 0.5106152296066284\n",
      "Validation: Epoch [15], Batch [349/938], Loss: 0.40695589780807495\n",
      "Validation: Epoch [15], Batch [350/938], Loss: 0.476997435092926\n",
      "Validation: Epoch [15], Batch [351/938], Loss: 0.40085285902023315\n",
      "Validation: Epoch [15], Batch [352/938], Loss: 0.493977427482605\n",
      "Validation: Epoch [15], Batch [353/938], Loss: 0.559334397315979\n",
      "Validation: Epoch [15], Batch [354/938], Loss: 0.36625927686691284\n",
      "Validation: Epoch [15], Batch [355/938], Loss: 0.4437949061393738\n",
      "Validation: Epoch [15], Batch [356/938], Loss: 0.5026718378067017\n",
      "Validation: Epoch [15], Batch [357/938], Loss: 0.33217689394950867\n",
      "Validation: Epoch [15], Batch [358/938], Loss: 0.40425729751586914\n",
      "Validation: Epoch [15], Batch [359/938], Loss: 0.407843679189682\n",
      "Validation: Epoch [15], Batch [360/938], Loss: 0.5123253464698792\n",
      "Validation: Epoch [15], Batch [361/938], Loss: 0.606647253036499\n",
      "Validation: Epoch [15], Batch [362/938], Loss: 0.5486479997634888\n",
      "Validation: Epoch [15], Batch [363/938], Loss: 0.5935233235359192\n",
      "Validation: Epoch [15], Batch [364/938], Loss: 0.48126596212387085\n",
      "Validation: Epoch [15], Batch [365/938], Loss: 0.31295186281204224\n",
      "Validation: Epoch [15], Batch [366/938], Loss: 0.4606548547744751\n",
      "Validation: Epoch [15], Batch [367/938], Loss: 0.3970085382461548\n",
      "Validation: Epoch [15], Batch [368/938], Loss: 0.43546345829963684\n",
      "Validation: Epoch [15], Batch [369/938], Loss: 0.6107041835784912\n",
      "Validation: Epoch [15], Batch [370/938], Loss: 0.30840879678726196\n",
      "Validation: Epoch [15], Batch [371/938], Loss: 0.3883851170539856\n",
      "Validation: Epoch [15], Batch [372/938], Loss: 0.31516772508621216\n",
      "Validation: Epoch [15], Batch [373/938], Loss: 0.34234312176704407\n",
      "Validation: Epoch [15], Batch [374/938], Loss: 0.3404676914215088\n",
      "Validation: Epoch [15], Batch [375/938], Loss: 0.3607292175292969\n",
      "Validation: Epoch [15], Batch [376/938], Loss: 0.5085803866386414\n",
      "Validation: Epoch [15], Batch [377/938], Loss: 0.32695645093917847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [378/938], Loss: 0.3781288266181946\n",
      "Validation: Epoch [15], Batch [379/938], Loss: 0.2839753031730652\n",
      "Validation: Epoch [15], Batch [380/938], Loss: 0.37980780005455017\n",
      "Validation: Epoch [15], Batch [381/938], Loss: 0.5205012559890747\n",
      "Validation: Epoch [15], Batch [382/938], Loss: 0.461549311876297\n",
      "Validation: Epoch [15], Batch [383/938], Loss: 0.46548783779144287\n",
      "Validation: Epoch [15], Batch [384/938], Loss: 0.475526362657547\n",
      "Validation: Epoch [15], Batch [385/938], Loss: 0.3568228781223297\n",
      "Validation: Epoch [15], Batch [386/938], Loss: 0.42456144094467163\n",
      "Validation: Epoch [15], Batch [387/938], Loss: 0.4011051058769226\n",
      "Validation: Epoch [15], Batch [388/938], Loss: 0.3571609854698181\n",
      "Validation: Epoch [15], Batch [389/938], Loss: 0.5787758231163025\n",
      "Validation: Epoch [15], Batch [390/938], Loss: 0.5254120826721191\n",
      "Validation: Epoch [15], Batch [391/938], Loss: 0.43257081508636475\n",
      "Validation: Epoch [15], Batch [392/938], Loss: 0.5324289798736572\n",
      "Validation: Epoch [15], Batch [393/938], Loss: 0.46802592277526855\n",
      "Validation: Epoch [15], Batch [394/938], Loss: 0.6186283230781555\n",
      "Validation: Epoch [15], Batch [395/938], Loss: 0.3893403708934784\n",
      "Validation: Epoch [15], Batch [396/938], Loss: 0.46519216895103455\n",
      "Validation: Epoch [15], Batch [397/938], Loss: 0.44543078541755676\n",
      "Validation: Epoch [15], Batch [398/938], Loss: 0.39615222811698914\n",
      "Validation: Epoch [15], Batch [399/938], Loss: 0.31477436423301697\n",
      "Validation: Epoch [15], Batch [400/938], Loss: 0.5899730324745178\n",
      "Validation: Epoch [15], Batch [401/938], Loss: 0.3761235475540161\n",
      "Validation: Epoch [15], Batch [402/938], Loss: 0.4531498849391937\n",
      "Validation: Epoch [15], Batch [403/938], Loss: 0.5027239918708801\n",
      "Validation: Epoch [15], Batch [404/938], Loss: 0.32007044553756714\n",
      "Validation: Epoch [15], Batch [405/938], Loss: 0.5095710754394531\n",
      "Validation: Epoch [15], Batch [406/938], Loss: 0.6809374094009399\n",
      "Validation: Epoch [15], Batch [407/938], Loss: 0.3642110228538513\n",
      "Validation: Epoch [15], Batch [408/938], Loss: 0.3516775965690613\n",
      "Validation: Epoch [15], Batch [409/938], Loss: 0.3338712453842163\n",
      "Validation: Epoch [15], Batch [410/938], Loss: 0.5492722988128662\n",
      "Validation: Epoch [15], Batch [411/938], Loss: 0.586014986038208\n",
      "Validation: Epoch [15], Batch [412/938], Loss: 0.48627951741218567\n",
      "Validation: Epoch [15], Batch [413/938], Loss: 0.4531409740447998\n",
      "Validation: Epoch [15], Batch [414/938], Loss: 0.5127756595611572\n",
      "Validation: Epoch [15], Batch [415/938], Loss: 0.4247139096260071\n",
      "Validation: Epoch [15], Batch [416/938], Loss: 0.32600903511047363\n",
      "Validation: Epoch [15], Batch [417/938], Loss: 0.261190801858902\n",
      "Validation: Epoch [15], Batch [418/938], Loss: 0.45991888642311096\n",
      "Validation: Epoch [15], Batch [419/938], Loss: 0.5149216651916504\n",
      "Validation: Epoch [15], Batch [420/938], Loss: 0.5872358083724976\n",
      "Validation: Epoch [15], Batch [421/938], Loss: 0.2522866725921631\n",
      "Validation: Epoch [15], Batch [422/938], Loss: 0.5416797399520874\n",
      "Validation: Epoch [15], Batch [423/938], Loss: 0.5173704624176025\n",
      "Validation: Epoch [15], Batch [424/938], Loss: 0.3896757662296295\n",
      "Validation: Epoch [15], Batch [425/938], Loss: 0.41475650668144226\n",
      "Validation: Epoch [15], Batch [426/938], Loss: 0.5305522084236145\n",
      "Validation: Epoch [15], Batch [427/938], Loss: 0.4041535258293152\n",
      "Validation: Epoch [15], Batch [428/938], Loss: 0.49164336919784546\n",
      "Validation: Epoch [15], Batch [429/938], Loss: 0.6715912222862244\n",
      "Validation: Epoch [15], Batch [430/938], Loss: 0.3734693229198456\n",
      "Validation: Epoch [15], Batch [431/938], Loss: 0.3570365905761719\n",
      "Validation: Epoch [15], Batch [432/938], Loss: 0.4263087511062622\n",
      "Validation: Epoch [15], Batch [433/938], Loss: 0.47821611166000366\n",
      "Validation: Epoch [15], Batch [434/938], Loss: 0.331940621137619\n",
      "Validation: Epoch [15], Batch [435/938], Loss: 0.40070632100105286\n",
      "Validation: Epoch [15], Batch [436/938], Loss: 0.6028751134872437\n",
      "Validation: Epoch [15], Batch [437/938], Loss: 0.43935447931289673\n",
      "Validation: Epoch [15], Batch [438/938], Loss: 0.4209420382976532\n",
      "Validation: Epoch [15], Batch [439/938], Loss: 0.463166743516922\n",
      "Validation: Epoch [15], Batch [440/938], Loss: 0.6530784964561462\n",
      "Validation: Epoch [15], Batch [441/938], Loss: 0.373751163482666\n",
      "Validation: Epoch [15], Batch [442/938], Loss: 0.44100692868232727\n",
      "Validation: Epoch [15], Batch [443/938], Loss: 0.5242593288421631\n",
      "Validation: Epoch [15], Batch [444/938], Loss: 0.3419490456581116\n",
      "Validation: Epoch [15], Batch [445/938], Loss: 0.45907729864120483\n",
      "Validation: Epoch [15], Batch [446/938], Loss: 0.3826633095741272\n",
      "Validation: Epoch [15], Batch [447/938], Loss: 0.36266565322875977\n",
      "Validation: Epoch [15], Batch [448/938], Loss: 0.3992832899093628\n",
      "Validation: Epoch [15], Batch [449/938], Loss: 0.5899077653884888\n",
      "Validation: Epoch [15], Batch [450/938], Loss: 0.7048934698104858\n",
      "Validation: Epoch [15], Batch [451/938], Loss: 0.38136330246925354\n",
      "Validation: Epoch [15], Batch [452/938], Loss: 0.3749830722808838\n",
      "Validation: Epoch [15], Batch [453/938], Loss: 0.4232978820800781\n",
      "Validation: Epoch [15], Batch [454/938], Loss: 0.6015557646751404\n",
      "Validation: Epoch [15], Batch [455/938], Loss: 0.4216211438179016\n",
      "Validation: Epoch [15], Batch [456/938], Loss: 0.3849680423736572\n",
      "Validation: Epoch [15], Batch [457/938], Loss: 0.33314409852027893\n",
      "Validation: Epoch [15], Batch [458/938], Loss: 0.4087347686290741\n",
      "Validation: Epoch [15], Batch [459/938], Loss: 0.36534351110458374\n",
      "Validation: Epoch [15], Batch [460/938], Loss: 0.515671968460083\n",
      "Validation: Epoch [15], Batch [461/938], Loss: 0.4514670968055725\n",
      "Validation: Epoch [15], Batch [462/938], Loss: 0.4304662346839905\n",
      "Validation: Epoch [15], Batch [463/938], Loss: 0.39320021867752075\n",
      "Validation: Epoch [15], Batch [464/938], Loss: 0.4742627739906311\n",
      "Validation: Epoch [15], Batch [465/938], Loss: 0.3963632583618164\n",
      "Validation: Epoch [15], Batch [466/938], Loss: 0.3712438642978668\n",
      "Validation: Epoch [15], Batch [467/938], Loss: 0.3643473982810974\n",
      "Validation: Epoch [15], Batch [468/938], Loss: 0.4341258704662323\n",
      "Validation: Epoch [15], Batch [469/938], Loss: 0.4141055941581726\n",
      "Validation: Epoch [15], Batch [470/938], Loss: 0.5622408986091614\n",
      "Validation: Epoch [15], Batch [471/938], Loss: 0.34762269258499146\n",
      "Validation: Epoch [15], Batch [472/938], Loss: 0.38584989309310913\n",
      "Validation: Epoch [15], Batch [473/938], Loss: 0.5411481261253357\n",
      "Validation: Epoch [15], Batch [474/938], Loss: 0.47174733877182007\n",
      "Validation: Epoch [15], Batch [475/938], Loss: 0.5181095600128174\n",
      "Validation: Epoch [15], Batch [476/938], Loss: 0.41542187333106995\n",
      "Validation: Epoch [15], Batch [477/938], Loss: 0.6096272468566895\n",
      "Validation: Epoch [15], Batch [478/938], Loss: 0.3613443374633789\n",
      "Validation: Epoch [15], Batch [479/938], Loss: 0.5169690251350403\n",
      "Validation: Epoch [15], Batch [480/938], Loss: 0.3167465329170227\n",
      "Validation: Epoch [15], Batch [481/938], Loss: 0.4804304242134094\n",
      "Validation: Epoch [15], Batch [482/938], Loss: 0.3240338861942291\n",
      "Validation: Epoch [15], Batch [483/938], Loss: 0.5168803930282593\n",
      "Validation: Epoch [15], Batch [484/938], Loss: 0.36161625385284424\n",
      "Validation: Epoch [15], Batch [485/938], Loss: 0.5602160692214966\n",
      "Validation: Epoch [15], Batch [486/938], Loss: 0.48264259099960327\n",
      "Validation: Epoch [15], Batch [487/938], Loss: 0.3616747260093689\n",
      "Validation: Epoch [15], Batch [488/938], Loss: 0.4888225495815277\n",
      "Validation: Epoch [15], Batch [489/938], Loss: 0.6454030275344849\n",
      "Validation: Epoch [15], Batch [490/938], Loss: 0.3527408242225647\n",
      "Validation: Epoch [15], Batch [491/938], Loss: 0.3262787163257599\n",
      "Validation: Epoch [15], Batch [492/938], Loss: 0.5333315134048462\n",
      "Validation: Epoch [15], Batch [493/938], Loss: 0.3759619891643524\n",
      "Validation: Epoch [15], Batch [494/938], Loss: 0.39011457562446594\n",
      "Validation: Epoch [15], Batch [495/938], Loss: 0.4603736400604248\n",
      "Validation: Epoch [15], Batch [496/938], Loss: 0.48972874879837036\n",
      "Validation: Epoch [15], Batch [497/938], Loss: 0.3203079402446747\n",
      "Validation: Epoch [15], Batch [498/938], Loss: 0.5455445647239685\n",
      "Validation: Epoch [15], Batch [499/938], Loss: 0.30038216710090637\n",
      "Validation: Epoch [15], Batch [500/938], Loss: 0.4042980968952179\n",
      "Validation: Epoch [15], Batch [501/938], Loss: 0.7385021448135376\n",
      "Validation: Epoch [15], Batch [502/938], Loss: 0.3717547357082367\n",
      "Validation: Epoch [15], Batch [503/938], Loss: 0.3881635069847107\n",
      "Validation: Epoch [15], Batch [504/938], Loss: 0.3504137694835663\n",
      "Validation: Epoch [15], Batch [505/938], Loss: 0.3504599332809448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [506/938], Loss: 0.42135292291641235\n",
      "Validation: Epoch [15], Batch [507/938], Loss: 0.36785268783569336\n",
      "Validation: Epoch [15], Batch [508/938], Loss: 0.39400750398635864\n",
      "Validation: Epoch [15], Batch [509/938], Loss: 0.55046546459198\n",
      "Validation: Epoch [15], Batch [510/938], Loss: 0.563496470451355\n",
      "Validation: Epoch [15], Batch [511/938], Loss: 0.4749351441860199\n",
      "Validation: Epoch [15], Batch [512/938], Loss: 0.40276873111724854\n",
      "Validation: Epoch [15], Batch [513/938], Loss: 0.550869345664978\n",
      "Validation: Epoch [15], Batch [514/938], Loss: 0.7153112292289734\n",
      "Validation: Epoch [15], Batch [515/938], Loss: 0.28117913007736206\n",
      "Validation: Epoch [15], Batch [516/938], Loss: 0.38584789633750916\n",
      "Validation: Epoch [15], Batch [517/938], Loss: 0.42333728075027466\n",
      "Validation: Epoch [15], Batch [518/938], Loss: 0.5713549256324768\n",
      "Validation: Epoch [15], Batch [519/938], Loss: 0.48430073261260986\n",
      "Validation: Epoch [15], Batch [520/938], Loss: 0.6615957021713257\n",
      "Validation: Epoch [15], Batch [521/938], Loss: 0.4729240834712982\n",
      "Validation: Epoch [15], Batch [522/938], Loss: 0.3374474346637726\n",
      "Validation: Epoch [15], Batch [523/938], Loss: 0.3600255250930786\n",
      "Validation: Epoch [15], Batch [524/938], Loss: 0.31386256217956543\n",
      "Validation: Epoch [15], Batch [525/938], Loss: 0.6845614910125732\n",
      "Validation: Epoch [15], Batch [526/938], Loss: 0.4138793349266052\n",
      "Validation: Epoch [15], Batch [527/938], Loss: 0.37919843196868896\n",
      "Validation: Epoch [15], Batch [528/938], Loss: 0.3895147442817688\n",
      "Validation: Epoch [15], Batch [529/938], Loss: 0.48167935013771057\n",
      "Validation: Epoch [15], Batch [530/938], Loss: 0.37196624279022217\n",
      "Validation: Epoch [15], Batch [531/938], Loss: 0.43706178665161133\n",
      "Validation: Epoch [15], Batch [532/938], Loss: 0.5249854922294617\n",
      "Validation: Epoch [15], Batch [533/938], Loss: 0.3504830002784729\n",
      "Validation: Epoch [15], Batch [534/938], Loss: 0.450395792722702\n",
      "Validation: Epoch [15], Batch [535/938], Loss: 0.4572848081588745\n",
      "Validation: Epoch [15], Batch [536/938], Loss: 0.5080993175506592\n",
      "Validation: Epoch [15], Batch [537/938], Loss: 0.6493865251541138\n",
      "Validation: Epoch [15], Batch [538/938], Loss: 0.44395798444747925\n",
      "Validation: Epoch [15], Batch [539/938], Loss: 0.6971904039382935\n",
      "Validation: Epoch [15], Batch [540/938], Loss: 0.41885459423065186\n",
      "Validation: Epoch [15], Batch [541/938], Loss: 0.43348366022109985\n",
      "Validation: Epoch [15], Batch [542/938], Loss: 0.8531497120857239\n",
      "Validation: Epoch [15], Batch [543/938], Loss: 0.38734835386276245\n",
      "Validation: Epoch [15], Batch [544/938], Loss: 0.3932816982269287\n",
      "Validation: Epoch [15], Batch [545/938], Loss: 0.3923555314540863\n",
      "Validation: Epoch [15], Batch [546/938], Loss: 0.4569714069366455\n",
      "Validation: Epoch [15], Batch [547/938], Loss: 0.33024367690086365\n",
      "Validation: Epoch [15], Batch [548/938], Loss: 0.3901287913322449\n",
      "Validation: Epoch [15], Batch [549/938], Loss: 0.4726054072380066\n",
      "Validation: Epoch [15], Batch [550/938], Loss: 0.4143712520599365\n",
      "Validation: Epoch [15], Batch [551/938], Loss: 0.4830425977706909\n",
      "Validation: Epoch [15], Batch [552/938], Loss: 0.4168042540550232\n",
      "Validation: Epoch [15], Batch [553/938], Loss: 0.581132173538208\n",
      "Validation: Epoch [15], Batch [554/938], Loss: 0.44998496770858765\n",
      "Validation: Epoch [15], Batch [555/938], Loss: 0.5077785849571228\n",
      "Validation: Epoch [15], Batch [556/938], Loss: 0.4501456618309021\n",
      "Validation: Epoch [15], Batch [557/938], Loss: 0.3304038643836975\n",
      "Validation: Epoch [15], Batch [558/938], Loss: 0.4551549255847931\n",
      "Validation: Epoch [15], Batch [559/938], Loss: 0.6895837783813477\n",
      "Validation: Epoch [15], Batch [560/938], Loss: 0.378619909286499\n",
      "Validation: Epoch [15], Batch [561/938], Loss: 0.3595788776874542\n",
      "Validation: Epoch [15], Batch [562/938], Loss: 0.3682911694049835\n",
      "Validation: Epoch [15], Batch [563/938], Loss: 0.46790286898612976\n",
      "Validation: Epoch [15], Batch [564/938], Loss: 0.4626137912273407\n",
      "Validation: Epoch [15], Batch [565/938], Loss: 0.48070377111434937\n",
      "Validation: Epoch [15], Batch [566/938], Loss: 0.3713199496269226\n",
      "Validation: Epoch [15], Batch [567/938], Loss: 0.44632449746131897\n",
      "Validation: Epoch [15], Batch [568/938], Loss: 0.5895364880561829\n",
      "Validation: Epoch [15], Batch [569/938], Loss: 0.35402458906173706\n",
      "Validation: Epoch [15], Batch [570/938], Loss: 0.5217806100845337\n",
      "Validation: Epoch [15], Batch [571/938], Loss: 0.5552756786346436\n",
      "Validation: Epoch [15], Batch [572/938], Loss: 0.32084381580352783\n",
      "Validation: Epoch [15], Batch [573/938], Loss: 0.4465661346912384\n",
      "Validation: Epoch [15], Batch [574/938], Loss: 0.4595784544944763\n",
      "Validation: Epoch [15], Batch [575/938], Loss: 0.35416775941848755\n",
      "Validation: Epoch [15], Batch [576/938], Loss: 0.32502061128616333\n",
      "Validation: Epoch [15], Batch [577/938], Loss: 0.35954514145851135\n",
      "Validation: Epoch [15], Batch [578/938], Loss: 0.4087887406349182\n",
      "Validation: Epoch [15], Batch [579/938], Loss: 0.4025213420391083\n",
      "Validation: Epoch [15], Batch [580/938], Loss: 0.3799000382423401\n",
      "Validation: Epoch [15], Batch [581/938], Loss: 0.42208558320999146\n",
      "Validation: Epoch [15], Batch [582/938], Loss: 0.431957483291626\n",
      "Validation: Epoch [15], Batch [583/938], Loss: 0.46252548694610596\n",
      "Validation: Epoch [15], Batch [584/938], Loss: 0.30956700444221497\n",
      "Validation: Epoch [15], Batch [585/938], Loss: 0.4948064684867859\n",
      "Validation: Epoch [15], Batch [586/938], Loss: 0.7016963362693787\n",
      "Validation: Epoch [15], Batch [587/938], Loss: 0.28776195645332336\n",
      "Validation: Epoch [15], Batch [588/938], Loss: 0.3867575526237488\n",
      "Validation: Epoch [15], Batch [589/938], Loss: 0.4366362392902374\n",
      "Validation: Epoch [15], Batch [590/938], Loss: 0.4544292390346527\n",
      "Validation: Epoch [15], Batch [591/938], Loss: 0.430850088596344\n",
      "Validation: Epoch [15], Batch [592/938], Loss: 0.43857264518737793\n",
      "Validation: Epoch [15], Batch [593/938], Loss: 0.5961662530899048\n",
      "Validation: Epoch [15], Batch [594/938], Loss: 0.35234037041664124\n",
      "Validation: Epoch [15], Batch [595/938], Loss: 0.4422229826450348\n",
      "Validation: Epoch [15], Batch [596/938], Loss: 0.4069676101207733\n",
      "Validation: Epoch [15], Batch [597/938], Loss: 0.5052376389503479\n",
      "Validation: Epoch [15], Batch [598/938], Loss: 0.49547478556632996\n",
      "Validation: Epoch [15], Batch [599/938], Loss: 0.4624933898448944\n",
      "Validation: Epoch [15], Batch [600/938], Loss: 0.49957913160324097\n",
      "Validation: Epoch [15], Batch [601/938], Loss: 0.48321136832237244\n",
      "Validation: Epoch [15], Batch [602/938], Loss: 0.5865823030471802\n",
      "Validation: Epoch [15], Batch [603/938], Loss: 0.3077470064163208\n",
      "Validation: Epoch [15], Batch [604/938], Loss: 0.41507619619369507\n",
      "Validation: Epoch [15], Batch [605/938], Loss: 0.4162684679031372\n",
      "Validation: Epoch [15], Batch [606/938], Loss: 0.5746988654136658\n",
      "Validation: Epoch [15], Batch [607/938], Loss: 0.43558835983276367\n",
      "Validation: Epoch [15], Batch [608/938], Loss: 0.316932737827301\n",
      "Validation: Epoch [15], Batch [609/938], Loss: 0.6172261834144592\n",
      "Validation: Epoch [15], Batch [610/938], Loss: 0.4903312921524048\n",
      "Validation: Epoch [15], Batch [611/938], Loss: 0.4064715504646301\n",
      "Validation: Epoch [15], Batch [612/938], Loss: 0.32305851578712463\n",
      "Validation: Epoch [15], Batch [613/938], Loss: 0.6590185761451721\n",
      "Validation: Epoch [15], Batch [614/938], Loss: 0.34592318534851074\n",
      "Validation: Epoch [15], Batch [615/938], Loss: 0.4910730719566345\n",
      "Validation: Epoch [15], Batch [616/938], Loss: 0.3653661012649536\n",
      "Validation: Epoch [15], Batch [617/938], Loss: 0.46932482719421387\n",
      "Validation: Epoch [15], Batch [618/938], Loss: 0.3962441086769104\n",
      "Validation: Epoch [15], Batch [619/938], Loss: 0.4235946536064148\n",
      "Validation: Epoch [15], Batch [620/938], Loss: 0.4765542149543762\n",
      "Validation: Epoch [15], Batch [621/938], Loss: 0.5166065692901611\n",
      "Validation: Epoch [15], Batch [622/938], Loss: 0.3214913010597229\n",
      "Validation: Epoch [15], Batch [623/938], Loss: 0.43399378657341003\n",
      "Validation: Epoch [15], Batch [624/938], Loss: 0.5793896913528442\n",
      "Validation: Epoch [15], Batch [625/938], Loss: 0.36584800481796265\n",
      "Validation: Epoch [15], Batch [626/938], Loss: 0.470492959022522\n",
      "Validation: Epoch [15], Batch [627/938], Loss: 0.3157866895198822\n",
      "Validation: Epoch [15], Batch [628/938], Loss: 0.48716044425964355\n",
      "Validation: Epoch [15], Batch [629/938], Loss: 0.4532051682472229\n",
      "Validation: Epoch [15], Batch [630/938], Loss: 0.3206445574760437\n",
      "Validation: Epoch [15], Batch [631/938], Loss: 0.47084131836891174\n",
      "Validation: Epoch [15], Batch [632/938], Loss: 0.6436374187469482\n",
      "Validation: Epoch [15], Batch [633/938], Loss: 0.427249938249588\n",
      "Validation: Epoch [15], Batch [634/938], Loss: 0.622566819190979\n",
      "Validation: Epoch [15], Batch [635/938], Loss: 0.467586874961853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [636/938], Loss: 0.4056164622306824\n",
      "Validation: Epoch [15], Batch [637/938], Loss: 0.38399219512939453\n",
      "Validation: Epoch [15], Batch [638/938], Loss: 0.4235045909881592\n",
      "Validation: Epoch [15], Batch [639/938], Loss: 0.5656962990760803\n",
      "Validation: Epoch [15], Batch [640/938], Loss: 0.5228714942932129\n",
      "Validation: Epoch [15], Batch [641/938], Loss: 0.39894571900367737\n",
      "Validation: Epoch [15], Batch [642/938], Loss: 0.3531980514526367\n",
      "Validation: Epoch [15], Batch [643/938], Loss: 0.4455375075340271\n",
      "Validation: Epoch [15], Batch [644/938], Loss: 0.35800474882125854\n",
      "Validation: Epoch [15], Batch [645/938], Loss: 0.2952033281326294\n",
      "Validation: Epoch [15], Batch [646/938], Loss: 0.45368972420692444\n",
      "Validation: Epoch [15], Batch [647/938], Loss: 0.5400729775428772\n",
      "Validation: Epoch [15], Batch [648/938], Loss: 0.48203808069229126\n",
      "Validation: Epoch [15], Batch [649/938], Loss: 0.4320644736289978\n",
      "Validation: Epoch [15], Batch [650/938], Loss: 0.5410470366477966\n",
      "Validation: Epoch [15], Batch [651/938], Loss: 0.30879905819892883\n",
      "Validation: Epoch [15], Batch [652/938], Loss: 0.6564946174621582\n",
      "Validation: Epoch [15], Batch [653/938], Loss: 0.4332565367221832\n",
      "Validation: Epoch [15], Batch [654/938], Loss: 0.4188337028026581\n",
      "Validation: Epoch [15], Batch [655/938], Loss: 0.32458412647247314\n",
      "Validation: Epoch [15], Batch [656/938], Loss: 0.415297269821167\n",
      "Validation: Epoch [15], Batch [657/938], Loss: 0.3851025402545929\n",
      "Validation: Epoch [15], Batch [658/938], Loss: 0.4981578290462494\n",
      "Validation: Epoch [15], Batch [659/938], Loss: 0.5248348712921143\n",
      "Validation: Epoch [15], Batch [660/938], Loss: 0.3363853096961975\n",
      "Validation: Epoch [15], Batch [661/938], Loss: 0.38419926166534424\n",
      "Validation: Epoch [15], Batch [662/938], Loss: 0.2842476963996887\n",
      "Validation: Epoch [15], Batch [663/938], Loss: 0.3506487011909485\n",
      "Validation: Epoch [15], Batch [664/938], Loss: 0.4742904305458069\n",
      "Validation: Epoch [15], Batch [665/938], Loss: 0.5796582698822021\n",
      "Validation: Epoch [15], Batch [666/938], Loss: 0.48252037167549133\n",
      "Validation: Epoch [15], Batch [667/938], Loss: 0.3246939778327942\n",
      "Validation: Epoch [15], Batch [668/938], Loss: 0.34562546014785767\n",
      "Validation: Epoch [15], Batch [669/938], Loss: 0.4008631706237793\n",
      "Validation: Epoch [15], Batch [670/938], Loss: 0.694144606590271\n",
      "Validation: Epoch [15], Batch [671/938], Loss: 0.40253788232803345\n",
      "Validation: Epoch [15], Batch [672/938], Loss: 0.5267190933227539\n",
      "Validation: Epoch [15], Batch [673/938], Loss: 0.3760303258895874\n",
      "Validation: Epoch [15], Batch [674/938], Loss: 0.5154526233673096\n",
      "Validation: Epoch [15], Batch [675/938], Loss: 0.6203486919403076\n",
      "Validation: Epoch [15], Batch [676/938], Loss: 0.49127906560897827\n",
      "Validation: Epoch [15], Batch [677/938], Loss: 0.41552746295928955\n",
      "Validation: Epoch [15], Batch [678/938], Loss: 0.283774197101593\n",
      "Validation: Epoch [15], Batch [679/938], Loss: 0.5028035044670105\n",
      "Validation: Epoch [15], Batch [680/938], Loss: 0.4829970598220825\n",
      "Validation: Epoch [15], Batch [681/938], Loss: 0.5433785915374756\n",
      "Validation: Epoch [15], Batch [682/938], Loss: 0.6477769613265991\n",
      "Validation: Epoch [15], Batch [683/938], Loss: 0.6254065036773682\n",
      "Validation: Epoch [15], Batch [684/938], Loss: 0.5835874080657959\n",
      "Validation: Epoch [15], Batch [685/938], Loss: 0.4567056894302368\n",
      "Validation: Epoch [15], Batch [686/938], Loss: 0.4739399552345276\n",
      "Validation: Epoch [15], Batch [687/938], Loss: 0.43520617485046387\n",
      "Validation: Epoch [15], Batch [688/938], Loss: 0.5954982042312622\n",
      "Validation: Epoch [15], Batch [689/938], Loss: 0.3218163847923279\n",
      "Validation: Epoch [15], Batch [690/938], Loss: 0.44582459330558777\n",
      "Validation: Epoch [15], Batch [691/938], Loss: 0.41318440437316895\n",
      "Validation: Epoch [15], Batch [692/938], Loss: 0.3841322064399719\n",
      "Validation: Epoch [15], Batch [693/938], Loss: 0.47968241572380066\n",
      "Validation: Epoch [15], Batch [694/938], Loss: 0.38252440094947815\n",
      "Validation: Epoch [15], Batch [695/938], Loss: 0.5861824154853821\n",
      "Validation: Epoch [15], Batch [696/938], Loss: 0.3651045560836792\n",
      "Validation: Epoch [15], Batch [697/938], Loss: 0.36900925636291504\n",
      "Validation: Epoch [15], Batch [698/938], Loss: 0.39305055141448975\n",
      "Validation: Epoch [15], Batch [699/938], Loss: 0.5879784822463989\n",
      "Validation: Epoch [15], Batch [700/938], Loss: 0.3775622248649597\n",
      "Validation: Epoch [15], Batch [701/938], Loss: 0.3987557590007782\n",
      "Validation: Epoch [15], Batch [702/938], Loss: 0.5873703360557556\n",
      "Validation: Epoch [15], Batch [703/938], Loss: 0.4469498097896576\n",
      "Validation: Epoch [15], Batch [704/938], Loss: 0.3915116786956787\n",
      "Validation: Epoch [15], Batch [705/938], Loss: 0.40876463055610657\n",
      "Validation: Epoch [15], Batch [706/938], Loss: 0.7026978731155396\n",
      "Validation: Epoch [15], Batch [707/938], Loss: 0.5722212195396423\n",
      "Validation: Epoch [15], Batch [708/938], Loss: 0.4743039309978485\n",
      "Validation: Epoch [15], Batch [709/938], Loss: 0.34341394901275635\n",
      "Validation: Epoch [15], Batch [710/938], Loss: 0.39281952381134033\n",
      "Validation: Epoch [15], Batch [711/938], Loss: 0.4918019771575928\n",
      "Validation: Epoch [15], Batch [712/938], Loss: 0.33231398463249207\n",
      "Validation: Epoch [15], Batch [713/938], Loss: 0.36315423250198364\n",
      "Validation: Epoch [15], Batch [714/938], Loss: 0.4501529335975647\n",
      "Validation: Epoch [15], Batch [715/938], Loss: 0.32429006695747375\n",
      "Validation: Epoch [15], Batch [716/938], Loss: 0.4682883024215698\n",
      "Validation: Epoch [15], Batch [717/938], Loss: 0.38409343361854553\n",
      "Validation: Epoch [15], Batch [718/938], Loss: 0.45280224084854126\n",
      "Validation: Epoch [15], Batch [719/938], Loss: 0.4432380795478821\n",
      "Validation: Epoch [15], Batch [720/938], Loss: 0.5114052891731262\n",
      "Validation: Epoch [15], Batch [721/938], Loss: 0.2896791696548462\n",
      "Validation: Epoch [15], Batch [722/938], Loss: 0.5006041526794434\n",
      "Validation: Epoch [15], Batch [723/938], Loss: 0.5247311592102051\n",
      "Validation: Epoch [15], Batch [724/938], Loss: 0.22087883949279785\n",
      "Validation: Epoch [15], Batch [725/938], Loss: 0.3486926853656769\n",
      "Validation: Epoch [15], Batch [726/938], Loss: 0.4229244589805603\n",
      "Validation: Epoch [15], Batch [727/938], Loss: 0.2918740510940552\n",
      "Validation: Epoch [15], Batch [728/938], Loss: 0.4402378797531128\n",
      "Validation: Epoch [15], Batch [729/938], Loss: 0.3469996154308319\n",
      "Validation: Epoch [15], Batch [730/938], Loss: 0.425227552652359\n",
      "Validation: Epoch [15], Batch [731/938], Loss: 0.6286903619766235\n",
      "Validation: Epoch [15], Batch [732/938], Loss: 0.43363919854164124\n",
      "Validation: Epoch [15], Batch [733/938], Loss: 0.5698639750480652\n",
      "Validation: Epoch [15], Batch [734/938], Loss: 0.4946325123310089\n",
      "Validation: Epoch [15], Batch [735/938], Loss: 0.3990796208381653\n",
      "Validation: Epoch [15], Batch [736/938], Loss: 0.32880672812461853\n",
      "Validation: Epoch [15], Batch [737/938], Loss: 0.448908269405365\n",
      "Validation: Epoch [15], Batch [738/938], Loss: 0.3586275577545166\n",
      "Validation: Epoch [15], Batch [739/938], Loss: 0.32376205921173096\n",
      "Validation: Epoch [15], Batch [740/938], Loss: 0.5037732124328613\n",
      "Validation: Epoch [15], Batch [741/938], Loss: 0.3932109773159027\n",
      "Validation: Epoch [15], Batch [742/938], Loss: 0.4105374217033386\n",
      "Validation: Epoch [15], Batch [743/938], Loss: 0.3614727854728699\n",
      "Validation: Epoch [15], Batch [744/938], Loss: 0.6137675046920776\n",
      "Validation: Epoch [15], Batch [745/938], Loss: 0.505834698677063\n",
      "Validation: Epoch [15], Batch [746/938], Loss: 0.2822399437427521\n",
      "Validation: Epoch [15], Batch [747/938], Loss: 0.3531178832054138\n",
      "Validation: Epoch [15], Batch [748/938], Loss: 0.4715253710746765\n",
      "Validation: Epoch [15], Batch [749/938], Loss: 0.4755232334136963\n",
      "Validation: Epoch [15], Batch [750/938], Loss: 0.47376811504364014\n",
      "Validation: Epoch [15], Batch [751/938], Loss: 0.42737793922424316\n",
      "Validation: Epoch [15], Batch [752/938], Loss: 0.6711640357971191\n",
      "Validation: Epoch [15], Batch [753/938], Loss: 0.40631988644599915\n",
      "Validation: Epoch [15], Batch [754/938], Loss: 0.5267794132232666\n",
      "Validation: Epoch [15], Batch [755/938], Loss: 0.41536790132522583\n",
      "Validation: Epoch [15], Batch [756/938], Loss: 0.33706748485565186\n",
      "Validation: Epoch [15], Batch [757/938], Loss: 0.35344699025154114\n",
      "Validation: Epoch [15], Batch [758/938], Loss: 0.6224410533905029\n",
      "Validation: Epoch [15], Batch [759/938], Loss: 0.40421128273010254\n",
      "Validation: Epoch [15], Batch [760/938], Loss: 0.5556091070175171\n",
      "Validation: Epoch [15], Batch [761/938], Loss: 0.575609564781189\n",
      "Validation: Epoch [15], Batch [762/938], Loss: 0.361014187335968\n",
      "Validation: Epoch [15], Batch [763/938], Loss: 0.49988141655921936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [764/938], Loss: 0.31243929266929626\n",
      "Validation: Epoch [15], Batch [765/938], Loss: 0.39669108390808105\n",
      "Validation: Epoch [15], Batch [766/938], Loss: 0.6600074768066406\n",
      "Validation: Epoch [15], Batch [767/938], Loss: 0.42174825072288513\n",
      "Validation: Epoch [15], Batch [768/938], Loss: 0.30827128887176514\n",
      "Validation: Epoch [15], Batch [769/938], Loss: 0.2944493889808655\n",
      "Validation: Epoch [15], Batch [770/938], Loss: 0.33272093534469604\n",
      "Validation: Epoch [15], Batch [771/938], Loss: 0.31781813502311707\n",
      "Validation: Epoch [15], Batch [772/938], Loss: 0.404813289642334\n",
      "Validation: Epoch [15], Batch [773/938], Loss: 0.3184930086135864\n",
      "Validation: Epoch [15], Batch [774/938], Loss: 0.46352335810661316\n",
      "Validation: Epoch [15], Batch [775/938], Loss: 0.5003807544708252\n",
      "Validation: Epoch [15], Batch [776/938], Loss: 0.5745892524719238\n",
      "Validation: Epoch [15], Batch [777/938], Loss: 0.3713441491127014\n",
      "Validation: Epoch [15], Batch [778/938], Loss: 0.5803780555725098\n",
      "Validation: Epoch [15], Batch [779/938], Loss: 0.5926589965820312\n",
      "Validation: Epoch [15], Batch [780/938], Loss: 0.3007532060146332\n",
      "Validation: Epoch [15], Batch [781/938], Loss: 0.6057256460189819\n",
      "Validation: Epoch [15], Batch [782/938], Loss: 0.4297441244125366\n",
      "Validation: Epoch [15], Batch [783/938], Loss: 0.4261297881603241\n",
      "Validation: Epoch [15], Batch [784/938], Loss: 0.36655908823013306\n",
      "Validation: Epoch [15], Batch [785/938], Loss: 0.37856918573379517\n",
      "Validation: Epoch [15], Batch [786/938], Loss: 0.528934121131897\n",
      "Validation: Epoch [15], Batch [787/938], Loss: 0.26194530725479126\n",
      "Validation: Epoch [15], Batch [788/938], Loss: 0.4648243486881256\n",
      "Validation: Epoch [15], Batch [789/938], Loss: 0.38868996500968933\n",
      "Validation: Epoch [15], Batch [790/938], Loss: 0.38525718450546265\n",
      "Validation: Epoch [15], Batch [791/938], Loss: 0.4597857892513275\n",
      "Validation: Epoch [15], Batch [792/938], Loss: 0.5451844930648804\n",
      "Validation: Epoch [15], Batch [793/938], Loss: 0.4657314419746399\n",
      "Validation: Epoch [15], Batch [794/938], Loss: 0.6695600748062134\n",
      "Validation: Epoch [15], Batch [795/938], Loss: 0.4729102849960327\n",
      "Validation: Epoch [15], Batch [796/938], Loss: 0.34515681862831116\n",
      "Validation: Epoch [15], Batch [797/938], Loss: 0.5631105303764343\n",
      "Validation: Epoch [15], Batch [798/938], Loss: 0.4277667701244354\n",
      "Validation: Epoch [15], Batch [799/938], Loss: 0.40515971183776855\n",
      "Validation: Epoch [15], Batch [800/938], Loss: 0.5769909620285034\n",
      "Validation: Epoch [15], Batch [801/938], Loss: 0.4141998291015625\n",
      "Validation: Epoch [15], Batch [802/938], Loss: 0.41678258776664734\n",
      "Validation: Epoch [15], Batch [803/938], Loss: 0.649308443069458\n",
      "Validation: Epoch [15], Batch [804/938], Loss: 0.40571075677871704\n",
      "Validation: Epoch [15], Batch [805/938], Loss: 0.4765971004962921\n",
      "Validation: Epoch [15], Batch [806/938], Loss: 0.5024127960205078\n",
      "Validation: Epoch [15], Batch [807/938], Loss: 0.3504011332988739\n",
      "Validation: Epoch [15], Batch [808/938], Loss: 0.5414688587188721\n",
      "Validation: Epoch [15], Batch [809/938], Loss: 0.5791196823120117\n",
      "Validation: Epoch [15], Batch [810/938], Loss: 0.46947020292282104\n",
      "Validation: Epoch [15], Batch [811/938], Loss: 0.4369209408760071\n",
      "Validation: Epoch [15], Batch [812/938], Loss: 0.45825880765914917\n",
      "Validation: Epoch [15], Batch [813/938], Loss: 0.468266099691391\n",
      "Validation: Epoch [15], Batch [814/938], Loss: 0.5168177485466003\n",
      "Validation: Epoch [15], Batch [815/938], Loss: 0.3500431180000305\n",
      "Validation: Epoch [15], Batch [816/938], Loss: 0.42400485277175903\n",
      "Validation: Epoch [15], Batch [817/938], Loss: 0.47516822814941406\n",
      "Validation: Epoch [15], Batch [818/938], Loss: 0.3197592496871948\n",
      "Validation: Epoch [15], Batch [819/938], Loss: 0.5922356843948364\n",
      "Validation: Epoch [15], Batch [820/938], Loss: 0.78315669298172\n",
      "Validation: Epoch [15], Batch [821/938], Loss: 0.4229481816291809\n",
      "Validation: Epoch [15], Batch [822/938], Loss: 0.4494606554508209\n",
      "Validation: Epoch [15], Batch [823/938], Loss: 0.5761711597442627\n",
      "Validation: Epoch [15], Batch [824/938], Loss: 0.4964931905269623\n",
      "Validation: Epoch [15], Batch [825/938], Loss: 0.35165780782699585\n",
      "Validation: Epoch [15], Batch [826/938], Loss: 0.46783876419067383\n",
      "Validation: Epoch [15], Batch [827/938], Loss: 0.558077335357666\n",
      "Validation: Epoch [15], Batch [828/938], Loss: 0.2867105007171631\n",
      "Validation: Epoch [15], Batch [829/938], Loss: 0.33712533116340637\n",
      "Validation: Epoch [15], Batch [830/938], Loss: 0.521041989326477\n",
      "Validation: Epoch [15], Batch [831/938], Loss: 0.37059301137924194\n",
      "Validation: Epoch [15], Batch [832/938], Loss: 0.39321595430374146\n",
      "Validation: Epoch [15], Batch [833/938], Loss: 0.623682975769043\n",
      "Validation: Epoch [15], Batch [834/938], Loss: 0.4976285696029663\n",
      "Validation: Epoch [15], Batch [835/938], Loss: 0.318050742149353\n",
      "Validation: Epoch [15], Batch [836/938], Loss: 0.48779475688934326\n",
      "Validation: Epoch [15], Batch [837/938], Loss: 0.37844347953796387\n",
      "Validation: Epoch [15], Batch [838/938], Loss: 0.5562849044799805\n",
      "Validation: Epoch [15], Batch [839/938], Loss: 0.35762977600097656\n",
      "Validation: Epoch [15], Batch [840/938], Loss: 0.6258714199066162\n",
      "Validation: Epoch [15], Batch [841/938], Loss: 0.4616306722164154\n",
      "Validation: Epoch [15], Batch [842/938], Loss: 0.32799819111824036\n",
      "Validation: Epoch [15], Batch [843/938], Loss: 0.3260502815246582\n",
      "Validation: Epoch [15], Batch [844/938], Loss: 0.5300308465957642\n",
      "Validation: Epoch [15], Batch [845/938], Loss: 0.5976138114929199\n",
      "Validation: Epoch [15], Batch [846/938], Loss: 0.5754914283752441\n",
      "Validation: Epoch [15], Batch [847/938], Loss: 0.46799010038375854\n",
      "Validation: Epoch [15], Batch [848/938], Loss: 0.39372020959854126\n",
      "Validation: Epoch [15], Batch [849/938], Loss: 0.5146440267562866\n",
      "Validation: Epoch [15], Batch [850/938], Loss: 0.4276655912399292\n",
      "Validation: Epoch [15], Batch [851/938], Loss: 0.600213348865509\n",
      "Validation: Epoch [15], Batch [852/938], Loss: 0.3765503168106079\n",
      "Validation: Epoch [15], Batch [853/938], Loss: 0.3866446316242218\n",
      "Validation: Epoch [15], Batch [854/938], Loss: 0.39528313279151917\n",
      "Validation: Epoch [15], Batch [855/938], Loss: 0.2269839644432068\n",
      "Validation: Epoch [15], Batch [856/938], Loss: 0.4098033308982849\n",
      "Validation: Epoch [15], Batch [857/938], Loss: 0.4000711441040039\n",
      "Validation: Epoch [15], Batch [858/938], Loss: 0.45982515811920166\n",
      "Validation: Epoch [15], Batch [859/938], Loss: 0.3953474760055542\n",
      "Validation: Epoch [15], Batch [860/938], Loss: 0.5530080795288086\n",
      "Validation: Epoch [15], Batch [861/938], Loss: 0.37198108434677124\n",
      "Validation: Epoch [15], Batch [862/938], Loss: 0.5473229885101318\n",
      "Validation: Epoch [15], Batch [863/938], Loss: 0.47069787979125977\n",
      "Validation: Epoch [15], Batch [864/938], Loss: 0.487140953540802\n",
      "Validation: Epoch [15], Batch [865/938], Loss: 0.4601169228553772\n",
      "Validation: Epoch [15], Batch [866/938], Loss: 0.5076680779457092\n",
      "Validation: Epoch [15], Batch [867/938], Loss: 0.4472264051437378\n",
      "Validation: Epoch [15], Batch [868/938], Loss: 0.380837082862854\n",
      "Validation: Epoch [15], Batch [869/938], Loss: 0.5079343914985657\n",
      "Validation: Epoch [15], Batch [870/938], Loss: 0.6013360023498535\n",
      "Validation: Epoch [15], Batch [871/938], Loss: 0.5272363424301147\n",
      "Validation: Epoch [15], Batch [872/938], Loss: 0.39961689710617065\n",
      "Validation: Epoch [15], Batch [873/938], Loss: 0.43865835666656494\n",
      "Validation: Epoch [15], Batch [874/938], Loss: 0.37862348556518555\n",
      "Validation: Epoch [15], Batch [875/938], Loss: 0.3112112879753113\n",
      "Validation: Epoch [15], Batch [876/938], Loss: 0.4517516493797302\n",
      "Validation: Epoch [15], Batch [877/938], Loss: 0.5990715026855469\n",
      "Validation: Epoch [15], Batch [878/938], Loss: 0.5315182209014893\n",
      "Validation: Epoch [15], Batch [879/938], Loss: 0.3474416136741638\n",
      "Validation: Epoch [15], Batch [880/938], Loss: 0.33907249569892883\n",
      "Validation: Epoch [15], Batch [881/938], Loss: 0.49968060851097107\n",
      "Validation: Epoch [15], Batch [882/938], Loss: 0.47538185119628906\n",
      "Validation: Epoch [15], Batch [883/938], Loss: 0.6183984279632568\n",
      "Validation: Epoch [15], Batch [884/938], Loss: 0.4267784655094147\n",
      "Validation: Epoch [15], Batch [885/938], Loss: 0.47726449370384216\n",
      "Validation: Epoch [15], Batch [886/938], Loss: 0.4626622498035431\n",
      "Validation: Epoch [15], Batch [887/938], Loss: 0.5069574117660522\n",
      "Validation: Epoch [15], Batch [888/938], Loss: 0.41410142183303833\n",
      "Validation: Epoch [15], Batch [889/938], Loss: 0.2943296432495117\n",
      "Validation: Epoch [15], Batch [890/938], Loss: 0.44462379813194275\n",
      "Validation: Epoch [15], Batch [891/938], Loss: 0.4331982731819153\n",
      "Validation: Epoch [15], Batch [892/938], Loss: 0.3598583936691284\n",
      "Validation: Epoch [15], Batch [893/938], Loss: 0.7760584354400635\n",
      "Validation: Epoch [15], Batch [894/938], Loss: 0.3557724952697754\n",
      "Validation: Epoch [15], Batch [895/938], Loss: 0.44786080718040466\n",
      "Validation: Epoch [15], Batch [896/938], Loss: 0.5240528583526611\n",
      "Validation: Epoch [15], Batch [897/938], Loss: 0.40925973653793335\n",
      "Validation: Epoch [15], Batch [898/938], Loss: 0.6642004251480103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [15], Batch [899/938], Loss: 0.42748555541038513\n",
      "Validation: Epoch [15], Batch [900/938], Loss: 0.5634949803352356\n",
      "Validation: Epoch [15], Batch [901/938], Loss: 0.40648823976516724\n",
      "Validation: Epoch [15], Batch [902/938], Loss: 0.6346838474273682\n",
      "Validation: Epoch [15], Batch [903/938], Loss: 0.5093578100204468\n",
      "Validation: Epoch [15], Batch [904/938], Loss: 0.47488752007484436\n",
      "Validation: Epoch [15], Batch [905/938], Loss: 0.38578641414642334\n",
      "Validation: Epoch [15], Batch [906/938], Loss: 0.45385175943374634\n",
      "Validation: Epoch [15], Batch [907/938], Loss: 0.5106362700462341\n",
      "Validation: Epoch [15], Batch [908/938], Loss: 0.44698435068130493\n",
      "Validation: Epoch [15], Batch [909/938], Loss: 0.3363209366798401\n",
      "Validation: Epoch [15], Batch [910/938], Loss: 0.5530459880828857\n",
      "Validation: Epoch [15], Batch [911/938], Loss: 0.4334985613822937\n",
      "Validation: Epoch [15], Batch [912/938], Loss: 0.48502081632614136\n",
      "Validation: Epoch [15], Batch [913/938], Loss: 0.426997572183609\n",
      "Validation: Epoch [15], Batch [914/938], Loss: 0.6007053852081299\n",
      "Validation: Epoch [15], Batch [915/938], Loss: 0.4079449474811554\n",
      "Validation: Epoch [15], Batch [916/938], Loss: 0.4350143074989319\n",
      "Validation: Epoch [15], Batch [917/938], Loss: 0.4688042998313904\n",
      "Validation: Epoch [15], Batch [918/938], Loss: 0.6534909009933472\n",
      "Validation: Epoch [15], Batch [919/938], Loss: 0.3224417567253113\n",
      "Validation: Epoch [15], Batch [920/938], Loss: 0.48517942428588867\n",
      "Validation: Epoch [15], Batch [921/938], Loss: 0.4314192533493042\n",
      "Validation: Epoch [15], Batch [922/938], Loss: 0.2708987593650818\n",
      "Validation: Epoch [15], Batch [923/938], Loss: 0.4507043659687042\n",
      "Validation: Epoch [15], Batch [924/938], Loss: 0.3610799014568329\n",
      "Validation: Epoch [15], Batch [925/938], Loss: 0.2783399820327759\n",
      "Validation: Epoch [15], Batch [926/938], Loss: 0.3616842031478882\n",
      "Validation: Epoch [15], Batch [927/938], Loss: 0.3737686574459076\n",
      "Validation: Epoch [15], Batch [928/938], Loss: 0.8727841377258301\n",
      "Validation: Epoch [15], Batch [929/938], Loss: 0.4325486421585083\n",
      "Validation: Epoch [15], Batch [930/938], Loss: 0.5144506692886353\n",
      "Validation: Epoch [15], Batch [931/938], Loss: 0.6291242837905884\n",
      "Validation: Epoch [15], Batch [932/938], Loss: 0.5769217014312744\n",
      "Validation: Epoch [15], Batch [933/938], Loss: 0.4839429259300232\n",
      "Validation: Epoch [15], Batch [934/938], Loss: 0.7378666996955872\n",
      "Validation: Epoch [15], Batch [935/938], Loss: 0.49280285835266113\n",
      "Validation: Epoch [15], Batch [936/938], Loss: 0.4220072627067566\n",
      "Validation: Epoch [15], Batch [937/938], Loss: 0.4854242205619812\n",
      "Validation: Epoch [15], Batch [938/938], Loss: 0.3024299740791321\n",
      "Accuracy of test set: 0.84365\n",
      "Train: Epoch [16], Batch [1/938], Loss: 0.5693325996398926\n",
      "Train: Epoch [16], Batch [2/938], Loss: 0.5456492900848389\n",
      "Train: Epoch [16], Batch [3/938], Loss: 0.4976980686187744\n",
      "Train: Epoch [16], Batch [4/938], Loss: 0.4991917610168457\n",
      "Train: Epoch [16], Batch [5/938], Loss: 0.5004672408103943\n",
      "Train: Epoch [16], Batch [6/938], Loss: 0.4332020580768585\n",
      "Train: Epoch [16], Batch [7/938], Loss: 0.45585304498672485\n",
      "Train: Epoch [16], Batch [8/938], Loss: 0.511806070804596\n",
      "Train: Epoch [16], Batch [9/938], Loss: 0.44795167446136475\n",
      "Train: Epoch [16], Batch [10/938], Loss: 0.5935810804367065\n",
      "Train: Epoch [16], Batch [11/938], Loss: 0.42858535051345825\n",
      "Train: Epoch [16], Batch [12/938], Loss: 0.5298482179641724\n",
      "Train: Epoch [16], Batch [13/938], Loss: 0.5348799824714661\n",
      "Train: Epoch [16], Batch [14/938], Loss: 0.5109533667564392\n",
      "Train: Epoch [16], Batch [15/938], Loss: 0.5159516334533691\n",
      "Train: Epoch [16], Batch [16/938], Loss: 0.5617887377738953\n",
      "Train: Epoch [16], Batch [17/938], Loss: 0.4818746745586395\n",
      "Train: Epoch [16], Batch [18/938], Loss: 0.4378022849559784\n",
      "Train: Epoch [16], Batch [19/938], Loss: 0.593843936920166\n",
      "Train: Epoch [16], Batch [20/938], Loss: 0.5173447132110596\n",
      "Train: Epoch [16], Batch [21/938], Loss: 0.47245290875434875\n",
      "Train: Epoch [16], Batch [22/938], Loss: 0.6333226561546326\n",
      "Train: Epoch [16], Batch [23/938], Loss: 0.42350518703460693\n",
      "Train: Epoch [16], Batch [24/938], Loss: 0.3227344751358032\n",
      "Train: Epoch [16], Batch [25/938], Loss: 0.5863040685653687\n",
      "Train: Epoch [16], Batch [26/938], Loss: 0.4741280674934387\n",
      "Train: Epoch [16], Batch [27/938], Loss: 0.476620078086853\n",
      "Train: Epoch [16], Batch [28/938], Loss: 0.4599929451942444\n",
      "Train: Epoch [16], Batch [29/938], Loss: 0.5094178915023804\n",
      "Train: Epoch [16], Batch [30/938], Loss: 0.4754737615585327\n",
      "Train: Epoch [16], Batch [31/938], Loss: 0.5501940846443176\n",
      "Train: Epoch [16], Batch [32/938], Loss: 0.39222899079322815\n",
      "Train: Epoch [16], Batch [33/938], Loss: 0.3825882375240326\n",
      "Train: Epoch [16], Batch [34/938], Loss: 0.401946097612381\n",
      "Train: Epoch [16], Batch [35/938], Loss: 0.38932472467422485\n",
      "Train: Epoch [16], Batch [36/938], Loss: 0.4418575167655945\n",
      "Train: Epoch [16], Batch [37/938], Loss: 0.3392735719680786\n",
      "Train: Epoch [16], Batch [38/938], Loss: 0.4287818968296051\n",
      "Train: Epoch [16], Batch [39/938], Loss: 0.5244749784469604\n",
      "Train: Epoch [16], Batch [40/938], Loss: 0.4207228720188141\n",
      "Train: Epoch [16], Batch [41/938], Loss: 0.4065026640892029\n",
      "Train: Epoch [16], Batch [42/938], Loss: 0.3939076066017151\n",
      "Train: Epoch [16], Batch [43/938], Loss: 0.40189334750175476\n",
      "Train: Epoch [16], Batch [44/938], Loss: 0.4465448260307312\n",
      "Train: Epoch [16], Batch [45/938], Loss: 0.32107818126678467\n",
      "Train: Epoch [16], Batch [46/938], Loss: 0.3832806646823883\n",
      "Train: Epoch [16], Batch [47/938], Loss: 0.4601854681968689\n",
      "Train: Epoch [16], Batch [48/938], Loss: 0.5385390520095825\n",
      "Train: Epoch [16], Batch [49/938], Loss: 0.40252038836479187\n",
      "Train: Epoch [16], Batch [50/938], Loss: 0.5635700821876526\n",
      "Train: Epoch [16], Batch [51/938], Loss: 0.5299361348152161\n",
      "Train: Epoch [16], Batch [52/938], Loss: 0.5845399498939514\n",
      "Train: Epoch [16], Batch [53/938], Loss: 0.37172335386276245\n",
      "Train: Epoch [16], Batch [54/938], Loss: 0.4689291715621948\n",
      "Train: Epoch [16], Batch [55/938], Loss: 0.49487829208374023\n",
      "Train: Epoch [16], Batch [56/938], Loss: 0.4601565897464752\n",
      "Train: Epoch [16], Batch [57/938], Loss: 0.598922610282898\n",
      "Train: Epoch [16], Batch [58/938], Loss: 0.6070141792297363\n",
      "Train: Epoch [16], Batch [59/938], Loss: 0.4286479949951172\n",
      "Train: Epoch [16], Batch [60/938], Loss: 0.614891767501831\n",
      "Train: Epoch [16], Batch [61/938], Loss: 0.528661847114563\n",
      "Train: Epoch [16], Batch [62/938], Loss: 0.3550320863723755\n",
      "Train: Epoch [16], Batch [63/938], Loss: 0.3832022547721863\n",
      "Train: Epoch [16], Batch [64/938], Loss: 0.5849437713623047\n",
      "Train: Epoch [16], Batch [65/938], Loss: 0.30061638355255127\n",
      "Train: Epoch [16], Batch [66/938], Loss: 0.45448949933052063\n",
      "Train: Epoch [16], Batch [67/938], Loss: 0.5715780258178711\n",
      "Train: Epoch [16], Batch [68/938], Loss: 0.4851703941822052\n",
      "Train: Epoch [16], Batch [69/938], Loss: 0.5664448142051697\n",
      "Train: Epoch [16], Batch [70/938], Loss: 0.3898598849773407\n",
      "Train: Epoch [16], Batch [71/938], Loss: 0.5064374208450317\n",
      "Train: Epoch [16], Batch [72/938], Loss: 0.5945134162902832\n",
      "Train: Epoch [16], Batch [73/938], Loss: 0.4131801724433899\n",
      "Train: Epoch [16], Batch [74/938], Loss: 0.417447030544281\n",
      "Train: Epoch [16], Batch [75/938], Loss: 0.4730021357536316\n",
      "Train: Epoch [16], Batch [76/938], Loss: 0.4840424954891205\n",
      "Train: Epoch [16], Batch [77/938], Loss: 0.513724684715271\n",
      "Train: Epoch [16], Batch [78/938], Loss: 0.40051665902137756\n",
      "Train: Epoch [16], Batch [79/938], Loss: 0.43656349182128906\n",
      "Train: Epoch [16], Batch [80/938], Loss: 0.3252163827419281\n",
      "Train: Epoch [16], Batch [81/938], Loss: 0.32040053606033325\n",
      "Train: Epoch [16], Batch [82/938], Loss: 0.35964179039001465\n",
      "Train: Epoch [16], Batch [83/938], Loss: 0.3196708559989929\n",
      "Train: Epoch [16], Batch [84/938], Loss: 0.28944075107574463\n",
      "Train: Epoch [16], Batch [85/938], Loss: 0.611777126789093\n",
      "Train: Epoch [16], Batch [86/938], Loss: 0.3537268042564392\n",
      "Train: Epoch [16], Batch [87/938], Loss: 0.5198960304260254\n",
      "Train: Epoch [16], Batch [88/938], Loss: 0.49421656131744385\n",
      "Train: Epoch [16], Batch [89/938], Loss: 0.46387848258018494\n",
      "Train: Epoch [16], Batch [90/938], Loss: 0.6733887195587158\n",
      "Train: Epoch [16], Batch [91/938], Loss: 0.621538519859314\n",
      "Train: Epoch [16], Batch [92/938], Loss: 0.5261296033859253\n",
      "Train: Epoch [16], Batch [93/938], Loss: 0.44451841711997986\n",
      "Train: Epoch [16], Batch [94/938], Loss: 0.4140482544898987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [16], Batch [95/938], Loss: 0.3796003758907318\n",
      "Train: Epoch [16], Batch [96/938], Loss: 0.5085611343383789\n",
      "Train: Epoch [16], Batch [97/938], Loss: 0.47878119349479675\n",
      "Train: Epoch [16], Batch [98/938], Loss: 0.6043978929519653\n",
      "Train: Epoch [16], Batch [99/938], Loss: 0.48453760147094727\n",
      "Train: Epoch [16], Batch [100/938], Loss: 0.4762893617153168\n",
      "Train: Epoch [16], Batch [101/938], Loss: 0.5123772621154785\n",
      "Train: Epoch [16], Batch [102/938], Loss: 0.43668508529663086\n",
      "Train: Epoch [16], Batch [103/938], Loss: 0.4243229925632477\n",
      "Train: Epoch [16], Batch [104/938], Loss: 0.5603294968605042\n",
      "Train: Epoch [16], Batch [105/938], Loss: 0.340205579996109\n",
      "Train: Epoch [16], Batch [106/938], Loss: 0.4734753966331482\n",
      "Train: Epoch [16], Batch [107/938], Loss: 0.45051029324531555\n",
      "Train: Epoch [16], Batch [108/938], Loss: 0.4455071687698364\n",
      "Train: Epoch [16], Batch [109/938], Loss: 0.5391725301742554\n",
      "Train: Epoch [16], Batch [110/938], Loss: 0.4843031167984009\n",
      "Train: Epoch [16], Batch [111/938], Loss: 0.4812842309474945\n",
      "Train: Epoch [16], Batch [112/938], Loss: 0.21928957104682922\n",
      "Train: Epoch [16], Batch [113/938], Loss: 0.44126343727111816\n",
      "Train: Epoch [16], Batch [114/938], Loss: 0.4626270532608032\n",
      "Train: Epoch [16], Batch [115/938], Loss: 0.29465529322624207\n",
      "Train: Epoch [16], Batch [116/938], Loss: 0.26155906915664673\n",
      "Train: Epoch [16], Batch [117/938], Loss: 0.6400331258773804\n",
      "Train: Epoch [16], Batch [118/938], Loss: 0.5369171500205994\n",
      "Train: Epoch [16], Batch [119/938], Loss: 0.7657991647720337\n",
      "Train: Epoch [16], Batch [120/938], Loss: 0.5281453132629395\n",
      "Train: Epoch [16], Batch [121/938], Loss: 0.2962076663970947\n",
      "Train: Epoch [16], Batch [122/938], Loss: 0.45902132987976074\n",
      "Train: Epoch [16], Batch [123/938], Loss: 0.3718937039375305\n",
      "Train: Epoch [16], Batch [124/938], Loss: 0.5193119049072266\n",
      "Train: Epoch [16], Batch [125/938], Loss: 0.36438584327697754\n",
      "Train: Epoch [16], Batch [126/938], Loss: 0.4669208526611328\n",
      "Train: Epoch [16], Batch [127/938], Loss: 0.6036370992660522\n",
      "Train: Epoch [16], Batch [128/938], Loss: 0.46986502408981323\n",
      "Train: Epoch [16], Batch [129/938], Loss: 0.4727235436439514\n",
      "Train: Epoch [16], Batch [130/938], Loss: 0.4829831123352051\n",
      "Train: Epoch [16], Batch [131/938], Loss: 0.40524160861968994\n",
      "Train: Epoch [16], Batch [132/938], Loss: 0.46993115544319153\n",
      "Train: Epoch [16], Batch [133/938], Loss: 0.4153165817260742\n",
      "Train: Epoch [16], Batch [134/938], Loss: 0.5156320333480835\n",
      "Train: Epoch [16], Batch [135/938], Loss: 0.33575838804244995\n",
      "Train: Epoch [16], Batch [136/938], Loss: 0.3397519290447235\n",
      "Train: Epoch [16], Batch [137/938], Loss: 0.3547290861606598\n",
      "Train: Epoch [16], Batch [138/938], Loss: 0.3140222132205963\n",
      "Train: Epoch [16], Batch [139/938], Loss: 0.5201115012168884\n",
      "Train: Epoch [16], Batch [140/938], Loss: 0.5000747442245483\n",
      "Train: Epoch [16], Batch [141/938], Loss: 0.4711136817932129\n",
      "Train: Epoch [16], Batch [142/938], Loss: 0.46671417355537415\n",
      "Train: Epoch [16], Batch [143/938], Loss: 0.6633318662643433\n",
      "Train: Epoch [16], Batch [144/938], Loss: 0.4665869474411011\n",
      "Train: Epoch [16], Batch [145/938], Loss: 0.4152086079120636\n",
      "Train: Epoch [16], Batch [146/938], Loss: 0.4390382170677185\n",
      "Train: Epoch [16], Batch [147/938], Loss: 0.43828558921813965\n",
      "Train: Epoch [16], Batch [148/938], Loss: 0.6261909604072571\n",
      "Train: Epoch [16], Batch [149/938], Loss: 0.449229896068573\n",
      "Train: Epoch [16], Batch [150/938], Loss: 0.47197458148002625\n",
      "Train: Epoch [16], Batch [151/938], Loss: 0.25364917516708374\n",
      "Train: Epoch [16], Batch [152/938], Loss: 0.4804335832595825\n",
      "Train: Epoch [16], Batch [153/938], Loss: 0.3716188371181488\n",
      "Train: Epoch [16], Batch [154/938], Loss: 0.5657119154930115\n",
      "Train: Epoch [16], Batch [155/938], Loss: 0.4966176450252533\n",
      "Train: Epoch [16], Batch [156/938], Loss: 0.4718543589115143\n",
      "Train: Epoch [16], Batch [157/938], Loss: 0.6204339265823364\n",
      "Train: Epoch [16], Batch [158/938], Loss: 0.4434957802295685\n",
      "Train: Epoch [16], Batch [159/938], Loss: 0.4650508463382721\n",
      "Train: Epoch [16], Batch [160/938], Loss: 0.5409524440765381\n",
      "Train: Epoch [16], Batch [161/938], Loss: 0.35932648181915283\n",
      "Train: Epoch [16], Batch [162/938], Loss: 0.38285762071609497\n",
      "Train: Epoch [16], Batch [163/938], Loss: 0.49924999475479126\n",
      "Train: Epoch [16], Batch [164/938], Loss: 0.6305166482925415\n",
      "Train: Epoch [16], Batch [165/938], Loss: 0.48502230644226074\n",
      "Train: Epoch [16], Batch [166/938], Loss: 0.4346211850643158\n",
      "Train: Epoch [16], Batch [167/938], Loss: 0.4354802966117859\n",
      "Train: Epoch [16], Batch [168/938], Loss: 0.5069472193717957\n",
      "Train: Epoch [16], Batch [169/938], Loss: 0.5765693187713623\n",
      "Train: Epoch [16], Batch [170/938], Loss: 0.4434080421924591\n",
      "Train: Epoch [16], Batch [171/938], Loss: 0.5166977047920227\n",
      "Train: Epoch [16], Batch [172/938], Loss: 0.362373948097229\n",
      "Train: Epoch [16], Batch [173/938], Loss: 0.4495813846588135\n",
      "Train: Epoch [16], Batch [174/938], Loss: 0.4054037928581238\n",
      "Train: Epoch [16], Batch [175/938], Loss: 0.37274110317230225\n",
      "Train: Epoch [16], Batch [176/938], Loss: 0.48138129711151123\n",
      "Train: Epoch [16], Batch [177/938], Loss: 0.5368925333023071\n",
      "Train: Epoch [16], Batch [178/938], Loss: 0.4260351061820984\n",
      "Train: Epoch [16], Batch [179/938], Loss: 0.4702974557876587\n",
      "Train: Epoch [16], Batch [180/938], Loss: 0.48338690400123596\n",
      "Train: Epoch [16], Batch [181/938], Loss: 0.4789277911186218\n",
      "Train: Epoch [16], Batch [182/938], Loss: 0.287700355052948\n",
      "Train: Epoch [16], Batch [183/938], Loss: 0.5377302169799805\n",
      "Train: Epoch [16], Batch [184/938], Loss: 0.4168614149093628\n",
      "Train: Epoch [16], Batch [185/938], Loss: 0.7776392698287964\n",
      "Train: Epoch [16], Batch [186/938], Loss: 0.5302613973617554\n",
      "Train: Epoch [16], Batch [187/938], Loss: 0.3749368190765381\n",
      "Train: Epoch [16], Batch [188/938], Loss: 0.3713947832584381\n",
      "Train: Epoch [16], Batch [189/938], Loss: 0.3326883018016815\n",
      "Train: Epoch [16], Batch [190/938], Loss: 0.6087708473205566\n",
      "Train: Epoch [16], Batch [191/938], Loss: 0.45464298129081726\n",
      "Train: Epoch [16], Batch [192/938], Loss: 0.7114540934562683\n",
      "Train: Epoch [16], Batch [193/938], Loss: 0.3603299856185913\n",
      "Train: Epoch [16], Batch [194/938], Loss: 0.28079620003700256\n",
      "Train: Epoch [16], Batch [195/938], Loss: 0.3672422170639038\n",
      "Train: Epoch [16], Batch [196/938], Loss: 0.3984287679195404\n",
      "Train: Epoch [16], Batch [197/938], Loss: 0.46496549248695374\n",
      "Train: Epoch [16], Batch [198/938], Loss: 0.3079184591770172\n",
      "Train: Epoch [16], Batch [199/938], Loss: 0.4063860774040222\n",
      "Train: Epoch [16], Batch [200/938], Loss: 0.34255194664001465\n",
      "Train: Epoch [16], Batch [201/938], Loss: 0.3757481276988983\n",
      "Train: Epoch [16], Batch [202/938], Loss: 0.4448050260543823\n",
      "Train: Epoch [16], Batch [203/938], Loss: 0.4261743426322937\n",
      "Train: Epoch [16], Batch [204/938], Loss: 0.7736910581588745\n",
      "Train: Epoch [16], Batch [205/938], Loss: 0.3544641435146332\n",
      "Train: Epoch [16], Batch [206/938], Loss: 0.438955157995224\n",
      "Train: Epoch [16], Batch [207/938], Loss: 0.2945256531238556\n",
      "Train: Epoch [16], Batch [208/938], Loss: 0.34995606541633606\n",
      "Train: Epoch [16], Batch [209/938], Loss: 0.26475080847740173\n",
      "Train: Epoch [16], Batch [210/938], Loss: 0.7423141002655029\n",
      "Train: Epoch [16], Batch [211/938], Loss: 0.3285568952560425\n",
      "Train: Epoch [16], Batch [212/938], Loss: 0.1473098248243332\n",
      "Train: Epoch [16], Batch [213/938], Loss: 0.40810227394104004\n",
      "Train: Epoch [16], Batch [214/938], Loss: 0.5151537656784058\n",
      "Train: Epoch [16], Batch [215/938], Loss: 0.4830455482006073\n",
      "Train: Epoch [16], Batch [216/938], Loss: 0.3829280734062195\n",
      "Train: Epoch [16], Batch [217/938], Loss: 0.5903699994087219\n",
      "Train: Epoch [16], Batch [218/938], Loss: 0.5211244821548462\n",
      "Train: Epoch [16], Batch [219/938], Loss: 0.4884556829929352\n",
      "Train: Epoch [16], Batch [220/938], Loss: 0.44120079278945923\n",
      "Train: Epoch [16], Batch [221/938], Loss: 0.5728787183761597\n",
      "Train: Epoch [16], Batch [222/938], Loss: 0.3738050162792206\n",
      "Train: Epoch [16], Batch [223/938], Loss: 0.5827053785324097\n",
      "Train: Epoch [16], Batch [224/938], Loss: 0.40114396810531616\n",
      "Train: Epoch [16], Batch [225/938], Loss: 0.39053308963775635\n",
      "Train: Epoch [16], Batch [226/938], Loss: 0.2497924417257309\n",
      "Train: Epoch [16], Batch [227/938], Loss: 0.5554545521736145\n",
      "Train: Epoch [16], Batch [228/938], Loss: 0.4760483205318451\n",
      "Train: Epoch [16], Batch [229/938], Loss: 0.6550127267837524\n",
      "Train: Epoch [16], Batch [230/938], Loss: 0.40142160654067993\n",
      "Train: Epoch [16], Batch [231/938], Loss: 0.3957225978374481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [16], Batch [232/938], Loss: 0.4346884489059448\n",
      "Train: Epoch [16], Batch [233/938], Loss: 0.35809922218322754\n",
      "Train: Epoch [16], Batch [234/938], Loss: 0.5835800170898438\n",
      "Train: Epoch [16], Batch [235/938], Loss: 0.731111466884613\n",
      "Train: Epoch [16], Batch [236/938], Loss: 0.4006679654121399\n",
      "Train: Epoch [16], Batch [237/938], Loss: 0.4120441675186157\n",
      "Train: Epoch [16], Batch [238/938], Loss: 0.34156471490859985\n",
      "Train: Epoch [16], Batch [239/938], Loss: 0.497520387172699\n",
      "Train: Epoch [16], Batch [240/938], Loss: 0.4879925847053528\n",
      "Train: Epoch [16], Batch [241/938], Loss: 0.44198518991470337\n",
      "Train: Epoch [16], Batch [242/938], Loss: 0.49075472354888916\n",
      "Train: Epoch [16], Batch [243/938], Loss: 0.5877237319946289\n",
      "Train: Epoch [16], Batch [244/938], Loss: 0.4266379773616791\n",
      "Train: Epoch [16], Batch [245/938], Loss: 0.5352442264556885\n",
      "Train: Epoch [16], Batch [246/938], Loss: 0.47058504819869995\n",
      "Train: Epoch [16], Batch [247/938], Loss: 0.4569833278656006\n",
      "Train: Epoch [16], Batch [248/938], Loss: 0.4875151515007019\n",
      "Train: Epoch [16], Batch [249/938], Loss: 0.402937650680542\n",
      "Train: Epoch [16], Batch [250/938], Loss: 0.3759586811065674\n",
      "Train: Epoch [16], Batch [251/938], Loss: 0.5791501998901367\n",
      "Train: Epoch [16], Batch [252/938], Loss: 0.48869964480400085\n",
      "Train: Epoch [16], Batch [253/938], Loss: 0.7425174713134766\n",
      "Train: Epoch [16], Batch [254/938], Loss: 0.4179651141166687\n",
      "Train: Epoch [16], Batch [255/938], Loss: 0.42538711428642273\n",
      "Train: Epoch [16], Batch [256/938], Loss: 0.4776621460914612\n",
      "Train: Epoch [16], Batch [257/938], Loss: 0.46385011076927185\n",
      "Train: Epoch [16], Batch [258/938], Loss: 0.631729245185852\n",
      "Train: Epoch [16], Batch [259/938], Loss: 0.3518407940864563\n",
      "Train: Epoch [16], Batch [260/938], Loss: 0.6654331684112549\n",
      "Train: Epoch [16], Batch [261/938], Loss: 0.35463517904281616\n",
      "Train: Epoch [16], Batch [262/938], Loss: 0.2840712070465088\n",
      "Train: Epoch [16], Batch [263/938], Loss: 0.3525141477584839\n",
      "Train: Epoch [16], Batch [264/938], Loss: 0.3337191939353943\n",
      "Train: Epoch [16], Batch [265/938], Loss: 0.3755253553390503\n",
      "Train: Epoch [16], Batch [266/938], Loss: 0.4584864377975464\n",
      "Train: Epoch [16], Batch [267/938], Loss: 0.4084644615650177\n",
      "Train: Epoch [16], Batch [268/938], Loss: 0.42066365480422974\n",
      "Train: Epoch [16], Batch [269/938], Loss: 0.32324257493019104\n",
      "Train: Epoch [16], Batch [270/938], Loss: 0.3563094139099121\n",
      "Train: Epoch [16], Batch [271/938], Loss: 0.43608182668685913\n",
      "Train: Epoch [16], Batch [272/938], Loss: 0.2965853214263916\n",
      "Train: Epoch [16], Batch [273/938], Loss: 0.4206258952617645\n",
      "Train: Epoch [16], Batch [274/938], Loss: 0.5730886459350586\n",
      "Train: Epoch [16], Batch [275/938], Loss: 0.3828592300415039\n",
      "Train: Epoch [16], Batch [276/938], Loss: 0.5184101462364197\n",
      "Train: Epoch [16], Batch [277/938], Loss: 0.29045137763023376\n",
      "Train: Epoch [16], Batch [278/938], Loss: 0.39659738540649414\n",
      "Train: Epoch [16], Batch [279/938], Loss: 0.3781801462173462\n",
      "Train: Epoch [16], Batch [280/938], Loss: 0.45301347970962524\n",
      "Train: Epoch [16], Batch [281/938], Loss: 0.42004120349884033\n",
      "Train: Epoch [16], Batch [282/938], Loss: 0.3181542754173279\n",
      "Train: Epoch [16], Batch [283/938], Loss: 0.6108193397521973\n",
      "Train: Epoch [16], Batch [284/938], Loss: 0.4382498264312744\n",
      "Train: Epoch [16], Batch [285/938], Loss: 0.4606667459011078\n",
      "Train: Epoch [16], Batch [286/938], Loss: 0.43039822578430176\n",
      "Train: Epoch [16], Batch [287/938], Loss: 0.47407960891723633\n",
      "Train: Epoch [16], Batch [288/938], Loss: 0.4705856442451477\n",
      "Train: Epoch [16], Batch [289/938], Loss: 0.46678492426872253\n",
      "Train: Epoch [16], Batch [290/938], Loss: 0.510302722454071\n",
      "Train: Epoch [16], Batch [291/938], Loss: 0.39236387610435486\n",
      "Train: Epoch [16], Batch [292/938], Loss: 0.5607589483261108\n",
      "Train: Epoch [16], Batch [293/938], Loss: 0.3481893539428711\n",
      "Train: Epoch [16], Batch [294/938], Loss: 0.25794005393981934\n",
      "Train: Epoch [16], Batch [295/938], Loss: 0.352020263671875\n",
      "Train: Epoch [16], Batch [296/938], Loss: 0.561612606048584\n",
      "Train: Epoch [16], Batch [297/938], Loss: 0.3576493263244629\n",
      "Train: Epoch [16], Batch [298/938], Loss: 0.3294258713722229\n",
      "Train: Epoch [16], Batch [299/938], Loss: 0.5312676429748535\n",
      "Train: Epoch [16], Batch [300/938], Loss: 0.47679173946380615\n",
      "Train: Epoch [16], Batch [301/938], Loss: 0.4049624502658844\n",
      "Train: Epoch [16], Batch [302/938], Loss: 0.383462518453598\n",
      "Train: Epoch [16], Batch [303/938], Loss: 0.4428061544895172\n",
      "Train: Epoch [16], Batch [304/938], Loss: 0.7010647058486938\n",
      "Train: Epoch [16], Batch [305/938], Loss: 0.5652235150337219\n",
      "Train: Epoch [16], Batch [306/938], Loss: 0.5416162610054016\n",
      "Train: Epoch [16], Batch [307/938], Loss: 0.4993835687637329\n",
      "Train: Epoch [16], Batch [308/938], Loss: 0.5613006949424744\n",
      "Train: Epoch [16], Batch [309/938], Loss: 0.4365696907043457\n",
      "Train: Epoch [16], Batch [310/938], Loss: 0.530260443687439\n",
      "Train: Epoch [16], Batch [311/938], Loss: 0.44326844811439514\n",
      "Train: Epoch [16], Batch [312/938], Loss: 0.5643039345741272\n",
      "Train: Epoch [16], Batch [313/938], Loss: 0.460037499666214\n",
      "Train: Epoch [16], Batch [314/938], Loss: 0.5017906427383423\n",
      "Train: Epoch [16], Batch [315/938], Loss: 0.4088191092014313\n",
      "Train: Epoch [16], Batch [316/938], Loss: 0.5475934743881226\n",
      "Train: Epoch [16], Batch [317/938], Loss: 0.27395278215408325\n",
      "Train: Epoch [16], Batch [318/938], Loss: 0.5614166855812073\n",
      "Train: Epoch [16], Batch [319/938], Loss: 0.42524978518486023\n",
      "Train: Epoch [16], Batch [320/938], Loss: 0.4745136499404907\n",
      "Train: Epoch [16], Batch [321/938], Loss: 0.43175560235977173\n",
      "Train: Epoch [16], Batch [322/938], Loss: 0.5116580724716187\n",
      "Train: Epoch [16], Batch [323/938], Loss: 0.46467891335487366\n",
      "Train: Epoch [16], Batch [324/938], Loss: 0.4885580539703369\n",
      "Train: Epoch [16], Batch [325/938], Loss: 0.5419385433197021\n",
      "Train: Epoch [16], Batch [326/938], Loss: 0.44179466366767883\n",
      "Train: Epoch [16], Batch [327/938], Loss: 0.5079195499420166\n",
      "Train: Epoch [16], Batch [328/938], Loss: 0.29657137393951416\n",
      "Train: Epoch [16], Batch [329/938], Loss: 0.5284798741340637\n",
      "Train: Epoch [16], Batch [330/938], Loss: 0.5182558298110962\n",
      "Train: Epoch [16], Batch [331/938], Loss: 0.40732675790786743\n",
      "Train: Epoch [16], Batch [332/938], Loss: 0.3485509157180786\n",
      "Train: Epoch [16], Batch [333/938], Loss: 0.42276129126548767\n",
      "Train: Epoch [16], Batch [334/938], Loss: 0.42920345067977905\n",
      "Train: Epoch [16], Batch [335/938], Loss: 0.35225391387939453\n",
      "Train: Epoch [16], Batch [336/938], Loss: 0.452373743057251\n",
      "Train: Epoch [16], Batch [337/938], Loss: 0.4886070489883423\n",
      "Train: Epoch [16], Batch [338/938], Loss: 0.6076153516769409\n",
      "Train: Epoch [16], Batch [339/938], Loss: 0.4763113260269165\n",
      "Train: Epoch [16], Batch [340/938], Loss: 0.36720800399780273\n",
      "Train: Epoch [16], Batch [341/938], Loss: 0.4048766791820526\n",
      "Train: Epoch [16], Batch [342/938], Loss: 0.39036357402801514\n",
      "Train: Epoch [16], Batch [343/938], Loss: 0.5594359636306763\n",
      "Train: Epoch [16], Batch [344/938], Loss: 0.4086853265762329\n",
      "Train: Epoch [16], Batch [345/938], Loss: 0.40930938720703125\n",
      "Train: Epoch [16], Batch [346/938], Loss: 0.641632616519928\n",
      "Train: Epoch [16], Batch [347/938], Loss: 0.5651004314422607\n",
      "Train: Epoch [16], Batch [348/938], Loss: 0.5039024353027344\n",
      "Train: Epoch [16], Batch [349/938], Loss: 0.31558889150619507\n",
      "Train: Epoch [16], Batch [350/938], Loss: 0.34036490321159363\n",
      "Train: Epoch [16], Batch [351/938], Loss: 0.5179923176765442\n",
      "Train: Epoch [16], Batch [352/938], Loss: 0.4642831087112427\n",
      "Train: Epoch [16], Batch [353/938], Loss: 0.5208070278167725\n",
      "Train: Epoch [16], Batch [354/938], Loss: 0.6265788078308105\n",
      "Train: Epoch [16], Batch [355/938], Loss: 0.4883991479873657\n",
      "Train: Epoch [16], Batch [356/938], Loss: 0.42092856764793396\n",
      "Train: Epoch [16], Batch [357/938], Loss: 0.6110888719558716\n",
      "Train: Epoch [16], Batch [358/938], Loss: 0.3770603537559509\n",
      "Train: Epoch [16], Batch [359/938], Loss: 0.5277141332626343\n",
      "Train: Epoch [16], Batch [360/938], Loss: 0.4410136342048645\n",
      "Train: Epoch [16], Batch [361/938], Loss: 0.35459035634994507\n",
      "Train: Epoch [16], Batch [362/938], Loss: 0.30403947830200195\n",
      "Train: Epoch [16], Batch [363/938], Loss: 0.30858391523361206\n",
      "Train: Epoch [16], Batch [364/938], Loss: 0.3153790235519409\n",
      "Train: Epoch [16], Batch [365/938], Loss: 0.4851776659488678\n",
      "Train: Epoch [16], Batch [366/938], Loss: 0.4579366445541382\n",
      "Train: Epoch [16], Batch [367/938], Loss: 0.43453314900398254\n",
      "Train: Epoch [16], Batch [368/938], Loss: 0.42399030923843384\n",
      "Train: Epoch [16], Batch [369/938], Loss: 0.46499568223953247\n",
      "Train: Epoch [16], Batch [370/938], Loss: 0.5335111618041992\n",
      "Train: Epoch [16], Batch [371/938], Loss: 0.3773452639579773\n",
      "Train: Epoch [16], Batch [372/938], Loss: 0.6239413022994995\n",
      "Train: Epoch [16], Batch [373/938], Loss: 0.3997902572154999\n",
      "Train: Epoch [16], Batch [374/938], Loss: 0.34131920337677\n",
      "Train: Epoch [16], Batch [375/938], Loss: 0.345734179019928\n",
      "Train: Epoch [16], Batch [376/938], Loss: 0.43341168761253357\n",
      "Train: Epoch [16], Batch [377/938], Loss: 0.39036810398101807\n",
      "Train: Epoch [16], Batch [378/938], Loss: 0.5001384019851685\n",
      "Train: Epoch [16], Batch [379/938], Loss: 0.44579222798347473\n",
      "Train: Epoch [16], Batch [380/938], Loss: 0.3795642852783203\n",
      "Train: Epoch [16], Batch [381/938], Loss: 0.39688941836357117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [16], Batch [382/938], Loss: 0.5297911763191223\n",
      "Train: Epoch [16], Batch [383/938], Loss: 0.46863794326782227\n",
      "Train: Epoch [16], Batch [384/938], Loss: 0.5378938317298889\n",
      "Train: Epoch [16], Batch [385/938], Loss: 0.5440847277641296\n",
      "Train: Epoch [16], Batch [386/938], Loss: 0.721301257610321\n",
      "Train: Epoch [16], Batch [387/938], Loss: 0.47665005922317505\n",
      "Train: Epoch [16], Batch [388/938], Loss: 0.4400336742401123\n",
      "Train: Epoch [16], Batch [389/938], Loss: 0.4773094952106476\n",
      "Train: Epoch [16], Batch [390/938], Loss: 0.4305894374847412\n",
      "Train: Epoch [16], Batch [391/938], Loss: 0.30192461609840393\n",
      "Train: Epoch [16], Batch [392/938], Loss: 0.4348059594631195\n",
      "Train: Epoch [16], Batch [393/938], Loss: 0.45331811904907227\n",
      "Train: Epoch [16], Batch [394/938], Loss: 0.46347543597221375\n",
      "Train: Epoch [16], Batch [395/938], Loss: 0.26725831627845764\n",
      "Train: Epoch [16], Batch [396/938], Loss: 0.46228504180908203\n",
      "Train: Epoch [16], Batch [397/938], Loss: 0.6077698469161987\n",
      "Train: Epoch [16], Batch [398/938], Loss: 0.46262019872665405\n",
      "Train: Epoch [16], Batch [399/938], Loss: 0.5903388261795044\n",
      "Train: Epoch [16], Batch [400/938], Loss: 0.33875739574432373\n",
      "Train: Epoch [16], Batch [401/938], Loss: 0.3928920030593872\n",
      "Train: Epoch [16], Batch [402/938], Loss: 0.4209609925746918\n",
      "Train: Epoch [16], Batch [403/938], Loss: 0.40960198640823364\n",
      "Train: Epoch [16], Batch [404/938], Loss: 0.47754591703414917\n",
      "Train: Epoch [16], Batch [405/938], Loss: 0.5058574080467224\n",
      "Train: Epoch [16], Batch [406/938], Loss: 0.5560912489891052\n",
      "Train: Epoch [16], Batch [407/938], Loss: 0.5244613885879517\n",
      "Train: Epoch [16], Batch [408/938], Loss: 0.25779545307159424\n",
      "Train: Epoch [16], Batch [409/938], Loss: 0.40969938039779663\n",
      "Train: Epoch [16], Batch [410/938], Loss: 0.4073210060596466\n",
      "Train: Epoch [16], Batch [411/938], Loss: 0.3015158176422119\n",
      "Train: Epoch [16], Batch [412/938], Loss: 0.31662532687187195\n",
      "Train: Epoch [16], Batch [413/938], Loss: 0.628342866897583\n",
      "Train: Epoch [16], Batch [414/938], Loss: 0.4129050374031067\n",
      "Train: Epoch [16], Batch [415/938], Loss: 0.38187071681022644\n",
      "Train: Epoch [16], Batch [416/938], Loss: 0.4801732897758484\n",
      "Train: Epoch [16], Batch [417/938], Loss: 0.345592200756073\n",
      "Train: Epoch [16], Batch [418/938], Loss: 0.4960481822490692\n",
      "Train: Epoch [16], Batch [419/938], Loss: 0.4060942530632019\n",
      "Train: Epoch [16], Batch [420/938], Loss: 0.37386631965637207\n",
      "Train: Epoch [16], Batch [421/938], Loss: 0.4522189199924469\n",
      "Train: Epoch [16], Batch [422/938], Loss: 0.5671031475067139\n",
      "Train: Epoch [16], Batch [423/938], Loss: 0.31011712551116943\n",
      "Train: Epoch [16], Batch [424/938], Loss: 0.21027177572250366\n",
      "Train: Epoch [16], Batch [425/938], Loss: 0.3768113851547241\n",
      "Train: Epoch [16], Batch [426/938], Loss: 0.46224507689476013\n",
      "Train: Epoch [16], Batch [427/938], Loss: 0.4064881205558777\n",
      "Train: Epoch [16], Batch [428/938], Loss: 0.6642128825187683\n",
      "Train: Epoch [16], Batch [429/938], Loss: 0.36358100175857544\n",
      "Train: Epoch [16], Batch [430/938], Loss: 0.3497723937034607\n",
      "Train: Epoch [16], Batch [431/938], Loss: 0.4241144359111786\n",
      "Train: Epoch [16], Batch [432/938], Loss: 0.5432523488998413\n",
      "Train: Epoch [16], Batch [433/938], Loss: 0.5350922346115112\n",
      "Train: Epoch [16], Batch [434/938], Loss: 0.6318569183349609\n",
      "Train: Epoch [16], Batch [435/938], Loss: 0.3370232582092285\n",
      "Train: Epoch [16], Batch [436/938], Loss: 0.38364291191101074\n",
      "Train: Epoch [16], Batch [437/938], Loss: 0.59730464220047\n",
      "Train: Epoch [16], Batch [438/938], Loss: 0.30476516485214233\n",
      "Train: Epoch [16], Batch [439/938], Loss: 0.5464739799499512\n",
      "Train: Epoch [16], Batch [440/938], Loss: 0.4671746492385864\n",
      "Train: Epoch [16], Batch [441/938], Loss: 0.3164161145687103\n",
      "Train: Epoch [16], Batch [442/938], Loss: 0.3420425057411194\n",
      "Train: Epoch [16], Batch [443/938], Loss: 0.40273812413215637\n",
      "Train: Epoch [16], Batch [444/938], Loss: 0.5079095363616943\n",
      "Train: Epoch [16], Batch [445/938], Loss: 0.5582448244094849\n",
      "Train: Epoch [16], Batch [446/938], Loss: 0.40041986107826233\n",
      "Train: Epoch [16], Batch [447/938], Loss: 0.30777087807655334\n",
      "Train: Epoch [16], Batch [448/938], Loss: 0.3129397928714752\n",
      "Train: Epoch [16], Batch [449/938], Loss: 0.48111382126808167\n",
      "Train: Epoch [16], Batch [450/938], Loss: 0.3110349774360657\n",
      "Train: Epoch [16], Batch [451/938], Loss: 0.332414448261261\n",
      "Train: Epoch [16], Batch [452/938], Loss: 0.496108740568161\n",
      "Train: Epoch [16], Batch [453/938], Loss: 0.4197880029678345\n",
      "Train: Epoch [16], Batch [454/938], Loss: 0.38311177492141724\n",
      "Train: Epoch [16], Batch [455/938], Loss: 0.44251036643981934\n",
      "Train: Epoch [16], Batch [456/938], Loss: 0.6985499858856201\n",
      "Train: Epoch [16], Batch [457/938], Loss: 0.46133869886398315\n",
      "Train: Epoch [16], Batch [458/938], Loss: 0.3421728312969208\n",
      "Train: Epoch [16], Batch [459/938], Loss: 0.44780707359313965\n",
      "Train: Epoch [16], Batch [460/938], Loss: 0.42844918370246887\n",
      "Train: Epoch [16], Batch [461/938], Loss: 0.4911356568336487\n",
      "Train: Epoch [16], Batch [462/938], Loss: 0.5264496803283691\n",
      "Train: Epoch [16], Batch [463/938], Loss: 0.45052897930145264\n",
      "Train: Epoch [16], Batch [464/938], Loss: 0.3947376012802124\n",
      "Train: Epoch [16], Batch [465/938], Loss: 0.31766802072525024\n",
      "Train: Epoch [16], Batch [466/938], Loss: 0.5997986197471619\n",
      "Train: Epoch [16], Batch [467/938], Loss: 0.3432927131652832\n",
      "Train: Epoch [16], Batch [468/938], Loss: 0.2901291847229004\n",
      "Train: Epoch [16], Batch [469/938], Loss: 0.3462727665901184\n",
      "Train: Epoch [16], Batch [470/938], Loss: 0.7429612874984741\n",
      "Train: Epoch [16], Batch [471/938], Loss: 0.49898549914360046\n",
      "Train: Epoch [16], Batch [472/938], Loss: 0.48404788970947266\n",
      "Train: Epoch [16], Batch [473/938], Loss: 0.43483003973960876\n",
      "Train: Epoch [16], Batch [474/938], Loss: 0.22989952564239502\n",
      "Train: Epoch [16], Batch [475/938], Loss: 0.3886764645576477\n",
      "Train: Epoch [16], Batch [476/938], Loss: 0.547725260257721\n",
      "Train: Epoch [16], Batch [477/938], Loss: 0.30970197916030884\n",
      "Train: Epoch [16], Batch [478/938], Loss: 0.3845938742160797\n",
      "Train: Epoch [16], Batch [479/938], Loss: 0.3630393445491791\n",
      "Train: Epoch [16], Batch [480/938], Loss: 0.5086955428123474\n",
      "Train: Epoch [16], Batch [481/938], Loss: 0.5552091002464294\n",
      "Train: Epoch [16], Batch [482/938], Loss: 0.5579051971435547\n",
      "Train: Epoch [16], Batch [483/938], Loss: 0.3697720468044281\n",
      "Train: Epoch [16], Batch [484/938], Loss: 0.4714205265045166\n",
      "Train: Epoch [16], Batch [485/938], Loss: 0.5979969501495361\n",
      "Train: Epoch [16], Batch [486/938], Loss: 0.4855433702468872\n",
      "Train: Epoch [16], Batch [487/938], Loss: 0.540023148059845\n",
      "Train: Epoch [16], Batch [488/938], Loss: 0.4027726948261261\n",
      "Train: Epoch [16], Batch [489/938], Loss: 0.5980994701385498\n",
      "Train: Epoch [16], Batch [490/938], Loss: 0.5033868551254272\n",
      "Train: Epoch [16], Batch [491/938], Loss: 0.47768479585647583\n",
      "Train: Epoch [16], Batch [492/938], Loss: 0.2706514894962311\n",
      "Train: Epoch [16], Batch [493/938], Loss: 0.48717430233955383\n",
      "Train: Epoch [16], Batch [494/938], Loss: 0.47612300515174866\n",
      "Train: Epoch [16], Batch [495/938], Loss: 0.5087906122207642\n",
      "Train: Epoch [16], Batch [496/938], Loss: 0.5401327610015869\n",
      "Train: Epoch [16], Batch [497/938], Loss: 0.3631853759288788\n",
      "Train: Epoch [16], Batch [498/938], Loss: 0.5752298831939697\n",
      "Train: Epoch [16], Batch [499/938], Loss: 0.4649665653705597\n",
      "Train: Epoch [16], Batch [500/938], Loss: 0.49780160188674927\n",
      "Train: Epoch [16], Batch [501/938], Loss: 0.2991848886013031\n",
      "Train: Epoch [16], Batch [502/938], Loss: 0.5453928112983704\n",
      "Train: Epoch [16], Batch [503/938], Loss: 0.3107268810272217\n",
      "Train: Epoch [16], Batch [504/938], Loss: 0.32159262895584106\n",
      "Train: Epoch [16], Batch [505/938], Loss: 0.42732205986976624\n",
      "Train: Epoch [16], Batch [506/938], Loss: 0.5032857060432434\n",
      "Train: Epoch [16], Batch [507/938], Loss: 0.42515137791633606\n",
      "Train: Epoch [16], Batch [508/938], Loss: 0.5978007316589355\n",
      "Train: Epoch [16], Batch [509/938], Loss: 0.5907347202301025\n",
      "Train: Epoch [16], Batch [510/938], Loss: 0.7483760118484497\n",
      "Train: Epoch [16], Batch [511/938], Loss: 0.7162713408470154\n",
      "Train: Epoch [16], Batch [512/938], Loss: 0.4047994017601013\n",
      "Train: Epoch [16], Batch [513/938], Loss: 0.5355929732322693\n",
      "Train: Epoch [16], Batch [514/938], Loss: 0.5721400380134583\n",
      "Train: Epoch [16], Batch [515/938], Loss: 0.6991915702819824\n",
      "Train: Epoch [16], Batch [516/938], Loss: 0.8003150224685669\n",
      "Train: Epoch [16], Batch [517/938], Loss: 0.37240085005760193\n",
      "Train: Epoch [16], Batch [518/938], Loss: 0.45910120010375977\n",
      "Train: Epoch [16], Batch [519/938], Loss: 0.3080274164676666\n",
      "Train: Epoch [16], Batch [520/938], Loss: 0.26231592893600464\n",
      "Train: Epoch [16], Batch [521/938], Loss: 0.35759401321411133\n",
      "Train: Epoch [16], Batch [522/938], Loss: 0.36862409114837646\n",
      "Train: Epoch [16], Batch [523/938], Loss: 0.5341124534606934\n",
      "Train: Epoch [16], Batch [524/938], Loss: 0.4753468930721283\n",
      "Train: Epoch [16], Batch [525/938], Loss: 0.3815428614616394\n",
      "Train: Epoch [16], Batch [526/938], Loss: 0.6404999494552612\n",
      "Train: Epoch [16], Batch [527/938], Loss: 0.6105319261550903\n",
      "Train: Epoch [16], Batch [528/938], Loss: 0.4844939112663269\n",
      "Train: Epoch [16], Batch [529/938], Loss: 0.7042831778526306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [16], Batch [530/938], Loss: 0.418438583612442\n",
      "Train: Epoch [16], Batch [531/938], Loss: 0.37830373644828796\n",
      "Train: Epoch [16], Batch [532/938], Loss: 0.3947638273239136\n",
      "Train: Epoch [16], Batch [533/938], Loss: 0.5254325866699219\n",
      "Train: Epoch [16], Batch [534/938], Loss: 0.3449651002883911\n",
      "Train: Epoch [16], Batch [535/938], Loss: 0.5098205804824829\n",
      "Train: Epoch [16], Batch [536/938], Loss: 0.47576677799224854\n",
      "Train: Epoch [16], Batch [537/938], Loss: 0.3224579393863678\n",
      "Train: Epoch [16], Batch [538/938], Loss: 0.5928629636764526\n",
      "Train: Epoch [16], Batch [539/938], Loss: 0.5028994083404541\n",
      "Train: Epoch [16], Batch [540/938], Loss: 0.2958015501499176\n",
      "Train: Epoch [16], Batch [541/938], Loss: 0.43336811661720276\n",
      "Train: Epoch [16], Batch [542/938], Loss: 0.38376277685165405\n",
      "Train: Epoch [16], Batch [543/938], Loss: 0.38256752490997314\n",
      "Train: Epoch [16], Batch [544/938], Loss: 0.37169551849365234\n",
      "Train: Epoch [16], Batch [545/938], Loss: 0.24901878833770752\n",
      "Train: Epoch [16], Batch [546/938], Loss: 0.35182619094848633\n",
      "Train: Epoch [16], Batch [547/938], Loss: 0.4446038603782654\n",
      "Train: Epoch [16], Batch [548/938], Loss: 0.5534137487411499\n",
      "Train: Epoch [16], Batch [549/938], Loss: 0.49190449714660645\n",
      "Train: Epoch [16], Batch [550/938], Loss: 0.3650038242340088\n",
      "Train: Epoch [16], Batch [551/938], Loss: 0.5725231170654297\n",
      "Train: Epoch [16], Batch [552/938], Loss: 0.4516701400279999\n",
      "Train: Epoch [16], Batch [553/938], Loss: 0.5616041421890259\n",
      "Train: Epoch [16], Batch [554/938], Loss: 0.3439076542854309\n",
      "Train: Epoch [16], Batch [555/938], Loss: 0.37522655725479126\n",
      "Train: Epoch [16], Batch [556/938], Loss: 0.323125958442688\n",
      "Train: Epoch [16], Batch [557/938], Loss: 0.3036520481109619\n",
      "Train: Epoch [16], Batch [558/938], Loss: 0.5095326900482178\n",
      "Train: Epoch [16], Batch [559/938], Loss: 0.5299311280250549\n",
      "Train: Epoch [16], Batch [560/938], Loss: 0.48264333605766296\n",
      "Train: Epoch [16], Batch [561/938], Loss: 0.3963637948036194\n",
      "Train: Epoch [16], Batch [562/938], Loss: 0.26387733221054077\n",
      "Train: Epoch [16], Batch [563/938], Loss: 0.5297489762306213\n",
      "Train: Epoch [16], Batch [564/938], Loss: 0.33059442043304443\n",
      "Train: Epoch [16], Batch [565/938], Loss: 0.5309205055236816\n",
      "Train: Epoch [16], Batch [566/938], Loss: 0.3772314190864563\n",
      "Train: Epoch [16], Batch [567/938], Loss: 0.6463715434074402\n",
      "Train: Epoch [16], Batch [568/938], Loss: 0.2914283871650696\n",
      "Train: Epoch [16], Batch [569/938], Loss: 0.45383912324905396\n",
      "Train: Epoch [16], Batch [570/938], Loss: 0.4369877874851227\n",
      "Train: Epoch [16], Batch [571/938], Loss: 0.29038408398628235\n",
      "Train: Epoch [16], Batch [572/938], Loss: 0.297851026058197\n",
      "Train: Epoch [16], Batch [573/938], Loss: 0.3536612391471863\n",
      "Train: Epoch [16], Batch [574/938], Loss: 0.43141883611679077\n",
      "Train: Epoch [16], Batch [575/938], Loss: 0.38818466663360596\n",
      "Train: Epoch [16], Batch [576/938], Loss: 0.49841931462287903\n",
      "Train: Epoch [16], Batch [577/938], Loss: 0.41002318263053894\n",
      "Train: Epoch [16], Batch [578/938], Loss: 0.374701589345932\n",
      "Train: Epoch [16], Batch [579/938], Loss: 0.5609630942344666\n",
      "Train: Epoch [16], Batch [580/938], Loss: 0.3754519820213318\n",
      "Train: Epoch [16], Batch [581/938], Loss: 0.5250908136367798\n",
      "Train: Epoch [16], Batch [582/938], Loss: 0.39962393045425415\n",
      "Train: Epoch [16], Batch [583/938], Loss: 0.46088773012161255\n",
      "Train: Epoch [16], Batch [584/938], Loss: 0.36893585324287415\n",
      "Train: Epoch [16], Batch [585/938], Loss: 0.28946608304977417\n",
      "Train: Epoch [16], Batch [586/938], Loss: 0.4363531470298767\n",
      "Train: Epoch [16], Batch [587/938], Loss: 0.48837053775787354\n",
      "Train: Epoch [16], Batch [588/938], Loss: 0.37518009543418884\n",
      "Train: Epoch [16], Batch [589/938], Loss: 0.22501367330551147\n",
      "Train: Epoch [16], Batch [590/938], Loss: 0.38093483448028564\n",
      "Train: Epoch [16], Batch [591/938], Loss: 0.4385669231414795\n",
      "Train: Epoch [16], Batch [592/938], Loss: 0.2893635034561157\n",
      "Train: Epoch [16], Batch [593/938], Loss: 0.31464675068855286\n",
      "Train: Epoch [16], Batch [594/938], Loss: 0.3263436555862427\n",
      "Train: Epoch [16], Batch [595/938], Loss: 0.448649525642395\n",
      "Train: Epoch [16], Batch [596/938], Loss: 0.3493027091026306\n",
      "Train: Epoch [16], Batch [597/938], Loss: 0.5373150706291199\n",
      "Train: Epoch [16], Batch [598/938], Loss: 0.37444984912872314\n",
      "Train: Epoch [16], Batch [599/938], Loss: 0.5948188900947571\n",
      "Train: Epoch [16], Batch [600/938], Loss: 0.27195173501968384\n",
      "Train: Epoch [16], Batch [601/938], Loss: 0.4584471583366394\n",
      "Train: Epoch [16], Batch [602/938], Loss: 0.4013630747795105\n",
      "Train: Epoch [16], Batch [603/938], Loss: 0.4379023313522339\n",
      "Train: Epoch [16], Batch [604/938], Loss: 0.3564450442790985\n",
      "Train: Epoch [16], Batch [605/938], Loss: 0.5252742171287537\n",
      "Train: Epoch [16], Batch [606/938], Loss: 0.3236733078956604\n",
      "Train: Epoch [16], Batch [607/938], Loss: 0.3915075957775116\n",
      "Train: Epoch [16], Batch [608/938], Loss: 0.5395472049713135\n",
      "Train: Epoch [16], Batch [609/938], Loss: 0.5158003568649292\n",
      "Train: Epoch [16], Batch [610/938], Loss: 0.44373661279678345\n",
      "Train: Epoch [16], Batch [611/938], Loss: 0.5250887870788574\n",
      "Train: Epoch [16], Batch [612/938], Loss: 0.665641725063324\n",
      "Train: Epoch [16], Batch [613/938], Loss: 0.48085713386535645\n",
      "Train: Epoch [16], Batch [614/938], Loss: 0.5004012584686279\n",
      "Train: Epoch [16], Batch [615/938], Loss: 0.26989278197288513\n",
      "Train: Epoch [16], Batch [616/938], Loss: 0.5064806342124939\n",
      "Train: Epoch [16], Batch [617/938], Loss: 0.39124295115470886\n",
      "Train: Epoch [16], Batch [618/938], Loss: 0.3330373466014862\n",
      "Train: Epoch [16], Batch [619/938], Loss: 0.5065652132034302\n",
      "Train: Epoch [16], Batch [620/938], Loss: 0.5297143459320068\n",
      "Train: Epoch [16], Batch [621/938], Loss: 0.2679063677787781\n",
      "Train: Epoch [16], Batch [622/938], Loss: 0.4334356486797333\n",
      "Train: Epoch [16], Batch [623/938], Loss: 0.6778030395507812\n",
      "Train: Epoch [16], Batch [624/938], Loss: 0.39639967679977417\n",
      "Train: Epoch [16], Batch [625/938], Loss: 0.563002347946167\n",
      "Train: Epoch [16], Batch [626/938], Loss: 0.5606526732444763\n",
      "Train: Epoch [16], Batch [627/938], Loss: 0.45118099451065063\n",
      "Train: Epoch [16], Batch [628/938], Loss: 0.42008402943611145\n",
      "Train: Epoch [16], Batch [629/938], Loss: 0.5393431186676025\n",
      "Train: Epoch [16], Batch [630/938], Loss: 0.3869062066078186\n",
      "Train: Epoch [16], Batch [631/938], Loss: 0.39387398958206177\n",
      "Train: Epoch [16], Batch [632/938], Loss: 0.5666794776916504\n",
      "Train: Epoch [16], Batch [633/938], Loss: 0.7824040055274963\n",
      "Train: Epoch [16], Batch [634/938], Loss: 0.3787493109703064\n",
      "Train: Epoch [16], Batch [635/938], Loss: 0.6137696504592896\n",
      "Train: Epoch [16], Batch [636/938], Loss: 0.4414289593696594\n",
      "Train: Epoch [16], Batch [637/938], Loss: 0.24639716744422913\n",
      "Train: Epoch [16], Batch [638/938], Loss: 0.370892733335495\n",
      "Train: Epoch [16], Batch [639/938], Loss: 0.41146183013916016\n",
      "Train: Epoch [16], Batch [640/938], Loss: 0.42741239070892334\n",
      "Train: Epoch [16], Batch [641/938], Loss: 0.4062228798866272\n",
      "Train: Epoch [16], Batch [642/938], Loss: 0.5023990273475647\n",
      "Train: Epoch [16], Batch [643/938], Loss: 0.35906529426574707\n",
      "Train: Epoch [16], Batch [644/938], Loss: 0.47650933265686035\n",
      "Train: Epoch [16], Batch [645/938], Loss: 0.5735520124435425\n",
      "Train: Epoch [16], Batch [646/938], Loss: 0.7657529711723328\n",
      "Train: Epoch [16], Batch [647/938], Loss: 0.33127182722091675\n",
      "Train: Epoch [16], Batch [648/938], Loss: 0.32451969385147095\n",
      "Train: Epoch [16], Batch [649/938], Loss: 0.37053465843200684\n",
      "Train: Epoch [16], Batch [650/938], Loss: 0.3361794352531433\n",
      "Train: Epoch [16], Batch [651/938], Loss: 0.643075704574585\n",
      "Train: Epoch [16], Batch [652/938], Loss: 0.44476717710494995\n",
      "Train: Epoch [16], Batch [653/938], Loss: 0.42346954345703125\n",
      "Train: Epoch [16], Batch [654/938], Loss: 0.3062722682952881\n",
      "Train: Epoch [16], Batch [655/938], Loss: 0.4821872413158417\n",
      "Train: Epoch [16], Batch [656/938], Loss: 0.34539350867271423\n",
      "Train: Epoch [16], Batch [657/938], Loss: 0.4345896542072296\n",
      "Train: Epoch [16], Batch [658/938], Loss: 0.33655601739883423\n",
      "Train: Epoch [16], Batch [659/938], Loss: 0.38829171657562256\n",
      "Train: Epoch [16], Batch [660/938], Loss: 0.5306145548820496\n",
      "Train: Epoch [16], Batch [661/938], Loss: 0.2542135715484619\n",
      "Train: Epoch [16], Batch [662/938], Loss: 0.415111780166626\n",
      "Train: Epoch [16], Batch [663/938], Loss: 0.5130638480186462\n",
      "Train: Epoch [16], Batch [664/938], Loss: 0.5039821267127991\n",
      "Train: Epoch [16], Batch [665/938], Loss: 0.43234893679618835\n",
      "Train: Epoch [16], Batch [666/938], Loss: 0.3863658905029297\n",
      "Train: Epoch [16], Batch [667/938], Loss: 0.3493203818798065\n",
      "Train: Epoch [16], Batch [668/938], Loss: 0.33199548721313477\n",
      "Train: Epoch [16], Batch [669/938], Loss: 0.4245862364768982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [16], Batch [670/938], Loss: 0.4081284999847412\n",
      "Train: Epoch [16], Batch [671/938], Loss: 0.38062041997909546\n",
      "Train: Epoch [16], Batch [672/938], Loss: 0.5796370506286621\n",
      "Train: Epoch [16], Batch [673/938], Loss: 0.2938583493232727\n",
      "Train: Epoch [16], Batch [674/938], Loss: 0.5647571086883545\n",
      "Train: Epoch [16], Batch [675/938], Loss: 0.38995617628097534\n",
      "Train: Epoch [16], Batch [676/938], Loss: 0.42841947078704834\n",
      "Train: Epoch [16], Batch [677/938], Loss: 0.509049117565155\n",
      "Train: Epoch [16], Batch [678/938], Loss: 0.4890905022621155\n",
      "Train: Epoch [16], Batch [679/938], Loss: 0.4042821228504181\n",
      "Train: Epoch [16], Batch [680/938], Loss: 0.4254322946071625\n",
      "Train: Epoch [16], Batch [681/938], Loss: 0.405020147562027\n",
      "Train: Epoch [16], Batch [682/938], Loss: 0.39538833498954773\n",
      "Train: Epoch [16], Batch [683/938], Loss: 0.5263160467147827\n",
      "Train: Epoch [16], Batch [684/938], Loss: 0.4777719974517822\n",
      "Train: Epoch [16], Batch [685/938], Loss: 0.6334642767906189\n",
      "Train: Epoch [16], Batch [686/938], Loss: 0.5181785821914673\n",
      "Train: Epoch [16], Batch [687/938], Loss: 0.409361332654953\n",
      "Train: Epoch [16], Batch [688/938], Loss: 0.3176264762878418\n",
      "Train: Epoch [16], Batch [689/938], Loss: 0.3500155210494995\n",
      "Train: Epoch [16], Batch [690/938], Loss: 0.4414823651313782\n",
      "Train: Epoch [16], Batch [691/938], Loss: 0.45283952355384827\n",
      "Train: Epoch [16], Batch [692/938], Loss: 0.3867074251174927\n",
      "Train: Epoch [16], Batch [693/938], Loss: 0.4185183048248291\n",
      "Train: Epoch [16], Batch [694/938], Loss: 0.26411160826683044\n",
      "Train: Epoch [16], Batch [695/938], Loss: 0.44559746980667114\n",
      "Train: Epoch [16], Batch [696/938], Loss: 0.40686875581741333\n",
      "Train: Epoch [16], Batch [697/938], Loss: 0.6950029134750366\n",
      "Train: Epoch [16], Batch [698/938], Loss: 0.3916836082935333\n",
      "Train: Epoch [16], Batch [699/938], Loss: 0.5250939130783081\n",
      "Train: Epoch [16], Batch [700/938], Loss: 0.2583811581134796\n",
      "Train: Epoch [16], Batch [701/938], Loss: 0.5609509944915771\n",
      "Train: Epoch [16], Batch [702/938], Loss: 0.2795710265636444\n",
      "Train: Epoch [16], Batch [703/938], Loss: 0.42573031783103943\n",
      "Train: Epoch [16], Batch [704/938], Loss: 0.5052284598350525\n",
      "Train: Epoch [16], Batch [705/938], Loss: 0.48621079325675964\n",
      "Train: Epoch [16], Batch [706/938], Loss: 0.31634530425071716\n",
      "Train: Epoch [16], Batch [707/938], Loss: 0.5315233469009399\n",
      "Train: Epoch [16], Batch [708/938], Loss: 0.49852004647254944\n",
      "Train: Epoch [16], Batch [709/938], Loss: 0.3868565857410431\n",
      "Train: Epoch [16], Batch [710/938], Loss: 0.3850938379764557\n",
      "Train: Epoch [16], Batch [711/938], Loss: 0.4144936203956604\n",
      "Train: Epoch [16], Batch [712/938], Loss: 0.2912358045578003\n",
      "Train: Epoch [16], Batch [713/938], Loss: 0.6945680379867554\n",
      "Train: Epoch [16], Batch [714/938], Loss: 0.2986920475959778\n",
      "Train: Epoch [16], Batch [715/938], Loss: 0.4514952301979065\n",
      "Train: Epoch [16], Batch [716/938], Loss: 0.5107705593109131\n",
      "Train: Epoch [16], Batch [717/938], Loss: 0.5267184972763062\n",
      "Train: Epoch [16], Batch [718/938], Loss: 0.5063163042068481\n",
      "Train: Epoch [16], Batch [719/938], Loss: 0.2642650604248047\n",
      "Train: Epoch [16], Batch [720/938], Loss: 0.4499741196632385\n",
      "Train: Epoch [16], Batch [721/938], Loss: 0.4614621698856354\n",
      "Train: Epoch [16], Batch [722/938], Loss: 0.4015008211135864\n",
      "Train: Epoch [16], Batch [723/938], Loss: 0.4426274001598358\n",
      "Train: Epoch [16], Batch [724/938], Loss: 0.5635117888450623\n",
      "Train: Epoch [16], Batch [725/938], Loss: 0.5375804305076599\n",
      "Train: Epoch [16], Batch [726/938], Loss: 0.5937963128089905\n",
      "Train: Epoch [16], Batch [727/938], Loss: 0.5476561784744263\n",
      "Train: Epoch [16], Batch [728/938], Loss: 0.4868863821029663\n",
      "Train: Epoch [16], Batch [729/938], Loss: 0.470352441072464\n",
      "Train: Epoch [16], Batch [730/938], Loss: 0.4077111482620239\n",
      "Train: Epoch [16], Batch [731/938], Loss: 0.577691912651062\n",
      "Train: Epoch [16], Batch [732/938], Loss: 0.36155468225479126\n",
      "Train: Epoch [16], Batch [733/938], Loss: 0.4862040877342224\n",
      "Train: Epoch [16], Batch [734/938], Loss: 0.3947649300098419\n",
      "Train: Epoch [16], Batch [735/938], Loss: 0.6716856360435486\n",
      "Train: Epoch [16], Batch [736/938], Loss: 0.4501662850379944\n",
      "Train: Epoch [16], Batch [737/938], Loss: 0.4837613105773926\n",
      "Train: Epoch [16], Batch [738/938], Loss: 0.5079448223114014\n",
      "Train: Epoch [16], Batch [739/938], Loss: 0.5529632568359375\n",
      "Train: Epoch [16], Batch [740/938], Loss: 0.35988134145736694\n",
      "Train: Epoch [16], Batch [741/938], Loss: 0.3955325484275818\n",
      "Train: Epoch [16], Batch [742/938], Loss: 0.33368468284606934\n",
      "Train: Epoch [16], Batch [743/938], Loss: 0.3522287905216217\n",
      "Train: Epoch [16], Batch [744/938], Loss: 0.4404314160346985\n",
      "Train: Epoch [16], Batch [745/938], Loss: 0.4435616731643677\n",
      "Train: Epoch [16], Batch [746/938], Loss: 0.5157030820846558\n",
      "Train: Epoch [16], Batch [747/938], Loss: 0.6602009534835815\n",
      "Train: Epoch [16], Batch [748/938], Loss: 0.40203413367271423\n",
      "Train: Epoch [16], Batch [749/938], Loss: 0.6030237674713135\n",
      "Train: Epoch [16], Batch [750/938], Loss: 0.28182554244995117\n",
      "Train: Epoch [16], Batch [751/938], Loss: 0.4672231674194336\n",
      "Train: Epoch [16], Batch [752/938], Loss: 0.6170097589492798\n",
      "Train: Epoch [16], Batch [753/938], Loss: 0.32515275478363037\n",
      "Train: Epoch [16], Batch [754/938], Loss: 0.3851865828037262\n",
      "Train: Epoch [16], Batch [755/938], Loss: 0.2896513044834137\n",
      "Train: Epoch [16], Batch [756/938], Loss: 0.3747672438621521\n",
      "Train: Epoch [16], Batch [757/938], Loss: 0.478841632604599\n",
      "Train: Epoch [16], Batch [758/938], Loss: 0.4753251075744629\n",
      "Train: Epoch [16], Batch [759/938], Loss: 0.3500525653362274\n",
      "Train: Epoch [16], Batch [760/938], Loss: 0.41668620705604553\n",
      "Train: Epoch [16], Batch [761/938], Loss: 0.34924423694610596\n",
      "Train: Epoch [16], Batch [762/938], Loss: 0.5631672739982605\n",
      "Train: Epoch [16], Batch [763/938], Loss: 0.4765809178352356\n",
      "Train: Epoch [16], Batch [764/938], Loss: 0.4007602334022522\n",
      "Train: Epoch [16], Batch [765/938], Loss: 0.5247700214385986\n",
      "Train: Epoch [16], Batch [766/938], Loss: 0.4712238907814026\n",
      "Train: Epoch [16], Batch [767/938], Loss: 0.35754549503326416\n",
      "Train: Epoch [16], Batch [768/938], Loss: 0.5175265669822693\n",
      "Train: Epoch [16], Batch [769/938], Loss: 0.5661046504974365\n",
      "Train: Epoch [16], Batch [770/938], Loss: 0.38317441940307617\n",
      "Train: Epoch [16], Batch [771/938], Loss: 0.3955334722995758\n",
      "Train: Epoch [16], Batch [772/938], Loss: 0.5105056166648865\n",
      "Train: Epoch [16], Batch [773/938], Loss: 0.4718897044658661\n",
      "Train: Epoch [16], Batch [774/938], Loss: 0.4072071313858032\n",
      "Train: Epoch [16], Batch [775/938], Loss: 0.32162484526634216\n",
      "Train: Epoch [16], Batch [776/938], Loss: 0.5522802472114563\n",
      "Train: Epoch [16], Batch [777/938], Loss: 0.40497854351997375\n",
      "Train: Epoch [16], Batch [778/938], Loss: 0.2614595890045166\n",
      "Train: Epoch [16], Batch [779/938], Loss: 0.4460497796535492\n",
      "Train: Epoch [16], Batch [780/938], Loss: 0.40352770686149597\n",
      "Train: Epoch [16], Batch [781/938], Loss: 0.40466028451919556\n",
      "Train: Epoch [16], Batch [782/938], Loss: 0.4991947114467621\n",
      "Train: Epoch [16], Batch [783/938], Loss: 0.523351788520813\n",
      "Train: Epoch [16], Batch [784/938], Loss: 0.6165362000465393\n",
      "Train: Epoch [16], Batch [785/938], Loss: 0.32559698820114136\n",
      "Train: Epoch [16], Batch [786/938], Loss: 0.4849948585033417\n",
      "Train: Epoch [16], Batch [787/938], Loss: 0.5799059867858887\n",
      "Train: Epoch [16], Batch [788/938], Loss: 0.3983818292617798\n",
      "Train: Epoch [16], Batch [789/938], Loss: 0.7035053372383118\n",
      "Train: Epoch [16], Batch [790/938], Loss: 0.35625511407852173\n",
      "Train: Epoch [16], Batch [791/938], Loss: 0.5571240186691284\n",
      "Train: Epoch [16], Batch [792/938], Loss: 0.3104877471923828\n",
      "Train: Epoch [16], Batch [793/938], Loss: 0.3188903331756592\n",
      "Train: Epoch [16], Batch [794/938], Loss: 0.47752848267555237\n",
      "Train: Epoch [16], Batch [795/938], Loss: 0.5584297776222229\n",
      "Train: Epoch [16], Batch [796/938], Loss: 0.4502425193786621\n",
      "Train: Epoch [16], Batch [797/938], Loss: 0.3708595037460327\n",
      "Train: Epoch [16], Batch [798/938], Loss: 0.34990638494491577\n",
      "Train: Epoch [16], Batch [799/938], Loss: 0.37613773345947266\n",
      "Train: Epoch [16], Batch [800/938], Loss: 0.5774695873260498\n",
      "Train: Epoch [16], Batch [801/938], Loss: 0.5671497583389282\n",
      "Train: Epoch [16], Batch [802/938], Loss: 0.5142920613288879\n",
      "Train: Epoch [16], Batch [803/938], Loss: 0.44017285108566284\n",
      "Train: Epoch [16], Batch [804/938], Loss: 0.3180457353591919\n",
      "Train: Epoch [16], Batch [805/938], Loss: 0.5294373035430908\n",
      "Train: Epoch [16], Batch [806/938], Loss: 0.46367496252059937\n",
      "Train: Epoch [16], Batch [807/938], Loss: 0.4425744414329529\n",
      "Train: Epoch [16], Batch [808/938], Loss: 0.42541685700416565\n",
      "Train: Epoch [16], Batch [809/938], Loss: 0.5531764626502991\n",
      "Train: Epoch [16], Batch [810/938], Loss: 0.49391841888427734\n",
      "Train: Epoch [16], Batch [811/938], Loss: 0.22649064660072327\n",
      "Train: Epoch [16], Batch [812/938], Loss: 0.5342243313789368\n",
      "Train: Epoch [16], Batch [813/938], Loss: 0.8533979058265686\n",
      "Train: Epoch [16], Batch [814/938], Loss: 0.3824414610862732\n",
      "Train: Epoch [16], Batch [815/938], Loss: 0.3623977601528168\n",
      "Train: Epoch [16], Batch [816/938], Loss: 0.3806449770927429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [16], Batch [817/938], Loss: 0.39118492603302\n",
      "Train: Epoch [16], Batch [818/938], Loss: 0.6031575202941895\n",
      "Train: Epoch [16], Batch [819/938], Loss: 0.4181740880012512\n",
      "Train: Epoch [16], Batch [820/938], Loss: 0.45662832260131836\n",
      "Train: Epoch [16], Batch [821/938], Loss: 0.37066030502319336\n",
      "Train: Epoch [16], Batch [822/938], Loss: 0.39145588874816895\n",
      "Train: Epoch [16], Batch [823/938], Loss: 0.5022569894790649\n",
      "Train: Epoch [16], Batch [824/938], Loss: 0.46824848651885986\n",
      "Train: Epoch [16], Batch [825/938], Loss: 0.34833088517189026\n",
      "Train: Epoch [16], Batch [826/938], Loss: 0.49127164483070374\n",
      "Train: Epoch [16], Batch [827/938], Loss: 0.5775772929191589\n",
      "Train: Epoch [16], Batch [828/938], Loss: 0.4109746813774109\n",
      "Train: Epoch [16], Batch [829/938], Loss: 0.5138934850692749\n",
      "Train: Epoch [16], Batch [830/938], Loss: 0.3391076624393463\n",
      "Train: Epoch [16], Batch [831/938], Loss: 0.31659531593322754\n",
      "Train: Epoch [16], Batch [832/938], Loss: 0.35914477705955505\n",
      "Train: Epoch [16], Batch [833/938], Loss: 0.4045218229293823\n",
      "Train: Epoch [16], Batch [834/938], Loss: 0.5264015197753906\n",
      "Train: Epoch [16], Batch [835/938], Loss: 0.31997114419937134\n",
      "Train: Epoch [16], Batch [836/938], Loss: 0.56235671043396\n",
      "Train: Epoch [16], Batch [837/938], Loss: 0.6836621761322021\n",
      "Train: Epoch [16], Batch [838/938], Loss: 0.28902894258499146\n",
      "Train: Epoch [16], Batch [839/938], Loss: 0.33416587114334106\n",
      "Train: Epoch [16], Batch [840/938], Loss: 0.5546054840087891\n",
      "Train: Epoch [16], Batch [841/938], Loss: 0.4055008590221405\n",
      "Train: Epoch [16], Batch [842/938], Loss: 0.4795820415019989\n",
      "Train: Epoch [16], Batch [843/938], Loss: 0.44374412298202515\n",
      "Train: Epoch [16], Batch [844/938], Loss: 0.5003203749656677\n",
      "Train: Epoch [16], Batch [845/938], Loss: 0.48467159271240234\n",
      "Train: Epoch [16], Batch [846/938], Loss: 0.5583427548408508\n",
      "Train: Epoch [16], Batch [847/938], Loss: 0.534031867980957\n",
      "Train: Epoch [16], Batch [848/938], Loss: 0.4897952675819397\n",
      "Train: Epoch [16], Batch [849/938], Loss: 0.4996231198310852\n",
      "Train: Epoch [16], Batch [850/938], Loss: 0.41903796792030334\n",
      "Train: Epoch [16], Batch [851/938], Loss: 0.6114656925201416\n",
      "Train: Epoch [16], Batch [852/938], Loss: 0.42237651348114014\n",
      "Train: Epoch [16], Batch [853/938], Loss: 0.4900315999984741\n",
      "Train: Epoch [16], Batch [854/938], Loss: 0.27791348099708557\n",
      "Train: Epoch [16], Batch [855/938], Loss: 0.39159584045410156\n",
      "Train: Epoch [16], Batch [856/938], Loss: 0.5827691555023193\n",
      "Train: Epoch [16], Batch [857/938], Loss: 0.534436047077179\n",
      "Train: Epoch [16], Batch [858/938], Loss: 0.6201342344284058\n",
      "Train: Epoch [16], Batch [859/938], Loss: 0.5211726427078247\n",
      "Train: Epoch [16], Batch [860/938], Loss: 0.4790363311767578\n",
      "Train: Epoch [16], Batch [861/938], Loss: 0.447986900806427\n",
      "Train: Epoch [16], Batch [862/938], Loss: 0.3973234295845032\n",
      "Train: Epoch [16], Batch [863/938], Loss: 0.39250773191452026\n",
      "Train: Epoch [16], Batch [864/938], Loss: 0.5164951086044312\n",
      "Train: Epoch [16], Batch [865/938], Loss: 0.5594593286514282\n",
      "Train: Epoch [16], Batch [866/938], Loss: 0.4691546857357025\n",
      "Train: Epoch [16], Batch [867/938], Loss: 0.44161558151245117\n",
      "Train: Epoch [16], Batch [868/938], Loss: 0.5954452753067017\n",
      "Train: Epoch [16], Batch [869/938], Loss: 0.4598066210746765\n",
      "Train: Epoch [16], Batch [870/938], Loss: 0.36665838956832886\n",
      "Train: Epoch [16], Batch [871/938], Loss: 0.4893302619457245\n",
      "Train: Epoch [16], Batch [872/938], Loss: 0.4884043335914612\n",
      "Train: Epoch [16], Batch [873/938], Loss: 0.34432530403137207\n",
      "Train: Epoch [16], Batch [874/938], Loss: 0.3514995574951172\n",
      "Train: Epoch [16], Batch [875/938], Loss: 0.560121476650238\n",
      "Train: Epoch [16], Batch [876/938], Loss: 0.31233102083206177\n",
      "Train: Epoch [16], Batch [877/938], Loss: 0.4789050221443176\n",
      "Train: Epoch [16], Batch [878/938], Loss: 0.40376341342926025\n",
      "Train: Epoch [16], Batch [879/938], Loss: 0.3592659831047058\n",
      "Train: Epoch [16], Batch [880/938], Loss: 0.504945695400238\n",
      "Train: Epoch [16], Batch [881/938], Loss: 0.6073294878005981\n",
      "Train: Epoch [16], Batch [882/938], Loss: 0.5528497695922852\n",
      "Train: Epoch [16], Batch [883/938], Loss: 0.550301194190979\n",
      "Train: Epoch [16], Batch [884/938], Loss: 0.4939202666282654\n",
      "Train: Epoch [16], Batch [885/938], Loss: 0.5249660611152649\n",
      "Train: Epoch [16], Batch [886/938], Loss: 0.37958720326423645\n",
      "Train: Epoch [16], Batch [887/938], Loss: 0.44690072536468506\n",
      "Train: Epoch [16], Batch [888/938], Loss: 0.3506624102592468\n",
      "Train: Epoch [16], Batch [889/938], Loss: 0.6134952902793884\n",
      "Train: Epoch [16], Batch [890/938], Loss: 0.37330949306488037\n",
      "Train: Epoch [16], Batch [891/938], Loss: 0.5402454137802124\n",
      "Train: Epoch [16], Batch [892/938], Loss: 0.4719458818435669\n",
      "Train: Epoch [16], Batch [893/938], Loss: 0.5356824398040771\n",
      "Train: Epoch [16], Batch [894/938], Loss: 0.4163737893104553\n",
      "Train: Epoch [16], Batch [895/938], Loss: 0.3475855886936188\n",
      "Train: Epoch [16], Batch [896/938], Loss: 0.4840882122516632\n",
      "Train: Epoch [16], Batch [897/938], Loss: 0.597386360168457\n",
      "Train: Epoch [16], Batch [898/938], Loss: 0.3359682857990265\n",
      "Train: Epoch [16], Batch [899/938], Loss: 0.5078908205032349\n",
      "Train: Epoch [16], Batch [900/938], Loss: 0.7144279479980469\n",
      "Train: Epoch [16], Batch [901/938], Loss: 0.37183380126953125\n",
      "Train: Epoch [16], Batch [902/938], Loss: 0.5040474534034729\n",
      "Train: Epoch [16], Batch [903/938], Loss: 0.35319042205810547\n",
      "Train: Epoch [16], Batch [904/938], Loss: 0.3586866855621338\n",
      "Train: Epoch [16], Batch [905/938], Loss: 0.5153717994689941\n",
      "Train: Epoch [16], Batch [906/938], Loss: 0.3484102189540863\n",
      "Train: Epoch [16], Batch [907/938], Loss: 0.7806276082992554\n",
      "Train: Epoch [16], Batch [908/938], Loss: 0.3550998866558075\n",
      "Train: Epoch [16], Batch [909/938], Loss: 0.3985966444015503\n",
      "Train: Epoch [16], Batch [910/938], Loss: 0.5557422637939453\n",
      "Train: Epoch [16], Batch [911/938], Loss: 0.23391234874725342\n",
      "Train: Epoch [16], Batch [912/938], Loss: 0.3420654535293579\n",
      "Train: Epoch [16], Batch [913/938], Loss: 0.6231082081794739\n",
      "Train: Epoch [16], Batch [914/938], Loss: 0.39842289686203003\n",
      "Train: Epoch [16], Batch [915/938], Loss: 0.29789847135543823\n",
      "Train: Epoch [16], Batch [916/938], Loss: 0.3215174674987793\n",
      "Train: Epoch [16], Batch [917/938], Loss: 0.349660187959671\n",
      "Train: Epoch [16], Batch [918/938], Loss: 0.4872404634952545\n",
      "Train: Epoch [16], Batch [919/938], Loss: 0.47912389039993286\n",
      "Train: Epoch [16], Batch [920/938], Loss: 0.5839166641235352\n",
      "Train: Epoch [16], Batch [921/938], Loss: 0.4840473234653473\n",
      "Train: Epoch [16], Batch [922/938], Loss: 0.3817959725856781\n",
      "Train: Epoch [16], Batch [923/938], Loss: 0.4743158519268036\n",
      "Train: Epoch [16], Batch [924/938], Loss: 0.31420189142227173\n",
      "Train: Epoch [16], Batch [925/938], Loss: 0.3896367847919464\n",
      "Train: Epoch [16], Batch [926/938], Loss: 0.593920111656189\n",
      "Train: Epoch [16], Batch [927/938], Loss: 0.7719118595123291\n",
      "Train: Epoch [16], Batch [928/938], Loss: 0.346113383769989\n",
      "Train: Epoch [16], Batch [929/938], Loss: 0.3826303482055664\n",
      "Train: Epoch [16], Batch [930/938], Loss: 0.4876146912574768\n",
      "Train: Epoch [16], Batch [931/938], Loss: 0.546526312828064\n",
      "Train: Epoch [16], Batch [932/938], Loss: 0.4610922932624817\n",
      "Train: Epoch [16], Batch [933/938], Loss: 0.29466521739959717\n",
      "Train: Epoch [16], Batch [934/938], Loss: 0.4795237183570862\n",
      "Train: Epoch [16], Batch [935/938], Loss: 0.4618065357208252\n",
      "Train: Epoch [16], Batch [936/938], Loss: 0.44781067967414856\n",
      "Train: Epoch [16], Batch [937/938], Loss: 0.3224576711654663\n",
      "Train: Epoch [16], Batch [938/938], Loss: 0.4220569133758545\n",
      "Accuracy of train set: 0.8428166666666667\n",
      "Validation: Epoch [16], Batch [1/938], Loss: 0.32743898034095764\n",
      "Validation: Epoch [16], Batch [2/938], Loss: 0.500957727432251\n",
      "Validation: Epoch [16], Batch [3/938], Loss: 0.43416303396224976\n",
      "Validation: Epoch [16], Batch [4/938], Loss: 0.5743669271469116\n",
      "Validation: Epoch [16], Batch [5/938], Loss: 0.3360188603401184\n",
      "Validation: Epoch [16], Batch [6/938], Loss: 0.4680236876010895\n",
      "Validation: Epoch [16], Batch [7/938], Loss: 0.5460953116416931\n",
      "Validation: Epoch [16], Batch [8/938], Loss: 0.48232176899909973\n",
      "Validation: Epoch [16], Batch [9/938], Loss: 0.5233261585235596\n",
      "Validation: Epoch [16], Batch [10/938], Loss: 0.4180777072906494\n",
      "Validation: Epoch [16], Batch [11/938], Loss: 0.49000734090805054\n",
      "Validation: Epoch [16], Batch [12/938], Loss: 0.6484454870223999\n",
      "Validation: Epoch [16], Batch [13/938], Loss: 0.40919721126556396\n",
      "Validation: Epoch [16], Batch [14/938], Loss: 0.5565481781959534\n",
      "Validation: Epoch [16], Batch [15/938], Loss: 0.44025933742523193\n",
      "Validation: Epoch [16], Batch [16/938], Loss: 0.38906753063201904\n",
      "Validation: Epoch [16], Batch [17/938], Loss: 0.5059818029403687\n",
      "Validation: Epoch [16], Batch [18/938], Loss: 0.3215910792350769\n",
      "Validation: Epoch [16], Batch [19/938], Loss: 0.4678003787994385\n",
      "Validation: Epoch [16], Batch [20/938], Loss: 0.29523205757141113\n",
      "Validation: Epoch [16], Batch [21/938], Loss: 0.4666686952114105\n",
      "Validation: Epoch [16], Batch [22/938], Loss: 0.45971834659576416\n",
      "Validation: Epoch [16], Batch [23/938], Loss: 0.42234280705451965\n",
      "Validation: Epoch [16], Batch [24/938], Loss: 0.6133748292922974\n",
      "Validation: Epoch [16], Batch [25/938], Loss: 0.24074038863182068\n",
      "Validation: Epoch [16], Batch [26/938], Loss: 0.4298740029335022\n",
      "Validation: Epoch [16], Batch [27/938], Loss: 0.463664174079895\n",
      "Validation: Epoch [16], Batch [28/938], Loss: 0.4464515447616577\n",
      "Validation: Epoch [16], Batch [29/938], Loss: 0.5064774751663208\n",
      "Validation: Epoch [16], Batch [30/938], Loss: 0.45872238278388977\n",
      "Validation: Epoch [16], Batch [31/938], Loss: 0.4249376654624939\n",
      "Validation: Epoch [16], Batch [32/938], Loss: 0.5723907351493835\n",
      "Validation: Epoch [16], Batch [33/938], Loss: 0.5395435094833374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [34/938], Loss: 0.32472482323646545\n",
      "Validation: Epoch [16], Batch [35/938], Loss: 0.5555844902992249\n",
      "Validation: Epoch [16], Batch [36/938], Loss: 0.5888760089874268\n",
      "Validation: Epoch [16], Batch [37/938], Loss: 0.31313782930374146\n",
      "Validation: Epoch [16], Batch [38/938], Loss: 0.7696502804756165\n",
      "Validation: Epoch [16], Batch [39/938], Loss: 0.4527450501918793\n",
      "Validation: Epoch [16], Batch [40/938], Loss: 0.6115595102310181\n",
      "Validation: Epoch [16], Batch [41/938], Loss: 0.26600176095962524\n",
      "Validation: Epoch [16], Batch [42/938], Loss: 0.5568934679031372\n",
      "Validation: Epoch [16], Batch [43/938], Loss: 0.47263216972351074\n",
      "Validation: Epoch [16], Batch [44/938], Loss: 0.6001468896865845\n",
      "Validation: Epoch [16], Batch [45/938], Loss: 0.4786439836025238\n",
      "Validation: Epoch [16], Batch [46/938], Loss: 0.3523806631565094\n",
      "Validation: Epoch [16], Batch [47/938], Loss: 0.7849804162979126\n",
      "Validation: Epoch [16], Batch [48/938], Loss: 0.573381781578064\n",
      "Validation: Epoch [16], Batch [49/938], Loss: 0.42396071553230286\n",
      "Validation: Epoch [16], Batch [50/938], Loss: 0.28765320777893066\n",
      "Validation: Epoch [16], Batch [51/938], Loss: 0.5682873725891113\n",
      "Validation: Epoch [16], Batch [52/938], Loss: 0.4994494616985321\n",
      "Validation: Epoch [16], Batch [53/938], Loss: 0.5293688774108887\n",
      "Validation: Epoch [16], Batch [54/938], Loss: 0.6256833076477051\n",
      "Validation: Epoch [16], Batch [55/938], Loss: 0.5859348773956299\n",
      "Validation: Epoch [16], Batch [56/938], Loss: 0.510744035243988\n",
      "Validation: Epoch [16], Batch [57/938], Loss: 0.5875783562660217\n",
      "Validation: Epoch [16], Batch [58/938], Loss: 0.27251434326171875\n",
      "Validation: Epoch [16], Batch [59/938], Loss: 0.35278239846229553\n",
      "Validation: Epoch [16], Batch [60/938], Loss: 0.6261821985244751\n",
      "Validation: Epoch [16], Batch [61/938], Loss: 0.4575903117656708\n",
      "Validation: Epoch [16], Batch [62/938], Loss: 0.5742710828781128\n",
      "Validation: Epoch [16], Batch [63/938], Loss: 0.45897752046585083\n",
      "Validation: Epoch [16], Batch [64/938], Loss: 0.41579556465148926\n",
      "Validation: Epoch [16], Batch [65/938], Loss: 0.5686419010162354\n",
      "Validation: Epoch [16], Batch [66/938], Loss: 0.48165780305862427\n",
      "Validation: Epoch [16], Batch [67/938], Loss: 0.3837794065475464\n",
      "Validation: Epoch [16], Batch [68/938], Loss: 0.3673210144042969\n",
      "Validation: Epoch [16], Batch [69/938], Loss: 0.34094762802124023\n",
      "Validation: Epoch [16], Batch [70/938], Loss: 0.3594903349876404\n",
      "Validation: Epoch [16], Batch [71/938], Loss: 0.5358870029449463\n",
      "Validation: Epoch [16], Batch [72/938], Loss: 0.5928143858909607\n",
      "Validation: Epoch [16], Batch [73/938], Loss: 0.40784937143325806\n",
      "Validation: Epoch [16], Batch [74/938], Loss: 0.3030399680137634\n",
      "Validation: Epoch [16], Batch [75/938], Loss: 0.5860061645507812\n",
      "Validation: Epoch [16], Batch [76/938], Loss: 0.43788278102874756\n",
      "Validation: Epoch [16], Batch [77/938], Loss: 0.4858340322971344\n",
      "Validation: Epoch [16], Batch [78/938], Loss: 0.43307584524154663\n",
      "Validation: Epoch [16], Batch [79/938], Loss: 0.507338285446167\n",
      "Validation: Epoch [16], Batch [80/938], Loss: 0.44341039657592773\n",
      "Validation: Epoch [16], Batch [81/938], Loss: 0.3732854127883911\n",
      "Validation: Epoch [16], Batch [82/938], Loss: 0.4246665835380554\n",
      "Validation: Epoch [16], Batch [83/938], Loss: 0.4144435524940491\n",
      "Validation: Epoch [16], Batch [84/938], Loss: 0.46277689933776855\n",
      "Validation: Epoch [16], Batch [85/938], Loss: 0.4263317584991455\n",
      "Validation: Epoch [16], Batch [86/938], Loss: 0.37712618708610535\n",
      "Validation: Epoch [16], Batch [87/938], Loss: 0.4571334421634674\n",
      "Validation: Epoch [16], Batch [88/938], Loss: 0.2426413744688034\n",
      "Validation: Epoch [16], Batch [89/938], Loss: 0.5490002632141113\n",
      "Validation: Epoch [16], Batch [90/938], Loss: 0.6199893951416016\n",
      "Validation: Epoch [16], Batch [91/938], Loss: 0.43034666776657104\n",
      "Validation: Epoch [16], Batch [92/938], Loss: 0.6002615690231323\n",
      "Validation: Epoch [16], Batch [93/938], Loss: 0.3437889814376831\n",
      "Validation: Epoch [16], Batch [94/938], Loss: 0.2248983085155487\n",
      "Validation: Epoch [16], Batch [95/938], Loss: 0.41716325283050537\n",
      "Validation: Epoch [16], Batch [96/938], Loss: 0.4965316653251648\n",
      "Validation: Epoch [16], Batch [97/938], Loss: 0.35171449184417725\n",
      "Validation: Epoch [16], Batch [98/938], Loss: 0.32361385226249695\n",
      "Validation: Epoch [16], Batch [99/938], Loss: 0.6550052762031555\n",
      "Validation: Epoch [16], Batch [100/938], Loss: 0.3428669571876526\n",
      "Validation: Epoch [16], Batch [101/938], Loss: 0.5082942247390747\n",
      "Validation: Epoch [16], Batch [102/938], Loss: 0.2912753224372864\n",
      "Validation: Epoch [16], Batch [103/938], Loss: 0.4418308138847351\n",
      "Validation: Epoch [16], Batch [104/938], Loss: 0.26864513754844666\n",
      "Validation: Epoch [16], Batch [105/938], Loss: 0.4208170771598816\n",
      "Validation: Epoch [16], Batch [106/938], Loss: 0.28814607858657837\n",
      "Validation: Epoch [16], Batch [107/938], Loss: 0.39031338691711426\n",
      "Validation: Epoch [16], Batch [108/938], Loss: 0.3299245536327362\n",
      "Validation: Epoch [16], Batch [109/938], Loss: 0.5237784385681152\n",
      "Validation: Epoch [16], Batch [110/938], Loss: 0.4079582095146179\n",
      "Validation: Epoch [16], Batch [111/938], Loss: 0.3958209753036499\n",
      "Validation: Epoch [16], Batch [112/938], Loss: 0.6070624589920044\n",
      "Validation: Epoch [16], Batch [113/938], Loss: 0.5779387950897217\n",
      "Validation: Epoch [16], Batch [114/938], Loss: 0.4434981048107147\n",
      "Validation: Epoch [16], Batch [115/938], Loss: 0.43674522638320923\n",
      "Validation: Epoch [16], Batch [116/938], Loss: 0.5145387053489685\n",
      "Validation: Epoch [16], Batch [117/938], Loss: 0.3988605737686157\n",
      "Validation: Epoch [16], Batch [118/938], Loss: 0.4520305097103119\n",
      "Validation: Epoch [16], Batch [119/938], Loss: 0.38481712341308594\n",
      "Validation: Epoch [16], Batch [120/938], Loss: 0.7809733748435974\n",
      "Validation: Epoch [16], Batch [121/938], Loss: 0.4096023440361023\n",
      "Validation: Epoch [16], Batch [122/938], Loss: 0.4486061930656433\n",
      "Validation: Epoch [16], Batch [123/938], Loss: 0.28578540682792664\n",
      "Validation: Epoch [16], Batch [124/938], Loss: 0.553223729133606\n",
      "Validation: Epoch [16], Batch [125/938], Loss: 0.37191492319107056\n",
      "Validation: Epoch [16], Batch [126/938], Loss: 0.4568842053413391\n",
      "Validation: Epoch [16], Batch [127/938], Loss: 0.4019562900066376\n",
      "Validation: Epoch [16], Batch [128/938], Loss: 0.45646676421165466\n",
      "Validation: Epoch [16], Batch [129/938], Loss: 0.3650396168231964\n",
      "Validation: Epoch [16], Batch [130/938], Loss: 0.3767184913158417\n",
      "Validation: Epoch [16], Batch [131/938], Loss: 0.6675605773925781\n",
      "Validation: Epoch [16], Batch [132/938], Loss: 0.4143347144126892\n",
      "Validation: Epoch [16], Batch [133/938], Loss: 0.3184235692024231\n",
      "Validation: Epoch [16], Batch [134/938], Loss: 0.2259867638349533\n",
      "Validation: Epoch [16], Batch [135/938], Loss: 0.47879141569137573\n",
      "Validation: Epoch [16], Batch [136/938], Loss: 0.46056661009788513\n",
      "Validation: Epoch [16], Batch [137/938], Loss: 0.4564622640609741\n",
      "Validation: Epoch [16], Batch [138/938], Loss: 0.4473697543144226\n",
      "Validation: Epoch [16], Batch [139/938], Loss: 0.24135816097259521\n",
      "Validation: Epoch [16], Batch [140/938], Loss: 0.38201019167900085\n",
      "Validation: Epoch [16], Batch [141/938], Loss: 0.43092265725135803\n",
      "Validation: Epoch [16], Batch [142/938], Loss: 0.38578277826309204\n",
      "Validation: Epoch [16], Batch [143/938], Loss: 0.41163110733032227\n",
      "Validation: Epoch [16], Batch [144/938], Loss: 0.5634230375289917\n",
      "Validation: Epoch [16], Batch [145/938], Loss: 0.5332450866699219\n",
      "Validation: Epoch [16], Batch [146/938], Loss: 0.5596474409103394\n",
      "Validation: Epoch [16], Batch [147/938], Loss: 0.35543644428253174\n",
      "Validation: Epoch [16], Batch [148/938], Loss: 0.35713353753089905\n",
      "Validation: Epoch [16], Batch [149/938], Loss: 0.2500467300415039\n",
      "Validation: Epoch [16], Batch [150/938], Loss: 0.46533477306365967\n",
      "Validation: Epoch [16], Batch [151/938], Loss: 0.6058573126792908\n",
      "Validation: Epoch [16], Batch [152/938], Loss: 0.42597079277038574\n",
      "Validation: Epoch [16], Batch [153/938], Loss: 0.27768638730049133\n",
      "Validation: Epoch [16], Batch [154/938], Loss: 0.7748495936393738\n",
      "Validation: Epoch [16], Batch [155/938], Loss: 0.2921448051929474\n",
      "Validation: Epoch [16], Batch [156/938], Loss: 0.40454334020614624\n",
      "Validation: Epoch [16], Batch [157/938], Loss: 0.26336073875427246\n",
      "Validation: Epoch [16], Batch [158/938], Loss: 0.3299093544483185\n",
      "Validation: Epoch [16], Batch [159/938], Loss: 0.47159236669540405\n",
      "Validation: Epoch [16], Batch [160/938], Loss: 0.35021311044692993\n",
      "Validation: Epoch [16], Batch [161/938], Loss: 0.49437370896339417\n",
      "Validation: Epoch [16], Batch [162/938], Loss: 0.4577917754650116\n",
      "Validation: Epoch [16], Batch [163/938], Loss: 0.381141722202301\n",
      "Validation: Epoch [16], Batch [164/938], Loss: 0.5242389440536499\n",
      "Validation: Epoch [16], Batch [165/938], Loss: 0.28478115797042847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [166/938], Loss: 0.4470577538013458\n",
      "Validation: Epoch [16], Batch [167/938], Loss: 0.40491557121276855\n",
      "Validation: Epoch [16], Batch [168/938], Loss: 0.6502004265785217\n",
      "Validation: Epoch [16], Batch [169/938], Loss: 0.5387376546859741\n",
      "Validation: Epoch [16], Batch [170/938], Loss: 0.4120183289051056\n",
      "Validation: Epoch [16], Batch [171/938], Loss: 0.41667819023132324\n",
      "Validation: Epoch [16], Batch [172/938], Loss: 0.44938111305236816\n",
      "Validation: Epoch [16], Batch [173/938], Loss: 0.486053466796875\n",
      "Validation: Epoch [16], Batch [174/938], Loss: 0.29471880197525024\n",
      "Validation: Epoch [16], Batch [175/938], Loss: 0.39784348011016846\n",
      "Validation: Epoch [16], Batch [176/938], Loss: 0.5842791795730591\n",
      "Validation: Epoch [16], Batch [177/938], Loss: 0.5944691896438599\n",
      "Validation: Epoch [16], Batch [178/938], Loss: 0.4507746696472168\n",
      "Validation: Epoch [16], Batch [179/938], Loss: 0.40345776081085205\n",
      "Validation: Epoch [16], Batch [180/938], Loss: 0.5732161998748779\n",
      "Validation: Epoch [16], Batch [181/938], Loss: 0.4838999807834625\n",
      "Validation: Epoch [16], Batch [182/938], Loss: 0.3290673494338989\n",
      "Validation: Epoch [16], Batch [183/938], Loss: 0.4697328209877014\n",
      "Validation: Epoch [16], Batch [184/938], Loss: 0.6592084169387817\n",
      "Validation: Epoch [16], Batch [185/938], Loss: 0.42785993218421936\n",
      "Validation: Epoch [16], Batch [186/938], Loss: 0.43822598457336426\n",
      "Validation: Epoch [16], Batch [187/938], Loss: 0.6625305414199829\n",
      "Validation: Epoch [16], Batch [188/938], Loss: 0.34436219930648804\n",
      "Validation: Epoch [16], Batch [189/938], Loss: 0.40567928552627563\n",
      "Validation: Epoch [16], Batch [190/938], Loss: 0.38693907856941223\n",
      "Validation: Epoch [16], Batch [191/938], Loss: 0.3908793330192566\n",
      "Validation: Epoch [16], Batch [192/938], Loss: 0.4695858359336853\n",
      "Validation: Epoch [16], Batch [193/938], Loss: 0.4092036485671997\n",
      "Validation: Epoch [16], Batch [194/938], Loss: 0.4745977520942688\n",
      "Validation: Epoch [16], Batch [195/938], Loss: 0.4601268470287323\n",
      "Validation: Epoch [16], Batch [196/938], Loss: 0.47487300634384155\n",
      "Validation: Epoch [16], Batch [197/938], Loss: 0.5481387376785278\n",
      "Validation: Epoch [16], Batch [198/938], Loss: 0.46626734733581543\n",
      "Validation: Epoch [16], Batch [199/938], Loss: 0.7606416940689087\n",
      "Validation: Epoch [16], Batch [200/938], Loss: 0.49792659282684326\n",
      "Validation: Epoch [16], Batch [201/938], Loss: 0.5129581093788147\n",
      "Validation: Epoch [16], Batch [202/938], Loss: 0.46768665313720703\n",
      "Validation: Epoch [16], Batch [203/938], Loss: 0.30820316076278687\n",
      "Validation: Epoch [16], Batch [204/938], Loss: 0.34842807054519653\n",
      "Validation: Epoch [16], Batch [205/938], Loss: 0.3155573606491089\n",
      "Validation: Epoch [16], Batch [206/938], Loss: 0.43420541286468506\n",
      "Validation: Epoch [16], Batch [207/938], Loss: 0.36800408363342285\n",
      "Validation: Epoch [16], Batch [208/938], Loss: 0.4615042507648468\n",
      "Validation: Epoch [16], Batch [209/938], Loss: 0.5105793476104736\n",
      "Validation: Epoch [16], Batch [210/938], Loss: 0.37843629717826843\n",
      "Validation: Epoch [16], Batch [211/938], Loss: 0.4793470799922943\n",
      "Validation: Epoch [16], Batch [212/938], Loss: 0.38194721937179565\n",
      "Validation: Epoch [16], Batch [213/938], Loss: 0.39843618869781494\n",
      "Validation: Epoch [16], Batch [214/938], Loss: 0.3955995738506317\n",
      "Validation: Epoch [16], Batch [215/938], Loss: 0.425460547208786\n",
      "Validation: Epoch [16], Batch [216/938], Loss: 0.36986207962036133\n",
      "Validation: Epoch [16], Batch [217/938], Loss: 0.3944582939147949\n",
      "Validation: Epoch [16], Batch [218/938], Loss: 0.5405042171478271\n",
      "Validation: Epoch [16], Batch [219/938], Loss: 0.42936959862709045\n",
      "Validation: Epoch [16], Batch [220/938], Loss: 0.538966178894043\n",
      "Validation: Epoch [16], Batch [221/938], Loss: 0.5634163618087769\n",
      "Validation: Epoch [16], Batch [222/938], Loss: 0.4493265450000763\n",
      "Validation: Epoch [16], Batch [223/938], Loss: 0.5257877111434937\n",
      "Validation: Epoch [16], Batch [224/938], Loss: 0.4214646518230438\n",
      "Validation: Epoch [16], Batch [225/938], Loss: 0.2577541768550873\n",
      "Validation: Epoch [16], Batch [226/938], Loss: 0.5056375861167908\n",
      "Validation: Epoch [16], Batch [227/938], Loss: 0.4706439673900604\n",
      "Validation: Epoch [16], Batch [228/938], Loss: 0.31156909465789795\n",
      "Validation: Epoch [16], Batch [229/938], Loss: 0.36307281255722046\n",
      "Validation: Epoch [16], Batch [230/938], Loss: 0.3732910752296448\n",
      "Validation: Epoch [16], Batch [231/938], Loss: 0.4846492409706116\n",
      "Validation: Epoch [16], Batch [232/938], Loss: 0.2551003396511078\n",
      "Validation: Epoch [16], Batch [233/938], Loss: 0.40828657150268555\n",
      "Validation: Epoch [16], Batch [234/938], Loss: 0.5291203260421753\n",
      "Validation: Epoch [16], Batch [235/938], Loss: 0.23459221422672272\n",
      "Validation: Epoch [16], Batch [236/938], Loss: 0.39039093255996704\n",
      "Validation: Epoch [16], Batch [237/938], Loss: 0.5567673444747925\n",
      "Validation: Epoch [16], Batch [238/938], Loss: 0.6067406535148621\n",
      "Validation: Epoch [16], Batch [239/938], Loss: 0.5485116839408875\n",
      "Validation: Epoch [16], Batch [240/938], Loss: 0.5520920753479004\n",
      "Validation: Epoch [16], Batch [241/938], Loss: 0.33646294474601746\n",
      "Validation: Epoch [16], Batch [242/938], Loss: 0.47767695784568787\n",
      "Validation: Epoch [16], Batch [243/938], Loss: 0.5392318964004517\n",
      "Validation: Epoch [16], Batch [244/938], Loss: 0.3158109188079834\n",
      "Validation: Epoch [16], Batch [245/938], Loss: 0.34966278076171875\n",
      "Validation: Epoch [16], Batch [246/938], Loss: 0.4974651038646698\n",
      "Validation: Epoch [16], Batch [247/938], Loss: 0.5649352669715881\n",
      "Validation: Epoch [16], Batch [248/938], Loss: 0.4740009009838104\n",
      "Validation: Epoch [16], Batch [249/938], Loss: 0.47225040197372437\n",
      "Validation: Epoch [16], Batch [250/938], Loss: 0.5103064179420471\n",
      "Validation: Epoch [16], Batch [251/938], Loss: 0.626811146736145\n",
      "Validation: Epoch [16], Batch [252/938], Loss: 0.45384976267814636\n",
      "Validation: Epoch [16], Batch [253/938], Loss: 0.4238516390323639\n",
      "Validation: Epoch [16], Batch [254/938], Loss: 0.3688783049583435\n",
      "Validation: Epoch [16], Batch [255/938], Loss: 0.52805495262146\n",
      "Validation: Epoch [16], Batch [256/938], Loss: 0.5046427249908447\n",
      "Validation: Epoch [16], Batch [257/938], Loss: 0.43474358320236206\n",
      "Validation: Epoch [16], Batch [258/938], Loss: 0.5821764469146729\n",
      "Validation: Epoch [16], Batch [259/938], Loss: 0.42435920238494873\n",
      "Validation: Epoch [16], Batch [260/938], Loss: 0.47041720151901245\n",
      "Validation: Epoch [16], Batch [261/938], Loss: 0.4167104959487915\n",
      "Validation: Epoch [16], Batch [262/938], Loss: 0.5115116834640503\n",
      "Validation: Epoch [16], Batch [263/938], Loss: 0.33378249406814575\n",
      "Validation: Epoch [16], Batch [264/938], Loss: 0.3657853305339813\n",
      "Validation: Epoch [16], Batch [265/938], Loss: 0.47705158591270447\n",
      "Validation: Epoch [16], Batch [266/938], Loss: 0.48189201951026917\n",
      "Validation: Epoch [16], Batch [267/938], Loss: 0.6499884724617004\n",
      "Validation: Epoch [16], Batch [268/938], Loss: 0.3736908733844757\n",
      "Validation: Epoch [16], Batch [269/938], Loss: 0.3830077052116394\n",
      "Validation: Epoch [16], Batch [270/938], Loss: 0.3779865503311157\n",
      "Validation: Epoch [16], Batch [271/938], Loss: 0.3466421365737915\n",
      "Validation: Epoch [16], Batch [272/938], Loss: 0.2528180480003357\n",
      "Validation: Epoch [16], Batch [273/938], Loss: 0.3298717737197876\n",
      "Validation: Epoch [16], Batch [274/938], Loss: 0.5349595546722412\n",
      "Validation: Epoch [16], Batch [275/938], Loss: 0.4152601957321167\n",
      "Validation: Epoch [16], Batch [276/938], Loss: 0.3377174139022827\n",
      "Validation: Epoch [16], Batch [277/938], Loss: 0.4058721959590912\n",
      "Validation: Epoch [16], Batch [278/938], Loss: 0.37658706307411194\n",
      "Validation: Epoch [16], Batch [279/938], Loss: 0.42302870750427246\n",
      "Validation: Epoch [16], Batch [280/938], Loss: 0.5082359313964844\n",
      "Validation: Epoch [16], Batch [281/938], Loss: 0.4043584167957306\n",
      "Validation: Epoch [16], Batch [282/938], Loss: 0.43123960494995117\n",
      "Validation: Epoch [16], Batch [283/938], Loss: 0.40772750973701477\n",
      "Validation: Epoch [16], Batch [284/938], Loss: 0.3709636628627777\n",
      "Validation: Epoch [16], Batch [285/938], Loss: 0.49556049704551697\n",
      "Validation: Epoch [16], Batch [286/938], Loss: 0.4395662546157837\n",
      "Validation: Epoch [16], Batch [287/938], Loss: 0.39719927310943604\n",
      "Validation: Epoch [16], Batch [288/938], Loss: 0.5497847199440002\n",
      "Validation: Epoch [16], Batch [289/938], Loss: 0.4569363594055176\n",
      "Validation: Epoch [16], Batch [290/938], Loss: 0.3265818655490875\n",
      "Validation: Epoch [16], Batch [291/938], Loss: 0.4518367052078247\n",
      "Validation: Epoch [16], Batch [292/938], Loss: 0.3222711384296417\n",
      "Validation: Epoch [16], Batch [293/938], Loss: 0.4633084833621979\n",
      "Validation: Epoch [16], Batch [294/938], Loss: 0.7582250833511353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [295/938], Loss: 0.36211690306663513\n",
      "Validation: Epoch [16], Batch [296/938], Loss: 0.5284411907196045\n",
      "Validation: Epoch [16], Batch [297/938], Loss: 0.5318720936775208\n",
      "Validation: Epoch [16], Batch [298/938], Loss: 0.5412425994873047\n",
      "Validation: Epoch [16], Batch [299/938], Loss: 0.445434033870697\n",
      "Validation: Epoch [16], Batch [300/938], Loss: 0.5194408893585205\n",
      "Validation: Epoch [16], Batch [301/938], Loss: 0.32231950759887695\n",
      "Validation: Epoch [16], Batch [302/938], Loss: 0.3868882656097412\n",
      "Validation: Epoch [16], Batch [303/938], Loss: 0.27495282888412476\n",
      "Validation: Epoch [16], Batch [304/938], Loss: 0.3695146441459656\n",
      "Validation: Epoch [16], Batch [305/938], Loss: 0.34354525804519653\n",
      "Validation: Epoch [16], Batch [306/938], Loss: 0.3415718674659729\n",
      "Validation: Epoch [16], Batch [307/938], Loss: 0.32678496837615967\n",
      "Validation: Epoch [16], Batch [308/938], Loss: 0.42964354157447815\n",
      "Validation: Epoch [16], Batch [309/938], Loss: 0.47601109743118286\n",
      "Validation: Epoch [16], Batch [310/938], Loss: 0.7501699924468994\n",
      "Validation: Epoch [16], Batch [311/938], Loss: 0.4327193796634674\n",
      "Validation: Epoch [16], Batch [312/938], Loss: 0.25929319858551025\n",
      "Validation: Epoch [16], Batch [313/938], Loss: 0.4586763083934784\n",
      "Validation: Epoch [16], Batch [314/938], Loss: 0.559882640838623\n",
      "Validation: Epoch [16], Batch [315/938], Loss: 0.582760214805603\n",
      "Validation: Epoch [16], Batch [316/938], Loss: 0.7083914875984192\n",
      "Validation: Epoch [16], Batch [317/938], Loss: 0.5044048428535461\n",
      "Validation: Epoch [16], Batch [318/938], Loss: 0.46642208099365234\n",
      "Validation: Epoch [16], Batch [319/938], Loss: 0.3662983179092407\n",
      "Validation: Epoch [16], Batch [320/938], Loss: 0.38024407625198364\n",
      "Validation: Epoch [16], Batch [321/938], Loss: 0.4713174104690552\n",
      "Validation: Epoch [16], Batch [322/938], Loss: 0.4513411223888397\n",
      "Validation: Epoch [16], Batch [323/938], Loss: 0.4469783306121826\n",
      "Validation: Epoch [16], Batch [324/938], Loss: 0.40171879529953003\n",
      "Validation: Epoch [16], Batch [325/938], Loss: 0.5681082010269165\n",
      "Validation: Epoch [16], Batch [326/938], Loss: 0.6610357165336609\n",
      "Validation: Epoch [16], Batch [327/938], Loss: 0.4724195599555969\n",
      "Validation: Epoch [16], Batch [328/938], Loss: 0.3691067099571228\n",
      "Validation: Epoch [16], Batch [329/938], Loss: 0.5578198432922363\n",
      "Validation: Epoch [16], Batch [330/938], Loss: 0.4145004153251648\n",
      "Validation: Epoch [16], Batch [331/938], Loss: 0.5659900903701782\n",
      "Validation: Epoch [16], Batch [332/938], Loss: 0.4512439966201782\n",
      "Validation: Epoch [16], Batch [333/938], Loss: 0.34163233637809753\n",
      "Validation: Epoch [16], Batch [334/938], Loss: 0.4319363534450531\n",
      "Validation: Epoch [16], Batch [335/938], Loss: 0.3010031580924988\n",
      "Validation: Epoch [16], Batch [336/938], Loss: 0.28748011589050293\n",
      "Validation: Epoch [16], Batch [337/938], Loss: 0.45397964119911194\n",
      "Validation: Epoch [16], Batch [338/938], Loss: 0.4648522138595581\n",
      "Validation: Epoch [16], Batch [339/938], Loss: 0.5563384294509888\n",
      "Validation: Epoch [16], Batch [340/938], Loss: 0.43005117774009705\n",
      "Validation: Epoch [16], Batch [341/938], Loss: 0.45383694767951965\n",
      "Validation: Epoch [16], Batch [342/938], Loss: 0.2954372763633728\n",
      "Validation: Epoch [16], Batch [343/938], Loss: 0.37972715497016907\n",
      "Validation: Epoch [16], Batch [344/938], Loss: 0.5581023693084717\n",
      "Validation: Epoch [16], Batch [345/938], Loss: 0.5279098749160767\n",
      "Validation: Epoch [16], Batch [346/938], Loss: 0.4228680729866028\n",
      "Validation: Epoch [16], Batch [347/938], Loss: 0.27785104513168335\n",
      "Validation: Epoch [16], Batch [348/938], Loss: 0.34618622064590454\n",
      "Validation: Epoch [16], Batch [349/938], Loss: 0.49576443433761597\n",
      "Validation: Epoch [16], Batch [350/938], Loss: 0.7064199447631836\n",
      "Validation: Epoch [16], Batch [351/938], Loss: 0.27928659319877625\n",
      "Validation: Epoch [16], Batch [352/938], Loss: 0.3245355486869812\n",
      "Validation: Epoch [16], Batch [353/938], Loss: 0.44055604934692383\n",
      "Validation: Epoch [16], Batch [354/938], Loss: 0.6791161298751831\n",
      "Validation: Epoch [16], Batch [355/938], Loss: 0.3116832971572876\n",
      "Validation: Epoch [16], Batch [356/938], Loss: 0.3358272612094879\n",
      "Validation: Epoch [16], Batch [357/938], Loss: 0.47313815355300903\n",
      "Validation: Epoch [16], Batch [358/938], Loss: 0.3035967946052551\n",
      "Validation: Epoch [16], Batch [359/938], Loss: 0.5810250043869019\n",
      "Validation: Epoch [16], Batch [360/938], Loss: 0.2742795944213867\n",
      "Validation: Epoch [16], Batch [361/938], Loss: 0.5042954683303833\n",
      "Validation: Epoch [16], Batch [362/938], Loss: 0.4309353530406952\n",
      "Validation: Epoch [16], Batch [363/938], Loss: 0.23382222652435303\n",
      "Validation: Epoch [16], Batch [364/938], Loss: 0.6543516516685486\n",
      "Validation: Epoch [16], Batch [365/938], Loss: 0.5923073291778564\n",
      "Validation: Epoch [16], Batch [366/938], Loss: 0.6229138970375061\n",
      "Validation: Epoch [16], Batch [367/938], Loss: 0.4717397093772888\n",
      "Validation: Epoch [16], Batch [368/938], Loss: 0.589888334274292\n",
      "Validation: Epoch [16], Batch [369/938], Loss: 0.4268442690372467\n",
      "Validation: Epoch [16], Batch [370/938], Loss: 0.4681406021118164\n",
      "Validation: Epoch [16], Batch [371/938], Loss: 0.5147378444671631\n",
      "Validation: Epoch [16], Batch [372/938], Loss: 0.22118708491325378\n",
      "Validation: Epoch [16], Batch [373/938], Loss: 0.5058757066726685\n",
      "Validation: Epoch [16], Batch [374/938], Loss: 0.4204120635986328\n",
      "Validation: Epoch [16], Batch [375/938], Loss: 0.6600722074508667\n",
      "Validation: Epoch [16], Batch [376/938], Loss: 0.5142160058021545\n",
      "Validation: Epoch [16], Batch [377/938], Loss: 0.6381619572639465\n",
      "Validation: Epoch [16], Batch [378/938], Loss: 0.5243424773216248\n",
      "Validation: Epoch [16], Batch [379/938], Loss: 0.4969688951969147\n",
      "Validation: Epoch [16], Batch [380/938], Loss: 0.43885913491249084\n",
      "Validation: Epoch [16], Batch [381/938], Loss: 0.30185002088546753\n",
      "Validation: Epoch [16], Batch [382/938], Loss: 0.6280202865600586\n",
      "Validation: Epoch [16], Batch [383/938], Loss: 0.49351704120635986\n",
      "Validation: Epoch [16], Batch [384/938], Loss: 0.50754714012146\n",
      "Validation: Epoch [16], Batch [385/938], Loss: 0.41548025608062744\n",
      "Validation: Epoch [16], Batch [386/938], Loss: 0.33455199003219604\n",
      "Validation: Epoch [16], Batch [387/938], Loss: 0.47118330001831055\n",
      "Validation: Epoch [16], Batch [388/938], Loss: 0.6197085380554199\n",
      "Validation: Epoch [16], Batch [389/938], Loss: 0.403352290391922\n",
      "Validation: Epoch [16], Batch [390/938], Loss: 0.42594918608665466\n",
      "Validation: Epoch [16], Batch [391/938], Loss: 0.47721779346466064\n",
      "Validation: Epoch [16], Batch [392/938], Loss: 0.5094907879829407\n",
      "Validation: Epoch [16], Batch [393/938], Loss: 0.3664238750934601\n",
      "Validation: Epoch [16], Batch [394/938], Loss: 0.262473464012146\n",
      "Validation: Epoch [16], Batch [395/938], Loss: 0.47336024045944214\n",
      "Validation: Epoch [16], Batch [396/938], Loss: 0.2836952209472656\n",
      "Validation: Epoch [16], Batch [397/938], Loss: 0.34677138924598694\n",
      "Validation: Epoch [16], Batch [398/938], Loss: 0.536508321762085\n",
      "Validation: Epoch [16], Batch [399/938], Loss: 0.3469790518283844\n",
      "Validation: Epoch [16], Batch [400/938], Loss: 0.40236061811447144\n",
      "Validation: Epoch [16], Batch [401/938], Loss: 0.4520389139652252\n",
      "Validation: Epoch [16], Batch [402/938], Loss: 0.6869664192199707\n",
      "Validation: Epoch [16], Batch [403/938], Loss: 0.5912289023399353\n",
      "Validation: Epoch [16], Batch [404/938], Loss: 0.47181931138038635\n",
      "Validation: Epoch [16], Batch [405/938], Loss: 0.4221380054950714\n",
      "Validation: Epoch [16], Batch [406/938], Loss: 0.5569919943809509\n",
      "Validation: Epoch [16], Batch [407/938], Loss: 0.46525734663009644\n",
      "Validation: Epoch [16], Batch [408/938], Loss: 0.21045517921447754\n",
      "Validation: Epoch [16], Batch [409/938], Loss: 0.5206797122955322\n",
      "Validation: Epoch [16], Batch [410/938], Loss: 0.4578011929988861\n",
      "Validation: Epoch [16], Batch [411/938], Loss: 0.6731833219528198\n",
      "Validation: Epoch [16], Batch [412/938], Loss: 0.4421543478965759\n",
      "Validation: Epoch [16], Batch [413/938], Loss: 0.4627493619918823\n",
      "Validation: Epoch [16], Batch [414/938], Loss: 0.4037843942642212\n",
      "Validation: Epoch [16], Batch [415/938], Loss: 0.49477362632751465\n",
      "Validation: Epoch [16], Batch [416/938], Loss: 0.345878541469574\n",
      "Validation: Epoch [16], Batch [417/938], Loss: 0.31397750973701477\n",
      "Validation: Epoch [16], Batch [418/938], Loss: 0.4024667739868164\n",
      "Validation: Epoch [16], Batch [419/938], Loss: 0.5283149480819702\n",
      "Validation: Epoch [16], Batch [420/938], Loss: 0.4039474129676819\n",
      "Validation: Epoch [16], Batch [421/938], Loss: 0.3008943200111389\n",
      "Validation: Epoch [16], Batch [422/938], Loss: 0.6353933811187744\n",
      "Validation: Epoch [16], Batch [423/938], Loss: 0.4544491171836853\n",
      "Validation: Epoch [16], Batch [424/938], Loss: 0.29939955472946167\n",
      "Validation: Epoch [16], Batch [425/938], Loss: 0.5221900939941406\n",
      "Validation: Epoch [16], Batch [426/938], Loss: 0.3730229139328003\n",
      "Validation: Epoch [16], Batch [427/938], Loss: 0.4166454076766968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [428/938], Loss: 0.43051791191101074\n",
      "Validation: Epoch [16], Batch [429/938], Loss: 0.38847172260284424\n",
      "Validation: Epoch [16], Batch [430/938], Loss: 0.47456496953964233\n",
      "Validation: Epoch [16], Batch [431/938], Loss: 0.5770253539085388\n",
      "Validation: Epoch [16], Batch [432/938], Loss: 0.5680429935455322\n",
      "Validation: Epoch [16], Batch [433/938], Loss: 0.47965824604034424\n",
      "Validation: Epoch [16], Batch [434/938], Loss: 0.3518681526184082\n",
      "Validation: Epoch [16], Batch [435/938], Loss: 0.5293321013450623\n",
      "Validation: Epoch [16], Batch [436/938], Loss: 0.37198248505592346\n",
      "Validation: Epoch [16], Batch [437/938], Loss: 0.5886229276657104\n",
      "Validation: Epoch [16], Batch [438/938], Loss: 0.361235111951828\n",
      "Validation: Epoch [16], Batch [439/938], Loss: 0.5363696217536926\n",
      "Validation: Epoch [16], Batch [440/938], Loss: 0.39080002903938293\n",
      "Validation: Epoch [16], Batch [441/938], Loss: 0.34263524413108826\n",
      "Validation: Epoch [16], Batch [442/938], Loss: 0.4619026780128479\n",
      "Validation: Epoch [16], Batch [443/938], Loss: 0.3616923987865448\n",
      "Validation: Epoch [16], Batch [444/938], Loss: 0.4510282278060913\n",
      "Validation: Epoch [16], Batch [445/938], Loss: 0.41789019107818604\n",
      "Validation: Epoch [16], Batch [446/938], Loss: 0.32192158699035645\n",
      "Validation: Epoch [16], Batch [447/938], Loss: 0.5568006038665771\n",
      "Validation: Epoch [16], Batch [448/938], Loss: 0.31582894921302795\n",
      "Validation: Epoch [16], Batch [449/938], Loss: 0.521734356880188\n",
      "Validation: Epoch [16], Batch [450/938], Loss: 0.3805951476097107\n",
      "Validation: Epoch [16], Batch [451/938], Loss: 0.5061480402946472\n",
      "Validation: Epoch [16], Batch [452/938], Loss: 0.3668734133243561\n",
      "Validation: Epoch [16], Batch [453/938], Loss: 0.6326378583908081\n",
      "Validation: Epoch [16], Batch [454/938], Loss: 0.6033673286437988\n",
      "Validation: Epoch [16], Batch [455/938], Loss: 0.46370142698287964\n",
      "Validation: Epoch [16], Batch [456/938], Loss: 0.3170183598995209\n",
      "Validation: Epoch [16], Batch [457/938], Loss: 0.5226254463195801\n",
      "Validation: Epoch [16], Batch [458/938], Loss: 0.44894808530807495\n",
      "Validation: Epoch [16], Batch [459/938], Loss: 0.26364704966545105\n",
      "Validation: Epoch [16], Batch [460/938], Loss: 0.4513212740421295\n",
      "Validation: Epoch [16], Batch [461/938], Loss: 0.4366553723812103\n",
      "Validation: Epoch [16], Batch [462/938], Loss: 0.45749345421791077\n",
      "Validation: Epoch [16], Batch [463/938], Loss: 0.5330171585083008\n",
      "Validation: Epoch [16], Batch [464/938], Loss: 0.40898001194000244\n",
      "Validation: Epoch [16], Batch [465/938], Loss: 0.30793094635009766\n",
      "Validation: Epoch [16], Batch [466/938], Loss: 0.34037137031555176\n",
      "Validation: Epoch [16], Batch [467/938], Loss: 0.210737407207489\n",
      "Validation: Epoch [16], Batch [468/938], Loss: 0.4169289469718933\n",
      "Validation: Epoch [16], Batch [469/938], Loss: 0.4085633158683777\n",
      "Validation: Epoch [16], Batch [470/938], Loss: 0.5929981470108032\n",
      "Validation: Epoch [16], Batch [471/938], Loss: 0.5004858374595642\n",
      "Validation: Epoch [16], Batch [472/938], Loss: 0.39563673734664917\n",
      "Validation: Epoch [16], Batch [473/938], Loss: 0.4492816925048828\n",
      "Validation: Epoch [16], Batch [474/938], Loss: 0.4560784697532654\n",
      "Validation: Epoch [16], Batch [475/938], Loss: 0.3806626796722412\n",
      "Validation: Epoch [16], Batch [476/938], Loss: 0.3516824245452881\n",
      "Validation: Epoch [16], Batch [477/938], Loss: 0.3874298930168152\n",
      "Validation: Epoch [16], Batch [478/938], Loss: 0.4477763772010803\n",
      "Validation: Epoch [16], Batch [479/938], Loss: 0.38469773530960083\n",
      "Validation: Epoch [16], Batch [480/938], Loss: 0.4374546706676483\n",
      "Validation: Epoch [16], Batch [481/938], Loss: 0.3028431832790375\n",
      "Validation: Epoch [16], Batch [482/938], Loss: 0.555572509765625\n",
      "Validation: Epoch [16], Batch [483/938], Loss: 0.44732674956321716\n",
      "Validation: Epoch [16], Batch [484/938], Loss: 0.5520923137664795\n",
      "Validation: Epoch [16], Batch [485/938], Loss: 0.5414577126502991\n",
      "Validation: Epoch [16], Batch [486/938], Loss: 0.3379290699958801\n",
      "Validation: Epoch [16], Batch [487/938], Loss: 0.4244765341281891\n",
      "Validation: Epoch [16], Batch [488/938], Loss: 0.45802703499794006\n",
      "Validation: Epoch [16], Batch [489/938], Loss: 0.2971896827220917\n",
      "Validation: Epoch [16], Batch [490/938], Loss: 0.5944936275482178\n",
      "Validation: Epoch [16], Batch [491/938], Loss: 0.5850368738174438\n",
      "Validation: Epoch [16], Batch [492/938], Loss: 0.3566453456878662\n",
      "Validation: Epoch [16], Batch [493/938], Loss: 0.5237170457839966\n",
      "Validation: Epoch [16], Batch [494/938], Loss: 0.35112282633781433\n",
      "Validation: Epoch [16], Batch [495/938], Loss: 0.49184635281562805\n",
      "Validation: Epoch [16], Batch [496/938], Loss: 0.46656519174575806\n",
      "Validation: Epoch [16], Batch [497/938], Loss: 0.3486121594905853\n",
      "Validation: Epoch [16], Batch [498/938], Loss: 0.710634708404541\n",
      "Validation: Epoch [16], Batch [499/938], Loss: 0.5799716711044312\n",
      "Validation: Epoch [16], Batch [500/938], Loss: 0.29451924562454224\n",
      "Validation: Epoch [16], Batch [501/938], Loss: 0.5153692960739136\n",
      "Validation: Epoch [16], Batch [502/938], Loss: 0.42196404933929443\n",
      "Validation: Epoch [16], Batch [503/938], Loss: 0.4291859269142151\n",
      "Validation: Epoch [16], Batch [504/938], Loss: 0.3205346465110779\n",
      "Validation: Epoch [16], Batch [505/938], Loss: 0.5753688812255859\n",
      "Validation: Epoch [16], Batch [506/938], Loss: 0.5665662288665771\n",
      "Validation: Epoch [16], Batch [507/938], Loss: 0.36716485023498535\n",
      "Validation: Epoch [16], Batch [508/938], Loss: 0.4720761179924011\n",
      "Validation: Epoch [16], Batch [509/938], Loss: 0.39504531025886536\n",
      "Validation: Epoch [16], Batch [510/938], Loss: 0.46718868613243103\n",
      "Validation: Epoch [16], Batch [511/938], Loss: 0.35653799772262573\n",
      "Validation: Epoch [16], Batch [512/938], Loss: 0.4644545912742615\n",
      "Validation: Epoch [16], Batch [513/938], Loss: 0.3733994662761688\n",
      "Validation: Epoch [16], Batch [514/938], Loss: 0.4760369658470154\n",
      "Validation: Epoch [16], Batch [515/938], Loss: 0.5167238712310791\n",
      "Validation: Epoch [16], Batch [516/938], Loss: 0.427458256483078\n",
      "Validation: Epoch [16], Batch [517/938], Loss: 0.4031122326850891\n",
      "Validation: Epoch [16], Batch [518/938], Loss: 0.521350622177124\n",
      "Validation: Epoch [16], Batch [519/938], Loss: 0.4430096745491028\n",
      "Validation: Epoch [16], Batch [520/938], Loss: 0.45572298765182495\n",
      "Validation: Epoch [16], Batch [521/938], Loss: 0.35072100162506104\n",
      "Validation: Epoch [16], Batch [522/938], Loss: 0.3089682459831238\n",
      "Validation: Epoch [16], Batch [523/938], Loss: 0.345766544342041\n",
      "Validation: Epoch [16], Batch [524/938], Loss: 0.45415329933166504\n",
      "Validation: Epoch [16], Batch [525/938], Loss: 0.3688543736934662\n",
      "Validation: Epoch [16], Batch [526/938], Loss: 0.37951400876045227\n",
      "Validation: Epoch [16], Batch [527/938], Loss: 0.2800133228302002\n",
      "Validation: Epoch [16], Batch [528/938], Loss: 0.45696502923965454\n",
      "Validation: Epoch [16], Batch [529/938], Loss: 0.4075528085231781\n",
      "Validation: Epoch [16], Batch [530/938], Loss: 0.5869327187538147\n",
      "Validation: Epoch [16], Batch [531/938], Loss: 0.6118883490562439\n",
      "Validation: Epoch [16], Batch [532/938], Loss: 0.5868439078330994\n",
      "Validation: Epoch [16], Batch [533/938], Loss: 0.49214816093444824\n",
      "Validation: Epoch [16], Batch [534/938], Loss: 0.3690909743309021\n",
      "Validation: Epoch [16], Batch [535/938], Loss: 0.3063233494758606\n",
      "Validation: Epoch [16], Batch [536/938], Loss: 0.5392130613327026\n",
      "Validation: Epoch [16], Batch [537/938], Loss: 0.6251696944236755\n",
      "Validation: Epoch [16], Batch [538/938], Loss: 0.35811635851860046\n",
      "Validation: Epoch [16], Batch [539/938], Loss: 0.3244208097457886\n",
      "Validation: Epoch [16], Batch [540/938], Loss: 0.2530617415904999\n",
      "Validation: Epoch [16], Batch [541/938], Loss: 0.47390031814575195\n",
      "Validation: Epoch [16], Batch [542/938], Loss: 0.5586696863174438\n",
      "Validation: Epoch [16], Batch [543/938], Loss: 0.4813060164451599\n",
      "Validation: Epoch [16], Batch [544/938], Loss: 0.3362550735473633\n",
      "Validation: Epoch [16], Batch [545/938], Loss: 0.35658061504364014\n",
      "Validation: Epoch [16], Batch [546/938], Loss: 0.4508788287639618\n",
      "Validation: Epoch [16], Batch [547/938], Loss: 0.505735456943512\n",
      "Validation: Epoch [16], Batch [548/938], Loss: 0.34710925817489624\n",
      "Validation: Epoch [16], Batch [549/938], Loss: 0.5202285051345825\n",
      "Validation: Epoch [16], Batch [550/938], Loss: 0.5024449825286865\n",
      "Validation: Epoch [16], Batch [551/938], Loss: 0.4133637249469757\n",
      "Validation: Epoch [16], Batch [552/938], Loss: 0.40294384956359863\n",
      "Validation: Epoch [16], Batch [553/938], Loss: 0.44731825590133667\n",
      "Validation: Epoch [16], Batch [554/938], Loss: 0.4308781623840332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [555/938], Loss: 0.3671913146972656\n",
      "Validation: Epoch [16], Batch [556/938], Loss: 0.34275996685028076\n",
      "Validation: Epoch [16], Batch [557/938], Loss: 0.4215352535247803\n",
      "Validation: Epoch [16], Batch [558/938], Loss: 0.4683993458747864\n",
      "Validation: Epoch [16], Batch [559/938], Loss: 0.5040196776390076\n",
      "Validation: Epoch [16], Batch [560/938], Loss: 0.263799250125885\n",
      "Validation: Epoch [16], Batch [561/938], Loss: 0.5209082365036011\n",
      "Validation: Epoch [16], Batch [562/938], Loss: 0.6295296549797058\n",
      "Validation: Epoch [16], Batch [563/938], Loss: 0.4125854969024658\n",
      "Validation: Epoch [16], Batch [564/938], Loss: 0.49812042713165283\n",
      "Validation: Epoch [16], Batch [565/938], Loss: 0.47332215309143066\n",
      "Validation: Epoch [16], Batch [566/938], Loss: 0.4235217273235321\n",
      "Validation: Epoch [16], Batch [567/938], Loss: 0.5851240158081055\n",
      "Validation: Epoch [16], Batch [568/938], Loss: 0.361539751291275\n",
      "Validation: Epoch [16], Batch [569/938], Loss: 0.3574230670928955\n",
      "Validation: Epoch [16], Batch [570/938], Loss: 0.3596183955669403\n",
      "Validation: Epoch [16], Batch [571/938], Loss: 0.602365255355835\n",
      "Validation: Epoch [16], Batch [572/938], Loss: 0.4343067407608032\n",
      "Validation: Epoch [16], Batch [573/938], Loss: 0.4655240476131439\n",
      "Validation: Epoch [16], Batch [574/938], Loss: 0.6943460702896118\n",
      "Validation: Epoch [16], Batch [575/938], Loss: 0.5980910658836365\n",
      "Validation: Epoch [16], Batch [576/938], Loss: 0.3026193380355835\n",
      "Validation: Epoch [16], Batch [577/938], Loss: 0.48956766724586487\n",
      "Validation: Epoch [16], Batch [578/938], Loss: 0.34574776887893677\n",
      "Validation: Epoch [16], Batch [579/938], Loss: 0.6372451186180115\n",
      "Validation: Epoch [16], Batch [580/938], Loss: 0.39695119857788086\n",
      "Validation: Epoch [16], Batch [581/938], Loss: 0.2641940414905548\n",
      "Validation: Epoch [16], Batch [582/938], Loss: 0.3587482273578644\n",
      "Validation: Epoch [16], Batch [583/938], Loss: 0.3932201564311981\n",
      "Validation: Epoch [16], Batch [584/938], Loss: 0.5239993929862976\n",
      "Validation: Epoch [16], Batch [585/938], Loss: 0.38691049814224243\n",
      "Validation: Epoch [16], Batch [586/938], Loss: 0.4220951795578003\n",
      "Validation: Epoch [16], Batch [587/938], Loss: 0.403053879737854\n",
      "Validation: Epoch [16], Batch [588/938], Loss: 0.49341827630996704\n",
      "Validation: Epoch [16], Batch [589/938], Loss: 0.24471262097358704\n",
      "Validation: Epoch [16], Batch [590/938], Loss: 0.4359803795814514\n",
      "Validation: Epoch [16], Batch [591/938], Loss: 0.4948139488697052\n",
      "Validation: Epoch [16], Batch [592/938], Loss: 0.3268561065196991\n",
      "Validation: Epoch [16], Batch [593/938], Loss: 0.46750062704086304\n",
      "Validation: Epoch [16], Batch [594/938], Loss: 0.6682174205780029\n",
      "Validation: Epoch [16], Batch [595/938], Loss: 0.23608559370040894\n",
      "Validation: Epoch [16], Batch [596/938], Loss: 0.41231274604797363\n",
      "Validation: Epoch [16], Batch [597/938], Loss: 0.5218377113342285\n",
      "Validation: Epoch [16], Batch [598/938], Loss: 0.4843745231628418\n",
      "Validation: Epoch [16], Batch [599/938], Loss: 0.439744234085083\n",
      "Validation: Epoch [16], Batch [600/938], Loss: 0.645162045955658\n",
      "Validation: Epoch [16], Batch [601/938], Loss: 0.26519057154655457\n",
      "Validation: Epoch [16], Batch [602/938], Loss: 0.35188594460487366\n",
      "Validation: Epoch [16], Batch [603/938], Loss: 0.4703344702720642\n",
      "Validation: Epoch [16], Batch [604/938], Loss: 0.44940024614334106\n",
      "Validation: Epoch [16], Batch [605/938], Loss: 0.4810490310192108\n",
      "Validation: Epoch [16], Batch [606/938], Loss: 0.4096330404281616\n",
      "Validation: Epoch [16], Batch [607/938], Loss: 0.48185721039772034\n",
      "Validation: Epoch [16], Batch [608/938], Loss: 0.3138061463832855\n",
      "Validation: Epoch [16], Batch [609/938], Loss: 0.48138344287872314\n",
      "Validation: Epoch [16], Batch [610/938], Loss: 0.3834790885448456\n",
      "Validation: Epoch [16], Batch [611/938], Loss: 0.41836032271385193\n",
      "Validation: Epoch [16], Batch [612/938], Loss: 0.5097347497940063\n",
      "Validation: Epoch [16], Batch [613/938], Loss: 0.577577531337738\n",
      "Validation: Epoch [16], Batch [614/938], Loss: 0.5879834890365601\n",
      "Validation: Epoch [16], Batch [615/938], Loss: 0.3779945373535156\n",
      "Validation: Epoch [16], Batch [616/938], Loss: 0.38548585772514343\n",
      "Validation: Epoch [16], Batch [617/938], Loss: 0.47277122735977173\n",
      "Validation: Epoch [16], Batch [618/938], Loss: 0.28056231141090393\n",
      "Validation: Epoch [16], Batch [619/938], Loss: 0.47680017352104187\n",
      "Validation: Epoch [16], Batch [620/938], Loss: 0.2594432234764099\n",
      "Validation: Epoch [16], Batch [621/938], Loss: 0.49407342076301575\n",
      "Validation: Epoch [16], Batch [622/938], Loss: 0.3411697745323181\n",
      "Validation: Epoch [16], Batch [623/938], Loss: 0.4230037331581116\n",
      "Validation: Epoch [16], Batch [624/938], Loss: 0.43805134296417236\n",
      "Validation: Epoch [16], Batch [625/938], Loss: 0.49310269951820374\n",
      "Validation: Epoch [16], Batch [626/938], Loss: 0.46637099981307983\n",
      "Validation: Epoch [16], Batch [627/938], Loss: 0.5233531594276428\n",
      "Validation: Epoch [16], Batch [628/938], Loss: 0.3210456371307373\n",
      "Validation: Epoch [16], Batch [629/938], Loss: 0.3907046914100647\n",
      "Validation: Epoch [16], Batch [630/938], Loss: 0.3767126500606537\n",
      "Validation: Epoch [16], Batch [631/938], Loss: 0.9201788306236267\n",
      "Validation: Epoch [16], Batch [632/938], Loss: 0.3751254677772522\n",
      "Validation: Epoch [16], Batch [633/938], Loss: 0.36844393610954285\n",
      "Validation: Epoch [16], Batch [634/938], Loss: 0.4735373854637146\n",
      "Validation: Epoch [16], Batch [635/938], Loss: 0.39507973194122314\n",
      "Validation: Epoch [16], Batch [636/938], Loss: 0.49929916858673096\n",
      "Validation: Epoch [16], Batch [637/938], Loss: 0.44981539249420166\n",
      "Validation: Epoch [16], Batch [638/938], Loss: 0.3949448764324188\n",
      "Validation: Epoch [16], Batch [639/938], Loss: 0.4608601927757263\n",
      "Validation: Epoch [16], Batch [640/938], Loss: 0.5126515626907349\n",
      "Validation: Epoch [16], Batch [641/938], Loss: 0.4326059818267822\n",
      "Validation: Epoch [16], Batch [642/938], Loss: 0.2518988847732544\n",
      "Validation: Epoch [16], Batch [643/938], Loss: 0.553194522857666\n",
      "Validation: Epoch [16], Batch [644/938], Loss: 0.3882210850715637\n",
      "Validation: Epoch [16], Batch [645/938], Loss: 0.39647817611694336\n",
      "Validation: Epoch [16], Batch [646/938], Loss: 0.35242512822151184\n",
      "Validation: Epoch [16], Batch [647/938], Loss: 0.43721869587898254\n",
      "Validation: Epoch [16], Batch [648/938], Loss: 0.5519819259643555\n",
      "Validation: Epoch [16], Batch [649/938], Loss: 0.26030802726745605\n",
      "Validation: Epoch [16], Batch [650/938], Loss: 0.3372189402580261\n",
      "Validation: Epoch [16], Batch [651/938], Loss: 0.4221062958240509\n",
      "Validation: Epoch [16], Batch [652/938], Loss: 0.3588588237762451\n",
      "Validation: Epoch [16], Batch [653/938], Loss: 0.5404717326164246\n",
      "Validation: Epoch [16], Batch [654/938], Loss: 0.3916362524032593\n",
      "Validation: Epoch [16], Batch [655/938], Loss: 0.5454860925674438\n",
      "Validation: Epoch [16], Batch [656/938], Loss: 0.5316405296325684\n",
      "Validation: Epoch [16], Batch [657/938], Loss: 0.5149589776992798\n",
      "Validation: Epoch [16], Batch [658/938], Loss: 0.32785487174987793\n",
      "Validation: Epoch [16], Batch [659/938], Loss: 0.5144103765487671\n",
      "Validation: Epoch [16], Batch [660/938], Loss: 0.41351354122161865\n",
      "Validation: Epoch [16], Batch [661/938], Loss: 0.3867928087711334\n",
      "Validation: Epoch [16], Batch [662/938], Loss: 0.49351412057876587\n",
      "Validation: Epoch [16], Batch [663/938], Loss: 0.5403778553009033\n",
      "Validation: Epoch [16], Batch [664/938], Loss: 0.5624990463256836\n",
      "Validation: Epoch [16], Batch [665/938], Loss: 0.395974725484848\n",
      "Validation: Epoch [16], Batch [666/938], Loss: 0.3858288526535034\n",
      "Validation: Epoch [16], Batch [667/938], Loss: 0.3097293972969055\n",
      "Validation: Epoch [16], Batch [668/938], Loss: 0.42513740062713623\n",
      "Validation: Epoch [16], Batch [669/938], Loss: 0.42070135474205017\n",
      "Validation: Epoch [16], Batch [670/938], Loss: 0.31553763151168823\n",
      "Validation: Epoch [16], Batch [671/938], Loss: 0.24976074695587158\n",
      "Validation: Epoch [16], Batch [672/938], Loss: 0.5359582901000977\n",
      "Validation: Epoch [16], Batch [673/938], Loss: 0.6486603021621704\n",
      "Validation: Epoch [16], Batch [674/938], Loss: 0.5012468099594116\n",
      "Validation: Epoch [16], Batch [675/938], Loss: 0.3954579532146454\n",
      "Validation: Epoch [16], Batch [676/938], Loss: 0.33993348479270935\n",
      "Validation: Epoch [16], Batch [677/938], Loss: 0.37016329169273376\n",
      "Validation: Epoch [16], Batch [678/938], Loss: 0.35045650601387024\n",
      "Validation: Epoch [16], Batch [679/938], Loss: 0.43103915452957153\n",
      "Validation: Epoch [16], Batch [680/938], Loss: 0.4773084819316864\n",
      "Validation: Epoch [16], Batch [681/938], Loss: 0.3699488639831543\n",
      "Validation: Epoch [16], Batch [682/938], Loss: 0.33911094069480896\n",
      "Validation: Epoch [16], Batch [683/938], Loss: 0.2859647274017334\n",
      "Validation: Epoch [16], Batch [684/938], Loss: 0.35363417863845825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [685/938], Loss: 0.464857280254364\n",
      "Validation: Epoch [16], Batch [686/938], Loss: 0.37707167863845825\n",
      "Validation: Epoch [16], Batch [687/938], Loss: 0.3951351046562195\n",
      "Validation: Epoch [16], Batch [688/938], Loss: 0.38457655906677246\n",
      "Validation: Epoch [16], Batch [689/938], Loss: 0.49521952867507935\n",
      "Validation: Epoch [16], Batch [690/938], Loss: 0.5158811211585999\n",
      "Validation: Epoch [16], Batch [691/938], Loss: 0.3303210735321045\n",
      "Validation: Epoch [16], Batch [692/938], Loss: 0.2994026243686676\n",
      "Validation: Epoch [16], Batch [693/938], Loss: 0.5181039571762085\n",
      "Validation: Epoch [16], Batch [694/938], Loss: 0.680936872959137\n",
      "Validation: Epoch [16], Batch [695/938], Loss: 0.45338040590286255\n",
      "Validation: Epoch [16], Batch [696/938], Loss: 0.496518075466156\n",
      "Validation: Epoch [16], Batch [697/938], Loss: 0.3786749839782715\n",
      "Validation: Epoch [16], Batch [698/938], Loss: 0.3536340296268463\n",
      "Validation: Epoch [16], Batch [699/938], Loss: 0.49540919065475464\n",
      "Validation: Epoch [16], Batch [700/938], Loss: 0.3515309989452362\n",
      "Validation: Epoch [16], Batch [701/938], Loss: 0.5388092994689941\n",
      "Validation: Epoch [16], Batch [702/938], Loss: 0.5088237524032593\n",
      "Validation: Epoch [16], Batch [703/938], Loss: 0.33435189723968506\n",
      "Validation: Epoch [16], Batch [704/938], Loss: 0.6359965801239014\n",
      "Validation: Epoch [16], Batch [705/938], Loss: 0.542195200920105\n",
      "Validation: Epoch [16], Batch [706/938], Loss: 0.35146841406822205\n",
      "Validation: Epoch [16], Batch [707/938], Loss: 0.5647242069244385\n",
      "Validation: Epoch [16], Batch [708/938], Loss: 0.4928305745124817\n",
      "Validation: Epoch [16], Batch [709/938], Loss: 0.5325953364372253\n",
      "Validation: Epoch [16], Batch [710/938], Loss: 0.49603408575057983\n",
      "Validation: Epoch [16], Batch [711/938], Loss: 0.3692823648452759\n",
      "Validation: Epoch [16], Batch [712/938], Loss: 0.3769363760948181\n",
      "Validation: Epoch [16], Batch [713/938], Loss: 0.47699981927871704\n",
      "Validation: Epoch [16], Batch [714/938], Loss: 0.4574853181838989\n",
      "Validation: Epoch [16], Batch [715/938], Loss: 0.6026062965393066\n",
      "Validation: Epoch [16], Batch [716/938], Loss: 0.5019667744636536\n",
      "Validation: Epoch [16], Batch [717/938], Loss: 0.3244554102420807\n",
      "Validation: Epoch [16], Batch [718/938], Loss: 0.5315786004066467\n",
      "Validation: Epoch [16], Batch [719/938], Loss: 0.6092778444290161\n",
      "Validation: Epoch [16], Batch [720/938], Loss: 0.4482043385505676\n",
      "Validation: Epoch [16], Batch [721/938], Loss: 0.6603388786315918\n",
      "Validation: Epoch [16], Batch [722/938], Loss: 0.3401806950569153\n",
      "Validation: Epoch [16], Batch [723/938], Loss: 0.352271169424057\n",
      "Validation: Epoch [16], Batch [724/938], Loss: 0.26469507813453674\n",
      "Validation: Epoch [16], Batch [725/938], Loss: 0.39623743295669556\n",
      "Validation: Epoch [16], Batch [726/938], Loss: 0.49494725465774536\n",
      "Validation: Epoch [16], Batch [727/938], Loss: 0.3153113126754761\n",
      "Validation: Epoch [16], Batch [728/938], Loss: 0.33155712485313416\n",
      "Validation: Epoch [16], Batch [729/938], Loss: 0.3704049587249756\n",
      "Validation: Epoch [16], Batch [730/938], Loss: 0.5114774703979492\n",
      "Validation: Epoch [16], Batch [731/938], Loss: 0.44188499450683594\n",
      "Validation: Epoch [16], Batch [732/938], Loss: 0.3999013900756836\n",
      "Validation: Epoch [16], Batch [733/938], Loss: 0.7000232934951782\n",
      "Validation: Epoch [16], Batch [734/938], Loss: 0.6777103543281555\n",
      "Validation: Epoch [16], Batch [735/938], Loss: 0.39627835154533386\n",
      "Validation: Epoch [16], Batch [736/938], Loss: 0.5975331664085388\n",
      "Validation: Epoch [16], Batch [737/938], Loss: 0.48787999153137207\n",
      "Validation: Epoch [16], Batch [738/938], Loss: 0.35772567987442017\n",
      "Validation: Epoch [16], Batch [739/938], Loss: 0.412160724401474\n",
      "Validation: Epoch [16], Batch [740/938], Loss: 0.3119533061981201\n",
      "Validation: Epoch [16], Batch [741/938], Loss: 0.40619638562202454\n",
      "Validation: Epoch [16], Batch [742/938], Loss: 0.6252415776252747\n",
      "Validation: Epoch [16], Batch [743/938], Loss: 0.47316473722457886\n",
      "Validation: Epoch [16], Batch [744/938], Loss: 0.27499645948410034\n",
      "Validation: Epoch [16], Batch [745/938], Loss: 0.4636307954788208\n",
      "Validation: Epoch [16], Batch [746/938], Loss: 0.41765522956848145\n",
      "Validation: Epoch [16], Batch [747/938], Loss: 0.3917538821697235\n",
      "Validation: Epoch [16], Batch [748/938], Loss: 0.49342721700668335\n",
      "Validation: Epoch [16], Batch [749/938], Loss: 0.5445635914802551\n",
      "Validation: Epoch [16], Batch [750/938], Loss: 0.3940872251987457\n",
      "Validation: Epoch [16], Batch [751/938], Loss: 0.4743519425392151\n",
      "Validation: Epoch [16], Batch [752/938], Loss: 0.49094897508621216\n",
      "Validation: Epoch [16], Batch [753/938], Loss: 0.25380656123161316\n",
      "Validation: Epoch [16], Batch [754/938], Loss: 0.4024900496006012\n",
      "Validation: Epoch [16], Batch [755/938], Loss: 0.5268187522888184\n",
      "Validation: Epoch [16], Batch [756/938], Loss: 0.39407217502593994\n",
      "Validation: Epoch [16], Batch [757/938], Loss: 0.5030524730682373\n",
      "Validation: Epoch [16], Batch [758/938], Loss: 0.4326424300670624\n",
      "Validation: Epoch [16], Batch [759/938], Loss: 0.5225486755371094\n",
      "Validation: Epoch [16], Batch [760/938], Loss: 0.5594369173049927\n",
      "Validation: Epoch [16], Batch [761/938], Loss: 0.5338059067726135\n",
      "Validation: Epoch [16], Batch [762/938], Loss: 0.6274524927139282\n",
      "Validation: Epoch [16], Batch [763/938], Loss: 0.45074284076690674\n",
      "Validation: Epoch [16], Batch [764/938], Loss: 0.3434281349182129\n",
      "Validation: Epoch [16], Batch [765/938], Loss: 0.33161240816116333\n",
      "Validation: Epoch [16], Batch [766/938], Loss: 0.43131834268569946\n",
      "Validation: Epoch [16], Batch [767/938], Loss: 0.40974751114845276\n",
      "Validation: Epoch [16], Batch [768/938], Loss: 0.5012212991714478\n",
      "Validation: Epoch [16], Batch [769/938], Loss: 0.5491766929626465\n",
      "Validation: Epoch [16], Batch [770/938], Loss: 0.4203122854232788\n",
      "Validation: Epoch [16], Batch [771/938], Loss: 0.4778703451156616\n",
      "Validation: Epoch [16], Batch [772/938], Loss: 0.41893041133880615\n",
      "Validation: Epoch [16], Batch [773/938], Loss: 0.4229700565338135\n",
      "Validation: Epoch [16], Batch [774/938], Loss: 0.2954557538032532\n",
      "Validation: Epoch [16], Batch [775/938], Loss: 0.20881156623363495\n",
      "Validation: Epoch [16], Batch [776/938], Loss: 0.4299575686454773\n",
      "Validation: Epoch [16], Batch [777/938], Loss: 0.5399359464645386\n",
      "Validation: Epoch [16], Batch [778/938], Loss: 0.4544750452041626\n",
      "Validation: Epoch [16], Batch [779/938], Loss: 0.404221773147583\n",
      "Validation: Epoch [16], Batch [780/938], Loss: 0.5145294666290283\n",
      "Validation: Epoch [16], Batch [781/938], Loss: 0.33757901191711426\n",
      "Validation: Epoch [16], Batch [782/938], Loss: 0.4566270411014557\n",
      "Validation: Epoch [16], Batch [783/938], Loss: 0.4378308057785034\n",
      "Validation: Epoch [16], Batch [784/938], Loss: 0.6598899364471436\n",
      "Validation: Epoch [16], Batch [785/938], Loss: 0.4250233769416809\n",
      "Validation: Epoch [16], Batch [786/938], Loss: 0.40094566345214844\n",
      "Validation: Epoch [16], Batch [787/938], Loss: 0.40538716316223145\n",
      "Validation: Epoch [16], Batch [788/938], Loss: 0.479228138923645\n",
      "Validation: Epoch [16], Batch [789/938], Loss: 0.46634745597839355\n",
      "Validation: Epoch [16], Batch [790/938], Loss: 0.4685862362384796\n",
      "Validation: Epoch [16], Batch [791/938], Loss: 0.4889450669288635\n",
      "Validation: Epoch [16], Batch [792/938], Loss: 0.34314823150634766\n",
      "Validation: Epoch [16], Batch [793/938], Loss: 0.6140741109848022\n",
      "Validation: Epoch [16], Batch [794/938], Loss: 0.38340145349502563\n",
      "Validation: Epoch [16], Batch [795/938], Loss: 0.5436767339706421\n",
      "Validation: Epoch [16], Batch [796/938], Loss: 0.6688117980957031\n",
      "Validation: Epoch [16], Batch [797/938], Loss: 0.4059428572654724\n",
      "Validation: Epoch [16], Batch [798/938], Loss: 0.3476283550262451\n",
      "Validation: Epoch [16], Batch [799/938], Loss: 0.2543244957923889\n",
      "Validation: Epoch [16], Batch [800/938], Loss: 0.5333659648895264\n",
      "Validation: Epoch [16], Batch [801/938], Loss: 0.5531750917434692\n",
      "Validation: Epoch [16], Batch [802/938], Loss: 0.519432783126831\n",
      "Validation: Epoch [16], Batch [803/938], Loss: 0.4909082055091858\n",
      "Validation: Epoch [16], Batch [804/938], Loss: 0.515838623046875\n",
      "Validation: Epoch [16], Batch [805/938], Loss: 0.3851223587989807\n",
      "Validation: Epoch [16], Batch [806/938], Loss: 0.3993479907512665\n",
      "Validation: Epoch [16], Batch [807/938], Loss: 0.6057572364807129\n",
      "Validation: Epoch [16], Batch [808/938], Loss: 0.45881178975105286\n",
      "Validation: Epoch [16], Batch [809/938], Loss: 0.34360191226005554\n",
      "Validation: Epoch [16], Batch [810/938], Loss: 0.5385664701461792\n",
      "Validation: Epoch [16], Batch [811/938], Loss: 0.37670964002609253\n",
      "Validation: Epoch [16], Batch [812/938], Loss: 0.23796594142913818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [16], Batch [813/938], Loss: 0.40091702342033386\n",
      "Validation: Epoch [16], Batch [814/938], Loss: 0.39864927530288696\n",
      "Validation: Epoch [16], Batch [815/938], Loss: 0.3934021592140198\n",
      "Validation: Epoch [16], Batch [816/938], Loss: 0.3496211767196655\n",
      "Validation: Epoch [16], Batch [817/938], Loss: 0.2352532148361206\n",
      "Validation: Epoch [16], Batch [818/938], Loss: 0.3565293550491333\n",
      "Validation: Epoch [16], Batch [819/938], Loss: 0.4232475161552429\n",
      "Validation: Epoch [16], Batch [820/938], Loss: 0.5462015271186829\n",
      "Validation: Epoch [16], Batch [821/938], Loss: 0.41720107197761536\n",
      "Validation: Epoch [16], Batch [822/938], Loss: 0.482340931892395\n",
      "Validation: Epoch [16], Batch [823/938], Loss: 0.6352336406707764\n",
      "Validation: Epoch [16], Batch [824/938], Loss: 0.5914931893348694\n",
      "Validation: Epoch [16], Batch [825/938], Loss: 0.5694535970687866\n",
      "Validation: Epoch [16], Batch [826/938], Loss: 0.43497219681739807\n",
      "Validation: Epoch [16], Batch [827/938], Loss: 0.4372061491012573\n",
      "Validation: Epoch [16], Batch [828/938], Loss: 0.5989990234375\n",
      "Validation: Epoch [16], Batch [829/938], Loss: 0.4918481111526489\n",
      "Validation: Epoch [16], Batch [830/938], Loss: 0.36513829231262207\n",
      "Validation: Epoch [16], Batch [831/938], Loss: 0.3810529112815857\n",
      "Validation: Epoch [16], Batch [832/938], Loss: 0.664015531539917\n",
      "Validation: Epoch [16], Batch [833/938], Loss: 0.3911064565181732\n",
      "Validation: Epoch [16], Batch [834/938], Loss: 0.36551806330680847\n",
      "Validation: Epoch [16], Batch [835/938], Loss: 0.4702910780906677\n",
      "Validation: Epoch [16], Batch [836/938], Loss: 0.6219273209571838\n",
      "Validation: Epoch [16], Batch [837/938], Loss: 0.3292697072029114\n",
      "Validation: Epoch [16], Batch [838/938], Loss: 0.49487677216529846\n",
      "Validation: Epoch [16], Batch [839/938], Loss: 0.37009286880493164\n",
      "Validation: Epoch [16], Batch [840/938], Loss: 0.3553113341331482\n",
      "Validation: Epoch [16], Batch [841/938], Loss: 0.4321545958518982\n",
      "Validation: Epoch [16], Batch [842/938], Loss: 0.5534736514091492\n",
      "Validation: Epoch [16], Batch [843/938], Loss: 0.4245649576187134\n",
      "Validation: Epoch [16], Batch [844/938], Loss: 0.6151288151741028\n",
      "Validation: Epoch [16], Batch [845/938], Loss: 0.5255855321884155\n",
      "Validation: Epoch [16], Batch [846/938], Loss: 0.4059881567955017\n",
      "Validation: Epoch [16], Batch [847/938], Loss: 0.29290899634361267\n",
      "Validation: Epoch [16], Batch [848/938], Loss: 0.43314921855926514\n",
      "Validation: Epoch [16], Batch [849/938], Loss: 0.6062111854553223\n",
      "Validation: Epoch [16], Batch [850/938], Loss: 0.4753766357898712\n",
      "Validation: Epoch [16], Batch [851/938], Loss: 0.3592875301837921\n",
      "Validation: Epoch [16], Batch [852/938], Loss: 0.30297213792800903\n",
      "Validation: Epoch [16], Batch [853/938], Loss: 0.49394240975379944\n",
      "Validation: Epoch [16], Batch [854/938], Loss: 0.4379499852657318\n",
      "Validation: Epoch [16], Batch [855/938], Loss: 0.31204208731651306\n",
      "Validation: Epoch [16], Batch [856/938], Loss: 0.4719141125679016\n",
      "Validation: Epoch [16], Batch [857/938], Loss: 0.47814247012138367\n",
      "Validation: Epoch [16], Batch [858/938], Loss: 0.6593914031982422\n",
      "Validation: Epoch [16], Batch [859/938], Loss: 0.3143150806427002\n",
      "Validation: Epoch [16], Batch [860/938], Loss: 0.30550381541252136\n",
      "Validation: Epoch [16], Batch [861/938], Loss: 0.40025025606155396\n",
      "Validation: Epoch [16], Batch [862/938], Loss: 0.4462472200393677\n",
      "Validation: Epoch [16], Batch [863/938], Loss: 0.37554463744163513\n",
      "Validation: Epoch [16], Batch [864/938], Loss: 0.440986692905426\n",
      "Validation: Epoch [16], Batch [865/938], Loss: 0.3626614809036255\n",
      "Validation: Epoch [16], Batch [866/938], Loss: 0.39229685068130493\n",
      "Validation: Epoch [16], Batch [867/938], Loss: 0.27535372972488403\n",
      "Validation: Epoch [16], Batch [868/938], Loss: 0.4952142834663391\n",
      "Validation: Epoch [16], Batch [869/938], Loss: 0.2944890856742859\n",
      "Validation: Epoch [16], Batch [870/938], Loss: 0.358853280544281\n",
      "Validation: Epoch [16], Batch [871/938], Loss: 0.4858742952346802\n",
      "Validation: Epoch [16], Batch [872/938], Loss: 0.3700842261314392\n",
      "Validation: Epoch [16], Batch [873/938], Loss: 0.5494841933250427\n",
      "Validation: Epoch [16], Batch [874/938], Loss: 0.4471553564071655\n",
      "Validation: Epoch [16], Batch [875/938], Loss: 0.39869511127471924\n",
      "Validation: Epoch [16], Batch [876/938], Loss: 0.3273649215698242\n",
      "Validation: Epoch [16], Batch [877/938], Loss: 0.3278377950191498\n",
      "Validation: Epoch [16], Batch [878/938], Loss: 0.4552651345729828\n",
      "Validation: Epoch [16], Batch [879/938], Loss: 0.33038657903671265\n",
      "Validation: Epoch [16], Batch [880/938], Loss: 0.45734381675720215\n",
      "Validation: Epoch [16], Batch [881/938], Loss: 0.3452003002166748\n",
      "Validation: Epoch [16], Batch [882/938], Loss: 0.3677268326282501\n",
      "Validation: Epoch [16], Batch [883/938], Loss: 0.32550328969955444\n",
      "Validation: Epoch [16], Batch [884/938], Loss: 0.4333840012550354\n",
      "Validation: Epoch [16], Batch [885/938], Loss: 0.3596852421760559\n",
      "Validation: Epoch [16], Batch [886/938], Loss: 0.6082769632339478\n",
      "Validation: Epoch [16], Batch [887/938], Loss: 0.5337525010108948\n",
      "Validation: Epoch [16], Batch [888/938], Loss: 0.41514652967453003\n",
      "Validation: Epoch [16], Batch [889/938], Loss: 0.5207537412643433\n",
      "Validation: Epoch [16], Batch [890/938], Loss: 0.2598801851272583\n",
      "Validation: Epoch [16], Batch [891/938], Loss: 0.4304032325744629\n",
      "Validation: Epoch [16], Batch [892/938], Loss: 0.45635324716567993\n",
      "Validation: Epoch [16], Batch [893/938], Loss: 0.4579639732837677\n",
      "Validation: Epoch [16], Batch [894/938], Loss: 0.42084044218063354\n",
      "Validation: Epoch [16], Batch [895/938], Loss: 0.4215472936630249\n",
      "Validation: Epoch [16], Batch [896/938], Loss: 0.34787648916244507\n",
      "Validation: Epoch [16], Batch [897/938], Loss: 0.317952036857605\n",
      "Validation: Epoch [16], Batch [898/938], Loss: 0.46222665905952454\n",
      "Validation: Epoch [16], Batch [899/938], Loss: 0.4407346844673157\n",
      "Validation: Epoch [16], Batch [900/938], Loss: 0.32519829273223877\n",
      "Validation: Epoch [16], Batch [901/938], Loss: 0.4381501376628876\n",
      "Validation: Epoch [16], Batch [902/938], Loss: 0.3999020457267761\n",
      "Validation: Epoch [16], Batch [903/938], Loss: 0.5415956377983093\n",
      "Validation: Epoch [16], Batch [904/938], Loss: 0.28901636600494385\n",
      "Validation: Epoch [16], Batch [905/938], Loss: 0.5022953152656555\n",
      "Validation: Epoch [16], Batch [906/938], Loss: 0.33507949113845825\n",
      "Validation: Epoch [16], Batch [907/938], Loss: 0.31695881485939026\n",
      "Validation: Epoch [16], Batch [908/938], Loss: 0.2720678448677063\n",
      "Validation: Epoch [16], Batch [909/938], Loss: 0.5829133987426758\n",
      "Validation: Epoch [16], Batch [910/938], Loss: 0.37572216987609863\n",
      "Validation: Epoch [16], Batch [911/938], Loss: 0.6368539333343506\n",
      "Validation: Epoch [16], Batch [912/938], Loss: 0.38435879349708557\n",
      "Validation: Epoch [16], Batch [913/938], Loss: 0.5171754360198975\n",
      "Validation: Epoch [16], Batch [914/938], Loss: 0.6598421931266785\n",
      "Validation: Epoch [16], Batch [915/938], Loss: 0.43545183539390564\n",
      "Validation: Epoch [16], Batch [916/938], Loss: 0.4728390574455261\n",
      "Validation: Epoch [16], Batch [917/938], Loss: 0.5605929493904114\n",
      "Validation: Epoch [16], Batch [918/938], Loss: 0.22064265608787537\n",
      "Validation: Epoch [16], Batch [919/938], Loss: 0.3143649101257324\n",
      "Validation: Epoch [16], Batch [920/938], Loss: 0.3202240467071533\n",
      "Validation: Epoch [16], Batch [921/938], Loss: 0.4818328320980072\n",
      "Validation: Epoch [16], Batch [922/938], Loss: 0.49514663219451904\n",
      "Validation: Epoch [16], Batch [923/938], Loss: 0.4742629826068878\n",
      "Validation: Epoch [16], Batch [924/938], Loss: 0.396423876285553\n",
      "Validation: Epoch [16], Batch [925/938], Loss: 0.4721338450908661\n",
      "Validation: Epoch [16], Batch [926/938], Loss: 0.4988808333873749\n",
      "Validation: Epoch [16], Batch [927/938], Loss: 0.42683082818984985\n",
      "Validation: Epoch [16], Batch [928/938], Loss: 0.29033195972442627\n",
      "Validation: Epoch [16], Batch [929/938], Loss: 0.27563562989234924\n",
      "Validation: Epoch [16], Batch [930/938], Loss: 0.274009644985199\n",
      "Validation: Epoch [16], Batch [931/938], Loss: 0.40974533557891846\n",
      "Validation: Epoch [16], Batch [932/938], Loss: 0.30965086817741394\n",
      "Validation: Epoch [16], Batch [933/938], Loss: 0.37096279859542847\n",
      "Validation: Epoch [16], Batch [934/938], Loss: 0.5471441745758057\n",
      "Validation: Epoch [16], Batch [935/938], Loss: 0.28795215487480164\n",
      "Validation: Epoch [16], Batch [936/938], Loss: 0.21625205874443054\n",
      "Validation: Epoch [16], Batch [937/938], Loss: 0.3727506101131439\n",
      "Validation: Epoch [16], Batch [938/938], Loss: 0.34233179688453674\n",
      "Accuracy of test set: 0.84595\n",
      "Train: Epoch [17], Batch [1/938], Loss: 0.4350433349609375\n",
      "Train: Epoch [17], Batch [2/938], Loss: 0.5336093306541443\n",
      "Train: Epoch [17], Batch [3/938], Loss: 0.6123009324073792\n",
      "Train: Epoch [17], Batch [4/938], Loss: 0.5890713930130005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [5/938], Loss: 0.3409101366996765\n",
      "Train: Epoch [17], Batch [6/938], Loss: 0.40777307748794556\n",
      "Train: Epoch [17], Batch [7/938], Loss: 0.39418941736221313\n",
      "Train: Epoch [17], Batch [8/938], Loss: 0.47130441665649414\n",
      "Train: Epoch [17], Batch [9/938], Loss: 0.5026037693023682\n",
      "Train: Epoch [17], Batch [10/938], Loss: 0.44735631346702576\n",
      "Train: Epoch [17], Batch [11/938], Loss: 0.4518415927886963\n",
      "Train: Epoch [17], Batch [12/938], Loss: 0.580322802066803\n",
      "Train: Epoch [17], Batch [13/938], Loss: 0.3265419006347656\n",
      "Train: Epoch [17], Batch [14/938], Loss: 0.34127163887023926\n",
      "Train: Epoch [17], Batch [15/938], Loss: 0.35999050736427307\n",
      "Train: Epoch [17], Batch [16/938], Loss: 0.5564171075820923\n",
      "Train: Epoch [17], Batch [17/938], Loss: 0.38206836581230164\n",
      "Train: Epoch [17], Batch [18/938], Loss: 0.46714961528778076\n",
      "Train: Epoch [17], Batch [19/938], Loss: 0.4609795808792114\n",
      "Train: Epoch [17], Batch [20/938], Loss: 0.5706825256347656\n",
      "Train: Epoch [17], Batch [21/938], Loss: 0.4347172975540161\n",
      "Train: Epoch [17], Batch [22/938], Loss: 0.4107484817504883\n",
      "Train: Epoch [17], Batch [23/938], Loss: 0.6191678047180176\n",
      "Train: Epoch [17], Batch [24/938], Loss: 0.4055679142475128\n",
      "Train: Epoch [17], Batch [25/938], Loss: 0.295528769493103\n",
      "Train: Epoch [17], Batch [26/938], Loss: 0.5626941919326782\n",
      "Train: Epoch [17], Batch [27/938], Loss: 0.2870553135871887\n",
      "Train: Epoch [17], Batch [28/938], Loss: 0.32788729667663574\n",
      "Train: Epoch [17], Batch [29/938], Loss: 0.31441357731819153\n",
      "Train: Epoch [17], Batch [30/938], Loss: 0.3042605221271515\n",
      "Train: Epoch [17], Batch [31/938], Loss: 0.4663656949996948\n",
      "Train: Epoch [17], Batch [32/938], Loss: 0.5838842391967773\n",
      "Train: Epoch [17], Batch [33/938], Loss: 0.5742146968841553\n",
      "Train: Epoch [17], Batch [34/938], Loss: 0.382007360458374\n",
      "Train: Epoch [17], Batch [35/938], Loss: 0.4834885597229004\n",
      "Train: Epoch [17], Batch [36/938], Loss: 0.30495139956474304\n",
      "Train: Epoch [17], Batch [37/938], Loss: 0.4883272349834442\n",
      "Train: Epoch [17], Batch [38/938], Loss: 0.34980082511901855\n",
      "Train: Epoch [17], Batch [39/938], Loss: 0.4210401475429535\n",
      "Train: Epoch [17], Batch [40/938], Loss: 0.45122194290161133\n",
      "Train: Epoch [17], Batch [41/938], Loss: 0.39024996757507324\n",
      "Train: Epoch [17], Batch [42/938], Loss: 0.5193548202514648\n",
      "Train: Epoch [17], Batch [43/938], Loss: 0.5735886693000793\n",
      "Train: Epoch [17], Batch [44/938], Loss: 0.40386486053466797\n",
      "Train: Epoch [17], Batch [45/938], Loss: 0.5300911068916321\n",
      "Train: Epoch [17], Batch [46/938], Loss: 0.3228917121887207\n",
      "Train: Epoch [17], Batch [47/938], Loss: 0.37716203927993774\n",
      "Train: Epoch [17], Batch [48/938], Loss: 0.4425630569458008\n",
      "Train: Epoch [17], Batch [49/938], Loss: 0.6463319659233093\n",
      "Train: Epoch [17], Batch [50/938], Loss: 0.4146941304206848\n",
      "Train: Epoch [17], Batch [51/938], Loss: 0.29065772891044617\n",
      "Train: Epoch [17], Batch [52/938], Loss: 0.7134957313537598\n",
      "Train: Epoch [17], Batch [53/938], Loss: 0.5106474161148071\n",
      "Train: Epoch [17], Batch [54/938], Loss: 0.5512340068817139\n",
      "Train: Epoch [17], Batch [55/938], Loss: 0.40147462487220764\n",
      "Train: Epoch [17], Batch [56/938], Loss: 0.7460110187530518\n",
      "Train: Epoch [17], Batch [57/938], Loss: 0.41741061210632324\n",
      "Train: Epoch [17], Batch [58/938], Loss: 0.5079338550567627\n",
      "Train: Epoch [17], Batch [59/938], Loss: 0.41795599460601807\n",
      "Train: Epoch [17], Batch [60/938], Loss: 0.5080346465110779\n",
      "Train: Epoch [17], Batch [61/938], Loss: 0.4732699394226074\n",
      "Train: Epoch [17], Batch [62/938], Loss: 0.4159609377384186\n",
      "Train: Epoch [17], Batch [63/938], Loss: 0.37672680616378784\n",
      "Train: Epoch [17], Batch [64/938], Loss: 0.34858042001724243\n",
      "Train: Epoch [17], Batch [65/938], Loss: 0.6144813895225525\n",
      "Train: Epoch [17], Batch [66/938], Loss: 0.5293070077896118\n",
      "Train: Epoch [17], Batch [67/938], Loss: 0.34997308254241943\n",
      "Train: Epoch [17], Batch [68/938], Loss: 0.5030838251113892\n",
      "Train: Epoch [17], Batch [69/938], Loss: 0.41904786229133606\n",
      "Train: Epoch [17], Batch [70/938], Loss: 0.45981600880622864\n",
      "Train: Epoch [17], Batch [71/938], Loss: 0.6596649289131165\n",
      "Train: Epoch [17], Batch [72/938], Loss: 0.60161292552948\n",
      "Train: Epoch [17], Batch [73/938], Loss: 0.35437166690826416\n",
      "Train: Epoch [17], Batch [74/938], Loss: 0.47923821210861206\n",
      "Train: Epoch [17], Batch [75/938], Loss: 0.3550211787223816\n",
      "Train: Epoch [17], Batch [76/938], Loss: 0.5695343017578125\n",
      "Train: Epoch [17], Batch [77/938], Loss: 0.42935630679130554\n",
      "Train: Epoch [17], Batch [78/938], Loss: 0.5638278722763062\n",
      "Train: Epoch [17], Batch [79/938], Loss: 0.320196270942688\n",
      "Train: Epoch [17], Batch [80/938], Loss: 0.4687091112136841\n",
      "Train: Epoch [17], Batch [81/938], Loss: 0.5801044702529907\n",
      "Train: Epoch [17], Batch [82/938], Loss: 0.2897360324859619\n",
      "Train: Epoch [17], Batch [83/938], Loss: 0.5211467146873474\n",
      "Train: Epoch [17], Batch [84/938], Loss: 0.6471975445747375\n",
      "Train: Epoch [17], Batch [85/938], Loss: 0.30398306250572205\n",
      "Train: Epoch [17], Batch [86/938], Loss: 0.39118582010269165\n",
      "Train: Epoch [17], Batch [87/938], Loss: 0.4330444037914276\n",
      "Train: Epoch [17], Batch [88/938], Loss: 0.42525482177734375\n",
      "Train: Epoch [17], Batch [89/938], Loss: 0.4787524342536926\n",
      "Train: Epoch [17], Batch [90/938], Loss: 0.35769596695899963\n",
      "Train: Epoch [17], Batch [91/938], Loss: 0.3888465464115143\n",
      "Train: Epoch [17], Batch [92/938], Loss: 0.5456116199493408\n",
      "Train: Epoch [17], Batch [93/938], Loss: 0.510111391544342\n",
      "Train: Epoch [17], Batch [94/938], Loss: 0.4466148912906647\n",
      "Train: Epoch [17], Batch [95/938], Loss: 0.3327258825302124\n",
      "Train: Epoch [17], Batch [96/938], Loss: 0.5004727840423584\n",
      "Train: Epoch [17], Batch [97/938], Loss: 0.24605384469032288\n",
      "Train: Epoch [17], Batch [98/938], Loss: 0.342237651348114\n",
      "Train: Epoch [17], Batch [99/938], Loss: 0.3033619225025177\n",
      "Train: Epoch [17], Batch [100/938], Loss: 0.6059125661849976\n",
      "Train: Epoch [17], Batch [101/938], Loss: 0.530608057975769\n",
      "Train: Epoch [17], Batch [102/938], Loss: 0.30141714215278625\n",
      "Train: Epoch [17], Batch [103/938], Loss: 0.3495675325393677\n",
      "Train: Epoch [17], Batch [104/938], Loss: 0.6283199191093445\n",
      "Train: Epoch [17], Batch [105/938], Loss: 0.29029759764671326\n",
      "Train: Epoch [17], Batch [106/938], Loss: 0.4094589352607727\n",
      "Train: Epoch [17], Batch [107/938], Loss: 0.44221916794776917\n",
      "Train: Epoch [17], Batch [108/938], Loss: 0.5417779684066772\n",
      "Train: Epoch [17], Batch [109/938], Loss: 0.44371965527534485\n",
      "Train: Epoch [17], Batch [110/938], Loss: 0.5103152990341187\n",
      "Train: Epoch [17], Batch [111/938], Loss: 0.34201428294181824\n",
      "Train: Epoch [17], Batch [112/938], Loss: 0.30532559752464294\n",
      "Train: Epoch [17], Batch [113/938], Loss: 0.31145280599594116\n",
      "Train: Epoch [17], Batch [114/938], Loss: 0.3497410714626312\n",
      "Train: Epoch [17], Batch [115/938], Loss: 0.5889213681221008\n",
      "Train: Epoch [17], Batch [116/938], Loss: 0.5356239676475525\n",
      "Train: Epoch [17], Batch [117/938], Loss: 0.5965148210525513\n",
      "Train: Epoch [17], Batch [118/938], Loss: 0.43399766087532043\n",
      "Train: Epoch [17], Batch [119/938], Loss: 0.3459571897983551\n",
      "Train: Epoch [17], Batch [120/938], Loss: 0.2888149619102478\n",
      "Train: Epoch [17], Batch [121/938], Loss: 0.40046870708465576\n",
      "Train: Epoch [17], Batch [122/938], Loss: 0.26286569237709045\n",
      "Train: Epoch [17], Batch [123/938], Loss: 0.3474210798740387\n",
      "Train: Epoch [17], Batch [124/938], Loss: 0.36805281043052673\n",
      "Train: Epoch [17], Batch [125/938], Loss: 0.48188263177871704\n",
      "Train: Epoch [17], Batch [126/938], Loss: 0.4519854485988617\n",
      "Train: Epoch [17], Batch [127/938], Loss: 0.7476480007171631\n",
      "Train: Epoch [17], Batch [128/938], Loss: 0.35818716883659363\n",
      "Train: Epoch [17], Batch [129/938], Loss: 0.46213772892951965\n",
      "Train: Epoch [17], Batch [130/938], Loss: 0.3373625576496124\n",
      "Train: Epoch [17], Batch [131/938], Loss: 0.3456772267818451\n",
      "Train: Epoch [17], Batch [132/938], Loss: 0.38388538360595703\n",
      "Train: Epoch [17], Batch [133/938], Loss: 0.5402214527130127\n",
      "Train: Epoch [17], Batch [134/938], Loss: 0.5949766039848328\n",
      "Train: Epoch [17], Batch [135/938], Loss: 0.4946742355823517\n",
      "Train: Epoch [17], Batch [136/938], Loss: 0.25401759147644043\n",
      "Train: Epoch [17], Batch [137/938], Loss: 0.5846893787384033\n",
      "Train: Epoch [17], Batch [138/938], Loss: 0.41907644271850586\n",
      "Train: Epoch [17], Batch [139/938], Loss: 0.5250294208526611\n",
      "Train: Epoch [17], Batch [140/938], Loss: 0.2624736428260803\n",
      "Train: Epoch [17], Batch [141/938], Loss: 0.3008754849433899\n",
      "Train: Epoch [17], Batch [142/938], Loss: 0.63980633020401\n",
      "Train: Epoch [17], Batch [143/938], Loss: 0.338489294052124\n",
      "Train: Epoch [17], Batch [144/938], Loss: 0.4097558259963989\n",
      "Train: Epoch [17], Batch [145/938], Loss: 0.6168853044509888\n",
      "Train: Epoch [17], Batch [146/938], Loss: 0.5091370940208435\n",
      "Train: Epoch [17], Batch [147/938], Loss: 0.390442430973053\n",
      "Train: Epoch [17], Batch [148/938], Loss: 0.34268641471862793\n",
      "Train: Epoch [17], Batch [149/938], Loss: 0.4333764910697937\n",
      "Train: Epoch [17], Batch [150/938], Loss: 0.5037277936935425\n",
      "Train: Epoch [17], Batch [151/938], Loss: 0.40460240840911865\n",
      "Train: Epoch [17], Batch [152/938], Loss: 0.2939493656158447\n",
      "Train: Epoch [17], Batch [153/938], Loss: 0.5793659687042236\n",
      "Train: Epoch [17], Batch [154/938], Loss: 0.4964314103126526\n",
      "Train: Epoch [17], Batch [155/938], Loss: 0.37551796436309814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [156/938], Loss: 0.48405927419662476\n",
      "Train: Epoch [17], Batch [157/938], Loss: 0.350629985332489\n",
      "Train: Epoch [17], Batch [158/938], Loss: 0.44111189246177673\n",
      "Train: Epoch [17], Batch [159/938], Loss: 0.4438084661960602\n",
      "Train: Epoch [17], Batch [160/938], Loss: 0.4012140929698944\n",
      "Train: Epoch [17], Batch [161/938], Loss: 0.4841330647468567\n",
      "Train: Epoch [17], Batch [162/938], Loss: 0.6226685047149658\n",
      "Train: Epoch [17], Batch [163/938], Loss: 0.5380191802978516\n",
      "Train: Epoch [17], Batch [164/938], Loss: 0.49335777759552\n",
      "Train: Epoch [17], Batch [165/938], Loss: 0.407590389251709\n",
      "Train: Epoch [17], Batch [166/938], Loss: 0.37266772985458374\n",
      "Train: Epoch [17], Batch [167/938], Loss: 0.5973694324493408\n",
      "Train: Epoch [17], Batch [168/938], Loss: 0.6593244075775146\n",
      "Train: Epoch [17], Batch [169/938], Loss: 0.7740561366081238\n",
      "Train: Epoch [17], Batch [170/938], Loss: 0.3686639070510864\n",
      "Train: Epoch [17], Batch [171/938], Loss: 0.4118267595767975\n",
      "Train: Epoch [17], Batch [172/938], Loss: 0.391883909702301\n",
      "Train: Epoch [17], Batch [173/938], Loss: 0.31019753217697144\n",
      "Train: Epoch [17], Batch [174/938], Loss: 0.3881221413612366\n",
      "Train: Epoch [17], Batch [175/938], Loss: 0.46139687299728394\n",
      "Train: Epoch [17], Batch [176/938], Loss: 0.2885626256465912\n",
      "Train: Epoch [17], Batch [177/938], Loss: 0.25375041365623474\n",
      "Train: Epoch [17], Batch [178/938], Loss: 0.5266907215118408\n",
      "Train: Epoch [17], Batch [179/938], Loss: 0.4880222678184509\n",
      "Train: Epoch [17], Batch [180/938], Loss: 0.4264586269855499\n",
      "Train: Epoch [17], Batch [181/938], Loss: 0.5819137096405029\n",
      "Train: Epoch [17], Batch [182/938], Loss: 0.3596509099006653\n",
      "Train: Epoch [17], Batch [183/938], Loss: 0.3281131386756897\n",
      "Train: Epoch [17], Batch [184/938], Loss: 0.42256325483322144\n",
      "Train: Epoch [17], Batch [185/938], Loss: 0.38185152411460876\n",
      "Train: Epoch [17], Batch [186/938], Loss: 0.3375220000743866\n",
      "Train: Epoch [17], Batch [187/938], Loss: 0.4204339385032654\n",
      "Train: Epoch [17], Batch [188/938], Loss: 0.40934622287750244\n",
      "Train: Epoch [17], Batch [189/938], Loss: 0.299226850271225\n",
      "Train: Epoch [17], Batch [190/938], Loss: 0.44353413581848145\n",
      "Train: Epoch [17], Batch [191/938], Loss: 0.5129227638244629\n",
      "Train: Epoch [17], Batch [192/938], Loss: 0.5141982436180115\n",
      "Train: Epoch [17], Batch [193/938], Loss: 0.33682435750961304\n",
      "Train: Epoch [17], Batch [194/938], Loss: 0.37649181485176086\n",
      "Train: Epoch [17], Batch [195/938], Loss: 0.4182422161102295\n",
      "Train: Epoch [17], Batch [196/938], Loss: 0.36320960521698\n",
      "Train: Epoch [17], Batch [197/938], Loss: 0.36294054985046387\n",
      "Train: Epoch [17], Batch [198/938], Loss: 0.4145045280456543\n",
      "Train: Epoch [17], Batch [199/938], Loss: 0.4946526885032654\n",
      "Train: Epoch [17], Batch [200/938], Loss: 0.5083649754524231\n",
      "Train: Epoch [17], Batch [201/938], Loss: 0.43331974744796753\n",
      "Train: Epoch [17], Batch [202/938], Loss: 0.7114870548248291\n",
      "Train: Epoch [17], Batch [203/938], Loss: 0.31448066234588623\n",
      "Train: Epoch [17], Batch [204/938], Loss: 0.5237903594970703\n",
      "Train: Epoch [17], Batch [205/938], Loss: 0.6000568270683289\n",
      "Train: Epoch [17], Batch [206/938], Loss: 0.5326972007751465\n",
      "Train: Epoch [17], Batch [207/938], Loss: 0.3785412311553955\n",
      "Train: Epoch [17], Batch [208/938], Loss: 0.4087868928909302\n",
      "Train: Epoch [17], Batch [209/938], Loss: 0.37128719687461853\n",
      "Train: Epoch [17], Batch [210/938], Loss: 0.37954676151275635\n",
      "Train: Epoch [17], Batch [211/938], Loss: 0.42401984333992004\n",
      "Train: Epoch [17], Batch [212/938], Loss: 0.45124366879463196\n",
      "Train: Epoch [17], Batch [213/938], Loss: 0.5217469930648804\n",
      "Train: Epoch [17], Batch [214/938], Loss: 0.38413006067276\n",
      "Train: Epoch [17], Batch [215/938], Loss: 0.38799619674682617\n",
      "Train: Epoch [17], Batch [216/938], Loss: 0.3544429838657379\n",
      "Train: Epoch [17], Batch [217/938], Loss: 0.35334205627441406\n",
      "Train: Epoch [17], Batch [218/938], Loss: 0.34627199172973633\n",
      "Train: Epoch [17], Batch [219/938], Loss: 0.3438825011253357\n",
      "Train: Epoch [17], Batch [220/938], Loss: 0.5544436573982239\n",
      "Train: Epoch [17], Batch [221/938], Loss: 0.47050023078918457\n",
      "Train: Epoch [17], Batch [222/938], Loss: 0.4761175513267517\n",
      "Train: Epoch [17], Batch [223/938], Loss: 0.4969729483127594\n",
      "Train: Epoch [17], Batch [224/938], Loss: 0.5154684782028198\n",
      "Train: Epoch [17], Batch [225/938], Loss: 0.4760383367538452\n",
      "Train: Epoch [17], Batch [226/938], Loss: 0.40080323815345764\n",
      "Train: Epoch [17], Batch [227/938], Loss: 0.5599750876426697\n",
      "Train: Epoch [17], Batch [228/938], Loss: 0.4576614499092102\n",
      "Train: Epoch [17], Batch [229/938], Loss: 0.6498475074768066\n",
      "Train: Epoch [17], Batch [230/938], Loss: 0.3735736608505249\n",
      "Train: Epoch [17], Batch [231/938], Loss: 0.5875810384750366\n",
      "Train: Epoch [17], Batch [232/938], Loss: 0.3714755177497864\n",
      "Train: Epoch [17], Batch [233/938], Loss: 0.45735037326812744\n",
      "Train: Epoch [17], Batch [234/938], Loss: 0.4596281349658966\n",
      "Train: Epoch [17], Batch [235/938], Loss: 0.3497408926486969\n",
      "Train: Epoch [17], Batch [236/938], Loss: 0.2839382290840149\n",
      "Train: Epoch [17], Batch [237/938], Loss: 0.5595085620880127\n",
      "Train: Epoch [17], Batch [238/938], Loss: 0.4123683571815491\n",
      "Train: Epoch [17], Batch [239/938], Loss: 0.4496231973171234\n",
      "Train: Epoch [17], Batch [240/938], Loss: 0.41715097427368164\n",
      "Train: Epoch [17], Batch [241/938], Loss: 0.4656038284301758\n",
      "Train: Epoch [17], Batch [242/938], Loss: 0.4170636236667633\n",
      "Train: Epoch [17], Batch [243/938], Loss: 0.449964314699173\n",
      "Train: Epoch [17], Batch [244/938], Loss: 0.5020616054534912\n",
      "Train: Epoch [17], Batch [245/938], Loss: 0.49632352590560913\n",
      "Train: Epoch [17], Batch [246/938], Loss: 0.446022093296051\n",
      "Train: Epoch [17], Batch [247/938], Loss: 0.5442509651184082\n",
      "Train: Epoch [17], Batch [248/938], Loss: 0.15316587686538696\n",
      "Train: Epoch [17], Batch [249/938], Loss: 0.33131539821624756\n",
      "Train: Epoch [17], Batch [250/938], Loss: 0.27014732360839844\n",
      "Train: Epoch [17], Batch [251/938], Loss: 0.6845357418060303\n",
      "Train: Epoch [17], Batch [252/938], Loss: 0.3802444636821747\n",
      "Train: Epoch [17], Batch [253/938], Loss: 0.5105751156806946\n",
      "Train: Epoch [17], Batch [254/938], Loss: 0.4263683259487152\n",
      "Train: Epoch [17], Batch [255/938], Loss: 0.45001158118247986\n",
      "Train: Epoch [17], Batch [256/938], Loss: 0.5179980397224426\n",
      "Train: Epoch [17], Batch [257/938], Loss: 0.5146381855010986\n",
      "Train: Epoch [17], Batch [258/938], Loss: 0.34150204062461853\n",
      "Train: Epoch [17], Batch [259/938], Loss: 0.5049512982368469\n",
      "Train: Epoch [17], Batch [260/938], Loss: 0.37715426087379456\n",
      "Train: Epoch [17], Batch [261/938], Loss: 0.4057661294937134\n",
      "Train: Epoch [17], Batch [262/938], Loss: 0.44158968329429626\n",
      "Train: Epoch [17], Batch [263/938], Loss: 0.31339216232299805\n",
      "Train: Epoch [17], Batch [264/938], Loss: 0.4564729630947113\n",
      "Train: Epoch [17], Batch [265/938], Loss: 1.0373989343643188\n",
      "Train: Epoch [17], Batch [266/938], Loss: 0.5251892805099487\n",
      "Train: Epoch [17], Batch [267/938], Loss: 0.3871973156929016\n",
      "Train: Epoch [17], Batch [268/938], Loss: 0.5688867568969727\n",
      "Train: Epoch [17], Batch [269/938], Loss: 0.47917044162750244\n",
      "Train: Epoch [17], Batch [270/938], Loss: 0.4170381724834442\n",
      "Train: Epoch [17], Batch [271/938], Loss: 0.6509623527526855\n",
      "Train: Epoch [17], Batch [272/938], Loss: 0.5204522609710693\n",
      "Train: Epoch [17], Batch [273/938], Loss: 0.4401155114173889\n",
      "Train: Epoch [17], Batch [274/938], Loss: 0.46813803911209106\n",
      "Train: Epoch [17], Batch [275/938], Loss: 0.6048808693885803\n",
      "Train: Epoch [17], Batch [276/938], Loss: 0.6552647948265076\n",
      "Train: Epoch [17], Batch [277/938], Loss: 0.5644859075546265\n",
      "Train: Epoch [17], Batch [278/938], Loss: 0.4855060577392578\n",
      "Train: Epoch [17], Batch [279/938], Loss: 0.629905104637146\n",
      "Train: Epoch [17], Batch [280/938], Loss: 0.3975284993648529\n",
      "Train: Epoch [17], Batch [281/938], Loss: 0.4682590961456299\n",
      "Train: Epoch [17], Batch [282/938], Loss: 0.40878796577453613\n",
      "Train: Epoch [17], Batch [283/938], Loss: 0.722069501876831\n",
      "Train: Epoch [17], Batch [284/938], Loss: 0.4570133686065674\n",
      "Train: Epoch [17], Batch [285/938], Loss: 0.3876384496688843\n",
      "Train: Epoch [17], Batch [286/938], Loss: 0.5369322299957275\n",
      "Train: Epoch [17], Batch [287/938], Loss: 0.6022658348083496\n",
      "Train: Epoch [17], Batch [288/938], Loss: 0.4370957314968109\n",
      "Train: Epoch [17], Batch [289/938], Loss: 0.4703960120677948\n",
      "Train: Epoch [17], Batch [290/938], Loss: 0.4040323495864868\n",
      "Train: Epoch [17], Batch [291/938], Loss: 0.49105653166770935\n",
      "Train: Epoch [17], Batch [292/938], Loss: 0.6583676338195801\n",
      "Train: Epoch [17], Batch [293/938], Loss: 0.47298112511634827\n",
      "Train: Epoch [17], Batch [294/938], Loss: 0.5926828384399414\n",
      "Train: Epoch [17], Batch [295/938], Loss: 0.3799673914909363\n",
      "Train: Epoch [17], Batch [296/938], Loss: 0.588642954826355\n",
      "Train: Epoch [17], Batch [297/938], Loss: 0.3360544741153717\n",
      "Train: Epoch [17], Batch [298/938], Loss: 0.6131384372711182\n",
      "Train: Epoch [17], Batch [299/938], Loss: 0.2650218605995178\n",
      "Train: Epoch [17], Batch [300/938], Loss: 0.4406670928001404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [301/938], Loss: 0.4517216384410858\n",
      "Train: Epoch [17], Batch [302/938], Loss: 0.2776528298854828\n",
      "Train: Epoch [17], Batch [303/938], Loss: 0.4769465923309326\n",
      "Train: Epoch [17], Batch [304/938], Loss: 0.46227431297302246\n",
      "Train: Epoch [17], Batch [305/938], Loss: 0.5649769306182861\n",
      "Train: Epoch [17], Batch [306/938], Loss: 0.4183286428451538\n",
      "Train: Epoch [17], Batch [307/938], Loss: 0.3537081480026245\n",
      "Train: Epoch [17], Batch [308/938], Loss: 0.4025716781616211\n",
      "Train: Epoch [17], Batch [309/938], Loss: 0.3300155699253082\n",
      "Train: Epoch [17], Batch [310/938], Loss: 0.5047845244407654\n",
      "Train: Epoch [17], Batch [311/938], Loss: 0.47013404965400696\n",
      "Train: Epoch [17], Batch [312/938], Loss: 0.3199458420276642\n",
      "Train: Epoch [17], Batch [313/938], Loss: 0.32932233810424805\n",
      "Train: Epoch [17], Batch [314/938], Loss: 0.39692896604537964\n",
      "Train: Epoch [17], Batch [315/938], Loss: 0.4944702386856079\n",
      "Train: Epoch [17], Batch [316/938], Loss: 0.4966995120048523\n",
      "Train: Epoch [17], Batch [317/938], Loss: 0.5361520051956177\n",
      "Train: Epoch [17], Batch [318/938], Loss: 0.45353808999061584\n",
      "Train: Epoch [17], Batch [319/938], Loss: 0.3272736072540283\n",
      "Train: Epoch [17], Batch [320/938], Loss: 0.40506038069725037\n",
      "Train: Epoch [17], Batch [321/938], Loss: 0.5286088585853577\n",
      "Train: Epoch [17], Batch [322/938], Loss: 0.4830712378025055\n",
      "Train: Epoch [17], Batch [323/938], Loss: 0.45756661891937256\n",
      "Train: Epoch [17], Batch [324/938], Loss: 0.4004117250442505\n",
      "Train: Epoch [17], Batch [325/938], Loss: 0.42279261350631714\n",
      "Train: Epoch [17], Batch [326/938], Loss: 0.5920335054397583\n",
      "Train: Epoch [17], Batch [327/938], Loss: 0.6507823467254639\n",
      "Train: Epoch [17], Batch [328/938], Loss: 0.4813791513442993\n",
      "Train: Epoch [17], Batch [329/938], Loss: 0.6677742600440979\n",
      "Train: Epoch [17], Batch [330/938], Loss: 0.36476564407348633\n",
      "Train: Epoch [17], Batch [331/938], Loss: 0.34247708320617676\n",
      "Train: Epoch [17], Batch [332/938], Loss: 0.24372045695781708\n",
      "Train: Epoch [17], Batch [333/938], Loss: 0.4986274838447571\n",
      "Train: Epoch [17], Batch [334/938], Loss: 0.43450498580932617\n",
      "Train: Epoch [17], Batch [335/938], Loss: 0.41969338059425354\n",
      "Train: Epoch [17], Batch [336/938], Loss: 0.47855156660079956\n",
      "Train: Epoch [17], Batch [337/938], Loss: 0.5343286395072937\n",
      "Train: Epoch [17], Batch [338/938], Loss: 0.4062047004699707\n",
      "Train: Epoch [17], Batch [339/938], Loss: 0.45821359753608704\n",
      "Train: Epoch [17], Batch [340/938], Loss: 0.34363800287246704\n",
      "Train: Epoch [17], Batch [341/938], Loss: 0.6836671233177185\n",
      "Train: Epoch [17], Batch [342/938], Loss: 0.5929629802703857\n",
      "Train: Epoch [17], Batch [343/938], Loss: 0.4198777973651886\n",
      "Train: Epoch [17], Batch [344/938], Loss: 0.3861391544342041\n",
      "Train: Epoch [17], Batch [345/938], Loss: 0.4950665533542633\n",
      "Train: Epoch [17], Batch [346/938], Loss: 0.45896440744400024\n",
      "Train: Epoch [17], Batch [347/938], Loss: 0.6412643790245056\n",
      "Train: Epoch [17], Batch [348/938], Loss: 0.39837077260017395\n",
      "Train: Epoch [17], Batch [349/938], Loss: 0.5717391967773438\n",
      "Train: Epoch [17], Batch [350/938], Loss: 0.5561048984527588\n",
      "Train: Epoch [17], Batch [351/938], Loss: 0.34765344858169556\n",
      "Train: Epoch [17], Batch [352/938], Loss: 0.4918757677078247\n",
      "Train: Epoch [17], Batch [353/938], Loss: 0.5667243003845215\n",
      "Train: Epoch [17], Batch [354/938], Loss: 0.35882505774497986\n",
      "Train: Epoch [17], Batch [355/938], Loss: 0.2877650856971741\n",
      "Train: Epoch [17], Batch [356/938], Loss: 0.4576582610607147\n",
      "Train: Epoch [17], Batch [357/938], Loss: 0.42247939109802246\n",
      "Train: Epoch [17], Batch [358/938], Loss: 0.4268661439418793\n",
      "Train: Epoch [17], Batch [359/938], Loss: 0.34183984994888306\n",
      "Train: Epoch [17], Batch [360/938], Loss: 0.5316817760467529\n",
      "Train: Epoch [17], Batch [361/938], Loss: 0.3347204029560089\n",
      "Train: Epoch [17], Batch [362/938], Loss: 0.4834669828414917\n",
      "Train: Epoch [17], Batch [363/938], Loss: 0.7395409345626831\n",
      "Train: Epoch [17], Batch [364/938], Loss: 0.49086135625839233\n",
      "Train: Epoch [17], Batch [365/938], Loss: 0.4062083959579468\n",
      "Train: Epoch [17], Batch [366/938], Loss: 0.4922940135002136\n",
      "Train: Epoch [17], Batch [367/938], Loss: 0.5811169147491455\n",
      "Train: Epoch [17], Batch [368/938], Loss: 0.38807886838912964\n",
      "Train: Epoch [17], Batch [369/938], Loss: 0.2965121269226074\n",
      "Train: Epoch [17], Batch [370/938], Loss: 0.4032074213027954\n",
      "Train: Epoch [17], Batch [371/938], Loss: 0.6185546517372131\n",
      "Train: Epoch [17], Batch [372/938], Loss: 0.5485146045684814\n",
      "Train: Epoch [17], Batch [373/938], Loss: 0.435922771692276\n",
      "Train: Epoch [17], Batch [374/938], Loss: 0.29941827058792114\n",
      "Train: Epoch [17], Batch [375/938], Loss: 0.37033581733703613\n",
      "Train: Epoch [17], Batch [376/938], Loss: 0.3906705677509308\n",
      "Train: Epoch [17], Batch [377/938], Loss: 0.44083189964294434\n",
      "Train: Epoch [17], Batch [378/938], Loss: 0.4493231177330017\n",
      "Train: Epoch [17], Batch [379/938], Loss: 0.6254643201828003\n",
      "Train: Epoch [17], Batch [380/938], Loss: 0.46558576822280884\n",
      "Train: Epoch [17], Batch [381/938], Loss: 0.537988543510437\n",
      "Train: Epoch [17], Batch [382/938], Loss: 0.5159686803817749\n",
      "Train: Epoch [17], Batch [383/938], Loss: 0.48155999183654785\n",
      "Train: Epoch [17], Batch [384/938], Loss: 0.4797593355178833\n",
      "Train: Epoch [17], Batch [385/938], Loss: 0.3838614523410797\n",
      "Train: Epoch [17], Batch [386/938], Loss: 0.31133174896240234\n",
      "Train: Epoch [17], Batch [387/938], Loss: 0.4056774973869324\n",
      "Train: Epoch [17], Batch [388/938], Loss: 0.43231135606765747\n",
      "Train: Epoch [17], Batch [389/938], Loss: 0.4286242127418518\n",
      "Train: Epoch [17], Batch [390/938], Loss: 0.46449220180511475\n",
      "Train: Epoch [17], Batch [391/938], Loss: 0.35150137543678284\n",
      "Train: Epoch [17], Batch [392/938], Loss: 0.4945926368236542\n",
      "Train: Epoch [17], Batch [393/938], Loss: 0.6612465381622314\n",
      "Train: Epoch [17], Batch [394/938], Loss: 0.30515214800834656\n",
      "Train: Epoch [17], Batch [395/938], Loss: 0.40697723627090454\n",
      "Train: Epoch [17], Batch [396/938], Loss: 0.20213069021701813\n",
      "Train: Epoch [17], Batch [397/938], Loss: 0.2942022979259491\n",
      "Train: Epoch [17], Batch [398/938], Loss: 0.48175370693206787\n",
      "Train: Epoch [17], Batch [399/938], Loss: 0.4846964180469513\n",
      "Train: Epoch [17], Batch [400/938], Loss: 0.6522305011749268\n",
      "Train: Epoch [17], Batch [401/938], Loss: 0.3487364947795868\n",
      "Train: Epoch [17], Batch [402/938], Loss: 0.45758533477783203\n",
      "Train: Epoch [17], Batch [403/938], Loss: 0.3189656138420105\n",
      "Train: Epoch [17], Batch [404/938], Loss: 0.3907209038734436\n",
      "Train: Epoch [17], Batch [405/938], Loss: 0.48871910572052\n",
      "Train: Epoch [17], Batch [406/938], Loss: 0.47045332193374634\n",
      "Train: Epoch [17], Batch [407/938], Loss: 0.33852922916412354\n",
      "Train: Epoch [17], Batch [408/938], Loss: 0.6265348792076111\n",
      "Train: Epoch [17], Batch [409/938], Loss: 0.29558536410331726\n",
      "Train: Epoch [17], Batch [410/938], Loss: 0.38588014245033264\n",
      "Train: Epoch [17], Batch [411/938], Loss: 0.3376837372779846\n",
      "Train: Epoch [17], Batch [412/938], Loss: 0.33466485142707825\n",
      "Train: Epoch [17], Batch [413/938], Loss: 0.5755152702331543\n",
      "Train: Epoch [17], Batch [414/938], Loss: 0.3906099796295166\n",
      "Train: Epoch [17], Batch [415/938], Loss: 0.41320115327835083\n",
      "Train: Epoch [17], Batch [416/938], Loss: 0.31120893359184265\n",
      "Train: Epoch [17], Batch [417/938], Loss: 0.30849844217300415\n",
      "Train: Epoch [17], Batch [418/938], Loss: 0.5283341407775879\n",
      "Train: Epoch [17], Batch [419/938], Loss: 0.4870343804359436\n",
      "Train: Epoch [17], Batch [420/938], Loss: 0.43771839141845703\n",
      "Train: Epoch [17], Batch [421/938], Loss: 0.45385798811912537\n",
      "Train: Epoch [17], Batch [422/938], Loss: 0.4586534798145294\n",
      "Train: Epoch [17], Batch [423/938], Loss: 0.41558927297592163\n",
      "Train: Epoch [17], Batch [424/938], Loss: 0.38396042585372925\n",
      "Train: Epoch [17], Batch [425/938], Loss: 0.35263675451278687\n",
      "Train: Epoch [17], Batch [426/938], Loss: 0.5261693000793457\n",
      "Train: Epoch [17], Batch [427/938], Loss: 0.23851484060287476\n",
      "Train: Epoch [17], Batch [428/938], Loss: 0.2634889483451843\n",
      "Train: Epoch [17], Batch [429/938], Loss: 0.3011932075023651\n",
      "Train: Epoch [17], Batch [430/938], Loss: 0.46623340249061584\n",
      "Train: Epoch [17], Batch [431/938], Loss: 0.28934502601623535\n",
      "Train: Epoch [17], Batch [432/938], Loss: 0.3585681915283203\n",
      "Train: Epoch [17], Batch [433/938], Loss: 0.4736359715461731\n",
      "Train: Epoch [17], Batch [434/938], Loss: 0.3515109717845917\n",
      "Train: Epoch [17], Batch [435/938], Loss: 0.4357808828353882\n",
      "Train: Epoch [17], Batch [436/938], Loss: 0.49244454503059387\n",
      "Train: Epoch [17], Batch [437/938], Loss: 0.3930179476737976\n",
      "Train: Epoch [17], Batch [438/938], Loss: 0.42862120270729065\n",
      "Train: Epoch [17], Batch [439/938], Loss: 0.30738121271133423\n",
      "Train: Epoch [17], Batch [440/938], Loss: 0.3454747498035431\n",
      "Train: Epoch [17], Batch [441/938], Loss: 0.6204696297645569\n",
      "Train: Epoch [17], Batch [442/938], Loss: 0.32062214612960815\n",
      "Train: Epoch [17], Batch [443/938], Loss: 0.38252556324005127\n",
      "Train: Epoch [17], Batch [444/938], Loss: 0.4132280647754669\n",
      "Train: Epoch [17], Batch [445/938], Loss: 0.4229162335395813\n",
      "Train: Epoch [17], Batch [446/938], Loss: 0.40657341480255127\n",
      "Train: Epoch [17], Batch [447/938], Loss: 0.6017404794692993\n",
      "Train: Epoch [17], Batch [448/938], Loss: 0.3105013072490692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [449/938], Loss: 0.5090339183807373\n",
      "Train: Epoch [17], Batch [450/938], Loss: 0.5171325206756592\n",
      "Train: Epoch [17], Batch [451/938], Loss: 0.41487738490104675\n",
      "Train: Epoch [17], Batch [452/938], Loss: 0.4379206597805023\n",
      "Train: Epoch [17], Batch [453/938], Loss: 0.5874373912811279\n",
      "Train: Epoch [17], Batch [454/938], Loss: 0.46006906032562256\n",
      "Train: Epoch [17], Batch [455/938], Loss: 0.6457735896110535\n",
      "Train: Epoch [17], Batch [456/938], Loss: 0.3721643388271332\n",
      "Train: Epoch [17], Batch [457/938], Loss: 0.3411558270454407\n",
      "Train: Epoch [17], Batch [458/938], Loss: 0.3724004030227661\n",
      "Train: Epoch [17], Batch [459/938], Loss: 0.24184323847293854\n",
      "Train: Epoch [17], Batch [460/938], Loss: 0.63909912109375\n",
      "Train: Epoch [17], Batch [461/938], Loss: 0.3811147212982178\n",
      "Train: Epoch [17], Batch [462/938], Loss: 0.4860292077064514\n",
      "Train: Epoch [17], Batch [463/938], Loss: 0.36783671379089355\n",
      "Train: Epoch [17], Batch [464/938], Loss: 0.4501864016056061\n",
      "Train: Epoch [17], Batch [465/938], Loss: 0.3444982171058655\n",
      "Train: Epoch [17], Batch [466/938], Loss: 0.5111269354820251\n",
      "Train: Epoch [17], Batch [467/938], Loss: 0.36274415254592896\n",
      "Train: Epoch [17], Batch [468/938], Loss: 0.39326226711273193\n",
      "Train: Epoch [17], Batch [469/938], Loss: 0.3441844582557678\n",
      "Train: Epoch [17], Batch [470/938], Loss: 0.4705243408679962\n",
      "Train: Epoch [17], Batch [471/938], Loss: 0.5419535040855408\n",
      "Train: Epoch [17], Batch [472/938], Loss: 0.5106788873672485\n",
      "Train: Epoch [17], Batch [473/938], Loss: 0.5464110374450684\n",
      "Train: Epoch [17], Batch [474/938], Loss: 0.5318077206611633\n",
      "Train: Epoch [17], Batch [475/938], Loss: 0.2976762652397156\n",
      "Train: Epoch [17], Batch [476/938], Loss: 0.3895729184150696\n",
      "Train: Epoch [17], Batch [477/938], Loss: 0.33150526881217957\n",
      "Train: Epoch [17], Batch [478/938], Loss: 0.44541144371032715\n",
      "Train: Epoch [17], Batch [479/938], Loss: 0.39332666993141174\n",
      "Train: Epoch [17], Batch [480/938], Loss: 0.5773348808288574\n",
      "Train: Epoch [17], Batch [481/938], Loss: 0.49924883246421814\n",
      "Train: Epoch [17], Batch [482/938], Loss: 0.32016721367836\n",
      "Train: Epoch [17], Batch [483/938], Loss: 0.5075626969337463\n",
      "Train: Epoch [17], Batch [484/938], Loss: 0.26782935857772827\n",
      "Train: Epoch [17], Batch [485/938], Loss: 0.5149784088134766\n",
      "Train: Epoch [17], Batch [486/938], Loss: 0.4450721740722656\n",
      "Train: Epoch [17], Batch [487/938], Loss: 0.48389768600463867\n",
      "Train: Epoch [17], Batch [488/938], Loss: 0.4346802532672882\n",
      "Train: Epoch [17], Batch [489/938], Loss: 0.5667628049850464\n",
      "Train: Epoch [17], Batch [490/938], Loss: 0.8179036378860474\n",
      "Train: Epoch [17], Batch [491/938], Loss: 0.40953099727630615\n",
      "Train: Epoch [17], Batch [492/938], Loss: 0.3951878547668457\n",
      "Train: Epoch [17], Batch [493/938], Loss: 0.5001184940338135\n",
      "Train: Epoch [17], Batch [494/938], Loss: 0.24330730736255646\n",
      "Train: Epoch [17], Batch [495/938], Loss: 0.31286507844924927\n",
      "Train: Epoch [17], Batch [496/938], Loss: 0.5504366159439087\n",
      "Train: Epoch [17], Batch [497/938], Loss: 0.4508157968521118\n",
      "Train: Epoch [17], Batch [498/938], Loss: 0.23672695457935333\n",
      "Train: Epoch [17], Batch [499/938], Loss: 0.5904392004013062\n",
      "Train: Epoch [17], Batch [500/938], Loss: 0.4849686324596405\n",
      "Train: Epoch [17], Batch [501/938], Loss: 0.5470155477523804\n",
      "Train: Epoch [17], Batch [502/938], Loss: 0.3014800250530243\n",
      "Train: Epoch [17], Batch [503/938], Loss: 0.5085634589195251\n",
      "Train: Epoch [17], Batch [504/938], Loss: 0.3299309015274048\n",
      "Train: Epoch [17], Batch [505/938], Loss: 0.3658421039581299\n",
      "Train: Epoch [17], Batch [506/938], Loss: 0.3229331970214844\n",
      "Train: Epoch [17], Batch [507/938], Loss: 0.26952874660491943\n",
      "Train: Epoch [17], Batch [508/938], Loss: 0.4692903161048889\n",
      "Train: Epoch [17], Batch [509/938], Loss: 0.46330296993255615\n",
      "Train: Epoch [17], Batch [510/938], Loss: 0.44075706601142883\n",
      "Train: Epoch [17], Batch [511/938], Loss: 0.32768017053604126\n",
      "Train: Epoch [17], Batch [512/938], Loss: 0.5383824706077576\n",
      "Train: Epoch [17], Batch [513/938], Loss: 0.3674929141998291\n",
      "Train: Epoch [17], Batch [514/938], Loss: 0.558518648147583\n",
      "Train: Epoch [17], Batch [515/938], Loss: 0.36809906363487244\n",
      "Train: Epoch [17], Batch [516/938], Loss: 0.25165361166000366\n",
      "Train: Epoch [17], Batch [517/938], Loss: 0.4500241279602051\n",
      "Train: Epoch [17], Batch [518/938], Loss: 0.3980746567249298\n",
      "Train: Epoch [17], Batch [519/938], Loss: 0.4686490595340729\n",
      "Train: Epoch [17], Batch [520/938], Loss: 0.22740532457828522\n",
      "Train: Epoch [17], Batch [521/938], Loss: 0.4481469988822937\n",
      "Train: Epoch [17], Batch [522/938], Loss: 0.49331897497177124\n",
      "Train: Epoch [17], Batch [523/938], Loss: 0.45812156796455383\n",
      "Train: Epoch [17], Batch [524/938], Loss: 0.41270995140075684\n",
      "Train: Epoch [17], Batch [525/938], Loss: 0.36322158575057983\n",
      "Train: Epoch [17], Batch [526/938], Loss: 0.4002424478530884\n",
      "Train: Epoch [17], Batch [527/938], Loss: 0.44743087887763977\n",
      "Train: Epoch [17], Batch [528/938], Loss: 0.3735351264476776\n",
      "Train: Epoch [17], Batch [529/938], Loss: 0.3413354754447937\n",
      "Train: Epoch [17], Batch [530/938], Loss: 0.47702136635780334\n",
      "Train: Epoch [17], Batch [531/938], Loss: 0.43841007351875305\n",
      "Train: Epoch [17], Batch [532/938], Loss: 0.39321497082710266\n",
      "Train: Epoch [17], Batch [533/938], Loss: 0.3268989324569702\n",
      "Train: Epoch [17], Batch [534/938], Loss: 0.33984482288360596\n",
      "Train: Epoch [17], Batch [535/938], Loss: 0.5612268447875977\n",
      "Train: Epoch [17], Batch [536/938], Loss: 0.5187157392501831\n",
      "Train: Epoch [17], Batch [537/938], Loss: 0.5827193856239319\n",
      "Train: Epoch [17], Batch [538/938], Loss: 0.34600216150283813\n",
      "Train: Epoch [17], Batch [539/938], Loss: 0.5392577052116394\n",
      "Train: Epoch [17], Batch [540/938], Loss: 0.5947355031967163\n",
      "Train: Epoch [17], Batch [541/938], Loss: 0.5753158330917358\n",
      "Train: Epoch [17], Batch [542/938], Loss: 0.5209239721298218\n",
      "Train: Epoch [17], Batch [543/938], Loss: 0.3514479994773865\n",
      "Train: Epoch [17], Batch [544/938], Loss: 0.5990848541259766\n",
      "Train: Epoch [17], Batch [545/938], Loss: 0.3232453465461731\n",
      "Train: Epoch [17], Batch [546/938], Loss: 0.5411252975463867\n",
      "Train: Epoch [17], Batch [547/938], Loss: 0.42488405108451843\n",
      "Train: Epoch [17], Batch [548/938], Loss: 0.5411627292633057\n",
      "Train: Epoch [17], Batch [549/938], Loss: 0.38191425800323486\n",
      "Train: Epoch [17], Batch [550/938], Loss: 0.42075207829475403\n",
      "Train: Epoch [17], Batch [551/938], Loss: 0.39585164189338684\n",
      "Train: Epoch [17], Batch [552/938], Loss: 0.37539517879486084\n",
      "Train: Epoch [17], Batch [553/938], Loss: 0.40172386169433594\n",
      "Train: Epoch [17], Batch [554/938], Loss: 0.3876391053199768\n",
      "Train: Epoch [17], Batch [555/938], Loss: 0.28517335653305054\n",
      "Train: Epoch [17], Batch [556/938], Loss: 0.5024415254592896\n",
      "Train: Epoch [17], Batch [557/938], Loss: 0.41016489267349243\n",
      "Train: Epoch [17], Batch [558/938], Loss: 0.45269274711608887\n",
      "Train: Epoch [17], Batch [559/938], Loss: 0.4132041931152344\n",
      "Train: Epoch [17], Batch [560/938], Loss: 0.5557836294174194\n",
      "Train: Epoch [17], Batch [561/938], Loss: 0.3559935986995697\n",
      "Train: Epoch [17], Batch [562/938], Loss: 0.5028688907623291\n",
      "Train: Epoch [17], Batch [563/938], Loss: 0.526136040687561\n",
      "Train: Epoch [17], Batch [564/938], Loss: 0.22654899954795837\n",
      "Train: Epoch [17], Batch [565/938], Loss: 0.501807689666748\n",
      "Train: Epoch [17], Batch [566/938], Loss: 0.48392677307128906\n",
      "Train: Epoch [17], Batch [567/938], Loss: 0.4880599081516266\n",
      "Train: Epoch [17], Batch [568/938], Loss: 0.4239121377468109\n",
      "Train: Epoch [17], Batch [569/938], Loss: 0.41853243112564087\n",
      "Train: Epoch [17], Batch [570/938], Loss: 0.3537440598011017\n",
      "Train: Epoch [17], Batch [571/938], Loss: 0.6638739705085754\n",
      "Train: Epoch [17], Batch [572/938], Loss: 0.4170148968696594\n",
      "Train: Epoch [17], Batch [573/938], Loss: 0.5101121068000793\n",
      "Train: Epoch [17], Batch [574/938], Loss: 0.32966935634613037\n",
      "Train: Epoch [17], Batch [575/938], Loss: 0.5553025007247925\n",
      "Train: Epoch [17], Batch [576/938], Loss: 0.5405987501144409\n",
      "Train: Epoch [17], Batch [577/938], Loss: 0.42322707176208496\n",
      "Train: Epoch [17], Batch [578/938], Loss: 0.45368385314941406\n",
      "Train: Epoch [17], Batch [579/938], Loss: 0.4987773895263672\n",
      "Train: Epoch [17], Batch [580/938], Loss: 0.628268837928772\n",
      "Train: Epoch [17], Batch [581/938], Loss: 0.49235421419143677\n",
      "Train: Epoch [17], Batch [582/938], Loss: 0.4003523588180542\n",
      "Train: Epoch [17], Batch [583/938], Loss: 0.3992742896080017\n",
      "Train: Epoch [17], Batch [584/938], Loss: 0.4009641110897064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [585/938], Loss: 0.31630435585975647\n",
      "Train: Epoch [17], Batch [586/938], Loss: 0.6602620482444763\n",
      "Train: Epoch [17], Batch [587/938], Loss: 0.49006277322769165\n",
      "Train: Epoch [17], Batch [588/938], Loss: 0.6211552619934082\n",
      "Train: Epoch [17], Batch [589/938], Loss: 0.299358069896698\n",
      "Train: Epoch [17], Batch [590/938], Loss: 0.33465442061424255\n",
      "Train: Epoch [17], Batch [591/938], Loss: 0.5013816952705383\n",
      "Train: Epoch [17], Batch [592/938], Loss: 0.48905298113822937\n",
      "Train: Epoch [17], Batch [593/938], Loss: 0.47562292218208313\n",
      "Train: Epoch [17], Batch [594/938], Loss: 0.522042989730835\n",
      "Train: Epoch [17], Batch [595/938], Loss: 0.3706624507904053\n",
      "Train: Epoch [17], Batch [596/938], Loss: 0.5514882206916809\n",
      "Train: Epoch [17], Batch [597/938], Loss: 0.49314194917678833\n",
      "Train: Epoch [17], Batch [598/938], Loss: 0.39928510785102844\n",
      "Train: Epoch [17], Batch [599/938], Loss: 0.33540958166122437\n",
      "Train: Epoch [17], Batch [600/938], Loss: 0.43506234884262085\n",
      "Train: Epoch [17], Batch [601/938], Loss: 0.48434796929359436\n",
      "Train: Epoch [17], Batch [602/938], Loss: 0.38367533683776855\n",
      "Train: Epoch [17], Batch [603/938], Loss: 0.28400886058807373\n",
      "Train: Epoch [17], Batch [604/938], Loss: 0.3979294002056122\n",
      "Train: Epoch [17], Batch [605/938], Loss: 0.3032791316509247\n",
      "Train: Epoch [17], Batch [606/938], Loss: 0.5196159482002258\n",
      "Train: Epoch [17], Batch [607/938], Loss: 0.3973086476325989\n",
      "Train: Epoch [17], Batch [608/938], Loss: 0.28947174549102783\n",
      "Train: Epoch [17], Batch [609/938], Loss: 0.4233435094356537\n",
      "Train: Epoch [17], Batch [610/938], Loss: 0.3267044126987457\n",
      "Train: Epoch [17], Batch [611/938], Loss: 0.44225776195526123\n",
      "Train: Epoch [17], Batch [612/938], Loss: 0.41839534044265747\n",
      "Train: Epoch [17], Batch [613/938], Loss: 0.39086616039276123\n",
      "Train: Epoch [17], Batch [614/938], Loss: 0.4299113154411316\n",
      "Train: Epoch [17], Batch [615/938], Loss: 0.47048455476760864\n",
      "Train: Epoch [17], Batch [616/938], Loss: 0.4155268371105194\n",
      "Train: Epoch [17], Batch [617/938], Loss: 0.433261513710022\n",
      "Train: Epoch [17], Batch [618/938], Loss: 0.3735271990299225\n",
      "Train: Epoch [17], Batch [619/938], Loss: 0.47620004415512085\n",
      "Train: Epoch [17], Batch [620/938], Loss: 0.36306166648864746\n",
      "Train: Epoch [17], Batch [621/938], Loss: 0.39047324657440186\n",
      "Train: Epoch [17], Batch [622/938], Loss: 0.35616177320480347\n",
      "Train: Epoch [17], Batch [623/938], Loss: 0.32660195231437683\n",
      "Train: Epoch [17], Batch [624/938], Loss: 0.46393004059791565\n",
      "Train: Epoch [17], Batch [625/938], Loss: 0.4521453082561493\n",
      "Train: Epoch [17], Batch [626/938], Loss: 0.4701812267303467\n",
      "Train: Epoch [17], Batch [627/938], Loss: 0.6024940013885498\n",
      "Train: Epoch [17], Batch [628/938], Loss: 0.42584899067878723\n",
      "Train: Epoch [17], Batch [629/938], Loss: 0.6674132347106934\n",
      "Train: Epoch [17], Batch [630/938], Loss: 0.4011459946632385\n",
      "Train: Epoch [17], Batch [631/938], Loss: 0.5284361839294434\n",
      "Train: Epoch [17], Batch [632/938], Loss: 0.37567847967147827\n",
      "Train: Epoch [17], Batch [633/938], Loss: 0.46900615096092224\n",
      "Train: Epoch [17], Batch [634/938], Loss: 0.7263230085372925\n",
      "Train: Epoch [17], Batch [635/938], Loss: 0.4873524308204651\n",
      "Train: Epoch [17], Batch [636/938], Loss: 0.4618263840675354\n",
      "Train: Epoch [17], Batch [637/938], Loss: 0.42836177349090576\n",
      "Train: Epoch [17], Batch [638/938], Loss: 0.47755900025367737\n",
      "Train: Epoch [17], Batch [639/938], Loss: 0.4869539439678192\n",
      "Train: Epoch [17], Batch [640/938], Loss: 0.5795500874519348\n",
      "Train: Epoch [17], Batch [641/938], Loss: 0.4436050057411194\n",
      "Train: Epoch [17], Batch [642/938], Loss: 0.3147388696670532\n",
      "Train: Epoch [17], Batch [643/938], Loss: 0.6008017063140869\n",
      "Train: Epoch [17], Batch [644/938], Loss: 0.4214684069156647\n",
      "Train: Epoch [17], Batch [645/938], Loss: 0.47623300552368164\n",
      "Train: Epoch [17], Batch [646/938], Loss: 0.5401933193206787\n",
      "Train: Epoch [17], Batch [647/938], Loss: 0.2934950590133667\n",
      "Train: Epoch [17], Batch [648/938], Loss: 0.37930727005004883\n",
      "Train: Epoch [17], Batch [649/938], Loss: 0.35605281591415405\n",
      "Train: Epoch [17], Batch [650/938], Loss: 0.3891434669494629\n",
      "Train: Epoch [17], Batch [651/938], Loss: 0.5447065830230713\n",
      "Train: Epoch [17], Batch [652/938], Loss: 0.4231258034706116\n",
      "Train: Epoch [17], Batch [653/938], Loss: 0.37326616048812866\n",
      "Train: Epoch [17], Batch [654/938], Loss: 0.566641092300415\n",
      "Train: Epoch [17], Batch [655/938], Loss: 0.38603663444519043\n",
      "Train: Epoch [17], Batch [656/938], Loss: 0.3953900933265686\n",
      "Train: Epoch [17], Batch [657/938], Loss: 0.5695737600326538\n",
      "Train: Epoch [17], Batch [658/938], Loss: 0.3505917191505432\n",
      "Train: Epoch [17], Batch [659/938], Loss: 0.43233823776245117\n",
      "Train: Epoch [17], Batch [660/938], Loss: 0.48264625668525696\n",
      "Train: Epoch [17], Batch [661/938], Loss: 0.44992130994796753\n",
      "Train: Epoch [17], Batch [662/938], Loss: 0.49670958518981934\n",
      "Train: Epoch [17], Batch [663/938], Loss: 0.4202643632888794\n",
      "Train: Epoch [17], Batch [664/938], Loss: 0.30686575174331665\n",
      "Train: Epoch [17], Batch [665/938], Loss: 0.44794371724128723\n",
      "Train: Epoch [17], Batch [666/938], Loss: 0.5090591907501221\n",
      "Train: Epoch [17], Batch [667/938], Loss: 0.4699275493621826\n",
      "Train: Epoch [17], Batch [668/938], Loss: 0.3801250457763672\n",
      "Train: Epoch [17], Batch [669/938], Loss: 0.37447234988212585\n",
      "Train: Epoch [17], Batch [670/938], Loss: 0.406512975692749\n",
      "Train: Epoch [17], Batch [671/938], Loss: 0.4532366394996643\n",
      "Train: Epoch [17], Batch [672/938], Loss: 0.433373361825943\n",
      "Train: Epoch [17], Batch [673/938], Loss: 0.44031819701194763\n",
      "Train: Epoch [17], Batch [674/938], Loss: 0.34804168343544006\n",
      "Train: Epoch [17], Batch [675/938], Loss: 0.4545446038246155\n",
      "Train: Epoch [17], Batch [676/938], Loss: 0.4537671208381653\n",
      "Train: Epoch [17], Batch [677/938], Loss: 0.6103471517562866\n",
      "Train: Epoch [17], Batch [678/938], Loss: 0.3965739905834198\n",
      "Train: Epoch [17], Batch [679/938], Loss: 0.5538877248764038\n",
      "Train: Epoch [17], Batch [680/938], Loss: 0.48816201090812683\n",
      "Train: Epoch [17], Batch [681/938], Loss: 0.4516143500804901\n",
      "Train: Epoch [17], Batch [682/938], Loss: 0.4714556932449341\n",
      "Train: Epoch [17], Batch [683/938], Loss: 0.3650400936603546\n",
      "Train: Epoch [17], Batch [684/938], Loss: 0.3211755156517029\n",
      "Train: Epoch [17], Batch [685/938], Loss: 0.4777963161468506\n",
      "Train: Epoch [17], Batch [686/938], Loss: 0.4520924687385559\n",
      "Train: Epoch [17], Batch [687/938], Loss: 0.4533226490020752\n",
      "Train: Epoch [17], Batch [688/938], Loss: 0.5640215873718262\n",
      "Train: Epoch [17], Batch [689/938], Loss: 0.5640661716461182\n",
      "Train: Epoch [17], Batch [690/938], Loss: 0.49457794427871704\n",
      "Train: Epoch [17], Batch [691/938], Loss: 0.32220277190208435\n",
      "Train: Epoch [17], Batch [692/938], Loss: 0.3718358278274536\n",
      "Train: Epoch [17], Batch [693/938], Loss: 0.40612637996673584\n",
      "Train: Epoch [17], Batch [694/938], Loss: 0.28857749700546265\n",
      "Train: Epoch [17], Batch [695/938], Loss: 0.5838664770126343\n",
      "Train: Epoch [17], Batch [696/938], Loss: 0.43421682715415955\n",
      "Train: Epoch [17], Batch [697/938], Loss: 0.31883537769317627\n",
      "Train: Epoch [17], Batch [698/938], Loss: 0.2933686375617981\n",
      "Train: Epoch [17], Batch [699/938], Loss: 0.5040446519851685\n",
      "Train: Epoch [17], Batch [700/938], Loss: 0.341280460357666\n",
      "Train: Epoch [17], Batch [701/938], Loss: 0.4397222399711609\n",
      "Train: Epoch [17], Batch [702/938], Loss: 0.2795073986053467\n",
      "Train: Epoch [17], Batch [703/938], Loss: 0.5528392195701599\n",
      "Train: Epoch [17], Batch [704/938], Loss: 0.365464985370636\n",
      "Train: Epoch [17], Batch [705/938], Loss: 0.5030175447463989\n",
      "Train: Epoch [17], Batch [706/938], Loss: 0.5425072908401489\n",
      "Train: Epoch [17], Batch [707/938], Loss: 0.3631620407104492\n",
      "Train: Epoch [17], Batch [708/938], Loss: 0.36976689100265503\n",
      "Train: Epoch [17], Batch [709/938], Loss: 0.44167864322662354\n",
      "Train: Epoch [17], Batch [710/938], Loss: 0.5186235904693604\n",
      "Train: Epoch [17], Batch [711/938], Loss: 0.2832983136177063\n",
      "Train: Epoch [17], Batch [712/938], Loss: 0.5704940557479858\n",
      "Train: Epoch [17], Batch [713/938], Loss: 0.4238984286785126\n",
      "Train: Epoch [17], Batch [714/938], Loss: 0.4643045961856842\n",
      "Train: Epoch [17], Batch [715/938], Loss: 0.4356032609939575\n",
      "Train: Epoch [17], Batch [716/938], Loss: 0.4583492875099182\n",
      "Train: Epoch [17], Batch [717/938], Loss: 0.39538902044296265\n",
      "Train: Epoch [17], Batch [718/938], Loss: 0.5390658378601074\n",
      "Train: Epoch [17], Batch [719/938], Loss: 0.31895846128463745\n",
      "Train: Epoch [17], Batch [720/938], Loss: 0.5356690287590027\n",
      "Train: Epoch [17], Batch [721/938], Loss: 0.7174002528190613\n",
      "Train: Epoch [17], Batch [722/938], Loss: 0.5706642866134644\n",
      "Train: Epoch [17], Batch [723/938], Loss: 0.5211806297302246\n",
      "Train: Epoch [17], Batch [724/938], Loss: 0.23472969233989716\n",
      "Train: Epoch [17], Batch [725/938], Loss: 0.562472939491272\n",
      "Train: Epoch [17], Batch [726/938], Loss: 0.43702077865600586\n",
      "Train: Epoch [17], Batch [727/938], Loss: 0.6689387559890747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [728/938], Loss: 0.49696505069732666\n",
      "Train: Epoch [17], Batch [729/938], Loss: 0.3526320457458496\n",
      "Train: Epoch [17], Batch [730/938], Loss: 0.3041311502456665\n",
      "Train: Epoch [17], Batch [731/938], Loss: 0.5393862724304199\n",
      "Train: Epoch [17], Batch [732/938], Loss: 0.361115962266922\n",
      "Train: Epoch [17], Batch [733/938], Loss: 0.2742038667201996\n",
      "Train: Epoch [17], Batch [734/938], Loss: 0.5113943815231323\n",
      "Train: Epoch [17], Batch [735/938], Loss: 0.41408419609069824\n",
      "Train: Epoch [17], Batch [736/938], Loss: 0.41612550616264343\n",
      "Train: Epoch [17], Batch [737/938], Loss: 0.5860716700553894\n",
      "Train: Epoch [17], Batch [738/938], Loss: 0.47172802686691284\n",
      "Train: Epoch [17], Batch [739/938], Loss: 0.345353364944458\n",
      "Train: Epoch [17], Batch [740/938], Loss: 0.5210092663764954\n",
      "Train: Epoch [17], Batch [741/938], Loss: 0.4661121666431427\n",
      "Train: Epoch [17], Batch [742/938], Loss: 0.42364051938056946\n",
      "Train: Epoch [17], Batch [743/938], Loss: 0.3741036355495453\n",
      "Train: Epoch [17], Batch [744/938], Loss: 0.30786722898483276\n",
      "Train: Epoch [17], Batch [745/938], Loss: 0.44363707304000854\n",
      "Train: Epoch [17], Batch [746/938], Loss: 0.566680908203125\n",
      "Train: Epoch [17], Batch [747/938], Loss: 0.5391042232513428\n",
      "Train: Epoch [17], Batch [748/938], Loss: 0.4817790389060974\n",
      "Train: Epoch [17], Batch [749/938], Loss: 0.4254437983036041\n",
      "Train: Epoch [17], Batch [750/938], Loss: 0.3958207070827484\n",
      "Train: Epoch [17], Batch [751/938], Loss: 0.2869788110256195\n",
      "Train: Epoch [17], Batch [752/938], Loss: 0.35519254207611084\n",
      "Train: Epoch [17], Batch [753/938], Loss: 0.44950032234191895\n",
      "Train: Epoch [17], Batch [754/938], Loss: 0.517008900642395\n",
      "Train: Epoch [17], Batch [755/938], Loss: 0.5762834548950195\n",
      "Train: Epoch [17], Batch [756/938], Loss: 0.3763745427131653\n",
      "Train: Epoch [17], Batch [757/938], Loss: 0.4546338617801666\n",
      "Train: Epoch [17], Batch [758/938], Loss: 0.450438916683197\n",
      "Train: Epoch [17], Batch [759/938], Loss: 0.641099214553833\n",
      "Train: Epoch [17], Batch [760/938], Loss: 0.48946619033813477\n",
      "Train: Epoch [17], Batch [761/938], Loss: 0.47166407108306885\n",
      "Train: Epoch [17], Batch [762/938], Loss: 0.4199524521827698\n",
      "Train: Epoch [17], Batch [763/938], Loss: 0.4076579213142395\n",
      "Train: Epoch [17], Batch [764/938], Loss: 0.46697568893432617\n",
      "Train: Epoch [17], Batch [765/938], Loss: 0.3810459077358246\n",
      "Train: Epoch [17], Batch [766/938], Loss: 0.33645403385162354\n",
      "Train: Epoch [17], Batch [767/938], Loss: 0.29392242431640625\n",
      "Train: Epoch [17], Batch [768/938], Loss: 0.48181039094924927\n",
      "Train: Epoch [17], Batch [769/938], Loss: 0.2491236925125122\n",
      "Train: Epoch [17], Batch [770/938], Loss: 0.4808860421180725\n",
      "Train: Epoch [17], Batch [771/938], Loss: 0.3887370228767395\n",
      "Train: Epoch [17], Batch [772/938], Loss: 0.7057145833969116\n",
      "Train: Epoch [17], Batch [773/938], Loss: 0.432582288980484\n",
      "Train: Epoch [17], Batch [774/938], Loss: 0.3608572781085968\n",
      "Train: Epoch [17], Batch [775/938], Loss: 0.4623481333255768\n",
      "Train: Epoch [17], Batch [776/938], Loss: 0.4054464101791382\n",
      "Train: Epoch [17], Batch [777/938], Loss: 0.26526308059692383\n",
      "Train: Epoch [17], Batch [778/938], Loss: 0.32233166694641113\n",
      "Train: Epoch [17], Batch [779/938], Loss: 0.5248752236366272\n",
      "Train: Epoch [17], Batch [780/938], Loss: 0.6406246423721313\n",
      "Train: Epoch [17], Batch [781/938], Loss: 0.5180196762084961\n",
      "Train: Epoch [17], Batch [782/938], Loss: 0.38832366466522217\n",
      "Train: Epoch [17], Batch [783/938], Loss: 0.37580347061157227\n",
      "Train: Epoch [17], Batch [784/938], Loss: 0.6164380311965942\n",
      "Train: Epoch [17], Batch [785/938], Loss: 0.38868796825408936\n",
      "Train: Epoch [17], Batch [786/938], Loss: 0.4136560559272766\n",
      "Train: Epoch [17], Batch [787/938], Loss: 0.345262736082077\n",
      "Train: Epoch [17], Batch [788/938], Loss: 0.6257658004760742\n",
      "Train: Epoch [17], Batch [789/938], Loss: 0.41759544610977173\n",
      "Train: Epoch [17], Batch [790/938], Loss: 0.2968231439590454\n",
      "Train: Epoch [17], Batch [791/938], Loss: 0.3643702566623688\n",
      "Train: Epoch [17], Batch [792/938], Loss: 0.2799285650253296\n",
      "Train: Epoch [17], Batch [793/938], Loss: 0.4535999000072479\n",
      "Train: Epoch [17], Batch [794/938], Loss: 0.45096108317375183\n",
      "Train: Epoch [17], Batch [795/938], Loss: 0.3601643443107605\n",
      "Train: Epoch [17], Batch [796/938], Loss: 0.45491358637809753\n",
      "Train: Epoch [17], Batch [797/938], Loss: 0.3932209312915802\n",
      "Train: Epoch [17], Batch [798/938], Loss: 0.3961358666419983\n",
      "Train: Epoch [17], Batch [799/938], Loss: 0.6714815497398376\n",
      "Train: Epoch [17], Batch [800/938], Loss: 0.5168070197105408\n",
      "Train: Epoch [17], Batch [801/938], Loss: 0.5136908888816833\n",
      "Train: Epoch [17], Batch [802/938], Loss: 0.460548996925354\n",
      "Train: Epoch [17], Batch [803/938], Loss: 0.5635827779769897\n",
      "Train: Epoch [17], Batch [804/938], Loss: 0.43946969509124756\n",
      "Train: Epoch [17], Batch [805/938], Loss: 0.2722315192222595\n",
      "Train: Epoch [17], Batch [806/938], Loss: 0.34025681018829346\n",
      "Train: Epoch [17], Batch [807/938], Loss: 0.36573725938796997\n",
      "Train: Epoch [17], Batch [808/938], Loss: 0.49547648429870605\n",
      "Train: Epoch [17], Batch [809/938], Loss: 0.5979708433151245\n",
      "Train: Epoch [17], Batch [810/938], Loss: 0.40526553988456726\n",
      "Train: Epoch [17], Batch [811/938], Loss: 0.48088690638542175\n",
      "Train: Epoch [17], Batch [812/938], Loss: 0.5580735206604004\n",
      "Train: Epoch [17], Batch [813/938], Loss: 0.5528783798217773\n",
      "Train: Epoch [17], Batch [814/938], Loss: 0.36115193367004395\n",
      "Train: Epoch [17], Batch [815/938], Loss: 0.5492192506790161\n",
      "Train: Epoch [17], Batch [816/938], Loss: 0.26605069637298584\n",
      "Train: Epoch [17], Batch [817/938], Loss: 0.3089282214641571\n",
      "Train: Epoch [17], Batch [818/938], Loss: 0.5481808185577393\n",
      "Train: Epoch [17], Batch [819/938], Loss: 0.33505159616470337\n",
      "Train: Epoch [17], Batch [820/938], Loss: 0.368131548166275\n",
      "Train: Epoch [17], Batch [821/938], Loss: 0.5460230708122253\n",
      "Train: Epoch [17], Batch [822/938], Loss: 0.45897603034973145\n",
      "Train: Epoch [17], Batch [823/938], Loss: 0.34877336025238037\n",
      "Train: Epoch [17], Batch [824/938], Loss: 0.784005880355835\n",
      "Train: Epoch [17], Batch [825/938], Loss: 0.43310609459877014\n",
      "Train: Epoch [17], Batch [826/938], Loss: 0.39213651418685913\n",
      "Train: Epoch [17], Batch [827/938], Loss: 0.5612127780914307\n",
      "Train: Epoch [17], Batch [828/938], Loss: 0.43322908878326416\n",
      "Train: Epoch [17], Batch [829/938], Loss: 0.6446307897567749\n",
      "Train: Epoch [17], Batch [830/938], Loss: 0.2982443571090698\n",
      "Train: Epoch [17], Batch [831/938], Loss: 0.4916873574256897\n",
      "Train: Epoch [17], Batch [832/938], Loss: 0.25953325629234314\n",
      "Train: Epoch [17], Batch [833/938], Loss: 0.35977602005004883\n",
      "Train: Epoch [17], Batch [834/938], Loss: 0.4342319369316101\n",
      "Train: Epoch [17], Batch [835/938], Loss: 0.4190458357334137\n",
      "Train: Epoch [17], Batch [836/938], Loss: 0.34522226452827454\n",
      "Train: Epoch [17], Batch [837/938], Loss: 0.445669949054718\n",
      "Train: Epoch [17], Batch [838/938], Loss: 0.3104037046432495\n",
      "Train: Epoch [17], Batch [839/938], Loss: 0.530511200428009\n",
      "Train: Epoch [17], Batch [840/938], Loss: 0.4155158996582031\n",
      "Train: Epoch [17], Batch [841/938], Loss: 0.43085673451423645\n",
      "Train: Epoch [17], Batch [842/938], Loss: 0.41289421916007996\n",
      "Train: Epoch [17], Batch [843/938], Loss: 0.3485397398471832\n",
      "Train: Epoch [17], Batch [844/938], Loss: 0.6096916198730469\n",
      "Train: Epoch [17], Batch [845/938], Loss: 0.3695491552352905\n",
      "Train: Epoch [17], Batch [846/938], Loss: 0.4760192632675171\n",
      "Train: Epoch [17], Batch [847/938], Loss: 0.5988006591796875\n",
      "Train: Epoch [17], Batch [848/938], Loss: 0.5197402238845825\n",
      "Train: Epoch [17], Batch [849/938], Loss: 0.2656857371330261\n",
      "Train: Epoch [17], Batch [850/938], Loss: 0.38399776816368103\n",
      "Train: Epoch [17], Batch [851/938], Loss: 0.4431923031806946\n",
      "Train: Epoch [17], Batch [852/938], Loss: 0.36049598455429077\n",
      "Train: Epoch [17], Batch [853/938], Loss: 0.628092348575592\n",
      "Train: Epoch [17], Batch [854/938], Loss: 0.4397431015968323\n",
      "Train: Epoch [17], Batch [855/938], Loss: 0.43505722284317017\n",
      "Train: Epoch [17], Batch [856/938], Loss: 0.37623196840286255\n",
      "Train: Epoch [17], Batch [857/938], Loss: 0.43390119075775146\n",
      "Train: Epoch [17], Batch [858/938], Loss: 0.4484918415546417\n",
      "Train: Epoch [17], Batch [859/938], Loss: 0.3894118666648865\n",
      "Train: Epoch [17], Batch [860/938], Loss: 0.3228303790092468\n",
      "Train: Epoch [17], Batch [861/938], Loss: 0.38285213708877563\n",
      "Train: Epoch [17], Batch [862/938], Loss: 0.47935977578163147\n",
      "Train: Epoch [17], Batch [863/938], Loss: 0.6019709706306458\n",
      "Train: Epoch [17], Batch [864/938], Loss: 0.4939088225364685\n",
      "Train: Epoch [17], Batch [865/938], Loss: 0.3244764804840088\n",
      "Train: Epoch [17], Batch [866/938], Loss: 0.41112297773361206\n",
      "Train: Epoch [17], Batch [867/938], Loss: 0.32265594601631165\n",
      "Train: Epoch [17], Batch [868/938], Loss: 0.530428946018219\n",
      "Train: Epoch [17], Batch [869/938], Loss: 0.3342960476875305\n",
      "Train: Epoch [17], Batch [870/938], Loss: 0.43412119150161743\n",
      "Train: Epoch [17], Batch [871/938], Loss: 0.563637375831604\n",
      "Train: Epoch [17], Batch [872/938], Loss: 0.3818816840648651\n",
      "Train: Epoch [17], Batch [873/938], Loss: 0.4156079590320587\n",
      "Train: Epoch [17], Batch [874/938], Loss: 0.2159748375415802\n",
      "Train: Epoch [17], Batch [875/938], Loss: 0.5122288465499878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [17], Batch [876/938], Loss: 0.427009642124176\n",
      "Train: Epoch [17], Batch [877/938], Loss: 0.37195873260498047\n",
      "Train: Epoch [17], Batch [878/938], Loss: 0.4983885884284973\n",
      "Train: Epoch [17], Batch [879/938], Loss: 0.7473210096359253\n",
      "Train: Epoch [17], Batch [880/938], Loss: 0.2969192862510681\n",
      "Train: Epoch [17], Batch [881/938], Loss: 0.4283469617366791\n",
      "Train: Epoch [17], Batch [882/938], Loss: 0.4182613790035248\n",
      "Train: Epoch [17], Batch [883/938], Loss: 0.37000906467437744\n",
      "Train: Epoch [17], Batch [884/938], Loss: 0.45766371488571167\n",
      "Train: Epoch [17], Batch [885/938], Loss: 0.38436171412467957\n",
      "Train: Epoch [17], Batch [886/938], Loss: 0.5283877849578857\n",
      "Train: Epoch [17], Batch [887/938], Loss: 0.42491546273231506\n",
      "Train: Epoch [17], Batch [888/938], Loss: 0.5124865770339966\n",
      "Train: Epoch [17], Batch [889/938], Loss: 0.4502791166305542\n",
      "Train: Epoch [17], Batch [890/938], Loss: 0.5109272003173828\n",
      "Train: Epoch [17], Batch [891/938], Loss: 0.47243231534957886\n",
      "Train: Epoch [17], Batch [892/938], Loss: 0.4225234389305115\n",
      "Train: Epoch [17], Batch [893/938], Loss: 0.48165249824523926\n",
      "Train: Epoch [17], Batch [894/938], Loss: 0.3345779478549957\n",
      "Train: Epoch [17], Batch [895/938], Loss: 0.33542078733444214\n",
      "Train: Epoch [17], Batch [896/938], Loss: 0.4094856381416321\n",
      "Train: Epoch [17], Batch [897/938], Loss: 0.39406976103782654\n",
      "Train: Epoch [17], Batch [898/938], Loss: 0.31245291233062744\n",
      "Train: Epoch [17], Batch [899/938], Loss: 0.8214871287345886\n",
      "Train: Epoch [17], Batch [900/938], Loss: 0.3637869954109192\n",
      "Train: Epoch [17], Batch [901/938], Loss: 0.5129398107528687\n",
      "Train: Epoch [17], Batch [902/938], Loss: 0.3804739713668823\n",
      "Train: Epoch [17], Batch [903/938], Loss: 0.42270100116729736\n",
      "Train: Epoch [17], Batch [904/938], Loss: 0.3287257254123688\n",
      "Train: Epoch [17], Batch [905/938], Loss: 0.2836393713951111\n",
      "Train: Epoch [17], Batch [906/938], Loss: 0.5427090525627136\n",
      "Train: Epoch [17], Batch [907/938], Loss: 0.3168444335460663\n",
      "Train: Epoch [17], Batch [908/938], Loss: 0.6832188367843628\n",
      "Train: Epoch [17], Batch [909/938], Loss: 0.32031014561653137\n",
      "Train: Epoch [17], Batch [910/938], Loss: 0.3467260003089905\n",
      "Train: Epoch [17], Batch [911/938], Loss: 0.6703248023986816\n",
      "Train: Epoch [17], Batch [912/938], Loss: 0.44511979818344116\n",
      "Train: Epoch [17], Batch [913/938], Loss: 0.38310131430625916\n",
      "Train: Epoch [17], Batch [914/938], Loss: 0.6090938448905945\n",
      "Train: Epoch [17], Batch [915/938], Loss: 0.33421069383621216\n",
      "Train: Epoch [17], Batch [916/938], Loss: 0.4621773362159729\n",
      "Train: Epoch [17], Batch [917/938], Loss: 0.222359299659729\n",
      "Train: Epoch [17], Batch [918/938], Loss: 0.5596213340759277\n",
      "Train: Epoch [17], Batch [919/938], Loss: 0.44746825098991394\n",
      "Train: Epoch [17], Batch [920/938], Loss: 0.8824026584625244\n",
      "Train: Epoch [17], Batch [921/938], Loss: 0.4926491975784302\n",
      "Train: Epoch [17], Batch [922/938], Loss: 0.3434828519821167\n",
      "Train: Epoch [17], Batch [923/938], Loss: 0.5437329411506653\n",
      "Train: Epoch [17], Batch [924/938], Loss: 0.43280649185180664\n",
      "Train: Epoch [17], Batch [925/938], Loss: 0.5148465633392334\n",
      "Train: Epoch [17], Batch [926/938], Loss: 0.4592108130455017\n",
      "Train: Epoch [17], Batch [927/938], Loss: 0.5175847411155701\n",
      "Train: Epoch [17], Batch [928/938], Loss: 0.35464394092559814\n",
      "Train: Epoch [17], Batch [929/938], Loss: 0.38535076379776\n",
      "Train: Epoch [17], Batch [930/938], Loss: 0.37166112661361694\n",
      "Train: Epoch [17], Batch [931/938], Loss: 0.2922287583351135\n",
      "Train: Epoch [17], Batch [932/938], Loss: 0.2753858268260956\n",
      "Train: Epoch [17], Batch [933/938], Loss: 0.3046357035636902\n",
      "Train: Epoch [17], Batch [934/938], Loss: 0.4916638731956482\n",
      "Train: Epoch [17], Batch [935/938], Loss: 0.48211565613746643\n",
      "Train: Epoch [17], Batch [936/938], Loss: 0.5185410976409912\n",
      "Train: Epoch [17], Batch [937/938], Loss: 0.44958236813545227\n",
      "Train: Epoch [17], Batch [938/938], Loss: 0.4715656340122223\n",
      "Accuracy of train set: 0.8458833333333333\n",
      "Validation: Epoch [17], Batch [1/938], Loss: 0.5952438712120056\n",
      "Validation: Epoch [17], Batch [2/938], Loss: 0.34443199634552\n",
      "Validation: Epoch [17], Batch [3/938], Loss: 0.227615624666214\n",
      "Validation: Epoch [17], Batch [4/938], Loss: 0.5905694961547852\n",
      "Validation: Epoch [17], Batch [5/938], Loss: 0.40593451261520386\n",
      "Validation: Epoch [17], Batch [6/938], Loss: 0.3224223256111145\n",
      "Validation: Epoch [17], Batch [7/938], Loss: 0.5708221197128296\n",
      "Validation: Epoch [17], Batch [8/938], Loss: 0.36223918199539185\n",
      "Validation: Epoch [17], Batch [9/938], Loss: 0.36444488167762756\n",
      "Validation: Epoch [17], Batch [10/938], Loss: 0.39817625284194946\n",
      "Validation: Epoch [17], Batch [11/938], Loss: 0.36834582686424255\n",
      "Validation: Epoch [17], Batch [12/938], Loss: 0.7009875178337097\n",
      "Validation: Epoch [17], Batch [13/938], Loss: 0.27016183733940125\n",
      "Validation: Epoch [17], Batch [14/938], Loss: 0.5030848383903503\n",
      "Validation: Epoch [17], Batch [15/938], Loss: 0.4747141897678375\n",
      "Validation: Epoch [17], Batch [16/938], Loss: 0.5271481275558472\n",
      "Validation: Epoch [17], Batch [17/938], Loss: 0.44609907269477844\n",
      "Validation: Epoch [17], Batch [18/938], Loss: 0.5107748508453369\n",
      "Validation: Epoch [17], Batch [19/938], Loss: 0.37413060665130615\n",
      "Validation: Epoch [17], Batch [20/938], Loss: 0.6500329971313477\n",
      "Validation: Epoch [17], Batch [21/938], Loss: 0.4961903989315033\n",
      "Validation: Epoch [17], Batch [22/938], Loss: 0.38199615478515625\n",
      "Validation: Epoch [17], Batch [23/938], Loss: 0.44965630769729614\n",
      "Validation: Epoch [17], Batch [24/938], Loss: 0.40013644099235535\n",
      "Validation: Epoch [17], Batch [25/938], Loss: 0.5454144477844238\n",
      "Validation: Epoch [17], Batch [26/938], Loss: 0.7163829803466797\n",
      "Validation: Epoch [17], Batch [27/938], Loss: 0.4191354513168335\n",
      "Validation: Epoch [17], Batch [28/938], Loss: 0.6100082397460938\n",
      "Validation: Epoch [17], Batch [29/938], Loss: 0.4459236264228821\n",
      "Validation: Epoch [17], Batch [30/938], Loss: 0.5121235847473145\n",
      "Validation: Epoch [17], Batch [31/938], Loss: 0.4648168683052063\n",
      "Validation: Epoch [17], Batch [32/938], Loss: 0.38410526514053345\n",
      "Validation: Epoch [17], Batch [33/938], Loss: 0.33606553077697754\n",
      "Validation: Epoch [17], Batch [34/938], Loss: 0.400534063577652\n",
      "Validation: Epoch [17], Batch [35/938], Loss: 0.4490957260131836\n",
      "Validation: Epoch [17], Batch [36/938], Loss: 0.443498820066452\n",
      "Validation: Epoch [17], Batch [37/938], Loss: 0.3510028123855591\n",
      "Validation: Epoch [17], Batch [38/938], Loss: 0.44539836049079895\n",
      "Validation: Epoch [17], Batch [39/938], Loss: 0.4782724380493164\n",
      "Validation: Epoch [17], Batch [40/938], Loss: 0.5224786400794983\n",
      "Validation: Epoch [17], Batch [41/938], Loss: 0.4301440715789795\n",
      "Validation: Epoch [17], Batch [42/938], Loss: 0.41208624839782715\n",
      "Validation: Epoch [17], Batch [43/938], Loss: 0.425724059343338\n",
      "Validation: Epoch [17], Batch [44/938], Loss: 0.5025416612625122\n",
      "Validation: Epoch [17], Batch [45/938], Loss: 0.45071154832839966\n",
      "Validation: Epoch [17], Batch [46/938], Loss: 0.32323306798934937\n",
      "Validation: Epoch [17], Batch [47/938], Loss: 0.5856841206550598\n",
      "Validation: Epoch [17], Batch [48/938], Loss: 0.5524325370788574\n",
      "Validation: Epoch [17], Batch [49/938], Loss: 0.26543545722961426\n",
      "Validation: Epoch [17], Batch [50/938], Loss: 0.42198532819747925\n",
      "Validation: Epoch [17], Batch [51/938], Loss: 0.31551530957221985\n",
      "Validation: Epoch [17], Batch [52/938], Loss: 0.49183323979377747\n",
      "Validation: Epoch [17], Batch [53/938], Loss: 0.5497429370880127\n",
      "Validation: Epoch [17], Batch [54/938], Loss: 0.43921956419944763\n",
      "Validation: Epoch [17], Batch [55/938], Loss: 0.44783514738082886\n",
      "Validation: Epoch [17], Batch [56/938], Loss: 0.41806846857070923\n",
      "Validation: Epoch [17], Batch [57/938], Loss: 0.2473345398902893\n",
      "Validation: Epoch [17], Batch [58/938], Loss: 0.41101425886154175\n",
      "Validation: Epoch [17], Batch [59/938], Loss: 0.5158913135528564\n",
      "Validation: Epoch [17], Batch [60/938], Loss: 0.5190087556838989\n",
      "Validation: Epoch [17], Batch [61/938], Loss: 0.7236499190330505\n",
      "Validation: Epoch [17], Batch [62/938], Loss: 0.3674171566963196\n",
      "Validation: Epoch [17], Batch [63/938], Loss: 0.3984183073043823\n",
      "Validation: Epoch [17], Batch [64/938], Loss: 0.5825170278549194\n",
      "Validation: Epoch [17], Batch [65/938], Loss: 0.45116740465164185\n",
      "Validation: Epoch [17], Batch [66/938], Loss: 0.4458211064338684\n",
      "Validation: Epoch [17], Batch [67/938], Loss: 0.33718341588974\n",
      "Validation: Epoch [17], Batch [68/938], Loss: 0.39390087127685547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [69/938], Loss: 0.3679582476615906\n",
      "Validation: Epoch [17], Batch [70/938], Loss: 0.22935503721237183\n",
      "Validation: Epoch [17], Batch [71/938], Loss: 0.5056142807006836\n",
      "Validation: Epoch [17], Batch [72/938], Loss: 0.5171077251434326\n",
      "Validation: Epoch [17], Batch [73/938], Loss: 0.31278496980667114\n",
      "Validation: Epoch [17], Batch [74/938], Loss: 0.3813154995441437\n",
      "Validation: Epoch [17], Batch [75/938], Loss: 0.6013480424880981\n",
      "Validation: Epoch [17], Batch [76/938], Loss: 0.3705383539199829\n",
      "Validation: Epoch [17], Batch [77/938], Loss: 0.35652342438697815\n",
      "Validation: Epoch [17], Batch [78/938], Loss: 0.3481496572494507\n",
      "Validation: Epoch [17], Batch [79/938], Loss: 0.75433349609375\n",
      "Validation: Epoch [17], Batch [80/938], Loss: 0.2762698531150818\n",
      "Validation: Epoch [17], Batch [81/938], Loss: 0.24813547730445862\n",
      "Validation: Epoch [17], Batch [82/938], Loss: 0.44667014479637146\n",
      "Validation: Epoch [17], Batch [83/938], Loss: 0.3992863893508911\n",
      "Validation: Epoch [17], Batch [84/938], Loss: 0.42343124747276306\n",
      "Validation: Epoch [17], Batch [85/938], Loss: 0.6162818670272827\n",
      "Validation: Epoch [17], Batch [86/938], Loss: 0.3647516965866089\n",
      "Validation: Epoch [17], Batch [87/938], Loss: 0.4662896990776062\n",
      "Validation: Epoch [17], Batch [88/938], Loss: 0.3689194321632385\n",
      "Validation: Epoch [17], Batch [89/938], Loss: 0.537763237953186\n",
      "Validation: Epoch [17], Batch [90/938], Loss: 0.38204994797706604\n",
      "Validation: Epoch [17], Batch [91/938], Loss: 0.4701850116252899\n",
      "Validation: Epoch [17], Batch [92/938], Loss: 0.34939831495285034\n",
      "Validation: Epoch [17], Batch [93/938], Loss: 0.5058762431144714\n",
      "Validation: Epoch [17], Batch [94/938], Loss: 0.4390045702457428\n",
      "Validation: Epoch [17], Batch [95/938], Loss: 0.44195446372032166\n",
      "Validation: Epoch [17], Batch [96/938], Loss: 0.40505367517471313\n",
      "Validation: Epoch [17], Batch [97/938], Loss: 0.39129766821861267\n",
      "Validation: Epoch [17], Batch [98/938], Loss: 0.3550693690776825\n",
      "Validation: Epoch [17], Batch [99/938], Loss: 0.606950044631958\n",
      "Validation: Epoch [17], Batch [100/938], Loss: 0.405032217502594\n",
      "Validation: Epoch [17], Batch [101/938], Loss: 0.27522528171539307\n",
      "Validation: Epoch [17], Batch [102/938], Loss: 0.5338107347488403\n",
      "Validation: Epoch [17], Batch [103/938], Loss: 0.4707961082458496\n",
      "Validation: Epoch [17], Batch [104/938], Loss: 0.3271815776824951\n",
      "Validation: Epoch [17], Batch [105/938], Loss: 0.32543855905532837\n",
      "Validation: Epoch [17], Batch [106/938], Loss: 0.6131710410118103\n",
      "Validation: Epoch [17], Batch [107/938], Loss: 0.4818211793899536\n",
      "Validation: Epoch [17], Batch [108/938], Loss: 0.43602949380874634\n",
      "Validation: Epoch [17], Batch [109/938], Loss: 0.5257801413536072\n",
      "Validation: Epoch [17], Batch [110/938], Loss: 0.5496581196784973\n",
      "Validation: Epoch [17], Batch [111/938], Loss: 0.4911597967147827\n",
      "Validation: Epoch [17], Batch [112/938], Loss: 0.5220939517021179\n",
      "Validation: Epoch [17], Batch [113/938], Loss: 0.34283247590065\n",
      "Validation: Epoch [17], Batch [114/938], Loss: 0.3379300832748413\n",
      "Validation: Epoch [17], Batch [115/938], Loss: 0.47643065452575684\n",
      "Validation: Epoch [17], Batch [116/938], Loss: 0.4331876039505005\n",
      "Validation: Epoch [17], Batch [117/938], Loss: 0.5196859836578369\n",
      "Validation: Epoch [17], Batch [118/938], Loss: 0.5699849724769592\n",
      "Validation: Epoch [17], Batch [119/938], Loss: 0.4338308870792389\n",
      "Validation: Epoch [17], Batch [120/938], Loss: 0.4607093632221222\n",
      "Validation: Epoch [17], Batch [121/938], Loss: 0.5646607875823975\n",
      "Validation: Epoch [17], Batch [122/938], Loss: 0.47084754705429077\n",
      "Validation: Epoch [17], Batch [123/938], Loss: 0.46246588230133057\n",
      "Validation: Epoch [17], Batch [124/938], Loss: 0.36068299412727356\n",
      "Validation: Epoch [17], Batch [125/938], Loss: 0.34475815296173096\n",
      "Validation: Epoch [17], Batch [126/938], Loss: 0.4774653911590576\n",
      "Validation: Epoch [17], Batch [127/938], Loss: 0.3407304883003235\n",
      "Validation: Epoch [17], Batch [128/938], Loss: 0.49692192673683167\n",
      "Validation: Epoch [17], Batch [129/938], Loss: 0.48531776666641235\n",
      "Validation: Epoch [17], Batch [130/938], Loss: 0.534345805644989\n",
      "Validation: Epoch [17], Batch [131/938], Loss: 0.5551190972328186\n",
      "Validation: Epoch [17], Batch [132/938], Loss: 0.3190220892429352\n",
      "Validation: Epoch [17], Batch [133/938], Loss: 0.26020094752311707\n",
      "Validation: Epoch [17], Batch [134/938], Loss: 0.521857738494873\n",
      "Validation: Epoch [17], Batch [135/938], Loss: 0.5694794058799744\n",
      "Validation: Epoch [17], Batch [136/938], Loss: 0.6065454483032227\n",
      "Validation: Epoch [17], Batch [137/938], Loss: 0.6782827377319336\n",
      "Validation: Epoch [17], Batch [138/938], Loss: 0.3840881586074829\n",
      "Validation: Epoch [17], Batch [139/938], Loss: 0.438654363155365\n",
      "Validation: Epoch [17], Batch [140/938], Loss: 0.3528962731361389\n",
      "Validation: Epoch [17], Batch [141/938], Loss: 0.7449890375137329\n",
      "Validation: Epoch [17], Batch [142/938], Loss: 0.32052087783813477\n",
      "Validation: Epoch [17], Batch [143/938], Loss: 0.410391628742218\n",
      "Validation: Epoch [17], Batch [144/938], Loss: 0.4145568311214447\n",
      "Validation: Epoch [17], Batch [145/938], Loss: 0.40835267305374146\n",
      "Validation: Epoch [17], Batch [146/938], Loss: 0.2181806117296219\n",
      "Validation: Epoch [17], Batch [147/938], Loss: 0.3849113881587982\n",
      "Validation: Epoch [17], Batch [148/938], Loss: 0.4019690752029419\n",
      "Validation: Epoch [17], Batch [149/938], Loss: 0.309067964553833\n",
      "Validation: Epoch [17], Batch [150/938], Loss: 0.5980855822563171\n",
      "Validation: Epoch [17], Batch [151/938], Loss: 0.40709489583969116\n",
      "Validation: Epoch [17], Batch [152/938], Loss: 0.42992302775382996\n",
      "Validation: Epoch [17], Batch [153/938], Loss: 0.5312464833259583\n",
      "Validation: Epoch [17], Batch [154/938], Loss: 0.5365472435951233\n",
      "Validation: Epoch [17], Batch [155/938], Loss: 0.4446222186088562\n",
      "Validation: Epoch [17], Batch [156/938], Loss: 0.46541908383369446\n",
      "Validation: Epoch [17], Batch [157/938], Loss: 0.21057677268981934\n",
      "Validation: Epoch [17], Batch [158/938], Loss: 0.34574127197265625\n",
      "Validation: Epoch [17], Batch [159/938], Loss: 0.3853543698787689\n",
      "Validation: Epoch [17], Batch [160/938], Loss: 0.6098502278327942\n",
      "Validation: Epoch [17], Batch [161/938], Loss: 0.3563285171985626\n",
      "Validation: Epoch [17], Batch [162/938], Loss: 0.3584536910057068\n",
      "Validation: Epoch [17], Batch [163/938], Loss: 0.3483249545097351\n",
      "Validation: Epoch [17], Batch [164/938], Loss: 0.31402838230133057\n",
      "Validation: Epoch [17], Batch [165/938], Loss: 0.4635303020477295\n",
      "Validation: Epoch [17], Batch [166/938], Loss: 0.5261451005935669\n",
      "Validation: Epoch [17], Batch [167/938], Loss: 0.4069010317325592\n",
      "Validation: Epoch [17], Batch [168/938], Loss: 0.37280189990997314\n",
      "Validation: Epoch [17], Batch [169/938], Loss: 0.4441675543785095\n",
      "Validation: Epoch [17], Batch [170/938], Loss: 0.4501853585243225\n",
      "Validation: Epoch [17], Batch [171/938], Loss: 0.3744811415672302\n",
      "Validation: Epoch [17], Batch [172/938], Loss: 0.4433593153953552\n",
      "Validation: Epoch [17], Batch [173/938], Loss: 0.45250368118286133\n",
      "Validation: Epoch [17], Batch [174/938], Loss: 0.3478787839412689\n",
      "Validation: Epoch [17], Batch [175/938], Loss: 0.5038443207740784\n",
      "Validation: Epoch [17], Batch [176/938], Loss: 0.4099274277687073\n",
      "Validation: Epoch [17], Batch [177/938], Loss: 0.35080769658088684\n",
      "Validation: Epoch [17], Batch [178/938], Loss: 0.3446882367134094\n",
      "Validation: Epoch [17], Batch [179/938], Loss: 0.5033804774284363\n",
      "Validation: Epoch [17], Batch [180/938], Loss: 0.31192830204963684\n",
      "Validation: Epoch [17], Batch [181/938], Loss: 0.4768812656402588\n",
      "Validation: Epoch [17], Batch [182/938], Loss: 0.31605303287506104\n",
      "Validation: Epoch [17], Batch [183/938], Loss: 0.7295883297920227\n",
      "Validation: Epoch [17], Batch [184/938], Loss: 0.5379393100738525\n",
      "Validation: Epoch [17], Batch [185/938], Loss: 0.4207874536514282\n",
      "Validation: Epoch [17], Batch [186/938], Loss: 0.5019203424453735\n",
      "Validation: Epoch [17], Batch [187/938], Loss: 0.48061710596084595\n",
      "Validation: Epoch [17], Batch [188/938], Loss: 0.40471524000167847\n",
      "Validation: Epoch [17], Batch [189/938], Loss: 0.3289881944656372\n",
      "Validation: Epoch [17], Batch [190/938], Loss: 0.5226483941078186\n",
      "Validation: Epoch [17], Batch [191/938], Loss: 0.4748188257217407\n",
      "Validation: Epoch [17], Batch [192/938], Loss: 0.2719515562057495\n",
      "Validation: Epoch [17], Batch [193/938], Loss: 0.4746604859828949\n",
      "Validation: Epoch [17], Batch [194/938], Loss: 0.5232164263725281\n",
      "Validation: Epoch [17], Batch [195/938], Loss: 0.3274179697036743\n",
      "Validation: Epoch [17], Batch [196/938], Loss: 0.4735068678855896\n",
      "Validation: Epoch [17], Batch [197/938], Loss: 0.37255188822746277\n",
      "Validation: Epoch [17], Batch [198/938], Loss: 0.45773226022720337\n",
      "Validation: Epoch [17], Batch [199/938], Loss: 0.420248806476593\n",
      "Validation: Epoch [17], Batch [200/938], Loss: 0.4818990230560303\n",
      "Validation: Epoch [17], Batch [201/938], Loss: 0.45824524760246277\n",
      "Validation: Epoch [17], Batch [202/938], Loss: 0.398049920797348\n",
      "Validation: Epoch [17], Batch [203/938], Loss: 0.48304298520088196\n",
      "Validation: Epoch [17], Batch [204/938], Loss: 0.3928735852241516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [205/938], Loss: 0.4783598780632019\n",
      "Validation: Epoch [17], Batch [206/938], Loss: 0.24989332258701324\n",
      "Validation: Epoch [17], Batch [207/938], Loss: 0.35803869366645813\n",
      "Validation: Epoch [17], Batch [208/938], Loss: 0.34464046359062195\n",
      "Validation: Epoch [17], Batch [209/938], Loss: 0.411199688911438\n",
      "Validation: Epoch [17], Batch [210/938], Loss: 0.2546957731246948\n",
      "Validation: Epoch [17], Batch [211/938], Loss: 0.27785786986351013\n",
      "Validation: Epoch [17], Batch [212/938], Loss: 0.30860769748687744\n",
      "Validation: Epoch [17], Batch [213/938], Loss: 0.3425463140010834\n",
      "Validation: Epoch [17], Batch [214/938], Loss: 0.37578505277633667\n",
      "Validation: Epoch [17], Batch [215/938], Loss: 0.4853842556476593\n",
      "Validation: Epoch [17], Batch [216/938], Loss: 0.38966381549835205\n",
      "Validation: Epoch [17], Batch [217/938], Loss: 0.42286449670791626\n",
      "Validation: Epoch [17], Batch [218/938], Loss: 0.3335339426994324\n",
      "Validation: Epoch [17], Batch [219/938], Loss: 0.46543949842453003\n",
      "Validation: Epoch [17], Batch [220/938], Loss: 0.4071453809738159\n",
      "Validation: Epoch [17], Batch [221/938], Loss: 0.4049326181411743\n",
      "Validation: Epoch [17], Batch [222/938], Loss: 0.3973199725151062\n",
      "Validation: Epoch [17], Batch [223/938], Loss: 0.6189534068107605\n",
      "Validation: Epoch [17], Batch [224/938], Loss: 0.6434170603752136\n",
      "Validation: Epoch [17], Batch [225/938], Loss: 0.39097732305526733\n",
      "Validation: Epoch [17], Batch [226/938], Loss: 0.3171769678592682\n",
      "Validation: Epoch [17], Batch [227/938], Loss: 0.34681642055511475\n",
      "Validation: Epoch [17], Batch [228/938], Loss: 0.2595905363559723\n",
      "Validation: Epoch [17], Batch [229/938], Loss: 0.33074116706848145\n",
      "Validation: Epoch [17], Batch [230/938], Loss: 0.5101686716079712\n",
      "Validation: Epoch [17], Batch [231/938], Loss: 0.5389474630355835\n",
      "Validation: Epoch [17], Batch [232/938], Loss: 0.5364213585853577\n",
      "Validation: Epoch [17], Batch [233/938], Loss: 0.2991654872894287\n",
      "Validation: Epoch [17], Batch [234/938], Loss: 0.41673433780670166\n",
      "Validation: Epoch [17], Batch [235/938], Loss: 0.34684839844703674\n",
      "Validation: Epoch [17], Batch [236/938], Loss: 0.27870190143585205\n",
      "Validation: Epoch [17], Batch [237/938], Loss: 0.27639806270599365\n",
      "Validation: Epoch [17], Batch [238/938], Loss: 0.5944787859916687\n",
      "Validation: Epoch [17], Batch [239/938], Loss: 0.4057861566543579\n",
      "Validation: Epoch [17], Batch [240/938], Loss: 0.33020728826522827\n",
      "Validation: Epoch [17], Batch [241/938], Loss: 0.2414640337228775\n",
      "Validation: Epoch [17], Batch [242/938], Loss: 0.52005535364151\n",
      "Validation: Epoch [17], Batch [243/938], Loss: 0.4500512480735779\n",
      "Validation: Epoch [17], Batch [244/938], Loss: 0.39278364181518555\n",
      "Validation: Epoch [17], Batch [245/938], Loss: 0.3333215117454529\n",
      "Validation: Epoch [17], Batch [246/938], Loss: 0.3648608326911926\n",
      "Validation: Epoch [17], Batch [247/938], Loss: 0.3009089231491089\n",
      "Validation: Epoch [17], Batch [248/938], Loss: 0.35657382011413574\n",
      "Validation: Epoch [17], Batch [249/938], Loss: 0.33895379304885864\n",
      "Validation: Epoch [17], Batch [250/938], Loss: 0.614964485168457\n",
      "Validation: Epoch [17], Batch [251/938], Loss: 0.5317087173461914\n",
      "Validation: Epoch [17], Batch [252/938], Loss: 0.5320418477058411\n",
      "Validation: Epoch [17], Batch [253/938], Loss: 0.2572462558746338\n",
      "Validation: Epoch [17], Batch [254/938], Loss: 0.5511157512664795\n",
      "Validation: Epoch [17], Batch [255/938], Loss: 0.38634413480758667\n",
      "Validation: Epoch [17], Batch [256/938], Loss: 0.605584442615509\n",
      "Validation: Epoch [17], Batch [257/938], Loss: 0.5514898300170898\n",
      "Validation: Epoch [17], Batch [258/938], Loss: 0.4670525789260864\n",
      "Validation: Epoch [17], Batch [259/938], Loss: 0.5297563076019287\n",
      "Validation: Epoch [17], Batch [260/938], Loss: 0.4832163453102112\n",
      "Validation: Epoch [17], Batch [261/938], Loss: 0.5061125755310059\n",
      "Validation: Epoch [17], Batch [262/938], Loss: 0.41409334540367126\n",
      "Validation: Epoch [17], Batch [263/938], Loss: 0.30633100867271423\n",
      "Validation: Epoch [17], Batch [264/938], Loss: 0.49297574162483215\n",
      "Validation: Epoch [17], Batch [265/938], Loss: 0.4105342626571655\n",
      "Validation: Epoch [17], Batch [266/938], Loss: 0.383208692073822\n",
      "Validation: Epoch [17], Batch [267/938], Loss: 0.5856545567512512\n",
      "Validation: Epoch [17], Batch [268/938], Loss: 0.6077826023101807\n",
      "Validation: Epoch [17], Batch [269/938], Loss: 0.35994505882263184\n",
      "Validation: Epoch [17], Batch [270/938], Loss: 0.46971559524536133\n",
      "Validation: Epoch [17], Batch [271/938], Loss: 0.4561423361301422\n",
      "Validation: Epoch [17], Batch [272/938], Loss: 0.3662051558494568\n",
      "Validation: Epoch [17], Batch [273/938], Loss: 0.5292614102363586\n",
      "Validation: Epoch [17], Batch [274/938], Loss: 0.4252035915851593\n",
      "Validation: Epoch [17], Batch [275/938], Loss: 0.52419513463974\n",
      "Validation: Epoch [17], Batch [276/938], Loss: 0.4566674828529358\n",
      "Validation: Epoch [17], Batch [277/938], Loss: 0.4295942187309265\n",
      "Validation: Epoch [17], Batch [278/938], Loss: 0.301420658826828\n",
      "Validation: Epoch [17], Batch [279/938], Loss: 0.41257786750793457\n",
      "Validation: Epoch [17], Batch [280/938], Loss: 0.4721406400203705\n",
      "Validation: Epoch [17], Batch [281/938], Loss: 0.3742391765117645\n",
      "Validation: Epoch [17], Batch [282/938], Loss: 0.3935368061065674\n",
      "Validation: Epoch [17], Batch [283/938], Loss: 0.37674641609191895\n",
      "Validation: Epoch [17], Batch [284/938], Loss: 0.7125271558761597\n",
      "Validation: Epoch [17], Batch [285/938], Loss: 0.41123300790786743\n",
      "Validation: Epoch [17], Batch [286/938], Loss: 0.34504973888397217\n",
      "Validation: Epoch [17], Batch [287/938], Loss: 0.3513098955154419\n",
      "Validation: Epoch [17], Batch [288/938], Loss: 0.5798846483230591\n",
      "Validation: Epoch [17], Batch [289/938], Loss: 0.5018402338027954\n",
      "Validation: Epoch [17], Batch [290/938], Loss: 0.32155004143714905\n",
      "Validation: Epoch [17], Batch [291/938], Loss: 0.39969316124916077\n",
      "Validation: Epoch [17], Batch [292/938], Loss: 0.4087093770503998\n",
      "Validation: Epoch [17], Batch [293/938], Loss: 0.38725927472114563\n",
      "Validation: Epoch [17], Batch [294/938], Loss: 0.36238181591033936\n",
      "Validation: Epoch [17], Batch [295/938], Loss: 0.3877263069152832\n",
      "Validation: Epoch [17], Batch [296/938], Loss: 0.30161821842193604\n",
      "Validation: Epoch [17], Batch [297/938], Loss: 0.414509117603302\n",
      "Validation: Epoch [17], Batch [298/938], Loss: 0.3584189713001251\n",
      "Validation: Epoch [17], Batch [299/938], Loss: 0.4846861660480499\n",
      "Validation: Epoch [17], Batch [300/938], Loss: 0.47205883264541626\n",
      "Validation: Epoch [17], Batch [301/938], Loss: 0.4693957269191742\n",
      "Validation: Epoch [17], Batch [302/938], Loss: 0.3690524101257324\n",
      "Validation: Epoch [17], Batch [303/938], Loss: 0.49319756031036377\n",
      "Validation: Epoch [17], Batch [304/938], Loss: 0.42533522844314575\n",
      "Validation: Epoch [17], Batch [305/938], Loss: 0.39950281381607056\n",
      "Validation: Epoch [17], Batch [306/938], Loss: 0.3171268105506897\n",
      "Validation: Epoch [17], Batch [307/938], Loss: 0.6159799695014954\n",
      "Validation: Epoch [17], Batch [308/938], Loss: 0.5816352963447571\n",
      "Validation: Epoch [17], Batch [309/938], Loss: 0.46662968397140503\n",
      "Validation: Epoch [17], Batch [310/938], Loss: 0.3713777959346771\n",
      "Validation: Epoch [17], Batch [311/938], Loss: 0.7485071420669556\n",
      "Validation: Epoch [17], Batch [312/938], Loss: 0.35178810358047485\n",
      "Validation: Epoch [17], Batch [313/938], Loss: 0.548974871635437\n",
      "Validation: Epoch [17], Batch [314/938], Loss: 0.3736185133457184\n",
      "Validation: Epoch [17], Batch [315/938], Loss: 0.3634081482887268\n",
      "Validation: Epoch [17], Batch [316/938], Loss: 0.5868327617645264\n",
      "Validation: Epoch [17], Batch [317/938], Loss: 0.4435534179210663\n",
      "Validation: Epoch [17], Batch [318/938], Loss: 0.3382759690284729\n",
      "Validation: Epoch [17], Batch [319/938], Loss: 0.2618741989135742\n",
      "Validation: Epoch [17], Batch [320/938], Loss: 0.6997004151344299\n",
      "Validation: Epoch [17], Batch [321/938], Loss: 0.501419186592102\n",
      "Validation: Epoch [17], Batch [322/938], Loss: 0.397981733083725\n",
      "Validation: Epoch [17], Batch [323/938], Loss: 0.4162328541278839\n",
      "Validation: Epoch [17], Batch [324/938], Loss: 0.4832293391227722\n",
      "Validation: Epoch [17], Batch [325/938], Loss: 0.45794129371643066\n",
      "Validation: Epoch [17], Batch [326/938], Loss: 0.6375241279602051\n",
      "Validation: Epoch [17], Batch [327/938], Loss: 0.5286033153533936\n",
      "Validation: Epoch [17], Batch [328/938], Loss: 0.5399713516235352\n",
      "Validation: Epoch [17], Batch [329/938], Loss: 0.3148798942565918\n",
      "Validation: Epoch [17], Batch [330/938], Loss: 0.5073012113571167\n",
      "Validation: Epoch [17], Batch [331/938], Loss: 0.4911618232727051\n",
      "Validation: Epoch [17], Batch [332/938], Loss: 0.40898197889328003\n",
      "Validation: Epoch [17], Batch [333/938], Loss: 0.6218917369842529\n",
      "Validation: Epoch [17], Batch [334/938], Loss: 0.49301308393478394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [335/938], Loss: 0.42924368381500244\n",
      "Validation: Epoch [17], Batch [336/938], Loss: 0.40437179803848267\n",
      "Validation: Epoch [17], Batch [337/938], Loss: 0.3843560814857483\n",
      "Validation: Epoch [17], Batch [338/938], Loss: 0.35886120796203613\n",
      "Validation: Epoch [17], Batch [339/938], Loss: 0.4735199809074402\n",
      "Validation: Epoch [17], Batch [340/938], Loss: 0.3108753561973572\n",
      "Validation: Epoch [17], Batch [341/938], Loss: 0.6242720484733582\n",
      "Validation: Epoch [17], Batch [342/938], Loss: 0.4587108790874481\n",
      "Validation: Epoch [17], Batch [343/938], Loss: 0.37590593099594116\n",
      "Validation: Epoch [17], Batch [344/938], Loss: 0.4557344913482666\n",
      "Validation: Epoch [17], Batch [345/938], Loss: 0.5215411186218262\n",
      "Validation: Epoch [17], Batch [346/938], Loss: 0.3767531216144562\n",
      "Validation: Epoch [17], Batch [347/938], Loss: 0.32592374086380005\n",
      "Validation: Epoch [17], Batch [348/938], Loss: 0.5613722205162048\n",
      "Validation: Epoch [17], Batch [349/938], Loss: 0.6883776187896729\n",
      "Validation: Epoch [17], Batch [350/938], Loss: 0.5263949036598206\n",
      "Validation: Epoch [17], Batch [351/938], Loss: 0.4564081132411957\n",
      "Validation: Epoch [17], Batch [352/938], Loss: 0.3750530779361725\n",
      "Validation: Epoch [17], Batch [353/938], Loss: 0.5287753939628601\n",
      "Validation: Epoch [17], Batch [354/938], Loss: 0.5019164085388184\n",
      "Validation: Epoch [17], Batch [355/938], Loss: 0.37638625502586365\n",
      "Validation: Epoch [17], Batch [356/938], Loss: 0.395794153213501\n",
      "Validation: Epoch [17], Batch [357/938], Loss: 0.43515896797180176\n",
      "Validation: Epoch [17], Batch [358/938], Loss: 0.49598777294158936\n",
      "Validation: Epoch [17], Batch [359/938], Loss: 0.26348602771759033\n",
      "Validation: Epoch [17], Batch [360/938], Loss: 0.6834204792976379\n",
      "Validation: Epoch [17], Batch [361/938], Loss: 0.2885342240333557\n",
      "Validation: Epoch [17], Batch [362/938], Loss: 0.3358190655708313\n",
      "Validation: Epoch [17], Batch [363/938], Loss: 0.4368726313114166\n",
      "Validation: Epoch [17], Batch [364/938], Loss: 0.44155046343803406\n",
      "Validation: Epoch [17], Batch [365/938], Loss: 0.6295551061630249\n",
      "Validation: Epoch [17], Batch [366/938], Loss: 0.3590015769004822\n",
      "Validation: Epoch [17], Batch [367/938], Loss: 0.5821126103401184\n",
      "Validation: Epoch [17], Batch [368/938], Loss: 0.42348232865333557\n",
      "Validation: Epoch [17], Batch [369/938], Loss: 0.4265936315059662\n",
      "Validation: Epoch [17], Batch [370/938], Loss: 0.2699417471885681\n",
      "Validation: Epoch [17], Batch [371/938], Loss: 0.39753109216690063\n",
      "Validation: Epoch [17], Batch [372/938], Loss: 0.7789683938026428\n",
      "Validation: Epoch [17], Batch [373/938], Loss: 0.3137657642364502\n",
      "Validation: Epoch [17], Batch [374/938], Loss: 0.5369016528129578\n",
      "Validation: Epoch [17], Batch [375/938], Loss: 0.3838679790496826\n",
      "Validation: Epoch [17], Batch [376/938], Loss: 0.39054563641548157\n",
      "Validation: Epoch [17], Batch [377/938], Loss: 0.33482271432876587\n",
      "Validation: Epoch [17], Batch [378/938], Loss: 0.37791019678115845\n",
      "Validation: Epoch [17], Batch [379/938], Loss: 0.4623309075832367\n",
      "Validation: Epoch [17], Batch [380/938], Loss: 0.3591860234737396\n",
      "Validation: Epoch [17], Batch [381/938], Loss: 0.5413933992385864\n",
      "Validation: Epoch [17], Batch [382/938], Loss: 0.4607911705970764\n",
      "Validation: Epoch [17], Batch [383/938], Loss: 0.43895602226257324\n",
      "Validation: Epoch [17], Batch [384/938], Loss: 0.3179296851158142\n",
      "Validation: Epoch [17], Batch [385/938], Loss: 0.397319495677948\n",
      "Validation: Epoch [17], Batch [386/938], Loss: 0.5518895983695984\n",
      "Validation: Epoch [17], Batch [387/938], Loss: 0.3187248706817627\n",
      "Validation: Epoch [17], Batch [388/938], Loss: 0.4102157950401306\n",
      "Validation: Epoch [17], Batch [389/938], Loss: 0.44313308596611023\n",
      "Validation: Epoch [17], Batch [390/938], Loss: 0.3898082971572876\n",
      "Validation: Epoch [17], Batch [391/938], Loss: 0.4342349171638489\n",
      "Validation: Epoch [17], Batch [392/938], Loss: 0.4078989326953888\n",
      "Validation: Epoch [17], Batch [393/938], Loss: 0.3087480962276459\n",
      "Validation: Epoch [17], Batch [394/938], Loss: 0.3815954923629761\n",
      "Validation: Epoch [17], Batch [395/938], Loss: 0.4210715889930725\n",
      "Validation: Epoch [17], Batch [396/938], Loss: 0.5299167633056641\n",
      "Validation: Epoch [17], Batch [397/938], Loss: 0.44515812397003174\n",
      "Validation: Epoch [17], Batch [398/938], Loss: 0.5563206076622009\n",
      "Validation: Epoch [17], Batch [399/938], Loss: 0.40914440155029297\n",
      "Validation: Epoch [17], Batch [400/938], Loss: 0.49115121364593506\n",
      "Validation: Epoch [17], Batch [401/938], Loss: 0.4263361394405365\n",
      "Validation: Epoch [17], Batch [402/938], Loss: 0.4986306428909302\n",
      "Validation: Epoch [17], Batch [403/938], Loss: 0.4936767816543579\n",
      "Validation: Epoch [17], Batch [404/938], Loss: 0.6137311458587646\n",
      "Validation: Epoch [17], Batch [405/938], Loss: 0.36364123225212097\n",
      "Validation: Epoch [17], Batch [406/938], Loss: 0.30034705996513367\n",
      "Validation: Epoch [17], Batch [407/938], Loss: 0.526330292224884\n",
      "Validation: Epoch [17], Batch [408/938], Loss: 0.35008591413497925\n",
      "Validation: Epoch [17], Batch [409/938], Loss: 0.43022823333740234\n",
      "Validation: Epoch [17], Batch [410/938], Loss: 0.4130297601222992\n",
      "Validation: Epoch [17], Batch [411/938], Loss: 0.4299386143684387\n",
      "Validation: Epoch [17], Batch [412/938], Loss: 0.48086971044540405\n",
      "Validation: Epoch [17], Batch [413/938], Loss: 0.4678284525871277\n",
      "Validation: Epoch [17], Batch [414/938], Loss: 0.4284539222717285\n",
      "Validation: Epoch [17], Batch [415/938], Loss: 0.6251215934753418\n",
      "Validation: Epoch [17], Batch [416/938], Loss: 0.5360193848609924\n",
      "Validation: Epoch [17], Batch [417/938], Loss: 0.3889187276363373\n",
      "Validation: Epoch [17], Batch [418/938], Loss: 0.6222531795501709\n",
      "Validation: Epoch [17], Batch [419/938], Loss: 0.48575559258461\n",
      "Validation: Epoch [17], Batch [420/938], Loss: 0.5825940370559692\n",
      "Validation: Epoch [17], Batch [421/938], Loss: 0.4437630772590637\n",
      "Validation: Epoch [17], Batch [422/938], Loss: 0.4342539608478546\n",
      "Validation: Epoch [17], Batch [423/938], Loss: 0.5079362392425537\n",
      "Validation: Epoch [17], Batch [424/938], Loss: 0.5052419900894165\n",
      "Validation: Epoch [17], Batch [425/938], Loss: 0.40499740839004517\n",
      "Validation: Epoch [17], Batch [426/938], Loss: 0.31843268871307373\n",
      "Validation: Epoch [17], Batch [427/938], Loss: 0.30563902854919434\n",
      "Validation: Epoch [17], Batch [428/938], Loss: 0.3244644105434418\n",
      "Validation: Epoch [17], Batch [429/938], Loss: 0.36938250064849854\n",
      "Validation: Epoch [17], Batch [430/938], Loss: 0.5716433525085449\n",
      "Validation: Epoch [17], Batch [431/938], Loss: 0.6582657098770142\n",
      "Validation: Epoch [17], Batch [432/938], Loss: 0.4560854732990265\n",
      "Validation: Epoch [17], Batch [433/938], Loss: 0.3968394696712494\n",
      "Validation: Epoch [17], Batch [434/938], Loss: 0.42084944248199463\n",
      "Validation: Epoch [17], Batch [435/938], Loss: 0.4737296998500824\n",
      "Validation: Epoch [17], Batch [436/938], Loss: 0.6745439767837524\n",
      "Validation: Epoch [17], Batch [437/938], Loss: 0.19337597489356995\n",
      "Validation: Epoch [17], Batch [438/938], Loss: 0.4249587059020996\n",
      "Validation: Epoch [17], Batch [439/938], Loss: 0.45499828457832336\n",
      "Validation: Epoch [17], Batch [440/938], Loss: 0.37591537833213806\n",
      "Validation: Epoch [17], Batch [441/938], Loss: 0.4538635015487671\n",
      "Validation: Epoch [17], Batch [442/938], Loss: 0.30751749873161316\n",
      "Validation: Epoch [17], Batch [443/938], Loss: 0.3647233247756958\n",
      "Validation: Epoch [17], Batch [444/938], Loss: 0.473375529050827\n",
      "Validation: Epoch [17], Batch [445/938], Loss: 0.36466386914253235\n",
      "Validation: Epoch [17], Batch [446/938], Loss: 0.5479829907417297\n",
      "Validation: Epoch [17], Batch [447/938], Loss: 0.37709516286849976\n",
      "Validation: Epoch [17], Batch [448/938], Loss: 0.41173622012138367\n",
      "Validation: Epoch [17], Batch [449/938], Loss: 0.4877718687057495\n",
      "Validation: Epoch [17], Batch [450/938], Loss: 0.4858875274658203\n",
      "Validation: Epoch [17], Batch [451/938], Loss: 0.30833718180656433\n",
      "Validation: Epoch [17], Batch [452/938], Loss: 0.4919474124908447\n",
      "Validation: Epoch [17], Batch [453/938], Loss: 0.5557266473770142\n",
      "Validation: Epoch [17], Batch [454/938], Loss: 0.47286441922187805\n",
      "Validation: Epoch [17], Batch [455/938], Loss: 0.3565964698791504\n",
      "Validation: Epoch [17], Batch [456/938], Loss: 0.3769271969795227\n",
      "Validation: Epoch [17], Batch [457/938], Loss: 0.5696093440055847\n",
      "Validation: Epoch [17], Batch [458/938], Loss: 0.4285191297531128\n",
      "Validation: Epoch [17], Batch [459/938], Loss: 0.4071725010871887\n",
      "Validation: Epoch [17], Batch [460/938], Loss: 0.4068487882614136\n",
      "Validation: Epoch [17], Batch [461/938], Loss: 0.7672386765480042\n",
      "Validation: Epoch [17], Batch [462/938], Loss: 0.42950499057769775\n",
      "Validation: Epoch [17], Batch [463/938], Loss: 0.5206881165504456\n",
      "Validation: Epoch [17], Batch [464/938], Loss: 0.3443233072757721\n",
      "Validation: Epoch [17], Batch [465/938], Loss: 0.5127111077308655\n",
      "Validation: Epoch [17], Batch [466/938], Loss: 0.5365356206893921\n",
      "Validation: Epoch [17], Batch [467/938], Loss: 0.4582149386405945\n",
      "Validation: Epoch [17], Batch [468/938], Loss: 0.3939918875694275\n",
      "Validation: Epoch [17], Batch [469/938], Loss: 0.47777634859085083\n",
      "Validation: Epoch [17], Batch [470/938], Loss: 0.4149419665336609\n",
      "Validation: Epoch [17], Batch [471/938], Loss: 0.37517690658569336\n",
      "Validation: Epoch [17], Batch [472/938], Loss: 0.43497687578201294\n",
      "Validation: Epoch [17], Batch [473/938], Loss: 0.4047810733318329\n",
      "Validation: Epoch [17], Batch [474/938], Loss: 0.34335458278656006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [475/938], Loss: 0.5899699926376343\n",
      "Validation: Epoch [17], Batch [476/938], Loss: 0.47265616059303284\n",
      "Validation: Epoch [17], Batch [477/938], Loss: 0.3921308219432831\n",
      "Validation: Epoch [17], Batch [478/938], Loss: 0.348807156085968\n",
      "Validation: Epoch [17], Batch [479/938], Loss: 0.5766130685806274\n",
      "Validation: Epoch [17], Batch [480/938], Loss: 0.3771575391292572\n",
      "Validation: Epoch [17], Batch [481/938], Loss: 0.5565563440322876\n",
      "Validation: Epoch [17], Batch [482/938], Loss: 0.3820323944091797\n",
      "Validation: Epoch [17], Batch [483/938], Loss: 0.4195587635040283\n",
      "Validation: Epoch [17], Batch [484/938], Loss: 0.45417511463165283\n",
      "Validation: Epoch [17], Batch [485/938], Loss: 0.29561135172843933\n",
      "Validation: Epoch [17], Batch [486/938], Loss: 0.37376558780670166\n",
      "Validation: Epoch [17], Batch [487/938], Loss: 0.4287627935409546\n",
      "Validation: Epoch [17], Batch [488/938], Loss: 0.4539082646369934\n",
      "Validation: Epoch [17], Batch [489/938], Loss: 0.3287869095802307\n",
      "Validation: Epoch [17], Batch [490/938], Loss: 0.34185338020324707\n",
      "Validation: Epoch [17], Batch [491/938], Loss: 0.45449382066726685\n",
      "Validation: Epoch [17], Batch [492/938], Loss: 0.6296015977859497\n",
      "Validation: Epoch [17], Batch [493/938], Loss: 0.6593937873840332\n",
      "Validation: Epoch [17], Batch [494/938], Loss: 0.42246466875076294\n",
      "Validation: Epoch [17], Batch [495/938], Loss: 0.43972158432006836\n",
      "Validation: Epoch [17], Batch [496/938], Loss: 0.3773868680000305\n",
      "Validation: Epoch [17], Batch [497/938], Loss: 0.39433422684669495\n",
      "Validation: Epoch [17], Batch [498/938], Loss: 0.5400210618972778\n",
      "Validation: Epoch [17], Batch [499/938], Loss: 0.3985198140144348\n",
      "Validation: Epoch [17], Batch [500/938], Loss: 0.3753872513771057\n",
      "Validation: Epoch [17], Batch [501/938], Loss: 0.2961706519126892\n",
      "Validation: Epoch [17], Batch [502/938], Loss: 0.4274379014968872\n",
      "Validation: Epoch [17], Batch [503/938], Loss: 0.4766753017902374\n",
      "Validation: Epoch [17], Batch [504/938], Loss: 0.5332089066505432\n",
      "Validation: Epoch [17], Batch [505/938], Loss: 0.34780406951904297\n",
      "Validation: Epoch [17], Batch [506/938], Loss: 0.3492931127548218\n",
      "Validation: Epoch [17], Batch [507/938], Loss: 0.29428473114967346\n",
      "Validation: Epoch [17], Batch [508/938], Loss: 0.38522589206695557\n",
      "Validation: Epoch [17], Batch [509/938], Loss: 0.38053014874458313\n",
      "Validation: Epoch [17], Batch [510/938], Loss: 0.5777599811553955\n",
      "Validation: Epoch [17], Batch [511/938], Loss: 0.42625951766967773\n",
      "Validation: Epoch [17], Batch [512/938], Loss: 0.44176197052001953\n",
      "Validation: Epoch [17], Batch [513/938], Loss: 0.49582356214523315\n",
      "Validation: Epoch [17], Batch [514/938], Loss: 0.49745994806289673\n",
      "Validation: Epoch [17], Batch [515/938], Loss: 0.5925945043563843\n",
      "Validation: Epoch [17], Batch [516/938], Loss: 0.4471457600593567\n",
      "Validation: Epoch [17], Batch [517/938], Loss: 0.5100255012512207\n",
      "Validation: Epoch [17], Batch [518/938], Loss: 0.5505415201187134\n",
      "Validation: Epoch [17], Batch [519/938], Loss: 0.5009350180625916\n",
      "Validation: Epoch [17], Batch [520/938], Loss: 0.40039339661598206\n",
      "Validation: Epoch [17], Batch [521/938], Loss: 0.3620932400226593\n",
      "Validation: Epoch [17], Batch [522/938], Loss: 0.43140459060668945\n",
      "Validation: Epoch [17], Batch [523/938], Loss: 0.3733103275299072\n",
      "Validation: Epoch [17], Batch [524/938], Loss: 0.4860362708568573\n",
      "Validation: Epoch [17], Batch [525/938], Loss: 0.43735483288764954\n",
      "Validation: Epoch [17], Batch [526/938], Loss: 0.33896127343177795\n",
      "Validation: Epoch [17], Batch [527/938], Loss: 0.34796687960624695\n",
      "Validation: Epoch [17], Batch [528/938], Loss: 0.5229139924049377\n",
      "Validation: Epoch [17], Batch [529/938], Loss: 0.6147837042808533\n",
      "Validation: Epoch [17], Batch [530/938], Loss: 0.4056352972984314\n",
      "Validation: Epoch [17], Batch [531/938], Loss: 0.4826885163784027\n",
      "Validation: Epoch [17], Batch [532/938], Loss: 0.3299943804740906\n",
      "Validation: Epoch [17], Batch [533/938], Loss: 0.3216395378112793\n",
      "Validation: Epoch [17], Batch [534/938], Loss: 0.3719814717769623\n",
      "Validation: Epoch [17], Batch [535/938], Loss: 0.4198383092880249\n",
      "Validation: Epoch [17], Batch [536/938], Loss: 0.2970517873764038\n",
      "Validation: Epoch [17], Batch [537/938], Loss: 0.3748225271701813\n",
      "Validation: Epoch [17], Batch [538/938], Loss: 0.4451529383659363\n",
      "Validation: Epoch [17], Batch [539/938], Loss: 0.42743927240371704\n",
      "Validation: Epoch [17], Batch [540/938], Loss: 0.5080077052116394\n",
      "Validation: Epoch [17], Batch [541/938], Loss: 0.5354247093200684\n",
      "Validation: Epoch [17], Batch [542/938], Loss: 0.4539559781551361\n",
      "Validation: Epoch [17], Batch [543/938], Loss: 0.5374278426170349\n",
      "Validation: Epoch [17], Batch [544/938], Loss: 0.22823792695999146\n",
      "Validation: Epoch [17], Batch [545/938], Loss: 0.5491780638694763\n",
      "Validation: Epoch [17], Batch [546/938], Loss: 0.5027346014976501\n",
      "Validation: Epoch [17], Batch [547/938], Loss: 0.5402830839157104\n",
      "Validation: Epoch [17], Batch [548/938], Loss: 0.3778855800628662\n",
      "Validation: Epoch [17], Batch [549/938], Loss: 0.30009812116622925\n",
      "Validation: Epoch [17], Batch [550/938], Loss: 0.4579305648803711\n",
      "Validation: Epoch [17], Batch [551/938], Loss: 0.5859344005584717\n",
      "Validation: Epoch [17], Batch [552/938], Loss: 0.5038179159164429\n",
      "Validation: Epoch [17], Batch [553/938], Loss: 0.43952709436416626\n",
      "Validation: Epoch [17], Batch [554/938], Loss: 0.1748751699924469\n",
      "Validation: Epoch [17], Batch [555/938], Loss: 0.4508463442325592\n",
      "Validation: Epoch [17], Batch [556/938], Loss: 0.4117887616157532\n",
      "Validation: Epoch [17], Batch [557/938], Loss: 0.23015953600406647\n",
      "Validation: Epoch [17], Batch [558/938], Loss: 0.3114216923713684\n",
      "Validation: Epoch [17], Batch [559/938], Loss: 0.3731856644153595\n",
      "Validation: Epoch [17], Batch [560/938], Loss: 0.39705079793930054\n",
      "Validation: Epoch [17], Batch [561/938], Loss: 0.3006514310836792\n",
      "Validation: Epoch [17], Batch [562/938], Loss: 0.2726273536682129\n",
      "Validation: Epoch [17], Batch [563/938], Loss: 0.3610132932662964\n",
      "Validation: Epoch [17], Batch [564/938], Loss: 0.5576865673065186\n",
      "Validation: Epoch [17], Batch [565/938], Loss: 0.3132708668708801\n",
      "Validation: Epoch [17], Batch [566/938], Loss: 0.3932924270629883\n",
      "Validation: Epoch [17], Batch [567/938], Loss: 0.5799574851989746\n",
      "Validation: Epoch [17], Batch [568/938], Loss: 0.2974686324596405\n",
      "Validation: Epoch [17], Batch [569/938], Loss: 0.5134254693984985\n",
      "Validation: Epoch [17], Batch [570/938], Loss: 0.29011380672454834\n",
      "Validation: Epoch [17], Batch [571/938], Loss: 0.5294803380966187\n",
      "Validation: Epoch [17], Batch [572/938], Loss: 0.5096501111984253\n",
      "Validation: Epoch [17], Batch [573/938], Loss: 0.6166868805885315\n",
      "Validation: Epoch [17], Batch [574/938], Loss: 0.48930850625038147\n",
      "Validation: Epoch [17], Batch [575/938], Loss: 0.3405890464782715\n",
      "Validation: Epoch [17], Batch [576/938], Loss: 0.3505970537662506\n",
      "Validation: Epoch [17], Batch [577/938], Loss: 0.386691153049469\n",
      "Validation: Epoch [17], Batch [578/938], Loss: 0.3326362669467926\n",
      "Validation: Epoch [17], Batch [579/938], Loss: 0.37529614567756653\n",
      "Validation: Epoch [17], Batch [580/938], Loss: 0.32579973340034485\n",
      "Validation: Epoch [17], Batch [581/938], Loss: 0.6736253499984741\n",
      "Validation: Epoch [17], Batch [582/938], Loss: 0.35124915838241577\n",
      "Validation: Epoch [17], Batch [583/938], Loss: 0.3450942039489746\n",
      "Validation: Epoch [17], Batch [584/938], Loss: 0.4800732135772705\n",
      "Validation: Epoch [17], Batch [585/938], Loss: 0.4383620023727417\n",
      "Validation: Epoch [17], Batch [586/938], Loss: 0.4533815383911133\n",
      "Validation: Epoch [17], Batch [587/938], Loss: 0.4985531270503998\n",
      "Validation: Epoch [17], Batch [588/938], Loss: 0.3568369448184967\n",
      "Validation: Epoch [17], Batch [589/938], Loss: 0.3895469605922699\n",
      "Validation: Epoch [17], Batch [590/938], Loss: 0.3648909628391266\n",
      "Validation: Epoch [17], Batch [591/938], Loss: 0.5463292598724365\n",
      "Validation: Epoch [17], Batch [592/938], Loss: 0.41960835456848145\n",
      "Validation: Epoch [17], Batch [593/938], Loss: 0.39833369851112366\n",
      "Validation: Epoch [17], Batch [594/938], Loss: 0.40381765365600586\n",
      "Validation: Epoch [17], Batch [595/938], Loss: 0.42105531692504883\n",
      "Validation: Epoch [17], Batch [596/938], Loss: 0.487537145614624\n",
      "Validation: Epoch [17], Batch [597/938], Loss: 0.4886645972728729\n",
      "Validation: Epoch [17], Batch [598/938], Loss: 0.3377324342727661\n",
      "Validation: Epoch [17], Batch [599/938], Loss: 0.3805181384086609\n",
      "Validation: Epoch [17], Batch [600/938], Loss: 0.5338854193687439\n",
      "Validation: Epoch [17], Batch [601/938], Loss: 0.733283281326294\n",
      "Validation: Epoch [17], Batch [602/938], Loss: 0.2873713970184326\n",
      "Validation: Epoch [17], Batch [603/938], Loss: 0.5856434106826782\n",
      "Validation: Epoch [17], Batch [604/938], Loss: 0.45898061990737915\n",
      "Validation: Epoch [17], Batch [605/938], Loss: 0.5537506341934204\n",
      "Validation: Epoch [17], Batch [606/938], Loss: 0.25037962198257446\n",
      "Validation: Epoch [17], Batch [607/938], Loss: 0.33201348781585693\n",
      "Validation: Epoch [17], Batch [608/938], Loss: 0.5468755960464478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [609/938], Loss: 0.38284316658973694\n",
      "Validation: Epoch [17], Batch [610/938], Loss: 0.43938952684402466\n",
      "Validation: Epoch [17], Batch [611/938], Loss: 0.37952667474746704\n",
      "Validation: Epoch [17], Batch [612/938], Loss: 0.3092786371707916\n",
      "Validation: Epoch [17], Batch [613/938], Loss: 0.3029678463935852\n",
      "Validation: Epoch [17], Batch [614/938], Loss: 0.43604668974876404\n",
      "Validation: Epoch [17], Batch [615/938], Loss: 0.38441672921180725\n",
      "Validation: Epoch [17], Batch [616/938], Loss: 0.5495606660842896\n",
      "Validation: Epoch [17], Batch [617/938], Loss: 0.7655581831932068\n",
      "Validation: Epoch [17], Batch [618/938], Loss: 0.46235179901123047\n",
      "Validation: Epoch [17], Batch [619/938], Loss: 0.6693613529205322\n",
      "Validation: Epoch [17], Batch [620/938], Loss: 0.3362293243408203\n",
      "Validation: Epoch [17], Batch [621/938], Loss: 0.29335686564445496\n",
      "Validation: Epoch [17], Batch [622/938], Loss: 0.3582642674446106\n",
      "Validation: Epoch [17], Batch [623/938], Loss: 0.47563236951828003\n",
      "Validation: Epoch [17], Batch [624/938], Loss: 0.309508740901947\n",
      "Validation: Epoch [17], Batch [625/938], Loss: 0.5464721918106079\n",
      "Validation: Epoch [17], Batch [626/938], Loss: 0.529160737991333\n",
      "Validation: Epoch [17], Batch [627/938], Loss: 0.32140883803367615\n",
      "Validation: Epoch [17], Batch [628/938], Loss: 0.43723264336586\n",
      "Validation: Epoch [17], Batch [629/938], Loss: 0.3059437870979309\n",
      "Validation: Epoch [17], Batch [630/938], Loss: 0.3480563163757324\n",
      "Validation: Epoch [17], Batch [631/938], Loss: 0.6436454057693481\n",
      "Validation: Epoch [17], Batch [632/938], Loss: 0.5332796573638916\n",
      "Validation: Epoch [17], Batch [633/938], Loss: 0.4734685719013214\n",
      "Validation: Epoch [17], Batch [634/938], Loss: 0.42726606130599976\n",
      "Validation: Epoch [17], Batch [635/938], Loss: 0.5068998336791992\n",
      "Validation: Epoch [17], Batch [636/938], Loss: 0.3862781524658203\n",
      "Validation: Epoch [17], Batch [637/938], Loss: 0.4271981120109558\n",
      "Validation: Epoch [17], Batch [638/938], Loss: 0.5762506723403931\n",
      "Validation: Epoch [17], Batch [639/938], Loss: 0.5100085735321045\n",
      "Validation: Epoch [17], Batch [640/938], Loss: 0.38618701696395874\n",
      "Validation: Epoch [17], Batch [641/938], Loss: 0.34386223554611206\n",
      "Validation: Epoch [17], Batch [642/938], Loss: 0.42764782905578613\n",
      "Validation: Epoch [17], Batch [643/938], Loss: 0.5356334447860718\n",
      "Validation: Epoch [17], Batch [644/938], Loss: 0.3695129156112671\n",
      "Validation: Epoch [17], Batch [645/938], Loss: 0.3401772677898407\n",
      "Validation: Epoch [17], Batch [646/938], Loss: 0.5192992091178894\n",
      "Validation: Epoch [17], Batch [647/938], Loss: 0.40524426102638245\n",
      "Validation: Epoch [17], Batch [648/938], Loss: 0.4838690757751465\n",
      "Validation: Epoch [17], Batch [649/938], Loss: 0.5093708038330078\n",
      "Validation: Epoch [17], Batch [650/938], Loss: 0.5418975949287415\n",
      "Validation: Epoch [17], Batch [651/938], Loss: 0.4762207865715027\n",
      "Validation: Epoch [17], Batch [652/938], Loss: 0.44173663854599\n",
      "Validation: Epoch [17], Batch [653/938], Loss: 0.22961261868476868\n",
      "Validation: Epoch [17], Batch [654/938], Loss: 0.4004504084587097\n",
      "Validation: Epoch [17], Batch [655/938], Loss: 0.5059646368026733\n",
      "Validation: Epoch [17], Batch [656/938], Loss: 0.570036768913269\n",
      "Validation: Epoch [17], Batch [657/938], Loss: 0.37643924355506897\n",
      "Validation: Epoch [17], Batch [658/938], Loss: 0.3081650733947754\n",
      "Validation: Epoch [17], Batch [659/938], Loss: 0.4138261377811432\n",
      "Validation: Epoch [17], Batch [660/938], Loss: 0.3538825511932373\n",
      "Validation: Epoch [17], Batch [661/938], Loss: 0.3510357737541199\n",
      "Validation: Epoch [17], Batch [662/938], Loss: 0.3451325595378876\n",
      "Validation: Epoch [17], Batch [663/938], Loss: 0.3871701955795288\n",
      "Validation: Epoch [17], Batch [664/938], Loss: 0.4746320843696594\n",
      "Validation: Epoch [17], Batch [665/938], Loss: 0.3909342885017395\n",
      "Validation: Epoch [17], Batch [666/938], Loss: 0.4706661105155945\n",
      "Validation: Epoch [17], Batch [667/938], Loss: 0.47553855180740356\n",
      "Validation: Epoch [17], Batch [668/938], Loss: 0.35670205950737\n",
      "Validation: Epoch [17], Batch [669/938], Loss: 0.5320817232131958\n",
      "Validation: Epoch [17], Batch [670/938], Loss: 0.49531102180480957\n",
      "Validation: Epoch [17], Batch [671/938], Loss: 0.34423625469207764\n",
      "Validation: Epoch [17], Batch [672/938], Loss: 0.39167338609695435\n",
      "Validation: Epoch [17], Batch [673/938], Loss: 0.3875855803489685\n",
      "Validation: Epoch [17], Batch [674/938], Loss: 0.36859098076820374\n",
      "Validation: Epoch [17], Batch [675/938], Loss: 0.42660394310951233\n",
      "Validation: Epoch [17], Batch [676/938], Loss: 0.3073645234107971\n",
      "Validation: Epoch [17], Batch [677/938], Loss: 0.4602876603603363\n",
      "Validation: Epoch [17], Batch [678/938], Loss: 0.33968865871429443\n",
      "Validation: Epoch [17], Batch [679/938], Loss: 0.5093166828155518\n",
      "Validation: Epoch [17], Batch [680/938], Loss: 0.5122237205505371\n",
      "Validation: Epoch [17], Batch [681/938], Loss: 0.3698708415031433\n",
      "Validation: Epoch [17], Batch [682/938], Loss: 0.29814985394477844\n",
      "Validation: Epoch [17], Batch [683/938], Loss: 0.34294262528419495\n",
      "Validation: Epoch [17], Batch [684/938], Loss: 0.5111755132675171\n",
      "Validation: Epoch [17], Batch [685/938], Loss: 0.4682196378707886\n",
      "Validation: Epoch [17], Batch [686/938], Loss: 0.5252668857574463\n",
      "Validation: Epoch [17], Batch [687/938], Loss: 0.3794153332710266\n",
      "Validation: Epoch [17], Batch [688/938], Loss: 0.732630729675293\n",
      "Validation: Epoch [17], Batch [689/938], Loss: 0.2555021047592163\n",
      "Validation: Epoch [17], Batch [690/938], Loss: 0.39366185665130615\n",
      "Validation: Epoch [17], Batch [691/938], Loss: 0.41725215315818787\n",
      "Validation: Epoch [17], Batch [692/938], Loss: 0.6206369400024414\n",
      "Validation: Epoch [17], Batch [693/938], Loss: 0.2806372046470642\n",
      "Validation: Epoch [17], Batch [694/938], Loss: 0.5155436992645264\n",
      "Validation: Epoch [17], Batch [695/938], Loss: 0.48109009861946106\n",
      "Validation: Epoch [17], Batch [696/938], Loss: 0.422619491815567\n",
      "Validation: Epoch [17], Batch [697/938], Loss: 0.6267073154449463\n",
      "Validation: Epoch [17], Batch [698/938], Loss: 0.6027646660804749\n",
      "Validation: Epoch [17], Batch [699/938], Loss: 0.502954363822937\n",
      "Validation: Epoch [17], Batch [700/938], Loss: 0.36442339420318604\n",
      "Validation: Epoch [17], Batch [701/938], Loss: 0.4282080829143524\n",
      "Validation: Epoch [17], Batch [702/938], Loss: 0.4695839583873749\n",
      "Validation: Epoch [17], Batch [703/938], Loss: 0.39543068408966064\n",
      "Validation: Epoch [17], Batch [704/938], Loss: 0.4864254891872406\n",
      "Validation: Epoch [17], Batch [705/938], Loss: 0.34035882353782654\n",
      "Validation: Epoch [17], Batch [706/938], Loss: 0.5755850672721863\n",
      "Validation: Epoch [17], Batch [707/938], Loss: 0.49816417694091797\n",
      "Validation: Epoch [17], Batch [708/938], Loss: 0.46006298065185547\n",
      "Validation: Epoch [17], Batch [709/938], Loss: 0.4021417498588562\n",
      "Validation: Epoch [17], Batch [710/938], Loss: 0.46875879168510437\n",
      "Validation: Epoch [17], Batch [711/938], Loss: 0.3987714350223541\n",
      "Validation: Epoch [17], Batch [712/938], Loss: 0.3925643563270569\n",
      "Validation: Epoch [17], Batch [713/938], Loss: 0.4476703405380249\n",
      "Validation: Epoch [17], Batch [714/938], Loss: 0.46154651045799255\n",
      "Validation: Epoch [17], Batch [715/938], Loss: 0.3932749032974243\n",
      "Validation: Epoch [17], Batch [716/938], Loss: 0.5018547773361206\n",
      "Validation: Epoch [17], Batch [717/938], Loss: 0.27302420139312744\n",
      "Validation: Epoch [17], Batch [718/938], Loss: 0.4578116834163666\n",
      "Validation: Epoch [17], Batch [719/938], Loss: 0.41268885135650635\n",
      "Validation: Epoch [17], Batch [720/938], Loss: 0.49195095896720886\n",
      "Validation: Epoch [17], Batch [721/938], Loss: 0.41259512305259705\n",
      "Validation: Epoch [17], Batch [722/938], Loss: 0.35595041513442993\n",
      "Validation: Epoch [17], Batch [723/938], Loss: 0.4564075171947479\n",
      "Validation: Epoch [17], Batch [724/938], Loss: 0.5225359201431274\n",
      "Validation: Epoch [17], Batch [725/938], Loss: 0.42667490243911743\n",
      "Validation: Epoch [17], Batch [726/938], Loss: 0.3131560981273651\n",
      "Validation: Epoch [17], Batch [727/938], Loss: 0.44205141067504883\n",
      "Validation: Epoch [17], Batch [728/938], Loss: 0.28023701906204224\n",
      "Validation: Epoch [17], Batch [729/938], Loss: 0.5924974679946899\n",
      "Validation: Epoch [17], Batch [730/938], Loss: 0.40714892745018005\n",
      "Validation: Epoch [17], Batch [731/938], Loss: 0.3933165669441223\n",
      "Validation: Epoch [17], Batch [732/938], Loss: 0.5116975903511047\n",
      "Validation: Epoch [17], Batch [733/938], Loss: 0.37618374824523926\n",
      "Validation: Epoch [17], Batch [734/938], Loss: 0.5130602121353149\n",
      "Validation: Epoch [17], Batch [735/938], Loss: 0.41209009289741516\n",
      "Validation: Epoch [17], Batch [736/938], Loss: 0.3299720287322998\n",
      "Validation: Epoch [17], Batch [737/938], Loss: 0.5533418655395508\n",
      "Validation: Epoch [17], Batch [738/938], Loss: 0.37814754247665405\n",
      "Validation: Epoch [17], Batch [739/938], Loss: 0.42968490719795227\n",
      "Validation: Epoch [17], Batch [740/938], Loss: 0.3539281487464905\n",
      "Validation: Epoch [17], Batch [741/938], Loss: 0.4290012717247009\n",
      "Validation: Epoch [17], Batch [742/938], Loss: 0.30723899602890015\n",
      "Validation: Epoch [17], Batch [743/938], Loss: 0.5541340112686157\n",
      "Validation: Epoch [17], Batch [744/938], Loss: 0.43139415979385376\n",
      "Validation: Epoch [17], Batch [745/938], Loss: 0.31682881712913513\n",
      "Validation: Epoch [17], Batch [746/938], Loss: 0.4473345875740051\n",
      "Validation: Epoch [17], Batch [747/938], Loss: 0.3580108880996704\n",
      "Validation: Epoch [17], Batch [748/938], Loss: 0.328718364238739\n",
      "Validation: Epoch [17], Batch [749/938], Loss: 0.3554566204547882\n",
      "Validation: Epoch [17], Batch [750/938], Loss: 0.579916775226593\n",
      "Validation: Epoch [17], Batch [751/938], Loss: 0.41662612557411194\n",
      "Validation: Epoch [17], Batch [752/938], Loss: 0.39495494961738586\n",
      "Validation: Epoch [17], Batch [753/938], Loss: 0.3389412462711334\n",
      "Validation: Epoch [17], Batch [754/938], Loss: 0.33196884393692017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [755/938], Loss: 0.4036332666873932\n",
      "Validation: Epoch [17], Batch [756/938], Loss: 0.3480793237686157\n",
      "Validation: Epoch [17], Batch [757/938], Loss: 0.2862798869609833\n",
      "Validation: Epoch [17], Batch [758/938], Loss: 0.4490974545478821\n",
      "Validation: Epoch [17], Batch [759/938], Loss: 0.25425755977630615\n",
      "Validation: Epoch [17], Batch [760/938], Loss: 0.4934873580932617\n",
      "Validation: Epoch [17], Batch [761/938], Loss: 0.5289703607559204\n",
      "Validation: Epoch [17], Batch [762/938], Loss: 0.3929033875465393\n",
      "Validation: Epoch [17], Batch [763/938], Loss: 0.4572758972644806\n",
      "Validation: Epoch [17], Batch [764/938], Loss: 0.5660117864608765\n",
      "Validation: Epoch [17], Batch [765/938], Loss: 0.4876210689544678\n",
      "Validation: Epoch [17], Batch [766/938], Loss: 0.3043690621852875\n",
      "Validation: Epoch [17], Batch [767/938], Loss: 0.31972646713256836\n",
      "Validation: Epoch [17], Batch [768/938], Loss: 0.5839006304740906\n",
      "Validation: Epoch [17], Batch [769/938], Loss: 0.34527191519737244\n",
      "Validation: Epoch [17], Batch [770/938], Loss: 0.5304813981056213\n",
      "Validation: Epoch [17], Batch [771/938], Loss: 0.4527948200702667\n",
      "Validation: Epoch [17], Batch [772/938], Loss: 0.3308524489402771\n",
      "Validation: Epoch [17], Batch [773/938], Loss: 0.3127311170101166\n",
      "Validation: Epoch [17], Batch [774/938], Loss: 0.42885926365852356\n",
      "Validation: Epoch [17], Batch [775/938], Loss: 0.4263606667518616\n",
      "Validation: Epoch [17], Batch [776/938], Loss: 0.3319324851036072\n",
      "Validation: Epoch [17], Batch [777/938], Loss: 0.5210596919059753\n",
      "Validation: Epoch [17], Batch [778/938], Loss: 0.40323030948638916\n",
      "Validation: Epoch [17], Batch [779/938], Loss: 0.3483307659626007\n",
      "Validation: Epoch [17], Batch [780/938], Loss: 0.2850404381752014\n",
      "Validation: Epoch [17], Batch [781/938], Loss: 0.4205349087715149\n",
      "Validation: Epoch [17], Batch [782/938], Loss: 0.31396380066871643\n",
      "Validation: Epoch [17], Batch [783/938], Loss: 0.5192575454711914\n",
      "Validation: Epoch [17], Batch [784/938], Loss: 0.5165448188781738\n",
      "Validation: Epoch [17], Batch [785/938], Loss: 0.36140719056129456\n",
      "Validation: Epoch [17], Batch [786/938], Loss: 0.38873976469039917\n",
      "Validation: Epoch [17], Batch [787/938], Loss: 0.5438261032104492\n",
      "Validation: Epoch [17], Batch [788/938], Loss: 0.5637019276618958\n",
      "Validation: Epoch [17], Batch [789/938], Loss: 0.31660354137420654\n",
      "Validation: Epoch [17], Batch [790/938], Loss: 0.670984148979187\n",
      "Validation: Epoch [17], Batch [791/938], Loss: 0.5161197185516357\n",
      "Validation: Epoch [17], Batch [792/938], Loss: 0.6673474311828613\n",
      "Validation: Epoch [17], Batch [793/938], Loss: 0.3185238838195801\n",
      "Validation: Epoch [17], Batch [794/938], Loss: 0.3400823175907135\n",
      "Validation: Epoch [17], Batch [795/938], Loss: 0.49624159932136536\n",
      "Validation: Epoch [17], Batch [796/938], Loss: 0.39282190799713135\n",
      "Validation: Epoch [17], Batch [797/938], Loss: 0.5276951193809509\n",
      "Validation: Epoch [17], Batch [798/938], Loss: 0.5267224907875061\n",
      "Validation: Epoch [17], Batch [799/938], Loss: 0.4035221040248871\n",
      "Validation: Epoch [17], Batch [800/938], Loss: 0.41766566038131714\n",
      "Validation: Epoch [17], Batch [801/938], Loss: 0.3783578872680664\n",
      "Validation: Epoch [17], Batch [802/938], Loss: 0.5919700860977173\n",
      "Validation: Epoch [17], Batch [803/938], Loss: 0.4614483714103699\n",
      "Validation: Epoch [17], Batch [804/938], Loss: 0.3941628336906433\n",
      "Validation: Epoch [17], Batch [805/938], Loss: 0.3765004873275757\n",
      "Validation: Epoch [17], Batch [806/938], Loss: 0.34151047468185425\n",
      "Validation: Epoch [17], Batch [807/938], Loss: 0.3420799970626831\n",
      "Validation: Epoch [17], Batch [808/938], Loss: 0.5119844079017639\n",
      "Validation: Epoch [17], Batch [809/938], Loss: 0.4574918746948242\n",
      "Validation: Epoch [17], Batch [810/938], Loss: 0.3959561586380005\n",
      "Validation: Epoch [17], Batch [811/938], Loss: 0.44738873839378357\n",
      "Validation: Epoch [17], Batch [812/938], Loss: 0.1781415492296219\n",
      "Validation: Epoch [17], Batch [813/938], Loss: 0.4361656904220581\n",
      "Validation: Epoch [17], Batch [814/938], Loss: 0.32202470302581787\n",
      "Validation: Epoch [17], Batch [815/938], Loss: 0.30987483263015747\n",
      "Validation: Epoch [17], Batch [816/938], Loss: 0.5133748054504395\n",
      "Validation: Epoch [17], Batch [817/938], Loss: 0.5483028292655945\n",
      "Validation: Epoch [17], Batch [818/938], Loss: 0.36624380946159363\n",
      "Validation: Epoch [17], Batch [819/938], Loss: 0.38880717754364014\n",
      "Validation: Epoch [17], Batch [820/938], Loss: 0.4162275195121765\n",
      "Validation: Epoch [17], Batch [821/938], Loss: 0.4633291959762573\n",
      "Validation: Epoch [17], Batch [822/938], Loss: 0.46375399827957153\n",
      "Validation: Epoch [17], Batch [823/938], Loss: 0.39005234837532043\n",
      "Validation: Epoch [17], Batch [824/938], Loss: 0.4958568215370178\n",
      "Validation: Epoch [17], Batch [825/938], Loss: 0.5641533136367798\n",
      "Validation: Epoch [17], Batch [826/938], Loss: 0.5800944566726685\n",
      "Validation: Epoch [17], Batch [827/938], Loss: 0.4779422879219055\n",
      "Validation: Epoch [17], Batch [828/938], Loss: 0.31082993745803833\n",
      "Validation: Epoch [17], Batch [829/938], Loss: 0.5769962072372437\n",
      "Validation: Epoch [17], Batch [830/938], Loss: 0.32387489080429077\n",
      "Validation: Epoch [17], Batch [831/938], Loss: 0.34668290615081787\n",
      "Validation: Epoch [17], Batch [832/938], Loss: 0.6152951717376709\n",
      "Validation: Epoch [17], Batch [833/938], Loss: 0.5802491307258606\n",
      "Validation: Epoch [17], Batch [834/938], Loss: 0.5256615877151489\n",
      "Validation: Epoch [17], Batch [835/938], Loss: 0.6037184000015259\n",
      "Validation: Epoch [17], Batch [836/938], Loss: 0.46974343061447144\n",
      "Validation: Epoch [17], Batch [837/938], Loss: 0.475755512714386\n",
      "Validation: Epoch [17], Batch [838/938], Loss: 0.3899499773979187\n",
      "Validation: Epoch [17], Batch [839/938], Loss: 0.24352668225765228\n",
      "Validation: Epoch [17], Batch [840/938], Loss: 0.4529344439506531\n",
      "Validation: Epoch [17], Batch [841/938], Loss: 0.3230392336845398\n",
      "Validation: Epoch [17], Batch [842/938], Loss: 0.637596607208252\n",
      "Validation: Epoch [17], Batch [843/938], Loss: 0.507758617401123\n",
      "Validation: Epoch [17], Batch [844/938], Loss: 0.3151737153530121\n",
      "Validation: Epoch [17], Batch [845/938], Loss: 0.4326411485671997\n",
      "Validation: Epoch [17], Batch [846/938], Loss: 0.5007172226905823\n",
      "Validation: Epoch [17], Batch [847/938], Loss: 0.3067457675933838\n",
      "Validation: Epoch [17], Batch [848/938], Loss: 0.41871652007102966\n",
      "Validation: Epoch [17], Batch [849/938], Loss: 0.3980037569999695\n",
      "Validation: Epoch [17], Batch [850/938], Loss: 0.4730105400085449\n",
      "Validation: Epoch [17], Batch [851/938], Loss: 0.33638814091682434\n",
      "Validation: Epoch [17], Batch [852/938], Loss: 0.34921765327453613\n",
      "Validation: Epoch [17], Batch [853/938], Loss: 0.37611663341522217\n",
      "Validation: Epoch [17], Batch [854/938], Loss: 0.4904034733772278\n",
      "Validation: Epoch [17], Batch [855/938], Loss: 0.4114508628845215\n",
      "Validation: Epoch [17], Batch [856/938], Loss: 0.5256656408309937\n",
      "Validation: Epoch [17], Batch [857/938], Loss: 0.39255276322364807\n",
      "Validation: Epoch [17], Batch [858/938], Loss: 0.48800426721572876\n",
      "Validation: Epoch [17], Batch [859/938], Loss: 0.4023311138153076\n",
      "Validation: Epoch [17], Batch [860/938], Loss: 0.40042006969451904\n",
      "Validation: Epoch [17], Batch [861/938], Loss: 0.4028394818305969\n",
      "Validation: Epoch [17], Batch [862/938], Loss: 0.3773758113384247\n",
      "Validation: Epoch [17], Batch [863/938], Loss: 0.4390825033187866\n",
      "Validation: Epoch [17], Batch [864/938], Loss: 0.3569287061691284\n",
      "Validation: Epoch [17], Batch [865/938], Loss: 0.296845018863678\n",
      "Validation: Epoch [17], Batch [866/938], Loss: 0.24785777926445007\n",
      "Validation: Epoch [17], Batch [867/938], Loss: 0.4647177457809448\n",
      "Validation: Epoch [17], Batch [868/938], Loss: 0.4795166850090027\n",
      "Validation: Epoch [17], Batch [869/938], Loss: 0.5373293161392212\n",
      "Validation: Epoch [17], Batch [870/938], Loss: 0.28356391191482544\n",
      "Validation: Epoch [17], Batch [871/938], Loss: 0.3971884250640869\n",
      "Validation: Epoch [17], Batch [872/938], Loss: 0.39219993352890015\n",
      "Validation: Epoch [17], Batch [873/938], Loss: 0.38826704025268555\n",
      "Validation: Epoch [17], Batch [874/938], Loss: 0.4282335638999939\n",
      "Validation: Epoch [17], Batch [875/938], Loss: 0.38253486156463623\n",
      "Validation: Epoch [17], Batch [876/938], Loss: 0.3722306191921234\n",
      "Validation: Epoch [17], Batch [877/938], Loss: 0.3699641823768616\n",
      "Validation: Epoch [17], Batch [878/938], Loss: 0.5160046815872192\n",
      "Validation: Epoch [17], Batch [879/938], Loss: 0.3238978981971741\n",
      "Validation: Epoch [17], Batch [880/938], Loss: 0.3357601463794708\n",
      "Validation: Epoch [17], Batch [881/938], Loss: 0.4321906864643097\n",
      "Validation: Epoch [17], Batch [882/938], Loss: 0.3342454135417938\n",
      "Validation: Epoch [17], Batch [883/938], Loss: 0.35033994913101196\n",
      "Validation: Epoch [17], Batch [884/938], Loss: 0.5159648656845093\n",
      "Validation: Epoch [17], Batch [885/938], Loss: 0.40045079588890076\n",
      "Validation: Epoch [17], Batch [886/938], Loss: 0.43103909492492676\n",
      "Validation: Epoch [17], Batch [887/938], Loss: 0.38774967193603516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [17], Batch [888/938], Loss: 0.6459163427352905\n",
      "Validation: Epoch [17], Batch [889/938], Loss: 0.6177281141281128\n",
      "Validation: Epoch [17], Batch [890/938], Loss: 0.4733849763870239\n",
      "Validation: Epoch [17], Batch [891/938], Loss: 0.4603818655014038\n",
      "Validation: Epoch [17], Batch [892/938], Loss: 0.49708735942840576\n",
      "Validation: Epoch [17], Batch [893/938], Loss: 0.3735904097557068\n",
      "Validation: Epoch [17], Batch [894/938], Loss: 0.5179587602615356\n",
      "Validation: Epoch [17], Batch [895/938], Loss: 0.5328577160835266\n",
      "Validation: Epoch [17], Batch [896/938], Loss: 0.39695659279823303\n",
      "Validation: Epoch [17], Batch [897/938], Loss: 0.6416478753089905\n",
      "Validation: Epoch [17], Batch [898/938], Loss: 0.4928922653198242\n",
      "Validation: Epoch [17], Batch [899/938], Loss: 0.5706580877304077\n",
      "Validation: Epoch [17], Batch [900/938], Loss: 0.38681963086128235\n",
      "Validation: Epoch [17], Batch [901/938], Loss: 0.22506287693977356\n",
      "Validation: Epoch [17], Batch [902/938], Loss: 0.5487977266311646\n",
      "Validation: Epoch [17], Batch [903/938], Loss: 0.3247670829296112\n",
      "Validation: Epoch [17], Batch [904/938], Loss: 0.3124663829803467\n",
      "Validation: Epoch [17], Batch [905/938], Loss: 0.5470578670501709\n",
      "Validation: Epoch [17], Batch [906/938], Loss: 0.4855262339115143\n",
      "Validation: Epoch [17], Batch [907/938], Loss: 0.3582818806171417\n",
      "Validation: Epoch [17], Batch [908/938], Loss: 0.4323282241821289\n",
      "Validation: Epoch [17], Batch [909/938], Loss: 0.3427529036998749\n",
      "Validation: Epoch [17], Batch [910/938], Loss: 0.5817767381668091\n",
      "Validation: Epoch [17], Batch [911/938], Loss: 0.4508875012397766\n",
      "Validation: Epoch [17], Batch [912/938], Loss: 0.27867698669433594\n",
      "Validation: Epoch [17], Batch [913/938], Loss: 0.5073361396789551\n",
      "Validation: Epoch [17], Batch [914/938], Loss: 0.3034828007221222\n",
      "Validation: Epoch [17], Batch [915/938], Loss: 0.3760039210319519\n",
      "Validation: Epoch [17], Batch [916/938], Loss: 0.43123844265937805\n",
      "Validation: Epoch [17], Batch [917/938], Loss: 0.593515932559967\n",
      "Validation: Epoch [17], Batch [918/938], Loss: 0.4797106683254242\n",
      "Validation: Epoch [17], Batch [919/938], Loss: 0.6469649076461792\n",
      "Validation: Epoch [17], Batch [920/938], Loss: 0.497081458568573\n",
      "Validation: Epoch [17], Batch [921/938], Loss: 0.5257189273834229\n",
      "Validation: Epoch [17], Batch [922/938], Loss: 0.25868654251098633\n",
      "Validation: Epoch [17], Batch [923/938], Loss: 0.368908554315567\n",
      "Validation: Epoch [17], Batch [924/938], Loss: 0.5919302701950073\n",
      "Validation: Epoch [17], Batch [925/938], Loss: 0.3043135404586792\n",
      "Validation: Epoch [17], Batch [926/938], Loss: 0.3194518983364105\n",
      "Validation: Epoch [17], Batch [927/938], Loss: 0.2864748239517212\n",
      "Validation: Epoch [17], Batch [928/938], Loss: 0.27540653944015503\n",
      "Validation: Epoch [17], Batch [929/938], Loss: 0.469740629196167\n",
      "Validation: Epoch [17], Batch [930/938], Loss: 0.33161693811416626\n",
      "Validation: Epoch [17], Batch [931/938], Loss: 0.43814945220947266\n",
      "Validation: Epoch [17], Batch [932/938], Loss: 0.4807353615760803\n",
      "Validation: Epoch [17], Batch [933/938], Loss: 0.4086298644542694\n",
      "Validation: Epoch [17], Batch [934/938], Loss: 0.43443062901496887\n",
      "Validation: Epoch [17], Batch [935/938], Loss: 0.5775660276412964\n",
      "Validation: Epoch [17], Batch [936/938], Loss: 0.5449455976486206\n",
      "Validation: Epoch [17], Batch [937/938], Loss: 0.5581205487251282\n",
      "Validation: Epoch [17], Batch [938/938], Loss: 0.4182843565940857\n",
      "Accuracy of test set: 0.8464833333333334\n",
      "Train: Epoch [18], Batch [1/938], Loss: 0.5562270879745483\n",
      "Train: Epoch [18], Batch [2/938], Loss: 0.5347136855125427\n",
      "Train: Epoch [18], Batch [3/938], Loss: 0.2880012094974518\n",
      "Train: Epoch [18], Batch [4/938], Loss: 0.40720003843307495\n",
      "Train: Epoch [18], Batch [5/938], Loss: 0.5421554446220398\n",
      "Train: Epoch [18], Batch [6/938], Loss: 0.32762762904167175\n",
      "Train: Epoch [18], Batch [7/938], Loss: 0.412168025970459\n",
      "Train: Epoch [18], Batch [8/938], Loss: 0.46448785066604614\n",
      "Train: Epoch [18], Batch [9/938], Loss: 0.49055442214012146\n",
      "Train: Epoch [18], Batch [10/938], Loss: 0.6187141537666321\n",
      "Train: Epoch [18], Batch [11/938], Loss: 0.4051716923713684\n",
      "Train: Epoch [18], Batch [12/938], Loss: 0.3409569263458252\n",
      "Train: Epoch [18], Batch [13/938], Loss: 0.4366730749607086\n",
      "Train: Epoch [18], Batch [14/938], Loss: 0.3243280053138733\n",
      "Train: Epoch [18], Batch [15/938], Loss: 0.4683108329772949\n",
      "Train: Epoch [18], Batch [16/938], Loss: 0.40162861347198486\n",
      "Train: Epoch [18], Batch [17/938], Loss: 0.4454318881034851\n",
      "Train: Epoch [18], Batch [18/938], Loss: 0.3773718476295471\n",
      "Train: Epoch [18], Batch [19/938], Loss: 0.4268173277378082\n",
      "Train: Epoch [18], Batch [20/938], Loss: 0.39359360933303833\n",
      "Train: Epoch [18], Batch [21/938], Loss: 0.43992623686790466\n",
      "Train: Epoch [18], Batch [22/938], Loss: 0.5277179479598999\n",
      "Train: Epoch [18], Batch [23/938], Loss: 0.9241948127746582\n",
      "Train: Epoch [18], Batch [24/938], Loss: 0.7736169695854187\n",
      "Train: Epoch [18], Batch [25/938], Loss: 0.4313984513282776\n",
      "Train: Epoch [18], Batch [26/938], Loss: 0.7512819766998291\n",
      "Train: Epoch [18], Batch [27/938], Loss: 0.6252903938293457\n",
      "Train: Epoch [18], Batch [28/938], Loss: 0.30476197600364685\n",
      "Train: Epoch [18], Batch [29/938], Loss: 0.2815791368484497\n",
      "Train: Epoch [18], Batch [30/938], Loss: 0.5268669128417969\n",
      "Train: Epoch [18], Batch [31/938], Loss: 0.3747333288192749\n",
      "Train: Epoch [18], Batch [32/938], Loss: 0.7851228713989258\n",
      "Train: Epoch [18], Batch [33/938], Loss: 0.31368207931518555\n",
      "Train: Epoch [18], Batch [34/938], Loss: 0.34061282873153687\n",
      "Train: Epoch [18], Batch [35/938], Loss: 0.37635552883148193\n",
      "Train: Epoch [18], Batch [36/938], Loss: 0.41162335872650146\n",
      "Train: Epoch [18], Batch [37/938], Loss: 0.3852887749671936\n",
      "Train: Epoch [18], Batch [38/938], Loss: 0.3657461404800415\n",
      "Train: Epoch [18], Batch [39/938], Loss: 0.4636056423187256\n",
      "Train: Epoch [18], Batch [40/938], Loss: 0.2839142680168152\n",
      "Train: Epoch [18], Batch [41/938], Loss: 0.46773314476013184\n",
      "Train: Epoch [18], Batch [42/938], Loss: 0.6536695957183838\n",
      "Train: Epoch [18], Batch [43/938], Loss: 0.4992385506629944\n",
      "Train: Epoch [18], Batch [44/938], Loss: 0.3380354344844818\n",
      "Train: Epoch [18], Batch [45/938], Loss: 0.5110409259796143\n",
      "Train: Epoch [18], Batch [46/938], Loss: 0.46588578820228577\n",
      "Train: Epoch [18], Batch [47/938], Loss: 0.340748131275177\n",
      "Train: Epoch [18], Batch [48/938], Loss: 0.3409004807472229\n",
      "Train: Epoch [18], Batch [49/938], Loss: 0.4615458846092224\n",
      "Train: Epoch [18], Batch [50/938], Loss: 0.5351746082305908\n",
      "Train: Epoch [18], Batch [51/938], Loss: 0.578379213809967\n",
      "Train: Epoch [18], Batch [52/938], Loss: 0.7505393028259277\n",
      "Train: Epoch [18], Batch [53/938], Loss: 0.6308143138885498\n",
      "Train: Epoch [18], Batch [54/938], Loss: 0.4849478602409363\n",
      "Train: Epoch [18], Batch [55/938], Loss: 0.41258418560028076\n",
      "Train: Epoch [18], Batch [56/938], Loss: 0.3752346634864807\n",
      "Train: Epoch [18], Batch [57/938], Loss: 0.6124521493911743\n",
      "Train: Epoch [18], Batch [58/938], Loss: 0.3227125406265259\n",
      "Train: Epoch [18], Batch [59/938], Loss: 0.2668689489364624\n",
      "Train: Epoch [18], Batch [60/938], Loss: 0.5261465907096863\n",
      "Train: Epoch [18], Batch [61/938], Loss: 0.3602582514286041\n",
      "Train: Epoch [18], Batch [62/938], Loss: 0.3833581507205963\n",
      "Train: Epoch [18], Batch [63/938], Loss: 0.3466701805591583\n",
      "Train: Epoch [18], Batch [64/938], Loss: 0.5496463775634766\n",
      "Train: Epoch [18], Batch [65/938], Loss: 0.30380505323410034\n",
      "Train: Epoch [18], Batch [66/938], Loss: 0.4161476492881775\n",
      "Train: Epoch [18], Batch [67/938], Loss: 0.29302987456321716\n",
      "Train: Epoch [18], Batch [68/938], Loss: 0.5037292242050171\n",
      "Train: Epoch [18], Batch [69/938], Loss: 0.594274640083313\n",
      "Train: Epoch [18], Batch [70/938], Loss: 0.3016994595527649\n",
      "Train: Epoch [18], Batch [71/938], Loss: 0.4801032543182373\n",
      "Train: Epoch [18], Batch [72/938], Loss: 0.5355955958366394\n",
      "Train: Epoch [18], Batch [73/938], Loss: 0.2822188138961792\n",
      "Train: Epoch [18], Batch [74/938], Loss: 0.3352939486503601\n",
      "Train: Epoch [18], Batch [75/938], Loss: 0.4822781980037689\n",
      "Train: Epoch [18], Batch [76/938], Loss: 0.467731237411499\n",
      "Train: Epoch [18], Batch [77/938], Loss: 0.3751069903373718\n",
      "Train: Epoch [18], Batch [78/938], Loss: 0.47076112031936646\n",
      "Train: Epoch [18], Batch [79/938], Loss: 0.37490639090538025\n",
      "Train: Epoch [18], Batch [80/938], Loss: 0.25188106298446655\n",
      "Train: Epoch [18], Batch [81/938], Loss: 0.42169061303138733\n",
      "Train: Epoch [18], Batch [82/938], Loss: 0.5451387166976929\n",
      "Train: Epoch [18], Batch [83/938], Loss: 0.21545018255710602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [18], Batch [84/938], Loss: 0.46609818935394287\n",
      "Train: Epoch [18], Batch [85/938], Loss: 0.4563993513584137\n",
      "Train: Epoch [18], Batch [86/938], Loss: 0.3574187755584717\n",
      "Train: Epoch [18], Batch [87/938], Loss: 0.41761159896850586\n",
      "Train: Epoch [18], Batch [88/938], Loss: 0.4073884189128876\n",
      "Train: Epoch [18], Batch [89/938], Loss: 0.34585124254226685\n",
      "Train: Epoch [18], Batch [90/938], Loss: 0.19662123918533325\n",
      "Train: Epoch [18], Batch [91/938], Loss: 0.5876423120498657\n",
      "Train: Epoch [18], Batch [92/938], Loss: 0.42030584812164307\n",
      "Train: Epoch [18], Batch [93/938], Loss: 0.33453887701034546\n",
      "Train: Epoch [18], Batch [94/938], Loss: 0.447685182094574\n",
      "Train: Epoch [18], Batch [95/938], Loss: 0.44065654277801514\n",
      "Train: Epoch [18], Batch [96/938], Loss: 0.40604886412620544\n",
      "Train: Epoch [18], Batch [97/938], Loss: 0.6146228313446045\n",
      "Train: Epoch [18], Batch [98/938], Loss: 0.556747317314148\n",
      "Train: Epoch [18], Batch [99/938], Loss: 0.42534852027893066\n",
      "Train: Epoch [18], Batch [100/938], Loss: 0.49308931827545166\n",
      "Train: Epoch [18], Batch [101/938], Loss: 0.511351466178894\n",
      "Train: Epoch [18], Batch [102/938], Loss: 0.45348551869392395\n",
      "Train: Epoch [18], Batch [103/938], Loss: 0.5218213200569153\n",
      "Train: Epoch [18], Batch [104/938], Loss: 0.345479816198349\n",
      "Train: Epoch [18], Batch [105/938], Loss: 0.5080469846725464\n",
      "Train: Epoch [18], Batch [106/938], Loss: 0.6072935461997986\n",
      "Train: Epoch [18], Batch [107/938], Loss: 0.5198342800140381\n",
      "Train: Epoch [18], Batch [108/938], Loss: 0.25920045375823975\n",
      "Train: Epoch [18], Batch [109/938], Loss: 0.440470814704895\n",
      "Train: Epoch [18], Batch [110/938], Loss: 0.4314551055431366\n",
      "Train: Epoch [18], Batch [111/938], Loss: 0.47801291942596436\n",
      "Train: Epoch [18], Batch [112/938], Loss: 0.43971529603004456\n",
      "Train: Epoch [18], Batch [113/938], Loss: 0.29025977849960327\n",
      "Train: Epoch [18], Batch [114/938], Loss: 0.5316838026046753\n",
      "Train: Epoch [18], Batch [115/938], Loss: 0.3571629822254181\n",
      "Train: Epoch [18], Batch [116/938], Loss: 0.4049178957939148\n",
      "Train: Epoch [18], Batch [117/938], Loss: 0.541239321231842\n",
      "Train: Epoch [18], Batch [118/938], Loss: 0.3232787251472473\n",
      "Train: Epoch [18], Batch [119/938], Loss: 0.3821490406990051\n",
      "Train: Epoch [18], Batch [120/938], Loss: 0.3413979411125183\n",
      "Train: Epoch [18], Batch [121/938], Loss: 0.5972726345062256\n",
      "Train: Epoch [18], Batch [122/938], Loss: 0.32125669717788696\n",
      "Train: Epoch [18], Batch [123/938], Loss: 0.5804294347763062\n",
      "Train: Epoch [18], Batch [124/938], Loss: 0.5115424394607544\n",
      "Train: Epoch [18], Batch [125/938], Loss: 0.3558187782764435\n",
      "Train: Epoch [18], Batch [126/938], Loss: 0.5277971029281616\n",
      "Train: Epoch [18], Batch [127/938], Loss: 0.3148541748523712\n",
      "Train: Epoch [18], Batch [128/938], Loss: 0.43824267387390137\n",
      "Train: Epoch [18], Batch [129/938], Loss: 0.24782733619213104\n",
      "Train: Epoch [18], Batch [130/938], Loss: 0.2438238263130188\n",
      "Train: Epoch [18], Batch [131/938], Loss: 0.32922953367233276\n",
      "Train: Epoch [18], Batch [132/938], Loss: 0.43306148052215576\n",
      "Train: Epoch [18], Batch [133/938], Loss: 0.32062846422195435\n",
      "Train: Epoch [18], Batch [134/938], Loss: 0.6521152853965759\n",
      "Train: Epoch [18], Batch [135/938], Loss: 0.5568093061447144\n",
      "Train: Epoch [18], Batch [136/938], Loss: 0.5341359972953796\n",
      "Train: Epoch [18], Batch [137/938], Loss: 0.5196856260299683\n",
      "Train: Epoch [18], Batch [138/938], Loss: 0.39282485842704773\n",
      "Train: Epoch [18], Batch [139/938], Loss: 0.470201700925827\n",
      "Train: Epoch [18], Batch [140/938], Loss: 0.5664839744567871\n",
      "Train: Epoch [18], Batch [141/938], Loss: 0.49799907207489014\n",
      "Train: Epoch [18], Batch [142/938], Loss: 0.5031410455703735\n",
      "Train: Epoch [18], Batch [143/938], Loss: 0.5020278692245483\n",
      "Train: Epoch [18], Batch [144/938], Loss: 0.5786120891571045\n",
      "Train: Epoch [18], Batch [145/938], Loss: 0.46279042959213257\n",
      "Train: Epoch [18], Batch [146/938], Loss: 0.37893974781036377\n",
      "Train: Epoch [18], Batch [147/938], Loss: 0.4707874059677124\n",
      "Train: Epoch [18], Batch [148/938], Loss: 0.34125420451164246\n",
      "Train: Epoch [18], Batch [149/938], Loss: 0.3909778594970703\n",
      "Train: Epoch [18], Batch [150/938], Loss: 0.4007318615913391\n",
      "Train: Epoch [18], Batch [151/938], Loss: 0.5105841159820557\n",
      "Train: Epoch [18], Batch [152/938], Loss: 0.42089641094207764\n",
      "Train: Epoch [18], Batch [153/938], Loss: 0.37073731422424316\n",
      "Train: Epoch [18], Batch [154/938], Loss: 0.2833070158958435\n",
      "Train: Epoch [18], Batch [155/938], Loss: 0.3678302764892578\n",
      "Train: Epoch [18], Batch [156/938], Loss: 0.23471947014331818\n",
      "Train: Epoch [18], Batch [157/938], Loss: 0.35950595140457153\n",
      "Train: Epoch [18], Batch [158/938], Loss: 0.4251200258731842\n",
      "Train: Epoch [18], Batch [159/938], Loss: 0.3233666718006134\n",
      "Train: Epoch [18], Batch [160/938], Loss: 0.2797563970088959\n",
      "Train: Epoch [18], Batch [161/938], Loss: 0.5199771523475647\n",
      "Train: Epoch [18], Batch [162/938], Loss: 0.5012171268463135\n",
      "Train: Epoch [18], Batch [163/938], Loss: 0.44577473402023315\n",
      "Train: Epoch [18], Batch [164/938], Loss: 0.29074329137802124\n",
      "Train: Epoch [18], Batch [165/938], Loss: 0.5349097847938538\n",
      "Train: Epoch [18], Batch [166/938], Loss: 0.3868825435638428\n",
      "Train: Epoch [18], Batch [167/938], Loss: 0.47417837381362915\n",
      "Train: Epoch [18], Batch [168/938], Loss: 0.4320994019508362\n",
      "Train: Epoch [18], Batch [169/938], Loss: 0.3451922535896301\n",
      "Train: Epoch [18], Batch [170/938], Loss: 0.394232839345932\n",
      "Train: Epoch [18], Batch [171/938], Loss: 0.5263301134109497\n",
      "Train: Epoch [18], Batch [172/938], Loss: 0.3273812532424927\n",
      "Train: Epoch [18], Batch [173/938], Loss: 0.45444855093955994\n",
      "Train: Epoch [18], Batch [174/938], Loss: 0.3510827422142029\n",
      "Train: Epoch [18], Batch [175/938], Loss: 0.510032057762146\n",
      "Train: Epoch [18], Batch [176/938], Loss: 0.5021357536315918\n",
      "Train: Epoch [18], Batch [177/938], Loss: 0.43210989236831665\n",
      "Train: Epoch [18], Batch [178/938], Loss: 0.42832523584365845\n",
      "Train: Epoch [18], Batch [179/938], Loss: 0.41484135389328003\n",
      "Train: Epoch [18], Batch [180/938], Loss: 0.5708363056182861\n",
      "Train: Epoch [18], Batch [181/938], Loss: 0.5227531790733337\n",
      "Train: Epoch [18], Batch [182/938], Loss: 0.49518972635269165\n",
      "Train: Epoch [18], Batch [183/938], Loss: 0.44518154859542847\n",
      "Train: Epoch [18], Batch [184/938], Loss: 0.44253358244895935\n",
      "Train: Epoch [18], Batch [185/938], Loss: 0.5224548578262329\n",
      "Train: Epoch [18], Batch [186/938], Loss: 0.2806335985660553\n",
      "Train: Epoch [18], Batch [187/938], Loss: 0.4404567778110504\n",
      "Train: Epoch [18], Batch [188/938], Loss: 0.49264588952064514\n",
      "Train: Epoch [18], Batch [189/938], Loss: 0.4114578366279602\n",
      "Train: Epoch [18], Batch [190/938], Loss: 0.6242945194244385\n",
      "Train: Epoch [18], Batch [191/938], Loss: 0.5235898494720459\n",
      "Train: Epoch [18], Batch [192/938], Loss: 0.44604045152664185\n",
      "Train: Epoch [18], Batch [193/938], Loss: 0.2618812024593353\n",
      "Train: Epoch [18], Batch [194/938], Loss: 0.300889253616333\n",
      "Train: Epoch [18], Batch [195/938], Loss: 0.3739720582962036\n",
      "Train: Epoch [18], Batch [196/938], Loss: 0.3582363724708557\n",
      "Train: Epoch [18], Batch [197/938], Loss: 0.4946388900279999\n",
      "Train: Epoch [18], Batch [198/938], Loss: 0.5725391507148743\n",
      "Train: Epoch [18], Batch [199/938], Loss: 0.439583420753479\n",
      "Train: Epoch [18], Batch [200/938], Loss: 0.592674195766449\n",
      "Train: Epoch [18], Batch [201/938], Loss: 0.45413580536842346\n",
      "Train: Epoch [18], Batch [202/938], Loss: 0.5425662994384766\n",
      "Train: Epoch [18], Batch [203/938], Loss: 0.4565138816833496\n",
      "Train: Epoch [18], Batch [204/938], Loss: 0.4643813669681549\n",
      "Train: Epoch [18], Batch [205/938], Loss: 0.5008368492126465\n",
      "Train: Epoch [18], Batch [206/938], Loss: 0.5025683045387268\n",
      "Train: Epoch [18], Batch [207/938], Loss: 0.7478612661361694\n",
      "Train: Epoch [18], Batch [208/938], Loss: 0.4589827358722687\n",
      "Train: Epoch [18], Batch [209/938], Loss: 0.4260696768760681\n",
      "Train: Epoch [18], Batch [210/938], Loss: 0.34175145626068115\n",
      "Train: Epoch [18], Batch [211/938], Loss: 0.5511707663536072\n",
      "Train: Epoch [18], Batch [212/938], Loss: 0.2393283247947693\n",
      "Train: Epoch [18], Batch [213/938], Loss: 0.6123491525650024\n",
      "Train: Epoch [18], Batch [214/938], Loss: 0.41637665033340454\n",
      "Train: Epoch [18], Batch [215/938], Loss: 0.45475608110427856\n",
      "Train: Epoch [18], Batch [216/938], Loss: 0.2865648567676544\n",
      "Train: Epoch [18], Batch [217/938], Loss: 0.30454838275909424\n",
      "Train: Epoch [18], Batch [218/938], Loss: 0.3325585126876831\n",
      "Train: Epoch [18], Batch [219/938], Loss: 0.5321115851402283\n",
      "Train: Epoch [18], Batch [220/938], Loss: 0.33708709478378296\n",
      "Train: Epoch [18], Batch [221/938], Loss: 0.5313585996627808\n",
      "Train: Epoch [18], Batch [222/938], Loss: 0.7207858562469482\n",
      "Train: Epoch [18], Batch [223/938], Loss: 0.410480260848999\n",
      "Train: Epoch [18], Batch [224/938], Loss: 0.31320127844810486\n",
      "Train: Epoch [18], Batch [225/938], Loss: 0.4377134442329407\n",
      "Train: Epoch [18], Batch [226/938], Loss: 0.46944957971572876\n",
      "Train: Epoch [18], Batch [227/938], Loss: 0.3432554006576538\n",
      "Train: Epoch [18], Batch [228/938], Loss: 0.3478681445121765\n",
      "Train: Epoch [18], Batch [229/938], Loss: 0.5207836627960205\n",
      "Train: Epoch [18], Batch [230/938], Loss: 0.4881119728088379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [18], Batch [231/938], Loss: 0.4202575087547302\n",
      "Train: Epoch [18], Batch [232/938], Loss: 0.4308589994907379\n",
      "Train: Epoch [18], Batch [233/938], Loss: 0.45928284525871277\n",
      "Train: Epoch [18], Batch [234/938], Loss: 0.5068320631980896\n",
      "Train: Epoch [18], Batch [235/938], Loss: 0.4514794945716858\n",
      "Train: Epoch [18], Batch [236/938], Loss: 0.3464064300060272\n",
      "Train: Epoch [18], Batch [237/938], Loss: 0.40033456683158875\n",
      "Train: Epoch [18], Batch [238/938], Loss: 0.6223089694976807\n",
      "Train: Epoch [18], Batch [239/938], Loss: 0.7204217314720154\n",
      "Train: Epoch [18], Batch [240/938], Loss: 0.32359933853149414\n",
      "Train: Epoch [18], Batch [241/938], Loss: 0.7626903057098389\n",
      "Train: Epoch [18], Batch [242/938], Loss: 0.3189455568790436\n",
      "Train: Epoch [18], Batch [243/938], Loss: 0.4491403102874756\n",
      "Train: Epoch [18], Batch [244/938], Loss: 0.4877120852470398\n",
      "Train: Epoch [18], Batch [245/938], Loss: 0.3813135623931885\n",
      "Train: Epoch [18], Batch [246/938], Loss: 0.5340251922607422\n",
      "Train: Epoch [18], Batch [247/938], Loss: 0.2831558585166931\n",
      "Train: Epoch [18], Batch [248/938], Loss: 0.5486692786216736\n",
      "Train: Epoch [18], Batch [249/938], Loss: 0.5460764765739441\n",
      "Train: Epoch [18], Batch [250/938], Loss: 0.4875710904598236\n",
      "Train: Epoch [18], Batch [251/938], Loss: 0.3108716607093811\n",
      "Train: Epoch [18], Batch [252/938], Loss: 0.47801023721694946\n",
      "Train: Epoch [18], Batch [253/938], Loss: 0.5989131331443787\n",
      "Train: Epoch [18], Batch [254/938], Loss: 0.38845324516296387\n",
      "Train: Epoch [18], Batch [255/938], Loss: 0.529393196105957\n",
      "Train: Epoch [18], Batch [256/938], Loss: 0.4095723032951355\n",
      "Train: Epoch [18], Batch [257/938], Loss: 0.42527997493743896\n",
      "Train: Epoch [18], Batch [258/938], Loss: 0.478889524936676\n",
      "Train: Epoch [18], Batch [259/938], Loss: 0.5273563861846924\n",
      "Train: Epoch [18], Batch [260/938], Loss: 0.4273888170719147\n",
      "Train: Epoch [18], Batch [261/938], Loss: 0.3532940745353699\n",
      "Train: Epoch [18], Batch [262/938], Loss: 0.32511407136917114\n",
      "Train: Epoch [18], Batch [263/938], Loss: 0.5230520963668823\n",
      "Train: Epoch [18], Batch [264/938], Loss: 0.4799637198448181\n",
      "Train: Epoch [18], Batch [265/938], Loss: 0.5341807007789612\n",
      "Train: Epoch [18], Batch [266/938], Loss: 0.30075493454933167\n",
      "Train: Epoch [18], Batch [267/938], Loss: 0.6159111857414246\n",
      "Train: Epoch [18], Batch [268/938], Loss: 0.3670557141304016\n",
      "Train: Epoch [18], Batch [269/938], Loss: 0.3961767554283142\n",
      "Train: Epoch [18], Batch [270/938], Loss: 0.4857020378112793\n",
      "Train: Epoch [18], Batch [271/938], Loss: 0.43143177032470703\n",
      "Train: Epoch [18], Batch [272/938], Loss: 0.5847839117050171\n",
      "Train: Epoch [18], Batch [273/938], Loss: 0.5570708513259888\n",
      "Train: Epoch [18], Batch [274/938], Loss: 0.4899064302444458\n",
      "Train: Epoch [18], Batch [275/938], Loss: 0.40070992708206177\n",
      "Train: Epoch [18], Batch [276/938], Loss: 0.5211234092712402\n",
      "Train: Epoch [18], Batch [277/938], Loss: 0.3323718309402466\n",
      "Train: Epoch [18], Batch [278/938], Loss: 0.35568100214004517\n",
      "Train: Epoch [18], Batch [279/938], Loss: 0.393279105424881\n",
      "Train: Epoch [18], Batch [280/938], Loss: 0.6092894077301025\n",
      "Train: Epoch [18], Batch [281/938], Loss: 0.5813479423522949\n",
      "Train: Epoch [18], Batch [282/938], Loss: 0.5278688669204712\n",
      "Train: Epoch [18], Batch [283/938], Loss: 0.2956063151359558\n",
      "Train: Epoch [18], Batch [284/938], Loss: 0.37123095989227295\n",
      "Train: Epoch [18], Batch [285/938], Loss: 0.17445069551467896\n",
      "Train: Epoch [18], Batch [286/938], Loss: 0.3971770405769348\n",
      "Train: Epoch [18], Batch [287/938], Loss: 0.3388979732990265\n",
      "Train: Epoch [18], Batch [288/938], Loss: 0.46882322430610657\n",
      "Train: Epoch [18], Batch [289/938], Loss: 0.37681880593299866\n",
      "Train: Epoch [18], Batch [290/938], Loss: 0.4253612160682678\n",
      "Train: Epoch [18], Batch [291/938], Loss: 0.3674317002296448\n",
      "Train: Epoch [18], Batch [292/938], Loss: 0.3740360736846924\n",
      "Train: Epoch [18], Batch [293/938], Loss: 0.7248156070709229\n",
      "Train: Epoch [18], Batch [294/938], Loss: 0.4685633182525635\n",
      "Train: Epoch [18], Batch [295/938], Loss: 0.5388845205307007\n",
      "Train: Epoch [18], Batch [296/938], Loss: 0.398023784160614\n",
      "Train: Epoch [18], Batch [297/938], Loss: 0.36757686734199524\n",
      "Train: Epoch [18], Batch [298/938], Loss: 0.4404556453227997\n",
      "Train: Epoch [18], Batch [299/938], Loss: 0.45136758685112\n",
      "Train: Epoch [18], Batch [300/938], Loss: 0.45489951968193054\n",
      "Train: Epoch [18], Batch [301/938], Loss: 0.4427020847797394\n",
      "Train: Epoch [18], Batch [302/938], Loss: 0.5192649960517883\n",
      "Train: Epoch [18], Batch [303/938], Loss: 0.39260780811309814\n",
      "Train: Epoch [18], Batch [304/938], Loss: 0.2660009562969208\n",
      "Train: Epoch [18], Batch [305/938], Loss: 0.4615597128868103\n",
      "Train: Epoch [18], Batch [306/938], Loss: 0.5037901401519775\n",
      "Train: Epoch [18], Batch [307/938], Loss: 0.5214737057685852\n",
      "Train: Epoch [18], Batch [308/938], Loss: 0.4861152172088623\n",
      "Train: Epoch [18], Batch [309/938], Loss: 0.3990502655506134\n",
      "Train: Epoch [18], Batch [310/938], Loss: 0.3913983106613159\n",
      "Train: Epoch [18], Batch [311/938], Loss: 0.626057505607605\n",
      "Train: Epoch [18], Batch [312/938], Loss: 0.5391793251037598\n",
      "Train: Epoch [18], Batch [313/938], Loss: 0.32753491401672363\n",
      "Train: Epoch [18], Batch [314/938], Loss: 0.571694552898407\n",
      "Train: Epoch [18], Batch [315/938], Loss: 0.4802449941635132\n",
      "Train: Epoch [18], Batch [316/938], Loss: 0.36700665950775146\n",
      "Train: Epoch [18], Batch [317/938], Loss: 0.3079100251197815\n",
      "Train: Epoch [18], Batch [318/938], Loss: 0.4139896631240845\n",
      "Train: Epoch [18], Batch [319/938], Loss: 0.5236103534698486\n",
      "Train: Epoch [18], Batch [320/938], Loss: 0.43401116132736206\n",
      "Train: Epoch [18], Batch [321/938], Loss: 0.5206310749053955\n",
      "Train: Epoch [18], Batch [322/938], Loss: 0.33212393522262573\n",
      "Train: Epoch [18], Batch [323/938], Loss: 0.6095549464225769\n",
      "Train: Epoch [18], Batch [324/938], Loss: 0.33879220485687256\n",
      "Train: Epoch [18], Batch [325/938], Loss: 0.4516879916191101\n",
      "Train: Epoch [18], Batch [326/938], Loss: 0.5856100916862488\n",
      "Train: Epoch [18], Batch [327/938], Loss: 0.4505274295806885\n",
      "Train: Epoch [18], Batch [328/938], Loss: 0.3942315876483917\n",
      "Train: Epoch [18], Batch [329/938], Loss: 0.3376944959163666\n",
      "Train: Epoch [18], Batch [330/938], Loss: 0.35880908370018005\n",
      "Train: Epoch [18], Batch [331/938], Loss: 0.4028111696243286\n",
      "Train: Epoch [18], Batch [332/938], Loss: 0.46986013650894165\n",
      "Train: Epoch [18], Batch [333/938], Loss: 0.6178598403930664\n",
      "Train: Epoch [18], Batch [334/938], Loss: 0.29549503326416016\n",
      "Train: Epoch [18], Batch [335/938], Loss: 0.6102968454360962\n",
      "Train: Epoch [18], Batch [336/938], Loss: 0.5621052384376526\n",
      "Train: Epoch [18], Batch [337/938], Loss: 0.46474480628967285\n",
      "Train: Epoch [18], Batch [338/938], Loss: 0.33982545137405396\n",
      "Train: Epoch [18], Batch [339/938], Loss: 0.36809080839157104\n",
      "Train: Epoch [18], Batch [340/938], Loss: 0.5278016924858093\n",
      "Train: Epoch [18], Batch [341/938], Loss: 0.4448087513446808\n",
      "Train: Epoch [18], Batch [342/938], Loss: 0.2997034788131714\n",
      "Train: Epoch [18], Batch [343/938], Loss: 0.5698782205581665\n",
      "Train: Epoch [18], Batch [344/938], Loss: 0.43069207668304443\n",
      "Train: Epoch [18], Batch [345/938], Loss: 0.45670661330223083\n",
      "Train: Epoch [18], Batch [346/938], Loss: 0.4001010060310364\n",
      "Train: Epoch [18], Batch [347/938], Loss: 0.5662528276443481\n",
      "Train: Epoch [18], Batch [348/938], Loss: 0.4931051731109619\n",
      "Train: Epoch [18], Batch [349/938], Loss: 0.3811112940311432\n",
      "Train: Epoch [18], Batch [350/938], Loss: 0.5285141468048096\n",
      "Train: Epoch [18], Batch [351/938], Loss: 0.5715031623840332\n",
      "Train: Epoch [18], Batch [352/938], Loss: 0.5576961040496826\n",
      "Train: Epoch [18], Batch [353/938], Loss: 0.43365344405174255\n",
      "Train: Epoch [18], Batch [354/938], Loss: 0.29289764165878296\n",
      "Train: Epoch [18], Batch [355/938], Loss: 0.4115719795227051\n",
      "Train: Epoch [18], Batch [356/938], Loss: 0.6880007982254028\n",
      "Train: Epoch [18], Batch [357/938], Loss: 0.3718329071998596\n",
      "Train: Epoch [18], Batch [358/938], Loss: 0.32818400859832764\n",
      "Train: Epoch [18], Batch [359/938], Loss: 0.40756797790527344\n",
      "Train: Epoch [18], Batch [360/938], Loss: 0.3313629627227783\n",
      "Train: Epoch [18], Batch [361/938], Loss: 0.5946947336196899\n",
      "Train: Epoch [18], Batch [362/938], Loss: 0.4341675043106079\n",
      "Train: Epoch [18], Batch [363/938], Loss: 0.3641343414783478\n",
      "Train: Epoch [18], Batch [364/938], Loss: 0.41336143016815186\n",
      "Train: Epoch [18], Batch [365/938], Loss: 0.4231897294521332\n",
      "Train: Epoch [18], Batch [366/938], Loss: 0.4483304023742676\n",
      "Train: Epoch [18], Batch [367/938], Loss: 0.37674638628959656\n",
      "Train: Epoch [18], Batch [368/938], Loss: 0.432365745306015\n",
      "Train: Epoch [18], Batch [369/938], Loss: 0.31863483786582947\n",
      "Train: Epoch [18], Batch [370/938], Loss: 0.6164979934692383\n",
      "Train: Epoch [18], Batch [371/938], Loss: 0.41862815618515015\n",
      "Train: Epoch [18], Batch [372/938], Loss: 0.3463844656944275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [18], Batch [373/938], Loss: 0.5671590566635132\n",
      "Train: Epoch [18], Batch [374/938], Loss: 0.3347325921058655\n",
      "Train: Epoch [18], Batch [375/938], Loss: 0.324476033449173\n",
      "Train: Epoch [18], Batch [376/938], Loss: 0.33743035793304443\n",
      "Train: Epoch [18], Batch [377/938], Loss: 0.6762662529945374\n",
      "Train: Epoch [18], Batch [378/938], Loss: 0.41396564245224\n",
      "Train: Epoch [18], Batch [379/938], Loss: 0.2694188356399536\n",
      "Train: Epoch [18], Batch [380/938], Loss: 0.6829542517662048\n",
      "Train: Epoch [18], Batch [381/938], Loss: 0.3778734505176544\n",
      "Train: Epoch [18], Batch [382/938], Loss: 0.4081588089466095\n",
      "Train: Epoch [18], Batch [383/938], Loss: 0.5094642639160156\n",
      "Train: Epoch [18], Batch [384/938], Loss: 0.7976366281509399\n",
      "Train: Epoch [18], Batch [385/938], Loss: 0.4187089800834656\n",
      "Train: Epoch [18], Batch [386/938], Loss: 0.1695464700460434\n",
      "Train: Epoch [18], Batch [387/938], Loss: 0.38309937715530396\n",
      "Train: Epoch [18], Batch [388/938], Loss: 0.36367446184158325\n",
      "Train: Epoch [18], Batch [389/938], Loss: 0.49639996886253357\n",
      "Train: Epoch [18], Batch [390/938], Loss: 0.5145917534828186\n",
      "Train: Epoch [18], Batch [391/938], Loss: 0.2950911223888397\n",
      "Train: Epoch [18], Batch [392/938], Loss: 0.500030517578125\n",
      "Train: Epoch [18], Batch [393/938], Loss: 0.5155882835388184\n",
      "Train: Epoch [18], Batch [394/938], Loss: 0.48706164956092834\n",
      "Train: Epoch [18], Batch [395/938], Loss: 0.3949132561683655\n",
      "Train: Epoch [18], Batch [396/938], Loss: 0.3934100866317749\n",
      "Train: Epoch [18], Batch [397/938], Loss: 0.3724125623703003\n",
      "Train: Epoch [18], Batch [398/938], Loss: 0.5388859510421753\n",
      "Train: Epoch [18], Batch [399/938], Loss: 0.5297721028327942\n",
      "Train: Epoch [18], Batch [400/938], Loss: 0.3802250027656555\n",
      "Train: Epoch [18], Batch [401/938], Loss: 0.5885405540466309\n",
      "Train: Epoch [18], Batch [402/938], Loss: 0.42894038558006287\n",
      "Train: Epoch [18], Batch [403/938], Loss: 0.4880788326263428\n",
      "Train: Epoch [18], Batch [404/938], Loss: 0.4411897659301758\n",
      "Train: Epoch [18], Batch [405/938], Loss: 0.4784080386161804\n",
      "Train: Epoch [18], Batch [406/938], Loss: 0.28551557660102844\n",
      "Train: Epoch [18], Batch [407/938], Loss: 0.3386487364768982\n",
      "Train: Epoch [18], Batch [408/938], Loss: 0.4094173312187195\n",
      "Train: Epoch [18], Batch [409/938], Loss: 0.28110256791114807\n",
      "Train: Epoch [18], Batch [410/938], Loss: 0.4340667724609375\n",
      "Train: Epoch [18], Batch [411/938], Loss: 0.4310722053050995\n",
      "Train: Epoch [18], Batch [412/938], Loss: 0.5972952246665955\n",
      "Train: Epoch [18], Batch [413/938], Loss: 0.5833278894424438\n",
      "Train: Epoch [18], Batch [414/938], Loss: 0.4511396884918213\n",
      "Train: Epoch [18], Batch [415/938], Loss: 0.42206108570098877\n",
      "Train: Epoch [18], Batch [416/938], Loss: 0.4343166947364807\n",
      "Train: Epoch [18], Batch [417/938], Loss: 0.428087055683136\n",
      "Train: Epoch [18], Batch [418/938], Loss: 0.48185354471206665\n",
      "Train: Epoch [18], Batch [419/938], Loss: 0.4512721598148346\n",
      "Train: Epoch [18], Batch [420/938], Loss: 0.6606292724609375\n",
      "Train: Epoch [18], Batch [421/938], Loss: 0.36818137764930725\n",
      "Train: Epoch [18], Batch [422/938], Loss: 0.39532944560050964\n",
      "Train: Epoch [18], Batch [423/938], Loss: 0.24274000525474548\n",
      "Train: Epoch [18], Batch [424/938], Loss: 0.316874623298645\n",
      "Train: Epoch [18], Batch [425/938], Loss: 0.3888864815235138\n",
      "Train: Epoch [18], Batch [426/938], Loss: 0.3856933116912842\n",
      "Train: Epoch [18], Batch [427/938], Loss: 0.41502392292022705\n",
      "Train: Epoch [18], Batch [428/938], Loss: 0.41897261142730713\n",
      "Train: Epoch [18], Batch [429/938], Loss: 0.45163559913635254\n",
      "Train: Epoch [18], Batch [430/938], Loss: 0.3894062936306\n",
      "Train: Epoch [18], Batch [431/938], Loss: 0.5345337986946106\n",
      "Train: Epoch [18], Batch [432/938], Loss: 0.48830288648605347\n",
      "Train: Epoch [18], Batch [433/938], Loss: 0.49860361218452454\n",
      "Train: Epoch [18], Batch [434/938], Loss: 0.47619250416755676\n",
      "Train: Epoch [18], Batch [435/938], Loss: 0.3439536392688751\n",
      "Train: Epoch [18], Batch [436/938], Loss: 0.41838228702545166\n",
      "Train: Epoch [18], Batch [437/938], Loss: 0.4218498468399048\n",
      "Train: Epoch [18], Batch [438/938], Loss: 0.41884011030197144\n",
      "Train: Epoch [18], Batch [439/938], Loss: 0.44100821018218994\n",
      "Train: Epoch [18], Batch [440/938], Loss: 0.3924420475959778\n",
      "Train: Epoch [18], Batch [441/938], Loss: 0.6983333826065063\n",
      "Train: Epoch [18], Batch [442/938], Loss: 0.45567771792411804\n",
      "Train: Epoch [18], Batch [443/938], Loss: 0.3768496513366699\n",
      "Train: Epoch [18], Batch [444/938], Loss: 0.43655240535736084\n",
      "Train: Epoch [18], Batch [445/938], Loss: 0.40871956944465637\n",
      "Train: Epoch [18], Batch [446/938], Loss: 0.2596014142036438\n",
      "Train: Epoch [18], Batch [447/938], Loss: 0.4726932644844055\n",
      "Train: Epoch [18], Batch [448/938], Loss: 0.3903522491455078\n",
      "Train: Epoch [18], Batch [449/938], Loss: 0.4229915142059326\n",
      "Train: Epoch [18], Batch [450/938], Loss: 0.4434523582458496\n",
      "Train: Epoch [18], Batch [451/938], Loss: 0.4106074273586273\n",
      "Train: Epoch [18], Batch [452/938], Loss: 0.5182884931564331\n",
      "Train: Epoch [18], Batch [453/938], Loss: 0.5829170942306519\n",
      "Train: Epoch [18], Batch [454/938], Loss: 0.35216790437698364\n",
      "Train: Epoch [18], Batch [455/938], Loss: 0.5116593837738037\n",
      "Train: Epoch [18], Batch [456/938], Loss: 0.44760042428970337\n",
      "Train: Epoch [18], Batch [457/938], Loss: 0.48934686183929443\n",
      "Train: Epoch [18], Batch [458/938], Loss: 0.308627724647522\n",
      "Train: Epoch [18], Batch [459/938], Loss: 0.44287437200546265\n",
      "Train: Epoch [18], Batch [460/938], Loss: 0.4632088541984558\n",
      "Train: Epoch [18], Batch [461/938], Loss: 0.33153754472732544\n",
      "Train: Epoch [18], Batch [462/938], Loss: 0.27619290351867676\n",
      "Train: Epoch [18], Batch [463/938], Loss: 0.3179672956466675\n",
      "Train: Epoch [18], Batch [464/938], Loss: 0.44553008675575256\n",
      "Train: Epoch [18], Batch [465/938], Loss: 0.35459965467453003\n",
      "Train: Epoch [18], Batch [466/938], Loss: 0.4781872034072876\n",
      "Train: Epoch [18], Batch [467/938], Loss: 0.5219042301177979\n",
      "Train: Epoch [18], Batch [468/938], Loss: 0.46608829498291016\n",
      "Train: Epoch [18], Batch [469/938], Loss: 0.38117069005966187\n",
      "Train: Epoch [18], Batch [470/938], Loss: 0.5882059335708618\n",
      "Train: Epoch [18], Batch [471/938], Loss: 0.3410485088825226\n",
      "Train: Epoch [18], Batch [472/938], Loss: 0.46506330370903015\n",
      "Train: Epoch [18], Batch [473/938], Loss: 0.46097877621650696\n",
      "Train: Epoch [18], Batch [474/938], Loss: 0.5646131038665771\n",
      "Train: Epoch [18], Batch [475/938], Loss: 0.31861358880996704\n",
      "Train: Epoch [18], Batch [476/938], Loss: 0.46085768938064575\n",
      "Train: Epoch [18], Batch [477/938], Loss: 0.42551207542419434\n",
      "Train: Epoch [18], Batch [478/938], Loss: 0.38192248344421387\n",
      "Train: Epoch [18], Batch [479/938], Loss: 0.5929496884346008\n",
      "Train: Epoch [18], Batch [480/938], Loss: 0.41596823930740356\n",
      "Train: Epoch [18], Batch [481/938], Loss: 0.334672749042511\n",
      "Train: Epoch [18], Batch [482/938], Loss: 0.3431709408760071\n",
      "Train: Epoch [18], Batch [483/938], Loss: 0.4220292568206787\n",
      "Train: Epoch [18], Batch [484/938], Loss: 0.4912317395210266\n",
      "Train: Epoch [18], Batch [485/938], Loss: 0.27904120087623596\n",
      "Train: Epoch [18], Batch [486/938], Loss: 0.2605741322040558\n",
      "Train: Epoch [18], Batch [487/938], Loss: 0.41353839635849\n",
      "Train: Epoch [18], Batch [488/938], Loss: 0.4794180393218994\n",
      "Train: Epoch [18], Batch [489/938], Loss: 0.3150880038738251\n",
      "Train: Epoch [18], Batch [490/938], Loss: 0.46337300539016724\n",
      "Train: Epoch [18], Batch [491/938], Loss: 0.30868810415267944\n",
      "Train: Epoch [18], Batch [492/938], Loss: 0.3287450671195984\n",
      "Train: Epoch [18], Batch [493/938], Loss: 0.5018487572669983\n",
      "Train: Epoch [18], Batch [494/938], Loss: 0.430031418800354\n",
      "Train: Epoch [18], Batch [495/938], Loss: 0.451768696308136\n",
      "Train: Epoch [18], Batch [496/938], Loss: 0.5434023141860962\n",
      "Train: Epoch [18], Batch [497/938], Loss: 0.4078912138938904\n",
      "Train: Epoch [18], Batch [498/938], Loss: 0.4736971855163574\n",
      "Train: Epoch [18], Batch [499/938], Loss: 0.422824889421463\n",
      "Train: Epoch [18], Batch [500/938], Loss: 0.27192819118499756\n",
      "Train: Epoch [18], Batch [501/938], Loss: 0.49847912788391113\n",
      "Train: Epoch [18], Batch [502/938], Loss: 0.2201886624097824\n",
      "Train: Epoch [18], Batch [503/938], Loss: 0.3645017147064209\n",
      "Train: Epoch [18], Batch [504/938], Loss: 0.554580807685852\n",
      "Train: Epoch [18], Batch [505/938], Loss: 0.37488144636154175\n",
      "Train: Epoch [18], Batch [506/938], Loss: 0.4313535690307617\n",
      "Train: Epoch [18], Batch [507/938], Loss: 0.34156838059425354\n",
      "Train: Epoch [18], Batch [508/938], Loss: 0.402601957321167\n",
      "Train: Epoch [18], Batch [509/938], Loss: 0.37037044763565063\n",
      "Train: Epoch [18], Batch [510/938], Loss: 0.4260297119617462\n",
      "Train: Epoch [18], Batch [511/938], Loss: 0.31290334463119507\n",
      "Train: Epoch [18], Batch [512/938], Loss: 0.4319913387298584\n",
      "Train: Epoch [18], Batch [513/938], Loss: 0.44114774465560913\n",
      "Train: Epoch [18], Batch [514/938], Loss: 0.3609924912452698\n",
      "Train: Epoch [18], Batch [515/938], Loss: 0.2656651735305786\n",
      "Train: Epoch [18], Batch [516/938], Loss: 0.5411280393600464\n",
      "Train: Epoch [18], Batch [517/938], Loss: 0.31344184279441833\n",
      "Train: Epoch [18], Batch [518/938], Loss: 0.29939258098602295\n",
      "Train: Epoch [18], Batch [519/938], Loss: 0.3211497664451599\n",
      "Train: Epoch [18], Batch [520/938], Loss: 0.28624433279037476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [18], Batch [521/938], Loss: 0.4291097819805145\n",
      "Train: Epoch [18], Batch [522/938], Loss: 0.6824332475662231\n",
      "Train: Epoch [18], Batch [523/938], Loss: 0.5606439113616943\n",
      "Train: Epoch [18], Batch [524/938], Loss: 0.45960110425949097\n",
      "Train: Epoch [18], Batch [525/938], Loss: 0.3253731429576874\n",
      "Train: Epoch [18], Batch [526/938], Loss: 0.3526296615600586\n",
      "Train: Epoch [18], Batch [527/938], Loss: 0.382168710231781\n",
      "Train: Epoch [18], Batch [528/938], Loss: 0.3267328143119812\n",
      "Train: Epoch [18], Batch [529/938], Loss: 0.34465309977531433\n",
      "Train: Epoch [18], Batch [530/938], Loss: 0.24231980741024017\n",
      "Train: Epoch [18], Batch [531/938], Loss: 0.34994518756866455\n",
      "Train: Epoch [18], Batch [532/938], Loss: 0.3731762170791626\n",
      "Train: Epoch [18], Batch [533/938], Loss: 0.6912416815757751\n",
      "Train: Epoch [18], Batch [534/938], Loss: 0.518301248550415\n",
      "Train: Epoch [18], Batch [535/938], Loss: 0.3420025110244751\n",
      "Train: Epoch [18], Batch [536/938], Loss: 0.41195809841156006\n",
      "Train: Epoch [18], Batch [537/938], Loss: 0.3770948052406311\n",
      "Train: Epoch [18], Batch [538/938], Loss: 0.4683244526386261\n",
      "Train: Epoch [18], Batch [539/938], Loss: 0.3596813380718231\n",
      "Train: Epoch [18], Batch [540/938], Loss: 0.38972288370132446\n",
      "Train: Epoch [18], Batch [541/938], Loss: 0.39791595935821533\n",
      "Train: Epoch [18], Batch [542/938], Loss: 0.3379690647125244\n",
      "Train: Epoch [18], Batch [543/938], Loss: 0.49702757596969604\n",
      "Train: Epoch [18], Batch [544/938], Loss: 0.45142507553100586\n",
      "Train: Epoch [18], Batch [545/938], Loss: 0.6441705226898193\n",
      "Train: Epoch [18], Batch [546/938], Loss: 0.5328422784805298\n",
      "Train: Epoch [18], Batch [547/938], Loss: 0.3139837384223938\n",
      "Train: Epoch [18], Batch [548/938], Loss: 0.3338245749473572\n",
      "Train: Epoch [18], Batch [549/938], Loss: 0.4714161157608032\n",
      "Train: Epoch [18], Batch [550/938], Loss: 0.40410226583480835\n",
      "Train: Epoch [18], Batch [551/938], Loss: 0.5541951060295105\n",
      "Train: Epoch [18], Batch [552/938], Loss: 0.383002907037735\n",
      "Train: Epoch [18], Batch [553/938], Loss: 0.5636526346206665\n",
      "Train: Epoch [18], Batch [554/938], Loss: 0.5015352368354797\n",
      "Train: Epoch [18], Batch [555/938], Loss: 0.5119528770446777\n",
      "Train: Epoch [18], Batch [556/938], Loss: 0.4118567705154419\n",
      "Train: Epoch [18], Batch [557/938], Loss: 0.34685635566711426\n",
      "Train: Epoch [18], Batch [558/938], Loss: 0.4469204246997833\n",
      "Train: Epoch [18], Batch [559/938], Loss: 0.34530144929885864\n",
      "Train: Epoch [18], Batch [560/938], Loss: 0.5081515312194824\n",
      "Train: Epoch [18], Batch [561/938], Loss: 0.3619391918182373\n",
      "Train: Epoch [18], Batch [562/938], Loss: 0.42338353395462036\n",
      "Train: Epoch [18], Batch [563/938], Loss: 0.431404173374176\n",
      "Train: Epoch [18], Batch [564/938], Loss: 0.40518221259117126\n",
      "Train: Epoch [18], Batch [565/938], Loss: 0.48005393147468567\n",
      "Train: Epoch [18], Batch [566/938], Loss: 0.3865242600440979\n",
      "Train: Epoch [18], Batch [567/938], Loss: 0.5233646035194397\n",
      "Train: Epoch [18], Batch [568/938], Loss: 0.20225952565670013\n",
      "Train: Epoch [18], Batch [569/938], Loss: 0.37662720680236816\n",
      "Train: Epoch [18], Batch [570/938], Loss: 0.4669952392578125\n",
      "Train: Epoch [18], Batch [571/938], Loss: 0.44530415534973145\n",
      "Train: Epoch [18], Batch [572/938], Loss: 0.38744956254959106\n",
      "Train: Epoch [18], Batch [573/938], Loss: 0.4017796218395233\n",
      "Train: Epoch [18], Batch [574/938], Loss: 0.31564486026763916\n",
      "Train: Epoch [18], Batch [575/938], Loss: 0.4397086203098297\n",
      "Train: Epoch [18], Batch [576/938], Loss: 0.4014607071876526\n",
      "Train: Epoch [18], Batch [577/938], Loss: 0.3986809551715851\n",
      "Train: Epoch [18], Batch [578/938], Loss: 0.45134350657463074\n",
      "Train: Epoch [18], Batch [579/938], Loss: 0.49530938267707825\n",
      "Train: Epoch [18], Batch [580/938], Loss: 0.5425915122032166\n",
      "Train: Epoch [18], Batch [581/938], Loss: 0.43449604511260986\n",
      "Train: Epoch [18], Batch [582/938], Loss: 0.37273508310317993\n",
      "Train: Epoch [18], Batch [583/938], Loss: 0.514694094657898\n",
      "Train: Epoch [18], Batch [584/938], Loss: 0.46988412737846375\n",
      "Train: Epoch [18], Batch [585/938], Loss: 0.5202542543411255\n",
      "Train: Epoch [18], Batch [586/938], Loss: 0.3535521328449249\n",
      "Train: Epoch [18], Batch [587/938], Loss: 0.37703752517700195\n",
      "Train: Epoch [18], Batch [588/938], Loss: 0.3748783469200134\n",
      "Train: Epoch [18], Batch [589/938], Loss: 0.3851635754108429\n",
      "Train: Epoch [18], Batch [590/938], Loss: 0.43104103207588196\n",
      "Train: Epoch [18], Batch [591/938], Loss: 0.3337189555168152\n",
      "Train: Epoch [18], Batch [592/938], Loss: 0.3692421317100525\n",
      "Train: Epoch [18], Batch [593/938], Loss: 0.26710280776023865\n",
      "Train: Epoch [18], Batch [594/938], Loss: 0.48425644636154175\n",
      "Train: Epoch [18], Batch [595/938], Loss: 0.27166274189949036\n",
      "Train: Epoch [18], Batch [596/938], Loss: 0.3935825228691101\n",
      "Train: Epoch [18], Batch [597/938], Loss: 0.3822490870952606\n",
      "Train: Epoch [18], Batch [598/938], Loss: 0.37804704904556274\n",
      "Train: Epoch [18], Batch [599/938], Loss: 0.5131262540817261\n",
      "Train: Epoch [18], Batch [600/938], Loss: 0.3079400360584259\n",
      "Train: Epoch [18], Batch [601/938], Loss: 0.39431434869766235\n",
      "Train: Epoch [18], Batch [602/938], Loss: 0.5490275025367737\n",
      "Train: Epoch [18], Batch [603/938], Loss: 0.3979693055152893\n",
      "Train: Epoch [18], Batch [604/938], Loss: 0.540311872959137\n",
      "Train: Epoch [18], Batch [605/938], Loss: 0.5956324934959412\n",
      "Train: Epoch [18], Batch [606/938], Loss: 0.5395388007164001\n",
      "Train: Epoch [18], Batch [607/938], Loss: 0.44412219524383545\n",
      "Train: Epoch [18], Batch [608/938], Loss: 0.3793998062610626\n",
      "Train: Epoch [18], Batch [609/938], Loss: 0.4603242874145508\n",
      "Train: Epoch [18], Batch [610/938], Loss: 0.45792579650878906\n",
      "Train: Epoch [18], Batch [611/938], Loss: 0.46976709365844727\n",
      "Train: Epoch [18], Batch [612/938], Loss: 0.256513774394989\n",
      "Train: Epoch [18], Batch [613/938], Loss: 0.47122013568878174\n",
      "Train: Epoch [18], Batch [614/938], Loss: 0.42560458183288574\n",
      "Train: Epoch [18], Batch [615/938], Loss: 0.49994805455207825\n",
      "Train: Epoch [18], Batch [616/938], Loss: 0.3215082287788391\n",
      "Train: Epoch [18], Batch [617/938], Loss: 0.47440841794013977\n",
      "Train: Epoch [18], Batch [618/938], Loss: 0.3430664837360382\n",
      "Train: Epoch [18], Batch [619/938], Loss: 0.24228855967521667\n",
      "Train: Epoch [18], Batch [620/938], Loss: 0.6461523771286011\n",
      "Train: Epoch [18], Batch [621/938], Loss: 0.37893277406692505\n",
      "Train: Epoch [18], Batch [622/938], Loss: 0.46899017691612244\n",
      "Train: Epoch [18], Batch [623/938], Loss: 0.3286190927028656\n",
      "Train: Epoch [18], Batch [624/938], Loss: 0.4730456471443176\n",
      "Train: Epoch [18], Batch [625/938], Loss: 0.5392920970916748\n",
      "Train: Epoch [18], Batch [626/938], Loss: 0.5281232595443726\n",
      "Train: Epoch [18], Batch [627/938], Loss: 0.3427353501319885\n",
      "Train: Epoch [18], Batch [628/938], Loss: 0.4687182307243347\n",
      "Train: Epoch [18], Batch [629/938], Loss: 0.4075626730918884\n",
      "Train: Epoch [18], Batch [630/938], Loss: 0.577087938785553\n",
      "Train: Epoch [18], Batch [631/938], Loss: 0.44373464584350586\n",
      "Train: Epoch [18], Batch [632/938], Loss: 0.41795116662979126\n",
      "Train: Epoch [18], Batch [633/938], Loss: 0.4083595275878906\n",
      "Train: Epoch [18], Batch [634/938], Loss: 0.46000364422798157\n",
      "Train: Epoch [18], Batch [635/938], Loss: 0.37998005747795105\n",
      "Train: Epoch [18], Batch [636/938], Loss: 0.6043813824653625\n",
      "Train: Epoch [18], Batch [637/938], Loss: 0.4078958332538605\n",
      "Train: Epoch [18], Batch [638/938], Loss: 0.39301571249961853\n",
      "Train: Epoch [18], Batch [639/938], Loss: 0.42113736271858215\n",
      "Train: Epoch [18], Batch [640/938], Loss: 0.3680839240550995\n",
      "Train: Epoch [18], Batch [641/938], Loss: 0.4254094660282135\n",
      "Train: Epoch [18], Batch [642/938], Loss: 0.5844756960868835\n",
      "Train: Epoch [18], Batch [643/938], Loss: 0.6205311417579651\n",
      "Train: Epoch [18], Batch [644/938], Loss: 0.4239963889122009\n",
      "Train: Epoch [18], Batch [645/938], Loss: 0.5994778871536255\n",
      "Train: Epoch [18], Batch [646/938], Loss: 0.34156569838523865\n",
      "Train: Epoch [18], Batch [647/938], Loss: 0.5330939888954163\n",
      "Train: Epoch [18], Batch [648/938], Loss: 0.3798444867134094\n",
      "Train: Epoch [18], Batch [649/938], Loss: 0.6986789703369141\n",
      "Train: Epoch [18], Batch [650/938], Loss: 0.3361700773239136\n",
      "Train: Epoch [18], Batch [651/938], Loss: 0.32816997170448303\n",
      "Train: Epoch [18], Batch [652/938], Loss: 0.3708702325820923\n",
      "Train: Epoch [18], Batch [653/938], Loss: 0.333081990480423\n",
      "Train: Epoch [18], Batch [654/938], Loss: 0.2886659801006317\n",
      "Train: Epoch [18], Batch [655/938], Loss: 0.38666582107543945\n",
      "Train: Epoch [18], Batch [656/938], Loss: 0.4358111619949341\n",
      "Train: Epoch [18], Batch [657/938], Loss: 0.5345770716667175\n",
      "Train: Epoch [18], Batch [658/938], Loss: 0.5696144104003906\n",
      "Train: Epoch [18], Batch [659/938], Loss: 0.5479130148887634\n",
      "Train: Epoch [18], Batch [660/938], Loss: 0.5460573434829712\n",
      "Train: Epoch [18], Batch [661/938], Loss: 0.45308917760849\n",
      "Train: Epoch [18], Batch [662/938], Loss: 0.3241293728351593\n",
      "Train: Epoch [18], Batch [663/938], Loss: 0.42600417137145996\n",
      "Train: Epoch [18], Batch [664/938], Loss: 0.3500440716743469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [18], Batch [665/938], Loss: 0.2754751443862915\n",
      "Train: Epoch [18], Batch [666/938], Loss: 0.4141194522380829\n",
      "Train: Epoch [18], Batch [667/938], Loss: 0.4253714084625244\n",
      "Train: Epoch [18], Batch [668/938], Loss: 0.30232399702072144\n",
      "Train: Epoch [18], Batch [669/938], Loss: 0.5428838133811951\n",
      "Train: Epoch [18], Batch [670/938], Loss: 0.3874293565750122\n",
      "Train: Epoch [18], Batch [671/938], Loss: 0.5159081220626831\n",
      "Train: Epoch [18], Batch [672/938], Loss: 0.5550600290298462\n",
      "Train: Epoch [18], Batch [673/938], Loss: 0.2504488527774811\n",
      "Train: Epoch [18], Batch [674/938], Loss: 0.5387850999832153\n",
      "Train: Epoch [18], Batch [675/938], Loss: 0.6378960609436035\n",
      "Train: Epoch [18], Batch [676/938], Loss: 0.8964971899986267\n",
      "Train: Epoch [18], Batch [677/938], Loss: 0.2409975826740265\n",
      "Train: Epoch [18], Batch [678/938], Loss: 0.46277904510498047\n",
      "Train: Epoch [18], Batch [679/938], Loss: 0.3157242238521576\n",
      "Train: Epoch [18], Batch [680/938], Loss: 0.40288347005844116\n",
      "Train: Epoch [18], Batch [681/938], Loss: 0.35633647441864014\n",
      "Train: Epoch [18], Batch [682/938], Loss: 0.356715589761734\n",
      "Train: Epoch [18], Batch [683/938], Loss: 0.36897408962249756\n",
      "Train: Epoch [18], Batch [684/938], Loss: 0.24971845746040344\n",
      "Train: Epoch [18], Batch [685/938], Loss: 0.45150208473205566\n",
      "Train: Epoch [18], Batch [686/938], Loss: 0.42025238275527954\n",
      "Train: Epoch [18], Batch [687/938], Loss: 0.34780508279800415\n",
      "Train: Epoch [18], Batch [688/938], Loss: 0.5060995817184448\n",
      "Train: Epoch [18], Batch [689/938], Loss: 0.31756287813186646\n",
      "Train: Epoch [18], Batch [690/938], Loss: 0.3775319457054138\n",
      "Train: Epoch [18], Batch [691/938], Loss: 0.4779653251171112\n",
      "Train: Epoch [18], Batch [692/938], Loss: 0.38799265027046204\n",
      "Train: Epoch [18], Batch [693/938], Loss: 0.4781012237071991\n",
      "Train: Epoch [18], Batch [694/938], Loss: 0.3961103558540344\n",
      "Train: Epoch [18], Batch [695/938], Loss: 0.5106621980667114\n",
      "Train: Epoch [18], Batch [696/938], Loss: 0.3046407103538513\n",
      "Train: Epoch [18], Batch [697/938], Loss: 0.27032727003097534\n",
      "Train: Epoch [18], Batch [698/938], Loss: 0.35599082708358765\n",
      "Train: Epoch [18], Batch [699/938], Loss: 0.338242769241333\n",
      "Train: Epoch [18], Batch [700/938], Loss: 0.551108181476593\n",
      "Train: Epoch [18], Batch [701/938], Loss: 0.3297353982925415\n",
      "Train: Epoch [18], Batch [702/938], Loss: 0.5995429754257202\n",
      "Train: Epoch [18], Batch [703/938], Loss: 0.5466256737709045\n",
      "Train: Epoch [18], Batch [704/938], Loss: 0.5018376111984253\n",
      "Train: Epoch [18], Batch [705/938], Loss: 0.2670650780200958\n",
      "Train: Epoch [18], Batch [706/938], Loss: 0.4835355579853058\n",
      "Train: Epoch [18], Batch [707/938], Loss: 0.34561705589294434\n",
      "Train: Epoch [18], Batch [708/938], Loss: 0.3983350396156311\n",
      "Train: Epoch [18], Batch [709/938], Loss: 0.6734086275100708\n",
      "Train: Epoch [18], Batch [710/938], Loss: 0.36304712295532227\n",
      "Train: Epoch [18], Batch [711/938], Loss: 0.4709537625312805\n",
      "Train: Epoch [18], Batch [712/938], Loss: 0.3881986141204834\n",
      "Train: Epoch [18], Batch [713/938], Loss: 0.5880672931671143\n",
      "Train: Epoch [18], Batch [714/938], Loss: 0.23592856526374817\n",
      "Train: Epoch [18], Batch [715/938], Loss: 0.49730855226516724\n",
      "Train: Epoch [18], Batch [716/938], Loss: 0.5877640247344971\n",
      "Train: Epoch [18], Batch [717/938], Loss: 0.49389100074768066\n",
      "Train: Epoch [18], Batch [718/938], Loss: 0.3034866154193878\n",
      "Train: Epoch [18], Batch [719/938], Loss: 0.29998162388801575\n",
      "Train: Epoch [18], Batch [720/938], Loss: 0.4161055088043213\n",
      "Train: Epoch [18], Batch [721/938], Loss: 0.5412138104438782\n",
      "Train: Epoch [18], Batch [722/938], Loss: 0.3595435917377472\n",
      "Train: Epoch [18], Batch [723/938], Loss: 0.28768226504325867\n",
      "Train: Epoch [18], Batch [724/938], Loss: 0.43586257100105286\n",
      "Train: Epoch [18], Batch [725/938], Loss: 0.46686244010925293\n",
      "Train: Epoch [18], Batch [726/938], Loss: 0.35622185468673706\n",
      "Train: Epoch [18], Batch [727/938], Loss: 0.3794771432876587\n",
      "Train: Epoch [18], Batch [728/938], Loss: 0.15010303258895874\n",
      "Train: Epoch [18], Batch [729/938], Loss: 0.521369218826294\n",
      "Train: Epoch [18], Batch [730/938], Loss: 0.544975996017456\n",
      "Train: Epoch [18], Batch [731/938], Loss: 0.48906210064888\n",
      "Train: Epoch [18], Batch [732/938], Loss: 0.7404875755310059\n",
      "Train: Epoch [18], Batch [733/938], Loss: 0.4708535671234131\n",
      "Train: Epoch [18], Batch [734/938], Loss: 0.2835581302642822\n",
      "Train: Epoch [18], Batch [735/938], Loss: 0.285019189119339\n",
      "Train: Epoch [18], Batch [736/938], Loss: 0.44185715913772583\n",
      "Train: Epoch [18], Batch [737/938], Loss: 0.5081618428230286\n",
      "Train: Epoch [18], Batch [738/938], Loss: 0.3898475766181946\n",
      "Train: Epoch [18], Batch [739/938], Loss: 0.40949392318725586\n",
      "Train: Epoch [18], Batch [740/938], Loss: 0.32155412435531616\n",
      "Train: Epoch [18], Batch [741/938], Loss: 0.3421602249145508\n",
      "Train: Epoch [18], Batch [742/938], Loss: 0.2966100871562958\n",
      "Train: Epoch [18], Batch [743/938], Loss: 0.3061358630657196\n",
      "Train: Epoch [18], Batch [744/938], Loss: 0.429719477891922\n",
      "Train: Epoch [18], Batch [745/938], Loss: 0.4138374328613281\n",
      "Train: Epoch [18], Batch [746/938], Loss: 0.3722328543663025\n",
      "Train: Epoch [18], Batch [747/938], Loss: 0.5919594764709473\n",
      "Train: Epoch [18], Batch [748/938], Loss: 0.40548282861709595\n",
      "Train: Epoch [18], Batch [749/938], Loss: 0.5072333812713623\n",
      "Train: Epoch [18], Batch [750/938], Loss: 0.43363267183303833\n",
      "Train: Epoch [18], Batch [751/938], Loss: 0.6878356337547302\n",
      "Train: Epoch [18], Batch [752/938], Loss: 0.21119800209999084\n",
      "Train: Epoch [18], Batch [753/938], Loss: 0.30460628867149353\n",
      "Train: Epoch [18], Batch [754/938], Loss: 0.3589513897895813\n",
      "Train: Epoch [18], Batch [755/938], Loss: 0.4606177806854248\n",
      "Train: Epoch [18], Batch [756/938], Loss: 0.4740317463874817\n",
      "Train: Epoch [18], Batch [757/938], Loss: 0.22820644080638885\n",
      "Train: Epoch [18], Batch [758/938], Loss: 0.4095371961593628\n",
      "Train: Epoch [18], Batch [759/938], Loss: 0.5811065435409546\n",
      "Train: Epoch [18], Batch [760/938], Loss: 0.33495229482650757\n",
      "Train: Epoch [18], Batch [761/938], Loss: 0.6286336183547974\n",
      "Train: Epoch [18], Batch [762/938], Loss: 0.5898609757423401\n",
      "Train: Epoch [18], Batch [763/938], Loss: 0.38425004482269287\n",
      "Train: Epoch [18], Batch [764/938], Loss: 0.35827815532684326\n",
      "Train: Epoch [18], Batch [765/938], Loss: 0.3082110285758972\n",
      "Train: Epoch [18], Batch [766/938], Loss: 0.2911377549171448\n",
      "Train: Epoch [18], Batch [767/938], Loss: 0.5246102809906006\n",
      "Train: Epoch [18], Batch [768/938], Loss: 0.4267382025718689\n",
      "Train: Epoch [18], Batch [769/938], Loss: 0.42748552560806274\n",
      "Train: Epoch [18], Batch [770/938], Loss: 0.5738670825958252\n",
      "Train: Epoch [18], Batch [771/938], Loss: 0.4625162184238434\n",
      "Train: Epoch [18], Batch [772/938], Loss: 0.5799131393432617\n",
      "Train: Epoch [18], Batch [773/938], Loss: 0.4196515679359436\n",
      "Train: Epoch [18], Batch [774/938], Loss: 0.49098676443099976\n",
      "Train: Epoch [18], Batch [775/938], Loss: 0.5069831013679504\n",
      "Train: Epoch [18], Batch [776/938], Loss: 0.3279690146446228\n",
      "Train: Epoch [18], Batch [777/938], Loss: 0.5654265880584717\n",
      "Train: Epoch [18], Batch [778/938], Loss: 0.2937372922897339\n",
      "Train: Epoch [18], Batch [779/938], Loss: 0.44218385219573975\n",
      "Train: Epoch [18], Batch [780/938], Loss: 0.3113514482975006\n",
      "Train: Epoch [18], Batch [781/938], Loss: 0.6780409216880798\n",
      "Train: Epoch [18], Batch [782/938], Loss: 0.35148394107818604\n",
      "Train: Epoch [18], Batch [783/938], Loss: 0.36032581329345703\n",
      "Train: Epoch [18], Batch [784/938], Loss: 0.36444342136383057\n",
      "Train: Epoch [18], Batch [785/938], Loss: 0.48997676372528076\n",
      "Train: Epoch [18], Batch [786/938], Loss: 0.26156923174858093\n",
      "Train: Epoch [18], Batch [787/938], Loss: 0.5955734252929688\n",
      "Train: Epoch [18], Batch [788/938], Loss: 0.34069862961769104\n",
      "Train: Epoch [18], Batch [789/938], Loss: 0.5108231902122498\n",
      "Train: Epoch [18], Batch [790/938], Loss: 0.4183773696422577\n",
      "Train: Epoch [18], Batch [791/938], Loss: 0.472978800535202\n",
      "Train: Epoch [18], Batch [792/938], Loss: 0.49424096941947937\n",
      "Train: Epoch [18], Batch [793/938], Loss: 0.32543039321899414\n",
      "Train: Epoch [18], Batch [794/938], Loss: 0.32886093854904175\n",
      "Train: Epoch [18], Batch [795/938], Loss: 0.385328471660614\n",
      "Train: Epoch [18], Batch [796/938], Loss: 0.277709424495697\n",
      "Train: Epoch [18], Batch [797/938], Loss: 0.30912843346595764\n",
      "Train: Epoch [18], Batch [798/938], Loss: 0.5604425668716431\n",
      "Train: Epoch [18], Batch [799/938], Loss: 0.5229793787002563\n",
      "Train: Epoch [18], Batch [800/938], Loss: 0.5949545502662659\n",
      "Train: Epoch [18], Batch [801/938], Loss: 0.3372476100921631\n",
      "Train: Epoch [18], Batch [802/938], Loss: 0.26763981580734253\n",
      "Train: Epoch [18], Batch [803/938], Loss: 0.3604798913002014\n",
      "Train: Epoch [18], Batch [804/938], Loss: 0.3716791272163391\n",
      "Train: Epoch [18], Batch [805/938], Loss: 0.5803940296173096\n",
      "Train: Epoch [18], Batch [806/938], Loss: 0.5261662602424622\n",
      "Train: Epoch [18], Batch [807/938], Loss: 0.39989158511161804\n",
      "Train: Epoch [18], Batch [808/938], Loss: 0.43022334575653076\n",
      "Train: Epoch [18], Batch [809/938], Loss: 0.3920174241065979\n",
      "Train: Epoch [18], Batch [810/938], Loss: 0.2845587134361267\n",
      "Train: Epoch [18], Batch [811/938], Loss: 0.6441469192504883\n",
      "Train: Epoch [18], Batch [812/938], Loss: 0.32143568992614746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [18], Batch [813/938], Loss: 0.5489570498466492\n",
      "Train: Epoch [18], Batch [814/938], Loss: 0.33773547410964966\n",
      "Train: Epoch [18], Batch [815/938], Loss: 0.23017382621765137\n",
      "Train: Epoch [18], Batch [816/938], Loss: 0.40015408396720886\n",
      "Train: Epoch [18], Batch [817/938], Loss: 0.6152516603469849\n",
      "Train: Epoch [18], Batch [818/938], Loss: 0.34394264221191406\n",
      "Train: Epoch [18], Batch [819/938], Loss: 0.3268420100212097\n",
      "Train: Epoch [18], Batch [820/938], Loss: 0.346790611743927\n",
      "Train: Epoch [18], Batch [821/938], Loss: 0.4832990765571594\n",
      "Train: Epoch [18], Batch [822/938], Loss: 0.3073385953903198\n",
      "Train: Epoch [18], Batch [823/938], Loss: 0.34419912099838257\n",
      "Train: Epoch [18], Batch [824/938], Loss: 0.33661243319511414\n",
      "Train: Epoch [18], Batch [825/938], Loss: 0.5352129340171814\n",
      "Train: Epoch [18], Batch [826/938], Loss: 0.7347339391708374\n",
      "Train: Epoch [18], Batch [827/938], Loss: 0.24843141436576843\n",
      "Train: Epoch [18], Batch [828/938], Loss: 0.3661153316497803\n",
      "Train: Epoch [18], Batch [829/938], Loss: 0.3058999478816986\n",
      "Train: Epoch [18], Batch [830/938], Loss: 0.2320335954427719\n",
      "Train: Epoch [18], Batch [831/938], Loss: 0.5354175567626953\n",
      "Train: Epoch [18], Batch [832/938], Loss: 0.5168744325637817\n",
      "Train: Epoch [18], Batch [833/938], Loss: 0.47541993856430054\n",
      "Train: Epoch [18], Batch [834/938], Loss: 0.3260915279388428\n",
      "Train: Epoch [18], Batch [835/938], Loss: 0.28003108501434326\n",
      "Train: Epoch [18], Batch [836/938], Loss: 0.4174979329109192\n",
      "Train: Epoch [18], Batch [837/938], Loss: 0.6293754577636719\n",
      "Train: Epoch [18], Batch [838/938], Loss: 0.2930491864681244\n",
      "Train: Epoch [18], Batch [839/938], Loss: 0.5927392244338989\n",
      "Train: Epoch [18], Batch [840/938], Loss: 0.43739211559295654\n",
      "Train: Epoch [18], Batch [841/938], Loss: 0.36541181802749634\n",
      "Train: Epoch [18], Batch [842/938], Loss: 0.2841797471046448\n",
      "Train: Epoch [18], Batch [843/938], Loss: 0.32867804169654846\n",
      "Train: Epoch [18], Batch [844/938], Loss: 0.38916099071502686\n",
      "Train: Epoch [18], Batch [845/938], Loss: 0.3283669352531433\n",
      "Train: Epoch [18], Batch [846/938], Loss: 0.2416296750307083\n",
      "Train: Epoch [18], Batch [847/938], Loss: 0.3436290919780731\n",
      "Train: Epoch [18], Batch [848/938], Loss: 0.5692654848098755\n",
      "Train: Epoch [18], Batch [849/938], Loss: 0.21217213571071625\n",
      "Train: Epoch [18], Batch [850/938], Loss: 0.2582244575023651\n",
      "Train: Epoch [18], Batch [851/938], Loss: 0.4802874028682709\n",
      "Train: Epoch [18], Batch [852/938], Loss: 0.46514731645584106\n",
      "Train: Epoch [18], Batch [853/938], Loss: 0.3477352261543274\n",
      "Train: Epoch [18], Batch [854/938], Loss: 0.46547257900238037\n",
      "Train: Epoch [18], Batch [855/938], Loss: 0.3598611652851105\n",
      "Train: Epoch [18], Batch [856/938], Loss: 0.4351249039173126\n",
      "Train: Epoch [18], Batch [857/938], Loss: 0.3860207200050354\n",
      "Train: Epoch [18], Batch [858/938], Loss: 0.3324659466743469\n",
      "Train: Epoch [18], Batch [859/938], Loss: 0.571467399597168\n",
      "Train: Epoch [18], Batch [860/938], Loss: 0.46050071716308594\n",
      "Train: Epoch [18], Batch [861/938], Loss: 0.31716784834861755\n",
      "Train: Epoch [18], Batch [862/938], Loss: 0.5960544943809509\n",
      "Train: Epoch [18], Batch [863/938], Loss: 0.5129566192626953\n",
      "Train: Epoch [18], Batch [864/938], Loss: 0.4547175168991089\n",
      "Train: Epoch [18], Batch [865/938], Loss: 0.6349768042564392\n",
      "Train: Epoch [18], Batch [866/938], Loss: 0.5889397263526917\n",
      "Train: Epoch [18], Batch [867/938], Loss: 0.4398031234741211\n",
      "Train: Epoch [18], Batch [868/938], Loss: 0.2584633231163025\n",
      "Train: Epoch [18], Batch [869/938], Loss: 0.4162602424621582\n",
      "Train: Epoch [18], Batch [870/938], Loss: 0.5161985158920288\n",
      "Train: Epoch [18], Batch [871/938], Loss: 0.27316105365753174\n",
      "Train: Epoch [18], Batch [872/938], Loss: 0.4684453010559082\n",
      "Train: Epoch [18], Batch [873/938], Loss: 0.4037643074989319\n",
      "Train: Epoch [18], Batch [874/938], Loss: 0.4287445545196533\n",
      "Train: Epoch [18], Batch [875/938], Loss: 0.4854847192764282\n",
      "Train: Epoch [18], Batch [876/938], Loss: 0.34631502628326416\n",
      "Train: Epoch [18], Batch [877/938], Loss: 0.23522281646728516\n",
      "Train: Epoch [18], Batch [878/938], Loss: 0.33616626262664795\n",
      "Train: Epoch [18], Batch [879/938], Loss: 0.44442805647850037\n",
      "Train: Epoch [18], Batch [880/938], Loss: 0.5237171649932861\n",
      "Train: Epoch [18], Batch [881/938], Loss: 0.4000159502029419\n",
      "Train: Epoch [18], Batch [882/938], Loss: 0.4402708411216736\n",
      "Train: Epoch [18], Batch [883/938], Loss: 0.4145972430706024\n",
      "Train: Epoch [18], Batch [884/938], Loss: 0.47125914692878723\n",
      "Train: Epoch [18], Batch [885/938], Loss: 0.5491417050361633\n",
      "Train: Epoch [18], Batch [886/938], Loss: 0.5251160860061646\n",
      "Train: Epoch [18], Batch [887/938], Loss: 0.30216890573501587\n",
      "Train: Epoch [18], Batch [888/938], Loss: 0.4638778567314148\n",
      "Train: Epoch [18], Batch [889/938], Loss: 0.5221787691116333\n",
      "Train: Epoch [18], Batch [890/938], Loss: 0.61426842212677\n",
      "Train: Epoch [18], Batch [891/938], Loss: 0.5261836051940918\n",
      "Train: Epoch [18], Batch [892/938], Loss: 0.4783670902252197\n",
      "Train: Epoch [18], Batch [893/938], Loss: 0.4713558256626129\n",
      "Train: Epoch [18], Batch [894/938], Loss: 0.6771793961524963\n",
      "Train: Epoch [18], Batch [895/938], Loss: 0.6133474111557007\n",
      "Train: Epoch [18], Batch [896/938], Loss: 0.41901177167892456\n",
      "Train: Epoch [18], Batch [897/938], Loss: 0.3126944899559021\n",
      "Train: Epoch [18], Batch [898/938], Loss: 0.43227851390838623\n",
      "Train: Epoch [18], Batch [899/938], Loss: 0.35886266827583313\n",
      "Train: Epoch [18], Batch [900/938], Loss: 0.5886828303337097\n",
      "Train: Epoch [18], Batch [901/938], Loss: 0.43577805161476135\n",
      "Train: Epoch [18], Batch [902/938], Loss: 0.334303617477417\n",
      "Train: Epoch [18], Batch [903/938], Loss: 0.4624078869819641\n",
      "Train: Epoch [18], Batch [904/938], Loss: 0.28290945291519165\n",
      "Train: Epoch [18], Batch [905/938], Loss: 0.3698117136955261\n",
      "Train: Epoch [18], Batch [906/938], Loss: 0.4638107120990753\n",
      "Train: Epoch [18], Batch [907/938], Loss: 0.47637107968330383\n",
      "Train: Epoch [18], Batch [908/938], Loss: 0.3379879593849182\n",
      "Train: Epoch [18], Batch [909/938], Loss: 0.6034194231033325\n",
      "Train: Epoch [18], Batch [910/938], Loss: 0.3764054775238037\n",
      "Train: Epoch [18], Batch [911/938], Loss: 0.4882473051548004\n",
      "Train: Epoch [18], Batch [912/938], Loss: 0.44753557443618774\n",
      "Train: Epoch [18], Batch [913/938], Loss: 0.3937839865684509\n",
      "Train: Epoch [18], Batch [914/938], Loss: 0.3332980275154114\n",
      "Train: Epoch [18], Batch [915/938], Loss: 0.4826076626777649\n",
      "Train: Epoch [18], Batch [916/938], Loss: 0.4440652132034302\n",
      "Train: Epoch [18], Batch [917/938], Loss: 0.411354124546051\n",
      "Train: Epoch [18], Batch [918/938], Loss: 0.4089152216911316\n",
      "Train: Epoch [18], Batch [919/938], Loss: 0.4808003902435303\n",
      "Train: Epoch [18], Batch [920/938], Loss: 0.5054340362548828\n",
      "Train: Epoch [18], Batch [921/938], Loss: 0.34121212363243103\n",
      "Train: Epoch [18], Batch [922/938], Loss: 0.38153332471847534\n",
      "Train: Epoch [18], Batch [923/938], Loss: 0.38595321774482727\n",
      "Train: Epoch [18], Batch [924/938], Loss: 0.5229587554931641\n",
      "Train: Epoch [18], Batch [925/938], Loss: 0.4359767735004425\n",
      "Train: Epoch [18], Batch [926/938], Loss: 0.3414921760559082\n",
      "Train: Epoch [18], Batch [927/938], Loss: 0.5865278244018555\n",
      "Train: Epoch [18], Batch [928/938], Loss: 0.21826894581317902\n",
      "Train: Epoch [18], Batch [929/938], Loss: 0.40032264590263367\n",
      "Train: Epoch [18], Batch [930/938], Loss: 0.26592615246772766\n",
      "Train: Epoch [18], Batch [931/938], Loss: 0.3181138038635254\n",
      "Train: Epoch [18], Batch [932/938], Loss: 0.43476709723472595\n",
      "Train: Epoch [18], Batch [933/938], Loss: 0.2766168713569641\n",
      "Train: Epoch [18], Batch [934/938], Loss: 0.46201613545417786\n",
      "Train: Epoch [18], Batch [935/938], Loss: 0.3969298005104065\n",
      "Train: Epoch [18], Batch [936/938], Loss: 0.3644774854183197\n",
      "Train: Epoch [18], Batch [937/938], Loss: 0.35623812675476074\n",
      "Train: Epoch [18], Batch [938/938], Loss: 0.30781644582748413\n",
      "Accuracy of train set: 0.8494\n",
      "Validation: Epoch [18], Batch [1/938], Loss: 0.2839958965778351\n",
      "Validation: Epoch [18], Batch [2/938], Loss: 0.6079128384590149\n",
      "Validation: Epoch [18], Batch [3/938], Loss: 0.32585257291793823\n",
      "Validation: Epoch [18], Batch [4/938], Loss: 0.4487370252609253\n",
      "Validation: Epoch [18], Batch [5/938], Loss: 0.5376268029212952\n",
      "Validation: Epoch [18], Batch [6/938], Loss: 0.4295075237751007\n",
      "Validation: Epoch [18], Batch [7/938], Loss: 0.5142538547515869\n",
      "Validation: Epoch [18], Batch [8/938], Loss: 0.5207630395889282\n",
      "Validation: Epoch [18], Batch [9/938], Loss: 0.29667651653289795\n",
      "Validation: Epoch [18], Batch [10/938], Loss: 0.405059278011322\n",
      "Validation: Epoch [18], Batch [11/938], Loss: 0.42626458406448364\n",
      "Validation: Epoch [18], Batch [12/938], Loss: 0.383969783782959\n",
      "Validation: Epoch [18], Batch [13/938], Loss: 0.39362964034080505\n",
      "Validation: Epoch [18], Batch [14/938], Loss: 0.31477129459381104\n",
      "Validation: Epoch [18], Batch [15/938], Loss: 0.42896783351898193\n",
      "Validation: Epoch [18], Batch [16/938], Loss: 0.3876502513885498\n",
      "Validation: Epoch [18], Batch [17/938], Loss: 0.3239574730396271\n",
      "Validation: Epoch [18], Batch [18/938], Loss: 0.4998190402984619\n",
      "Validation: Epoch [18], Batch [19/938], Loss: 0.37914109230041504\n",
      "Validation: Epoch [18], Batch [20/938], Loss: 0.6524289846420288\n",
      "Validation: Epoch [18], Batch [21/938], Loss: 0.5041723251342773\n",
      "Validation: Epoch [18], Batch [22/938], Loss: 0.40251025557518005\n",
      "Validation: Epoch [18], Batch [23/938], Loss: 0.5474321842193604\n",
      "Validation: Epoch [18], Batch [24/938], Loss: 0.4655187129974365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [25/938], Loss: 0.46256813406944275\n",
      "Validation: Epoch [18], Batch [26/938], Loss: 0.22190141677856445\n",
      "Validation: Epoch [18], Batch [27/938], Loss: 0.3949086666107178\n",
      "Validation: Epoch [18], Batch [28/938], Loss: 0.3366433382034302\n",
      "Validation: Epoch [18], Batch [29/938], Loss: 0.18447542190551758\n",
      "Validation: Epoch [18], Batch [30/938], Loss: 0.5048841834068298\n",
      "Validation: Epoch [18], Batch [31/938], Loss: 0.48988327383995056\n",
      "Validation: Epoch [18], Batch [32/938], Loss: 0.44484928250312805\n",
      "Validation: Epoch [18], Batch [33/938], Loss: 0.3183077573776245\n",
      "Validation: Epoch [18], Batch [34/938], Loss: 0.4652838706970215\n",
      "Validation: Epoch [18], Batch [35/938], Loss: 0.3001406788825989\n",
      "Validation: Epoch [18], Batch [36/938], Loss: 0.4202747046947479\n",
      "Validation: Epoch [18], Batch [37/938], Loss: 0.2570457458496094\n",
      "Validation: Epoch [18], Batch [38/938], Loss: 0.4891708791255951\n",
      "Validation: Epoch [18], Batch [39/938], Loss: 0.5077977180480957\n",
      "Validation: Epoch [18], Batch [40/938], Loss: 0.36871060729026794\n",
      "Validation: Epoch [18], Batch [41/938], Loss: 0.45387572050094604\n",
      "Validation: Epoch [18], Batch [42/938], Loss: 0.4518626630306244\n",
      "Validation: Epoch [18], Batch [43/938], Loss: 0.4699193835258484\n",
      "Validation: Epoch [18], Batch [44/938], Loss: 0.4195355474948883\n",
      "Validation: Epoch [18], Batch [45/938], Loss: 0.4378063380718231\n",
      "Validation: Epoch [18], Batch [46/938], Loss: 0.6213740110397339\n",
      "Validation: Epoch [18], Batch [47/938], Loss: 0.4495773911476135\n",
      "Validation: Epoch [18], Batch [48/938], Loss: 0.33023959398269653\n",
      "Validation: Epoch [18], Batch [49/938], Loss: 0.31475067138671875\n",
      "Validation: Epoch [18], Batch [50/938], Loss: 0.40553635358810425\n",
      "Validation: Epoch [18], Batch [51/938], Loss: 0.2809760570526123\n",
      "Validation: Epoch [18], Batch [52/938], Loss: 0.37727832794189453\n",
      "Validation: Epoch [18], Batch [53/938], Loss: 0.35456469655036926\n",
      "Validation: Epoch [18], Batch [54/938], Loss: 0.34727439284324646\n",
      "Validation: Epoch [18], Batch [55/938], Loss: 0.4881342649459839\n",
      "Validation: Epoch [18], Batch [56/938], Loss: 0.4665971100330353\n",
      "Validation: Epoch [18], Batch [57/938], Loss: 0.3942458927631378\n",
      "Validation: Epoch [18], Batch [58/938], Loss: 0.3201570510864258\n",
      "Validation: Epoch [18], Batch [59/938], Loss: 0.27036187052726746\n",
      "Validation: Epoch [18], Batch [60/938], Loss: 0.5834802389144897\n",
      "Validation: Epoch [18], Batch [61/938], Loss: 0.3172152042388916\n",
      "Validation: Epoch [18], Batch [62/938], Loss: 0.21248336136341095\n",
      "Validation: Epoch [18], Batch [63/938], Loss: 0.6173419952392578\n",
      "Validation: Epoch [18], Batch [64/938], Loss: 0.4647327661514282\n",
      "Validation: Epoch [18], Batch [65/938], Loss: 0.3094886243343353\n",
      "Validation: Epoch [18], Batch [66/938], Loss: 0.29390373826026917\n",
      "Validation: Epoch [18], Batch [67/938], Loss: 0.4276201128959656\n",
      "Validation: Epoch [18], Batch [68/938], Loss: 0.5755423903465271\n",
      "Validation: Epoch [18], Batch [69/938], Loss: 0.2642917037010193\n",
      "Validation: Epoch [18], Batch [70/938], Loss: 0.316880464553833\n",
      "Validation: Epoch [18], Batch [71/938], Loss: 0.3990020155906677\n",
      "Validation: Epoch [18], Batch [72/938], Loss: 0.3975422978401184\n",
      "Validation: Epoch [18], Batch [73/938], Loss: 0.39431893825531006\n",
      "Validation: Epoch [18], Batch [74/938], Loss: 0.42950761318206787\n",
      "Validation: Epoch [18], Batch [75/938], Loss: 0.4900568425655365\n",
      "Validation: Epoch [18], Batch [76/938], Loss: 0.38912951946258545\n",
      "Validation: Epoch [18], Batch [77/938], Loss: 0.3978283107280731\n",
      "Validation: Epoch [18], Batch [78/938], Loss: 0.5335272550582886\n",
      "Validation: Epoch [18], Batch [79/938], Loss: 0.44147664308547974\n",
      "Validation: Epoch [18], Batch [80/938], Loss: 0.410434365272522\n",
      "Validation: Epoch [18], Batch [81/938], Loss: 0.29027193784713745\n",
      "Validation: Epoch [18], Batch [82/938], Loss: 0.3310573101043701\n",
      "Validation: Epoch [18], Batch [83/938], Loss: 0.7486300468444824\n",
      "Validation: Epoch [18], Batch [84/938], Loss: 0.27342724800109863\n",
      "Validation: Epoch [18], Batch [85/938], Loss: 0.49242380261421204\n",
      "Validation: Epoch [18], Batch [86/938], Loss: 0.3341080844402313\n",
      "Validation: Epoch [18], Batch [87/938], Loss: 0.4630623757839203\n",
      "Validation: Epoch [18], Batch [88/938], Loss: 0.34453991055488586\n",
      "Validation: Epoch [18], Batch [89/938], Loss: 0.4194651246070862\n",
      "Validation: Epoch [18], Batch [90/938], Loss: 0.46928083896636963\n",
      "Validation: Epoch [18], Batch [91/938], Loss: 0.48247015476226807\n",
      "Validation: Epoch [18], Batch [92/938], Loss: 0.3417842984199524\n",
      "Validation: Epoch [18], Batch [93/938], Loss: 0.5338802337646484\n",
      "Validation: Epoch [18], Batch [94/938], Loss: 0.4524431824684143\n",
      "Validation: Epoch [18], Batch [95/938], Loss: 0.41426998376846313\n",
      "Validation: Epoch [18], Batch [96/938], Loss: 0.7152378559112549\n",
      "Validation: Epoch [18], Batch [97/938], Loss: 0.34788596630096436\n",
      "Validation: Epoch [18], Batch [98/938], Loss: 0.27407240867614746\n",
      "Validation: Epoch [18], Batch [99/938], Loss: 0.26864397525787354\n",
      "Validation: Epoch [18], Batch [100/938], Loss: 0.3364979028701782\n",
      "Validation: Epoch [18], Batch [101/938], Loss: 0.6662147045135498\n",
      "Validation: Epoch [18], Batch [102/938], Loss: 0.42162904143333435\n",
      "Validation: Epoch [18], Batch [103/938], Loss: 0.5729460716247559\n",
      "Validation: Epoch [18], Batch [104/938], Loss: 0.4051854610443115\n",
      "Validation: Epoch [18], Batch [105/938], Loss: 0.48902201652526855\n",
      "Validation: Epoch [18], Batch [106/938], Loss: 0.29016411304473877\n",
      "Validation: Epoch [18], Batch [107/938], Loss: 0.30022570490837097\n",
      "Validation: Epoch [18], Batch [108/938], Loss: 0.4869064390659332\n",
      "Validation: Epoch [18], Batch [109/938], Loss: 0.5638490915298462\n",
      "Validation: Epoch [18], Batch [110/938], Loss: 0.3297344744205475\n",
      "Validation: Epoch [18], Batch [111/938], Loss: 0.4810338616371155\n",
      "Validation: Epoch [18], Batch [112/938], Loss: 0.35764360427856445\n",
      "Validation: Epoch [18], Batch [113/938], Loss: 0.41717249155044556\n",
      "Validation: Epoch [18], Batch [114/938], Loss: 0.45457568764686584\n",
      "Validation: Epoch [18], Batch [115/938], Loss: 0.6818257570266724\n",
      "Validation: Epoch [18], Batch [116/938], Loss: 0.5316169857978821\n",
      "Validation: Epoch [18], Batch [117/938], Loss: 0.28211113810539246\n",
      "Validation: Epoch [18], Batch [118/938], Loss: 0.3291928470134735\n",
      "Validation: Epoch [18], Batch [119/938], Loss: 0.4226542115211487\n",
      "Validation: Epoch [18], Batch [120/938], Loss: 0.3803834021091461\n",
      "Validation: Epoch [18], Batch [121/938], Loss: 0.5675277709960938\n",
      "Validation: Epoch [18], Batch [122/938], Loss: 0.624580442905426\n",
      "Validation: Epoch [18], Batch [123/938], Loss: 0.3811177909374237\n",
      "Validation: Epoch [18], Batch [124/938], Loss: 0.6067447066307068\n",
      "Validation: Epoch [18], Batch [125/938], Loss: 0.33676406741142273\n",
      "Validation: Epoch [18], Batch [126/938], Loss: 0.90906822681427\n",
      "Validation: Epoch [18], Batch [127/938], Loss: 0.2692331075668335\n",
      "Validation: Epoch [18], Batch [128/938], Loss: 0.5663504600524902\n",
      "Validation: Epoch [18], Batch [129/938], Loss: 0.78212571144104\n",
      "Validation: Epoch [18], Batch [130/938], Loss: 0.3293211758136749\n",
      "Validation: Epoch [18], Batch [131/938], Loss: 0.4927733242511749\n",
      "Validation: Epoch [18], Batch [132/938], Loss: 0.5465009212493896\n",
      "Validation: Epoch [18], Batch [133/938], Loss: 0.33515238761901855\n",
      "Validation: Epoch [18], Batch [134/938], Loss: 0.5964261889457703\n",
      "Validation: Epoch [18], Batch [135/938], Loss: 0.45101916790008545\n",
      "Validation: Epoch [18], Batch [136/938], Loss: 0.4849705100059509\n",
      "Validation: Epoch [18], Batch [137/938], Loss: 0.3423082232475281\n",
      "Validation: Epoch [18], Batch [138/938], Loss: 0.3472833037376404\n",
      "Validation: Epoch [18], Batch [139/938], Loss: 0.382878839969635\n",
      "Validation: Epoch [18], Batch [140/938], Loss: 0.4794189929962158\n",
      "Validation: Epoch [18], Batch [141/938], Loss: 0.3919825553894043\n",
      "Validation: Epoch [18], Batch [142/938], Loss: 0.40629830956459045\n",
      "Validation: Epoch [18], Batch [143/938], Loss: 0.43376749753952026\n",
      "Validation: Epoch [18], Batch [144/938], Loss: 0.589936375617981\n",
      "Validation: Epoch [18], Batch [145/938], Loss: 0.773411750793457\n",
      "Validation: Epoch [18], Batch [146/938], Loss: 0.4858686327934265\n",
      "Validation: Epoch [18], Batch [147/938], Loss: 0.37162071466445923\n",
      "Validation: Epoch [18], Batch [148/938], Loss: 0.37365156412124634\n",
      "Validation: Epoch [18], Batch [149/938], Loss: 0.5600504875183105\n",
      "Validation: Epoch [18], Batch [150/938], Loss: 0.2883216440677643\n",
      "Validation: Epoch [18], Batch [151/938], Loss: 0.34261029958724976\n",
      "Validation: Epoch [18], Batch [152/938], Loss: 0.32661643624305725\n",
      "Validation: Epoch [18], Batch [153/938], Loss: 0.3242861032485962\n",
      "Validation: Epoch [18], Batch [154/938], Loss: 0.519413411617279\n",
      "Validation: Epoch [18], Batch [155/938], Loss: 0.32966163754463196\n",
      "Validation: Epoch [18], Batch [156/938], Loss: 0.2948071360588074\n",
      "Validation: Epoch [18], Batch [157/938], Loss: 0.3073727786540985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [158/938], Loss: 0.6409847140312195\n",
      "Validation: Epoch [18], Batch [159/938], Loss: 0.28687602281570435\n",
      "Validation: Epoch [18], Batch [160/938], Loss: 0.3673393726348877\n",
      "Validation: Epoch [18], Batch [161/938], Loss: 0.2281167209148407\n",
      "Validation: Epoch [18], Batch [162/938], Loss: 0.355374813079834\n",
      "Validation: Epoch [18], Batch [163/938], Loss: 0.4675161838531494\n",
      "Validation: Epoch [18], Batch [164/938], Loss: 0.40532851219177246\n",
      "Validation: Epoch [18], Batch [165/938], Loss: 0.37012970447540283\n",
      "Validation: Epoch [18], Batch [166/938], Loss: 0.4009716510772705\n",
      "Validation: Epoch [18], Batch [167/938], Loss: 0.5464103817939758\n",
      "Validation: Epoch [18], Batch [168/938], Loss: 0.43968135118484497\n",
      "Validation: Epoch [18], Batch [169/938], Loss: 0.3913167715072632\n",
      "Validation: Epoch [18], Batch [170/938], Loss: 0.33840489387512207\n",
      "Validation: Epoch [18], Batch [171/938], Loss: 0.5147486925125122\n",
      "Validation: Epoch [18], Batch [172/938], Loss: 0.33499979972839355\n",
      "Validation: Epoch [18], Batch [173/938], Loss: 0.32799267768859863\n",
      "Validation: Epoch [18], Batch [174/938], Loss: 0.42080801725387573\n",
      "Validation: Epoch [18], Batch [175/938], Loss: 0.3550753593444824\n",
      "Validation: Epoch [18], Batch [176/938], Loss: 0.424837589263916\n",
      "Validation: Epoch [18], Batch [177/938], Loss: 0.4579836130142212\n",
      "Validation: Epoch [18], Batch [178/938], Loss: 0.3722640872001648\n",
      "Validation: Epoch [18], Batch [179/938], Loss: 0.523991584777832\n",
      "Validation: Epoch [18], Batch [180/938], Loss: 0.6349456906318665\n",
      "Validation: Epoch [18], Batch [181/938], Loss: 0.2881721258163452\n",
      "Validation: Epoch [18], Batch [182/938], Loss: 0.3262175917625427\n",
      "Validation: Epoch [18], Batch [183/938], Loss: 0.3822794556617737\n",
      "Validation: Epoch [18], Batch [184/938], Loss: 0.31183701753616333\n",
      "Validation: Epoch [18], Batch [185/938], Loss: 0.4137263894081116\n",
      "Validation: Epoch [18], Batch [186/938], Loss: 0.4632156789302826\n",
      "Validation: Epoch [18], Batch [187/938], Loss: 0.3065354824066162\n",
      "Validation: Epoch [18], Batch [188/938], Loss: 0.5913325548171997\n",
      "Validation: Epoch [18], Batch [189/938], Loss: 0.4232695996761322\n",
      "Validation: Epoch [18], Batch [190/938], Loss: 0.3578585982322693\n",
      "Validation: Epoch [18], Batch [191/938], Loss: 0.47643858194351196\n",
      "Validation: Epoch [18], Batch [192/938], Loss: 0.3715069591999054\n",
      "Validation: Epoch [18], Batch [193/938], Loss: 0.5508987307548523\n",
      "Validation: Epoch [18], Batch [194/938], Loss: 0.3256196975708008\n",
      "Validation: Epoch [18], Batch [195/938], Loss: 0.4506479501724243\n",
      "Validation: Epoch [18], Batch [196/938], Loss: 0.2668344974517822\n",
      "Validation: Epoch [18], Batch [197/938], Loss: 0.3424290120601654\n",
      "Validation: Epoch [18], Batch [198/938], Loss: 0.5929120182991028\n",
      "Validation: Epoch [18], Batch [199/938], Loss: 0.38812151551246643\n",
      "Validation: Epoch [18], Batch [200/938], Loss: 0.4894992709159851\n",
      "Validation: Epoch [18], Batch [201/938], Loss: 0.259905070066452\n",
      "Validation: Epoch [18], Batch [202/938], Loss: 0.5829825401306152\n",
      "Validation: Epoch [18], Batch [203/938], Loss: 0.3431261479854584\n",
      "Validation: Epoch [18], Batch [204/938], Loss: 0.33777329325675964\n",
      "Validation: Epoch [18], Batch [205/938], Loss: 0.4265129566192627\n",
      "Validation: Epoch [18], Batch [206/938], Loss: 0.5201674699783325\n",
      "Validation: Epoch [18], Batch [207/938], Loss: 0.3156730532646179\n",
      "Validation: Epoch [18], Batch [208/938], Loss: 0.4926908016204834\n",
      "Validation: Epoch [18], Batch [209/938], Loss: 0.4080926775932312\n",
      "Validation: Epoch [18], Batch [210/938], Loss: 0.4394834041595459\n",
      "Validation: Epoch [18], Batch [211/938], Loss: 0.5344520807266235\n",
      "Validation: Epoch [18], Batch [212/938], Loss: 0.7873846292495728\n",
      "Validation: Epoch [18], Batch [213/938], Loss: 0.452606201171875\n",
      "Validation: Epoch [18], Batch [214/938], Loss: 0.3034513294696808\n",
      "Validation: Epoch [18], Batch [215/938], Loss: 0.3600824773311615\n",
      "Validation: Epoch [18], Batch [216/938], Loss: 0.5574374794960022\n",
      "Validation: Epoch [18], Batch [217/938], Loss: 0.39579910039901733\n",
      "Validation: Epoch [18], Batch [218/938], Loss: 0.4221038520336151\n",
      "Validation: Epoch [18], Batch [219/938], Loss: 0.34562021493911743\n",
      "Validation: Epoch [18], Batch [220/938], Loss: 0.43847471475601196\n",
      "Validation: Epoch [18], Batch [221/938], Loss: 0.3177236318588257\n",
      "Validation: Epoch [18], Batch [222/938], Loss: 0.2672480046749115\n",
      "Validation: Epoch [18], Batch [223/938], Loss: 0.31297382712364197\n",
      "Validation: Epoch [18], Batch [224/938], Loss: 0.43307846784591675\n",
      "Validation: Epoch [18], Batch [225/938], Loss: 0.22410888969898224\n",
      "Validation: Epoch [18], Batch [226/938], Loss: 0.393837034702301\n",
      "Validation: Epoch [18], Batch [227/938], Loss: 0.6067342758178711\n",
      "Validation: Epoch [18], Batch [228/938], Loss: 0.2322489470243454\n",
      "Validation: Epoch [18], Batch [229/938], Loss: 0.4297195076942444\n",
      "Validation: Epoch [18], Batch [230/938], Loss: 0.39892590045928955\n",
      "Validation: Epoch [18], Batch [231/938], Loss: 0.39492547512054443\n",
      "Validation: Epoch [18], Batch [232/938], Loss: 0.4160417914390564\n",
      "Validation: Epoch [18], Batch [233/938], Loss: 0.4709857702255249\n",
      "Validation: Epoch [18], Batch [234/938], Loss: 0.5618828535079956\n",
      "Validation: Epoch [18], Batch [235/938], Loss: 0.39278024435043335\n",
      "Validation: Epoch [18], Batch [236/938], Loss: 0.31738191843032837\n",
      "Validation: Epoch [18], Batch [237/938], Loss: 0.5515027046203613\n",
      "Validation: Epoch [18], Batch [238/938], Loss: 0.4748268723487854\n",
      "Validation: Epoch [18], Batch [239/938], Loss: 0.45073559880256653\n",
      "Validation: Epoch [18], Batch [240/938], Loss: 0.39310044050216675\n",
      "Validation: Epoch [18], Batch [241/938], Loss: 0.38998931646347046\n",
      "Validation: Epoch [18], Batch [242/938], Loss: 0.5042170286178589\n",
      "Validation: Epoch [18], Batch [243/938], Loss: 0.44216302037239075\n",
      "Validation: Epoch [18], Batch [244/938], Loss: 0.43094152212142944\n",
      "Validation: Epoch [18], Batch [245/938], Loss: 0.33829155564308167\n",
      "Validation: Epoch [18], Batch [246/938], Loss: 0.5262636542320251\n",
      "Validation: Epoch [18], Batch [247/938], Loss: 0.4209858775138855\n",
      "Validation: Epoch [18], Batch [248/938], Loss: 0.28287267684936523\n",
      "Validation: Epoch [18], Batch [249/938], Loss: 0.4824160635471344\n",
      "Validation: Epoch [18], Batch [250/938], Loss: 0.33160334825515747\n",
      "Validation: Epoch [18], Batch [251/938], Loss: 0.4485078454017639\n",
      "Validation: Epoch [18], Batch [252/938], Loss: 0.42732781171798706\n",
      "Validation: Epoch [18], Batch [253/938], Loss: 0.3403792381286621\n",
      "Validation: Epoch [18], Batch [254/938], Loss: 0.32689541578292847\n",
      "Validation: Epoch [18], Batch [255/938], Loss: 0.6543172597885132\n",
      "Validation: Epoch [18], Batch [256/938], Loss: 0.4270704984664917\n",
      "Validation: Epoch [18], Batch [257/938], Loss: 0.34327489137649536\n",
      "Validation: Epoch [18], Batch [258/938], Loss: 0.5475873947143555\n",
      "Validation: Epoch [18], Batch [259/938], Loss: 0.36352694034576416\n",
      "Validation: Epoch [18], Batch [260/938], Loss: 0.3589614927768707\n",
      "Validation: Epoch [18], Batch [261/938], Loss: 0.6916829347610474\n",
      "Validation: Epoch [18], Batch [262/938], Loss: 0.4384905695915222\n",
      "Validation: Epoch [18], Batch [263/938], Loss: 0.38991406559944153\n",
      "Validation: Epoch [18], Batch [264/938], Loss: 0.43959376215934753\n",
      "Validation: Epoch [18], Batch [265/938], Loss: 0.5383637547492981\n",
      "Validation: Epoch [18], Batch [266/938], Loss: 0.5759905576705933\n",
      "Validation: Epoch [18], Batch [267/938], Loss: 0.4362170994281769\n",
      "Validation: Epoch [18], Batch [268/938], Loss: 0.34468546509742737\n",
      "Validation: Epoch [18], Batch [269/938], Loss: 0.424927294254303\n",
      "Validation: Epoch [18], Batch [270/938], Loss: 0.40940725803375244\n",
      "Validation: Epoch [18], Batch [271/938], Loss: 0.4175417423248291\n",
      "Validation: Epoch [18], Batch [272/938], Loss: 0.5272409915924072\n",
      "Validation: Epoch [18], Batch [273/938], Loss: 0.6319896578788757\n",
      "Validation: Epoch [18], Batch [274/938], Loss: 0.5194649696350098\n",
      "Validation: Epoch [18], Batch [275/938], Loss: 0.3045198917388916\n",
      "Validation: Epoch [18], Batch [276/938], Loss: 0.29858309030532837\n",
      "Validation: Epoch [18], Batch [277/938], Loss: 0.37345659732818604\n",
      "Validation: Epoch [18], Batch [278/938], Loss: 0.32665538787841797\n",
      "Validation: Epoch [18], Batch [279/938], Loss: 0.44373267889022827\n",
      "Validation: Epoch [18], Batch [280/938], Loss: 0.3929760754108429\n",
      "Validation: Epoch [18], Batch [281/938], Loss: 0.4385780990123749\n",
      "Validation: Epoch [18], Batch [282/938], Loss: 0.5132313966751099\n",
      "Validation: Epoch [18], Batch [283/938], Loss: 0.3083110451698303\n",
      "Validation: Epoch [18], Batch [284/938], Loss: 0.41465693712234497\n",
      "Validation: Epoch [18], Batch [285/938], Loss: 0.4163675308227539\n",
      "Validation: Epoch [18], Batch [286/938], Loss: 0.3894568681716919\n",
      "Validation: Epoch [18], Batch [287/938], Loss: 0.5031577348709106\n",
      "Validation: Epoch [18], Batch [288/938], Loss: 0.4186478555202484\n",
      "Validation: Epoch [18], Batch [289/938], Loss: 0.3465859293937683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [290/938], Loss: 0.3111291229724884\n",
      "Validation: Epoch [18], Batch [291/938], Loss: 0.2837863862514496\n",
      "Validation: Epoch [18], Batch [292/938], Loss: 0.23706454038619995\n",
      "Validation: Epoch [18], Batch [293/938], Loss: 0.41320905089378357\n",
      "Validation: Epoch [18], Batch [294/938], Loss: 0.3125923275947571\n",
      "Validation: Epoch [18], Batch [295/938], Loss: 0.3954628109931946\n",
      "Validation: Epoch [18], Batch [296/938], Loss: 0.4214857220649719\n",
      "Validation: Epoch [18], Batch [297/938], Loss: 0.5845590829849243\n",
      "Validation: Epoch [18], Batch [298/938], Loss: 0.34331151843070984\n",
      "Validation: Epoch [18], Batch [299/938], Loss: 0.32301998138427734\n",
      "Validation: Epoch [18], Batch [300/938], Loss: 0.5893086194992065\n",
      "Validation: Epoch [18], Batch [301/938], Loss: 0.5187883377075195\n",
      "Validation: Epoch [18], Batch [302/938], Loss: 0.47844579815864563\n",
      "Validation: Epoch [18], Batch [303/938], Loss: 0.5695716142654419\n",
      "Validation: Epoch [18], Batch [304/938], Loss: 0.46388888359069824\n",
      "Validation: Epoch [18], Batch [305/938], Loss: 0.3396869897842407\n",
      "Validation: Epoch [18], Batch [306/938], Loss: 0.48491382598876953\n",
      "Validation: Epoch [18], Batch [307/938], Loss: 0.3682177662849426\n",
      "Validation: Epoch [18], Batch [308/938], Loss: 0.4126732349395752\n",
      "Validation: Epoch [18], Batch [309/938], Loss: 0.4623717665672302\n",
      "Validation: Epoch [18], Batch [310/938], Loss: 0.4444718360900879\n",
      "Validation: Epoch [18], Batch [311/938], Loss: 0.33583101630210876\n",
      "Validation: Epoch [18], Batch [312/938], Loss: 0.42769092321395874\n",
      "Validation: Epoch [18], Batch [313/938], Loss: 0.31223583221435547\n",
      "Validation: Epoch [18], Batch [314/938], Loss: 0.8835007548332214\n",
      "Validation: Epoch [18], Batch [315/938], Loss: 0.5198128819465637\n",
      "Validation: Epoch [18], Batch [316/938], Loss: 0.5311399698257446\n",
      "Validation: Epoch [18], Batch [317/938], Loss: 0.5911718010902405\n",
      "Validation: Epoch [18], Batch [318/938], Loss: 0.35368239879608154\n",
      "Validation: Epoch [18], Batch [319/938], Loss: 0.5802452564239502\n",
      "Validation: Epoch [18], Batch [320/938], Loss: 0.35935845971107483\n",
      "Validation: Epoch [18], Batch [321/938], Loss: 0.2745427191257477\n",
      "Validation: Epoch [18], Batch [322/938], Loss: 0.3653991222381592\n",
      "Validation: Epoch [18], Batch [323/938], Loss: 0.46229496598243713\n",
      "Validation: Epoch [18], Batch [324/938], Loss: 0.46531420946121216\n",
      "Validation: Epoch [18], Batch [325/938], Loss: 0.5378676652908325\n",
      "Validation: Epoch [18], Batch [326/938], Loss: 0.2357211709022522\n",
      "Validation: Epoch [18], Batch [327/938], Loss: 0.4820060133934021\n",
      "Validation: Epoch [18], Batch [328/938], Loss: 0.38372212648391724\n",
      "Validation: Epoch [18], Batch [329/938], Loss: 0.4143475592136383\n",
      "Validation: Epoch [18], Batch [330/938], Loss: 0.3787387013435364\n",
      "Validation: Epoch [18], Batch [331/938], Loss: 0.35236167907714844\n",
      "Validation: Epoch [18], Batch [332/938], Loss: 0.3029145300388336\n",
      "Validation: Epoch [18], Batch [333/938], Loss: 0.4017634689807892\n",
      "Validation: Epoch [18], Batch [334/938], Loss: 0.3980003595352173\n",
      "Validation: Epoch [18], Batch [335/938], Loss: 0.4456232786178589\n",
      "Validation: Epoch [18], Batch [336/938], Loss: 0.3725503385066986\n",
      "Validation: Epoch [18], Batch [337/938], Loss: 0.5870187878608704\n",
      "Validation: Epoch [18], Batch [338/938], Loss: 0.40109434723854065\n",
      "Validation: Epoch [18], Batch [339/938], Loss: 0.5913405418395996\n",
      "Validation: Epoch [18], Batch [340/938], Loss: 0.2370116412639618\n",
      "Validation: Epoch [18], Batch [341/938], Loss: 0.266059547662735\n",
      "Validation: Epoch [18], Batch [342/938], Loss: 0.39971357583999634\n",
      "Validation: Epoch [18], Batch [343/938], Loss: 0.39101219177246094\n",
      "Validation: Epoch [18], Batch [344/938], Loss: 0.37052884697914124\n",
      "Validation: Epoch [18], Batch [345/938], Loss: 0.27497398853302\n",
      "Validation: Epoch [18], Batch [346/938], Loss: 0.2572592496871948\n",
      "Validation: Epoch [18], Batch [347/938], Loss: 0.3604195713996887\n",
      "Validation: Epoch [18], Batch [348/938], Loss: 0.2647444009780884\n",
      "Validation: Epoch [18], Batch [349/938], Loss: 0.4421181380748749\n",
      "Validation: Epoch [18], Batch [350/938], Loss: 0.5633538961410522\n",
      "Validation: Epoch [18], Batch [351/938], Loss: 0.5970657467842102\n",
      "Validation: Epoch [18], Batch [352/938], Loss: 0.5690562725067139\n",
      "Validation: Epoch [18], Batch [353/938], Loss: 0.5445199012756348\n",
      "Validation: Epoch [18], Batch [354/938], Loss: 0.3313017785549164\n",
      "Validation: Epoch [18], Batch [355/938], Loss: 0.6227288246154785\n",
      "Validation: Epoch [18], Batch [356/938], Loss: 0.5255956053733826\n",
      "Validation: Epoch [18], Batch [357/938], Loss: 0.4224952757358551\n",
      "Validation: Epoch [18], Batch [358/938], Loss: 0.420626163482666\n",
      "Validation: Epoch [18], Batch [359/938], Loss: 0.4915974736213684\n",
      "Validation: Epoch [18], Batch [360/938], Loss: 0.6177155375480652\n",
      "Validation: Epoch [18], Batch [361/938], Loss: 0.3353196382522583\n",
      "Validation: Epoch [18], Batch [362/938], Loss: 0.5097758769989014\n",
      "Validation: Epoch [18], Batch [363/938], Loss: 0.36885106563568115\n",
      "Validation: Epoch [18], Batch [364/938], Loss: 0.3948028087615967\n",
      "Validation: Epoch [18], Batch [365/938], Loss: 0.344370573759079\n",
      "Validation: Epoch [18], Batch [366/938], Loss: 0.3284187316894531\n",
      "Validation: Epoch [18], Batch [367/938], Loss: 0.4275420904159546\n",
      "Validation: Epoch [18], Batch [368/938], Loss: 0.48166388273239136\n",
      "Validation: Epoch [18], Batch [369/938], Loss: 0.3463234305381775\n",
      "Validation: Epoch [18], Batch [370/938], Loss: 0.44127535820007324\n",
      "Validation: Epoch [18], Batch [371/938], Loss: 0.29177793860435486\n",
      "Validation: Epoch [18], Batch [372/938], Loss: 0.3545932173728943\n",
      "Validation: Epoch [18], Batch [373/938], Loss: 0.39758431911468506\n",
      "Validation: Epoch [18], Batch [374/938], Loss: 0.628272294998169\n",
      "Validation: Epoch [18], Batch [375/938], Loss: 0.43727877736091614\n",
      "Validation: Epoch [18], Batch [376/938], Loss: 0.5587538480758667\n",
      "Validation: Epoch [18], Batch [377/938], Loss: 0.48084187507629395\n",
      "Validation: Epoch [18], Batch [378/938], Loss: 0.3517897129058838\n",
      "Validation: Epoch [18], Batch [379/938], Loss: 0.5088212490081787\n",
      "Validation: Epoch [18], Batch [380/938], Loss: 0.3024401068687439\n",
      "Validation: Epoch [18], Batch [381/938], Loss: 0.2847623825073242\n",
      "Validation: Epoch [18], Batch [382/938], Loss: 0.338397741317749\n",
      "Validation: Epoch [18], Batch [383/938], Loss: 0.45167022943496704\n",
      "Validation: Epoch [18], Batch [384/938], Loss: 0.375206857919693\n",
      "Validation: Epoch [18], Batch [385/938], Loss: 0.3529890179634094\n",
      "Validation: Epoch [18], Batch [386/938], Loss: 0.2990698218345642\n",
      "Validation: Epoch [18], Batch [387/938], Loss: 0.6435576677322388\n",
      "Validation: Epoch [18], Batch [388/938], Loss: 0.36432531476020813\n",
      "Validation: Epoch [18], Batch [389/938], Loss: 0.4761592149734497\n",
      "Validation: Epoch [18], Batch [390/938], Loss: 0.4784175157546997\n",
      "Validation: Epoch [18], Batch [391/938], Loss: 0.3552042543888092\n",
      "Validation: Epoch [18], Batch [392/938], Loss: 0.30544015765190125\n",
      "Validation: Epoch [18], Batch [393/938], Loss: 0.3199417293071747\n",
      "Validation: Epoch [18], Batch [394/938], Loss: 0.49842891097068787\n",
      "Validation: Epoch [18], Batch [395/938], Loss: 0.41340234875679016\n",
      "Validation: Epoch [18], Batch [396/938], Loss: 0.2970537543296814\n",
      "Validation: Epoch [18], Batch [397/938], Loss: 0.3846723139286041\n",
      "Validation: Epoch [18], Batch [398/938], Loss: 0.49028143286705017\n",
      "Validation: Epoch [18], Batch [399/938], Loss: 0.3180854022502899\n",
      "Validation: Epoch [18], Batch [400/938], Loss: 0.439988911151886\n",
      "Validation: Epoch [18], Batch [401/938], Loss: 0.5152904987335205\n",
      "Validation: Epoch [18], Batch [402/938], Loss: 0.3182300925254822\n",
      "Validation: Epoch [18], Batch [403/938], Loss: 0.42148298025131226\n",
      "Validation: Epoch [18], Batch [404/938], Loss: 0.38361579179763794\n",
      "Validation: Epoch [18], Batch [405/938], Loss: 0.40327024459838867\n",
      "Validation: Epoch [18], Batch [406/938], Loss: 0.39069974422454834\n",
      "Validation: Epoch [18], Batch [407/938], Loss: 0.5010536313056946\n",
      "Validation: Epoch [18], Batch [408/938], Loss: 0.40012335777282715\n",
      "Validation: Epoch [18], Batch [409/938], Loss: 0.42735475301742554\n",
      "Validation: Epoch [18], Batch [410/938], Loss: 0.41806817054748535\n",
      "Validation: Epoch [18], Batch [411/938], Loss: 0.4076288342475891\n",
      "Validation: Epoch [18], Batch [412/938], Loss: 0.4693562984466553\n",
      "Validation: Epoch [18], Batch [413/938], Loss: 0.41839471459388733\n",
      "Validation: Epoch [18], Batch [414/938], Loss: 0.34458500146865845\n",
      "Validation: Epoch [18], Batch [415/938], Loss: 0.42073380947113037\n",
      "Validation: Epoch [18], Batch [416/938], Loss: 0.5423446297645569\n",
      "Validation: Epoch [18], Batch [417/938], Loss: 0.6401211023330688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [418/938], Loss: 0.31156718730926514\n",
      "Validation: Epoch [18], Batch [419/938], Loss: 0.37491774559020996\n",
      "Validation: Epoch [18], Batch [420/938], Loss: 0.35648947954177856\n",
      "Validation: Epoch [18], Batch [421/938], Loss: 0.44927316904067993\n",
      "Validation: Epoch [18], Batch [422/938], Loss: 0.4613306522369385\n",
      "Validation: Epoch [18], Batch [423/938], Loss: 0.26322394609451294\n",
      "Validation: Epoch [18], Batch [424/938], Loss: 0.4835996627807617\n",
      "Validation: Epoch [18], Batch [425/938], Loss: 0.5225951671600342\n",
      "Validation: Epoch [18], Batch [426/938], Loss: 0.3608844578266144\n",
      "Validation: Epoch [18], Batch [427/938], Loss: 0.38278019428253174\n",
      "Validation: Epoch [18], Batch [428/938], Loss: 0.5116934776306152\n",
      "Validation: Epoch [18], Batch [429/938], Loss: 0.43745148181915283\n",
      "Validation: Epoch [18], Batch [430/938], Loss: 0.32721954584121704\n",
      "Validation: Epoch [18], Batch [431/938], Loss: 0.4595363438129425\n",
      "Validation: Epoch [18], Batch [432/938], Loss: 0.34679120779037476\n",
      "Validation: Epoch [18], Batch [433/938], Loss: 0.33670663833618164\n",
      "Validation: Epoch [18], Batch [434/938], Loss: 0.26214638352394104\n",
      "Validation: Epoch [18], Batch [435/938], Loss: 0.29847630858421326\n",
      "Validation: Epoch [18], Batch [436/938], Loss: 0.5978251099586487\n",
      "Validation: Epoch [18], Batch [437/938], Loss: 0.4782782793045044\n",
      "Validation: Epoch [18], Batch [438/938], Loss: 0.39133262634277344\n",
      "Validation: Epoch [18], Batch [439/938], Loss: 0.3662031292915344\n",
      "Validation: Epoch [18], Batch [440/938], Loss: 0.5936866998672485\n",
      "Validation: Epoch [18], Batch [441/938], Loss: 0.4404759705066681\n",
      "Validation: Epoch [18], Batch [442/938], Loss: 0.47334498167037964\n",
      "Validation: Epoch [18], Batch [443/938], Loss: 0.3595481216907501\n",
      "Validation: Epoch [18], Batch [444/938], Loss: 0.5156800746917725\n",
      "Validation: Epoch [18], Batch [445/938], Loss: 0.6135739088058472\n",
      "Validation: Epoch [18], Batch [446/938], Loss: 0.42990198731422424\n",
      "Validation: Epoch [18], Batch [447/938], Loss: 0.17594432830810547\n",
      "Validation: Epoch [18], Batch [448/938], Loss: 0.4073672890663147\n",
      "Validation: Epoch [18], Batch [449/938], Loss: 0.30386048555374146\n",
      "Validation: Epoch [18], Batch [450/938], Loss: 0.46347808837890625\n",
      "Validation: Epoch [18], Batch [451/938], Loss: 0.48244988918304443\n",
      "Validation: Epoch [18], Batch [452/938], Loss: 0.5314257144927979\n",
      "Validation: Epoch [18], Batch [453/938], Loss: 0.44622477889060974\n",
      "Validation: Epoch [18], Batch [454/938], Loss: 0.5000452399253845\n",
      "Validation: Epoch [18], Batch [455/938], Loss: 0.34802842140197754\n",
      "Validation: Epoch [18], Batch [456/938], Loss: 0.4396367073059082\n",
      "Validation: Epoch [18], Batch [457/938], Loss: 0.44769012928009033\n",
      "Validation: Epoch [18], Batch [458/938], Loss: 0.3255723714828491\n",
      "Validation: Epoch [18], Batch [459/938], Loss: 0.34418976306915283\n",
      "Validation: Epoch [18], Batch [460/938], Loss: 0.4665870666503906\n",
      "Validation: Epoch [18], Batch [461/938], Loss: 0.39649373292922974\n",
      "Validation: Epoch [18], Batch [462/938], Loss: 0.3111600875854492\n",
      "Validation: Epoch [18], Batch [463/938], Loss: 0.42484143376350403\n",
      "Validation: Epoch [18], Batch [464/938], Loss: 0.46407631039619446\n",
      "Validation: Epoch [18], Batch [465/938], Loss: 0.42457830905914307\n",
      "Validation: Epoch [18], Batch [466/938], Loss: 0.617770791053772\n",
      "Validation: Epoch [18], Batch [467/938], Loss: 0.4523991644382477\n",
      "Validation: Epoch [18], Batch [468/938], Loss: 0.359144926071167\n",
      "Validation: Epoch [18], Batch [469/938], Loss: 0.5050609111785889\n",
      "Validation: Epoch [18], Batch [470/938], Loss: 0.6342321634292603\n",
      "Validation: Epoch [18], Batch [471/938], Loss: 0.4113425314426422\n",
      "Validation: Epoch [18], Batch [472/938], Loss: 0.5402144193649292\n",
      "Validation: Epoch [18], Batch [473/938], Loss: 0.44253256916999817\n",
      "Validation: Epoch [18], Batch [474/938], Loss: 0.5718709826469421\n",
      "Validation: Epoch [18], Batch [475/938], Loss: 0.49809545278549194\n",
      "Validation: Epoch [18], Batch [476/938], Loss: 0.4187016487121582\n",
      "Validation: Epoch [18], Batch [477/938], Loss: 0.44442683458328247\n",
      "Validation: Epoch [18], Batch [478/938], Loss: 0.47224658727645874\n",
      "Validation: Epoch [18], Batch [479/938], Loss: 0.3572275638580322\n",
      "Validation: Epoch [18], Batch [480/938], Loss: 0.24432535469532013\n",
      "Validation: Epoch [18], Batch [481/938], Loss: 0.41697025299072266\n",
      "Validation: Epoch [18], Batch [482/938], Loss: 0.35573941469192505\n",
      "Validation: Epoch [18], Batch [483/938], Loss: 0.33100059628486633\n",
      "Validation: Epoch [18], Batch [484/938], Loss: 0.32204198837280273\n",
      "Validation: Epoch [18], Batch [485/938], Loss: 0.6137639284133911\n",
      "Validation: Epoch [18], Batch [486/938], Loss: 0.32026416063308716\n",
      "Validation: Epoch [18], Batch [487/938], Loss: 0.2346162497997284\n",
      "Validation: Epoch [18], Batch [488/938], Loss: 0.480801522731781\n",
      "Validation: Epoch [18], Batch [489/938], Loss: 0.6598826050758362\n",
      "Validation: Epoch [18], Batch [490/938], Loss: 0.2734009027481079\n",
      "Validation: Epoch [18], Batch [491/938], Loss: 0.3878413438796997\n",
      "Validation: Epoch [18], Batch [492/938], Loss: 0.5493102669715881\n",
      "Validation: Epoch [18], Batch [493/938], Loss: 0.6843616962432861\n",
      "Validation: Epoch [18], Batch [494/938], Loss: 0.5599371194839478\n",
      "Validation: Epoch [18], Batch [495/938], Loss: 0.4599892497062683\n",
      "Validation: Epoch [18], Batch [496/938], Loss: 0.3065158724784851\n",
      "Validation: Epoch [18], Batch [497/938], Loss: 0.33031582832336426\n",
      "Validation: Epoch [18], Batch [498/938], Loss: 0.5059219002723694\n",
      "Validation: Epoch [18], Batch [499/938], Loss: 0.43624597787857056\n",
      "Validation: Epoch [18], Batch [500/938], Loss: 0.6505631804466248\n",
      "Validation: Epoch [18], Batch [501/938], Loss: 0.33829930424690247\n",
      "Validation: Epoch [18], Batch [502/938], Loss: 0.5270355939865112\n",
      "Validation: Epoch [18], Batch [503/938], Loss: 0.34358036518096924\n",
      "Validation: Epoch [18], Batch [504/938], Loss: 0.35500970482826233\n",
      "Validation: Epoch [18], Batch [505/938], Loss: 0.35736218094825745\n",
      "Validation: Epoch [18], Batch [506/938], Loss: 0.6099108457565308\n",
      "Validation: Epoch [18], Batch [507/938], Loss: 0.5211081504821777\n",
      "Validation: Epoch [18], Batch [508/938], Loss: 0.4477824568748474\n",
      "Validation: Epoch [18], Batch [509/938], Loss: 0.3872155547142029\n",
      "Validation: Epoch [18], Batch [510/938], Loss: 0.3438683748245239\n",
      "Validation: Epoch [18], Batch [511/938], Loss: 0.3309630751609802\n",
      "Validation: Epoch [18], Batch [512/938], Loss: 0.40778714418411255\n",
      "Validation: Epoch [18], Batch [513/938], Loss: 0.36383554339408875\n",
      "Validation: Epoch [18], Batch [514/938], Loss: 0.3010377287864685\n",
      "Validation: Epoch [18], Batch [515/938], Loss: 0.3884977400302887\n",
      "Validation: Epoch [18], Batch [516/938], Loss: 0.5890157222747803\n",
      "Validation: Epoch [18], Batch [517/938], Loss: 0.29841530323028564\n",
      "Validation: Epoch [18], Batch [518/938], Loss: 0.3856985867023468\n",
      "Validation: Epoch [18], Batch [519/938], Loss: 0.26102888584136963\n",
      "Validation: Epoch [18], Batch [520/938], Loss: 0.4999326765537262\n",
      "Validation: Epoch [18], Batch [521/938], Loss: 0.6248370409011841\n",
      "Validation: Epoch [18], Batch [522/938], Loss: 0.2832285761833191\n",
      "Validation: Epoch [18], Batch [523/938], Loss: 0.42073169350624084\n",
      "Validation: Epoch [18], Batch [524/938], Loss: 0.550750732421875\n",
      "Validation: Epoch [18], Batch [525/938], Loss: 0.4648355543613434\n",
      "Validation: Epoch [18], Batch [526/938], Loss: 0.3550682067871094\n",
      "Validation: Epoch [18], Batch [527/938], Loss: 0.5039573311805725\n",
      "Validation: Epoch [18], Batch [528/938], Loss: 0.6444302797317505\n",
      "Validation: Epoch [18], Batch [529/938], Loss: 0.28336936235427856\n",
      "Validation: Epoch [18], Batch [530/938], Loss: 0.394212543964386\n",
      "Validation: Epoch [18], Batch [531/938], Loss: 0.2567894458770752\n",
      "Validation: Epoch [18], Batch [532/938], Loss: 0.362420916557312\n",
      "Validation: Epoch [18], Batch [533/938], Loss: 0.3540056347846985\n",
      "Validation: Epoch [18], Batch [534/938], Loss: 0.456861287355423\n",
      "Validation: Epoch [18], Batch [535/938], Loss: 0.40444833040237427\n",
      "Validation: Epoch [18], Batch [536/938], Loss: 0.2845434248447418\n",
      "Validation: Epoch [18], Batch [537/938], Loss: 0.49914905428886414\n",
      "Validation: Epoch [18], Batch [538/938], Loss: 0.3515661060810089\n",
      "Validation: Epoch [18], Batch [539/938], Loss: 0.3888709545135498\n",
      "Validation: Epoch [18], Batch [540/938], Loss: 0.4720075726509094\n",
      "Validation: Epoch [18], Batch [541/938], Loss: 0.5165289640426636\n",
      "Validation: Epoch [18], Batch [542/938], Loss: 0.24982121586799622\n",
      "Validation: Epoch [18], Batch [543/938], Loss: 0.47399914264678955\n",
      "Validation: Epoch [18], Batch [544/938], Loss: 0.5462682247161865\n",
      "Validation: Epoch [18], Batch [545/938], Loss: 0.5849305391311646\n",
      "Validation: Epoch [18], Batch [546/938], Loss: 0.17819763720035553\n",
      "Validation: Epoch [18], Batch [547/938], Loss: 0.5281358361244202\n",
      "Validation: Epoch [18], Batch [548/938], Loss: 0.49496129155158997\n",
      "Validation: Epoch [18], Batch [549/938], Loss: 0.6163187026977539\n",
      "Validation: Epoch [18], Batch [550/938], Loss: 0.3762141168117523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [551/938], Loss: 0.4321194887161255\n",
      "Validation: Epoch [18], Batch [552/938], Loss: 0.4001482129096985\n",
      "Validation: Epoch [18], Batch [553/938], Loss: 0.5154970288276672\n",
      "Validation: Epoch [18], Batch [554/938], Loss: 0.39684513211250305\n",
      "Validation: Epoch [18], Batch [555/938], Loss: 0.5206928253173828\n",
      "Validation: Epoch [18], Batch [556/938], Loss: 0.32237666845321655\n",
      "Validation: Epoch [18], Batch [557/938], Loss: 0.30394887924194336\n",
      "Validation: Epoch [18], Batch [558/938], Loss: 0.45271772146224976\n",
      "Validation: Epoch [18], Batch [559/938], Loss: 0.37962624430656433\n",
      "Validation: Epoch [18], Batch [560/938], Loss: 0.3525797426700592\n",
      "Validation: Epoch [18], Batch [561/938], Loss: 0.3843533992767334\n",
      "Validation: Epoch [18], Batch [562/938], Loss: 0.5190963745117188\n",
      "Validation: Epoch [18], Batch [563/938], Loss: 0.47642821073532104\n",
      "Validation: Epoch [18], Batch [564/938], Loss: 0.370699942111969\n",
      "Validation: Epoch [18], Batch [565/938], Loss: 0.3566957712173462\n",
      "Validation: Epoch [18], Batch [566/938], Loss: 0.3789149522781372\n",
      "Validation: Epoch [18], Batch [567/938], Loss: 0.40297240018844604\n",
      "Validation: Epoch [18], Batch [568/938], Loss: 0.3465641736984253\n",
      "Validation: Epoch [18], Batch [569/938], Loss: 0.44176292419433594\n",
      "Validation: Epoch [18], Batch [570/938], Loss: 0.4339200258255005\n",
      "Validation: Epoch [18], Batch [571/938], Loss: 0.2762197256088257\n",
      "Validation: Epoch [18], Batch [572/938], Loss: 0.25609251856803894\n",
      "Validation: Epoch [18], Batch [573/938], Loss: 0.4363641142845154\n",
      "Validation: Epoch [18], Batch [574/938], Loss: 0.42052531242370605\n",
      "Validation: Epoch [18], Batch [575/938], Loss: 0.4172897934913635\n",
      "Validation: Epoch [18], Batch [576/938], Loss: 0.4325783848762512\n",
      "Validation: Epoch [18], Batch [577/938], Loss: 0.35198360681533813\n",
      "Validation: Epoch [18], Batch [578/938], Loss: 0.32198721170425415\n",
      "Validation: Epoch [18], Batch [579/938], Loss: 0.40007492899894714\n",
      "Validation: Epoch [18], Batch [580/938], Loss: 0.5719678401947021\n",
      "Validation: Epoch [18], Batch [581/938], Loss: 0.4442700147628784\n",
      "Validation: Epoch [18], Batch [582/938], Loss: 0.4810192286968231\n",
      "Validation: Epoch [18], Batch [583/938], Loss: 0.3151980936527252\n",
      "Validation: Epoch [18], Batch [584/938], Loss: 0.5692449808120728\n",
      "Validation: Epoch [18], Batch [585/938], Loss: 0.6134833693504333\n",
      "Validation: Epoch [18], Batch [586/938], Loss: 0.5256674885749817\n",
      "Validation: Epoch [18], Batch [587/938], Loss: 0.557750403881073\n",
      "Validation: Epoch [18], Batch [588/938], Loss: 0.3665192425251007\n",
      "Validation: Epoch [18], Batch [589/938], Loss: 0.32867157459259033\n",
      "Validation: Epoch [18], Batch [590/938], Loss: 0.44750094413757324\n",
      "Validation: Epoch [18], Batch [591/938], Loss: 0.3530520796775818\n",
      "Validation: Epoch [18], Batch [592/938], Loss: 0.35562294721603394\n",
      "Validation: Epoch [18], Batch [593/938], Loss: 0.42822080850601196\n",
      "Validation: Epoch [18], Batch [594/938], Loss: 0.4986010789871216\n",
      "Validation: Epoch [18], Batch [595/938], Loss: 0.39373940229415894\n",
      "Validation: Epoch [18], Batch [596/938], Loss: 0.5269964933395386\n",
      "Validation: Epoch [18], Batch [597/938], Loss: 0.4254907965660095\n",
      "Validation: Epoch [18], Batch [598/938], Loss: 0.5008351802825928\n",
      "Validation: Epoch [18], Batch [599/938], Loss: 0.3468831181526184\n",
      "Validation: Epoch [18], Batch [600/938], Loss: 0.3742676079273224\n",
      "Validation: Epoch [18], Batch [601/938], Loss: 0.35205936431884766\n",
      "Validation: Epoch [18], Batch [602/938], Loss: 0.4638122618198395\n",
      "Validation: Epoch [18], Batch [603/938], Loss: 0.41320210695266724\n",
      "Validation: Epoch [18], Batch [604/938], Loss: 0.5059642791748047\n",
      "Validation: Epoch [18], Batch [605/938], Loss: 0.4811406433582306\n",
      "Validation: Epoch [18], Batch [606/938], Loss: 0.5102857351303101\n",
      "Validation: Epoch [18], Batch [607/938], Loss: 0.547109842300415\n",
      "Validation: Epoch [18], Batch [608/938], Loss: 0.4505458474159241\n",
      "Validation: Epoch [18], Batch [609/938], Loss: 0.3171774744987488\n",
      "Validation: Epoch [18], Batch [610/938], Loss: 0.29950740933418274\n",
      "Validation: Epoch [18], Batch [611/938], Loss: 0.4527674913406372\n",
      "Validation: Epoch [18], Batch [612/938], Loss: 0.2508564591407776\n",
      "Validation: Epoch [18], Batch [613/938], Loss: 0.403278112411499\n",
      "Validation: Epoch [18], Batch [614/938], Loss: 0.47349393367767334\n",
      "Validation: Epoch [18], Batch [615/938], Loss: 0.29964974522590637\n",
      "Validation: Epoch [18], Batch [616/938], Loss: 0.41378968954086304\n",
      "Validation: Epoch [18], Batch [617/938], Loss: 0.5082604885101318\n",
      "Validation: Epoch [18], Batch [618/938], Loss: 0.40235012769699097\n",
      "Validation: Epoch [18], Batch [619/938], Loss: 0.3629518747329712\n",
      "Validation: Epoch [18], Batch [620/938], Loss: 0.47896498441696167\n",
      "Validation: Epoch [18], Batch [621/938], Loss: 0.30939292907714844\n",
      "Validation: Epoch [18], Batch [622/938], Loss: 0.5700442790985107\n",
      "Validation: Epoch [18], Batch [623/938], Loss: 0.4306449294090271\n",
      "Validation: Epoch [18], Batch [624/938], Loss: 0.4941540062427521\n",
      "Validation: Epoch [18], Batch [625/938], Loss: 0.37818995118141174\n",
      "Validation: Epoch [18], Batch [626/938], Loss: 0.48056650161743164\n",
      "Validation: Epoch [18], Batch [627/938], Loss: 0.4232639968395233\n",
      "Validation: Epoch [18], Batch [628/938], Loss: 0.31005585193634033\n",
      "Validation: Epoch [18], Batch [629/938], Loss: 0.49346309900283813\n",
      "Validation: Epoch [18], Batch [630/938], Loss: 0.4424079358577728\n",
      "Validation: Epoch [18], Batch [631/938], Loss: 0.35957199335098267\n",
      "Validation: Epoch [18], Batch [632/938], Loss: 0.3972846269607544\n",
      "Validation: Epoch [18], Batch [633/938], Loss: 0.33133625984191895\n",
      "Validation: Epoch [18], Batch [634/938], Loss: 0.4279547333717346\n",
      "Validation: Epoch [18], Batch [635/938], Loss: 0.6246447563171387\n",
      "Validation: Epoch [18], Batch [636/938], Loss: 0.37095576524734497\n",
      "Validation: Epoch [18], Batch [637/938], Loss: 0.4003045856952667\n",
      "Validation: Epoch [18], Batch [638/938], Loss: 0.530202329158783\n",
      "Validation: Epoch [18], Batch [639/938], Loss: 0.374039888381958\n",
      "Validation: Epoch [18], Batch [640/938], Loss: 0.5461626052856445\n",
      "Validation: Epoch [18], Batch [641/938], Loss: 0.41366732120513916\n",
      "Validation: Epoch [18], Batch [642/938], Loss: 0.45116478204727173\n",
      "Validation: Epoch [18], Batch [643/938], Loss: 0.40706759691238403\n",
      "Validation: Epoch [18], Batch [644/938], Loss: 0.49725931882858276\n",
      "Validation: Epoch [18], Batch [645/938], Loss: 0.39587366580963135\n",
      "Validation: Epoch [18], Batch [646/938], Loss: 0.34153106808662415\n",
      "Validation: Epoch [18], Batch [647/938], Loss: 0.33990153670310974\n",
      "Validation: Epoch [18], Batch [648/938], Loss: 0.5426526069641113\n",
      "Validation: Epoch [18], Batch [649/938], Loss: 0.3069394826889038\n",
      "Validation: Epoch [18], Batch [650/938], Loss: 0.5615714192390442\n",
      "Validation: Epoch [18], Batch [651/938], Loss: 0.6275005340576172\n",
      "Validation: Epoch [18], Batch [652/938], Loss: 0.5819121599197388\n",
      "Validation: Epoch [18], Batch [653/938], Loss: 0.39828601479530334\n",
      "Validation: Epoch [18], Batch [654/938], Loss: 0.5818139314651489\n",
      "Validation: Epoch [18], Batch [655/938], Loss: 0.3316074013710022\n",
      "Validation: Epoch [18], Batch [656/938], Loss: 0.3304094970226288\n",
      "Validation: Epoch [18], Batch [657/938], Loss: 0.6120612621307373\n",
      "Validation: Epoch [18], Batch [658/938], Loss: 0.2798362970352173\n",
      "Validation: Epoch [18], Batch [659/938], Loss: 0.41751229763031006\n",
      "Validation: Epoch [18], Batch [660/938], Loss: 0.3540836274623871\n",
      "Validation: Epoch [18], Batch [661/938], Loss: 0.48247459530830383\n",
      "Validation: Epoch [18], Batch [662/938], Loss: 0.3899659216403961\n",
      "Validation: Epoch [18], Batch [663/938], Loss: 0.38988324999809265\n",
      "Validation: Epoch [18], Batch [664/938], Loss: 0.5300310850143433\n",
      "Validation: Epoch [18], Batch [665/938], Loss: 0.3398820161819458\n",
      "Validation: Epoch [18], Batch [666/938], Loss: 0.40018171072006226\n",
      "Validation: Epoch [18], Batch [667/938], Loss: 0.41887062788009644\n",
      "Validation: Epoch [18], Batch [668/938], Loss: 0.39987391233444214\n",
      "Validation: Epoch [18], Batch [669/938], Loss: 0.30107569694519043\n",
      "Validation: Epoch [18], Batch [670/938], Loss: 0.49724578857421875\n",
      "Validation: Epoch [18], Batch [671/938], Loss: 0.4060516953468323\n",
      "Validation: Epoch [18], Batch [672/938], Loss: 0.3902084529399872\n",
      "Validation: Epoch [18], Batch [673/938], Loss: 0.33433786034584045\n",
      "Validation: Epoch [18], Batch [674/938], Loss: 0.3760160803794861\n",
      "Validation: Epoch [18], Batch [675/938], Loss: 0.4697205126285553\n",
      "Validation: Epoch [18], Batch [676/938], Loss: 0.2709640860557556\n",
      "Validation: Epoch [18], Batch [677/938], Loss: 0.39223816990852356\n",
      "Validation: Epoch [18], Batch [678/938], Loss: 0.3614409267902374\n",
      "Validation: Epoch [18], Batch [679/938], Loss: 0.4076717793941498\n",
      "Validation: Epoch [18], Batch [680/938], Loss: 0.47228628396987915\n",
      "Validation: Epoch [18], Batch [681/938], Loss: 0.558845579624176\n",
      "Validation: Epoch [18], Batch [682/938], Loss: 0.29360952973365784\n",
      "Validation: Epoch [18], Batch [683/938], Loss: 0.5326322913169861\n",
      "Validation: Epoch [18], Batch [684/938], Loss: 0.37778031826019287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [685/938], Loss: 0.28266024589538574\n",
      "Validation: Epoch [18], Batch [686/938], Loss: 0.4303876757621765\n",
      "Validation: Epoch [18], Batch [687/938], Loss: 0.5752254724502563\n",
      "Validation: Epoch [18], Batch [688/938], Loss: 0.4066709280014038\n",
      "Validation: Epoch [18], Batch [689/938], Loss: 0.5275776386260986\n",
      "Validation: Epoch [18], Batch [690/938], Loss: 0.7357812523841858\n",
      "Validation: Epoch [18], Batch [691/938], Loss: 0.4802664816379547\n",
      "Validation: Epoch [18], Batch [692/938], Loss: 0.4977484345436096\n",
      "Validation: Epoch [18], Batch [693/938], Loss: 0.4433513283729553\n",
      "Validation: Epoch [18], Batch [694/938], Loss: 0.3864334225654602\n",
      "Validation: Epoch [18], Batch [695/938], Loss: 0.42183035612106323\n",
      "Validation: Epoch [18], Batch [696/938], Loss: 0.44109195470809937\n",
      "Validation: Epoch [18], Batch [697/938], Loss: 0.3553050756454468\n",
      "Validation: Epoch [18], Batch [698/938], Loss: 0.33282822370529175\n",
      "Validation: Epoch [18], Batch [699/938], Loss: 0.3729243278503418\n",
      "Validation: Epoch [18], Batch [700/938], Loss: 0.606283962726593\n",
      "Validation: Epoch [18], Batch [701/938], Loss: 0.47516849637031555\n",
      "Validation: Epoch [18], Batch [702/938], Loss: 0.3858875334262848\n",
      "Validation: Epoch [18], Batch [703/938], Loss: 0.4011247754096985\n",
      "Validation: Epoch [18], Batch [704/938], Loss: 0.4310302734375\n",
      "Validation: Epoch [18], Batch [705/938], Loss: 0.5614598989486694\n",
      "Validation: Epoch [18], Batch [706/938], Loss: 0.33480092883110046\n",
      "Validation: Epoch [18], Batch [707/938], Loss: 0.24947872757911682\n",
      "Validation: Epoch [18], Batch [708/938], Loss: 0.3104976713657379\n",
      "Validation: Epoch [18], Batch [709/938], Loss: 0.4391517639160156\n",
      "Validation: Epoch [18], Batch [710/938], Loss: 0.21931442618370056\n",
      "Validation: Epoch [18], Batch [711/938], Loss: 0.3567339777946472\n",
      "Validation: Epoch [18], Batch [712/938], Loss: 0.608026385307312\n",
      "Validation: Epoch [18], Batch [713/938], Loss: 0.3702147305011749\n",
      "Validation: Epoch [18], Batch [714/938], Loss: 0.4988684356212616\n",
      "Validation: Epoch [18], Batch [715/938], Loss: 0.4270872473716736\n",
      "Validation: Epoch [18], Batch [716/938], Loss: 0.4182334542274475\n",
      "Validation: Epoch [18], Batch [717/938], Loss: 0.3348311483860016\n",
      "Validation: Epoch [18], Batch [718/938], Loss: 0.5223227739334106\n",
      "Validation: Epoch [18], Batch [719/938], Loss: 0.44883066415786743\n",
      "Validation: Epoch [18], Batch [720/938], Loss: 0.4446754455566406\n",
      "Validation: Epoch [18], Batch [721/938], Loss: 0.3637644648551941\n",
      "Validation: Epoch [18], Batch [722/938], Loss: 0.31015005707740784\n",
      "Validation: Epoch [18], Batch [723/938], Loss: 0.41463983058929443\n",
      "Validation: Epoch [18], Batch [724/938], Loss: 0.45064741373062134\n",
      "Validation: Epoch [18], Batch [725/938], Loss: 0.3463728129863739\n",
      "Validation: Epoch [18], Batch [726/938], Loss: 0.2454715520143509\n",
      "Validation: Epoch [18], Batch [727/938], Loss: 0.44433271884918213\n",
      "Validation: Epoch [18], Batch [728/938], Loss: 0.3649192154407501\n",
      "Validation: Epoch [18], Batch [729/938], Loss: 0.4369999170303345\n",
      "Validation: Epoch [18], Batch [730/938], Loss: 0.29123297333717346\n",
      "Validation: Epoch [18], Batch [731/938], Loss: 0.39481282234191895\n",
      "Validation: Epoch [18], Batch [732/938], Loss: 0.4179413914680481\n",
      "Validation: Epoch [18], Batch [733/938], Loss: 0.3038078248500824\n",
      "Validation: Epoch [18], Batch [734/938], Loss: 0.7030643224716187\n",
      "Validation: Epoch [18], Batch [735/938], Loss: 0.29763349890708923\n",
      "Validation: Epoch [18], Batch [736/938], Loss: 0.3059275150299072\n",
      "Validation: Epoch [18], Batch [737/938], Loss: 0.3685392439365387\n",
      "Validation: Epoch [18], Batch [738/938], Loss: 0.5394855737686157\n",
      "Validation: Epoch [18], Batch [739/938], Loss: 0.26448050141334534\n",
      "Validation: Epoch [18], Batch [740/938], Loss: 0.3061378002166748\n",
      "Validation: Epoch [18], Batch [741/938], Loss: 0.40831562876701355\n",
      "Validation: Epoch [18], Batch [742/938], Loss: 0.49056196212768555\n",
      "Validation: Epoch [18], Batch [743/938], Loss: 0.3964579999446869\n",
      "Validation: Epoch [18], Batch [744/938], Loss: 0.26199108362197876\n",
      "Validation: Epoch [18], Batch [745/938], Loss: 0.31911754608154297\n",
      "Validation: Epoch [18], Batch [746/938], Loss: 0.5930088758468628\n",
      "Validation: Epoch [18], Batch [747/938], Loss: 0.3404597342014313\n",
      "Validation: Epoch [18], Batch [748/938], Loss: 0.40291208028793335\n",
      "Validation: Epoch [18], Batch [749/938], Loss: 0.4799903333187103\n",
      "Validation: Epoch [18], Batch [750/938], Loss: 0.39666128158569336\n",
      "Validation: Epoch [18], Batch [751/938], Loss: 0.45317041873931885\n",
      "Validation: Epoch [18], Batch [752/938], Loss: 0.593489408493042\n",
      "Validation: Epoch [18], Batch [753/938], Loss: 0.3144402503967285\n",
      "Validation: Epoch [18], Batch [754/938], Loss: 0.5624004602432251\n",
      "Validation: Epoch [18], Batch [755/938], Loss: 0.2965526282787323\n",
      "Validation: Epoch [18], Batch [756/938], Loss: 0.5899661779403687\n",
      "Validation: Epoch [18], Batch [757/938], Loss: 0.397108793258667\n",
      "Validation: Epoch [18], Batch [758/938], Loss: 0.45175763964653015\n",
      "Validation: Epoch [18], Batch [759/938], Loss: 0.4158874452114105\n",
      "Validation: Epoch [18], Batch [760/938], Loss: 0.7258482575416565\n",
      "Validation: Epoch [18], Batch [761/938], Loss: 0.3089037239551544\n",
      "Validation: Epoch [18], Batch [762/938], Loss: 0.47868216037750244\n",
      "Validation: Epoch [18], Batch [763/938], Loss: 0.6752934455871582\n",
      "Validation: Epoch [18], Batch [764/938], Loss: 0.3751985728740692\n",
      "Validation: Epoch [18], Batch [765/938], Loss: 0.5156083106994629\n",
      "Validation: Epoch [18], Batch [766/938], Loss: 0.4427158236503601\n",
      "Validation: Epoch [18], Batch [767/938], Loss: 0.32837122678756714\n",
      "Validation: Epoch [18], Batch [768/938], Loss: 0.5064525604248047\n",
      "Validation: Epoch [18], Batch [769/938], Loss: 0.4386090934276581\n",
      "Validation: Epoch [18], Batch [770/938], Loss: 0.3763732314109802\n",
      "Validation: Epoch [18], Batch [771/938], Loss: 0.38286343216896057\n",
      "Validation: Epoch [18], Batch [772/938], Loss: 0.3021268844604492\n",
      "Validation: Epoch [18], Batch [773/938], Loss: 0.3597668707370758\n",
      "Validation: Epoch [18], Batch [774/938], Loss: 0.4501687288284302\n",
      "Validation: Epoch [18], Batch [775/938], Loss: 0.2750644087791443\n",
      "Validation: Epoch [18], Batch [776/938], Loss: 0.3823564946651459\n",
      "Validation: Epoch [18], Batch [777/938], Loss: 0.27422666549682617\n",
      "Validation: Epoch [18], Batch [778/938], Loss: 0.6335816383361816\n",
      "Validation: Epoch [18], Batch [779/938], Loss: 0.39095622301101685\n",
      "Validation: Epoch [18], Batch [780/938], Loss: 0.8175286054611206\n",
      "Validation: Epoch [18], Batch [781/938], Loss: 0.45809251070022583\n",
      "Validation: Epoch [18], Batch [782/938], Loss: 0.44875985383987427\n",
      "Validation: Epoch [18], Batch [783/938], Loss: 0.2579059600830078\n",
      "Validation: Epoch [18], Batch [784/938], Loss: 0.6047093272209167\n",
      "Validation: Epoch [18], Batch [785/938], Loss: 0.5744729042053223\n",
      "Validation: Epoch [18], Batch [786/938], Loss: 0.3673838973045349\n",
      "Validation: Epoch [18], Batch [787/938], Loss: 0.5043786764144897\n",
      "Validation: Epoch [18], Batch [788/938], Loss: 0.31507426500320435\n",
      "Validation: Epoch [18], Batch [789/938], Loss: 0.27157774567604065\n",
      "Validation: Epoch [18], Batch [790/938], Loss: 0.29935386776924133\n",
      "Validation: Epoch [18], Batch [791/938], Loss: 0.4163722097873688\n",
      "Validation: Epoch [18], Batch [792/938], Loss: 0.29681628942489624\n",
      "Validation: Epoch [18], Batch [793/938], Loss: 0.2842583656311035\n",
      "Validation: Epoch [18], Batch [794/938], Loss: 0.36071038246154785\n",
      "Validation: Epoch [18], Batch [795/938], Loss: 0.31069737672805786\n",
      "Validation: Epoch [18], Batch [796/938], Loss: 0.45173609256744385\n",
      "Validation: Epoch [18], Batch [797/938], Loss: 0.4835852384567261\n",
      "Validation: Epoch [18], Batch [798/938], Loss: 0.38532349467277527\n",
      "Validation: Epoch [18], Batch [799/938], Loss: 0.29031747579574585\n",
      "Validation: Epoch [18], Batch [800/938], Loss: 0.7678945660591125\n",
      "Validation: Epoch [18], Batch [801/938], Loss: 0.49020451307296753\n",
      "Validation: Epoch [18], Batch [802/938], Loss: 0.5936322212219238\n",
      "Validation: Epoch [18], Batch [803/938], Loss: 0.5330753326416016\n",
      "Validation: Epoch [18], Batch [804/938], Loss: 0.32936763763427734\n",
      "Validation: Epoch [18], Batch [805/938], Loss: 0.3328000605106354\n",
      "Validation: Epoch [18], Batch [806/938], Loss: 0.37743568420410156\n",
      "Validation: Epoch [18], Batch [807/938], Loss: 0.5324647426605225\n",
      "Validation: Epoch [18], Batch [808/938], Loss: 0.577443540096283\n",
      "Validation: Epoch [18], Batch [809/938], Loss: 0.39843279123306274\n",
      "Validation: Epoch [18], Batch [810/938], Loss: 0.2770881950855255\n",
      "Validation: Epoch [18], Batch [811/938], Loss: 0.44047218561172485\n",
      "Validation: Epoch [18], Batch [812/938], Loss: 0.6267310380935669\n",
      "Validation: Epoch [18], Batch [813/938], Loss: 0.3859691619873047\n",
      "Validation: Epoch [18], Batch [814/938], Loss: 0.3320973515510559\n",
      "Validation: Epoch [18], Batch [815/938], Loss: 0.4629312753677368\n",
      "Validation: Epoch [18], Batch [816/938], Loss: 0.8241376876831055\n",
      "Validation: Epoch [18], Batch [817/938], Loss: 0.2852679193019867\n",
      "Validation: Epoch [18], Batch [818/938], Loss: 0.5348812341690063\n",
      "Validation: Epoch [18], Batch [819/938], Loss: 0.6085646152496338\n",
      "Validation: Epoch [18], Batch [820/938], Loss: 0.33087462186813354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [18], Batch [821/938], Loss: 0.42723894119262695\n",
      "Validation: Epoch [18], Batch [822/938], Loss: 0.3830694258213043\n",
      "Validation: Epoch [18], Batch [823/938], Loss: 0.2757367491722107\n",
      "Validation: Epoch [18], Batch [824/938], Loss: 0.3273833990097046\n",
      "Validation: Epoch [18], Batch [825/938], Loss: 0.4347606897354126\n",
      "Validation: Epoch [18], Batch [826/938], Loss: 0.44805294275283813\n",
      "Validation: Epoch [18], Batch [827/938], Loss: 0.42688918113708496\n",
      "Validation: Epoch [18], Batch [828/938], Loss: 0.37835946679115295\n",
      "Validation: Epoch [18], Batch [829/938], Loss: 0.6495082378387451\n",
      "Validation: Epoch [18], Batch [830/938], Loss: 0.5772089958190918\n",
      "Validation: Epoch [18], Batch [831/938], Loss: 0.5755599141120911\n",
      "Validation: Epoch [18], Batch [832/938], Loss: 0.352742999792099\n",
      "Validation: Epoch [18], Batch [833/938], Loss: 0.5282962322235107\n",
      "Validation: Epoch [18], Batch [834/938], Loss: 0.4045088589191437\n",
      "Validation: Epoch [18], Batch [835/938], Loss: 0.5192346572875977\n",
      "Validation: Epoch [18], Batch [836/938], Loss: 0.3272043466567993\n",
      "Validation: Epoch [18], Batch [837/938], Loss: 0.483675092458725\n",
      "Validation: Epoch [18], Batch [838/938], Loss: 0.410089910030365\n",
      "Validation: Epoch [18], Batch [839/938], Loss: 0.7289397120475769\n",
      "Validation: Epoch [18], Batch [840/938], Loss: 0.3420768976211548\n",
      "Validation: Epoch [18], Batch [841/938], Loss: 0.2374609410762787\n",
      "Validation: Epoch [18], Batch [842/938], Loss: 0.3038315773010254\n",
      "Validation: Epoch [18], Batch [843/938], Loss: 0.4381324052810669\n",
      "Validation: Epoch [18], Batch [844/938], Loss: 0.3154548406600952\n",
      "Validation: Epoch [18], Batch [845/938], Loss: 0.5353024005889893\n",
      "Validation: Epoch [18], Batch [846/938], Loss: 0.35047242045402527\n",
      "Validation: Epoch [18], Batch [847/938], Loss: 0.4374271631240845\n",
      "Validation: Epoch [18], Batch [848/938], Loss: 0.4846402406692505\n",
      "Validation: Epoch [18], Batch [849/938], Loss: 0.398463636636734\n",
      "Validation: Epoch [18], Batch [850/938], Loss: 0.48466891050338745\n",
      "Validation: Epoch [18], Batch [851/938], Loss: 0.4418335556983948\n",
      "Validation: Epoch [18], Batch [852/938], Loss: 0.7148745059967041\n",
      "Validation: Epoch [18], Batch [853/938], Loss: 0.358259916305542\n",
      "Validation: Epoch [18], Batch [854/938], Loss: 0.572364091873169\n",
      "Validation: Epoch [18], Batch [855/938], Loss: 0.4170152246952057\n",
      "Validation: Epoch [18], Batch [856/938], Loss: 0.32851332426071167\n",
      "Validation: Epoch [18], Batch [857/938], Loss: 0.32286787033081055\n",
      "Validation: Epoch [18], Batch [858/938], Loss: 0.45188793540000916\n",
      "Validation: Epoch [18], Batch [859/938], Loss: 0.36797577142715454\n",
      "Validation: Epoch [18], Batch [860/938], Loss: 0.33926695585250854\n",
      "Validation: Epoch [18], Batch [861/938], Loss: 0.6329517364501953\n",
      "Validation: Epoch [18], Batch [862/938], Loss: 0.28121787309646606\n",
      "Validation: Epoch [18], Batch [863/938], Loss: 0.4507592022418976\n",
      "Validation: Epoch [18], Batch [864/938], Loss: 0.5003116130828857\n",
      "Validation: Epoch [18], Batch [865/938], Loss: 0.5596861839294434\n",
      "Validation: Epoch [18], Batch [866/938], Loss: 0.4392285943031311\n",
      "Validation: Epoch [18], Batch [867/938], Loss: 0.5246137380599976\n",
      "Validation: Epoch [18], Batch [868/938], Loss: 0.30098509788513184\n",
      "Validation: Epoch [18], Batch [869/938], Loss: 0.48549020290374756\n",
      "Validation: Epoch [18], Batch [870/938], Loss: 0.25910770893096924\n",
      "Validation: Epoch [18], Batch [871/938], Loss: 0.3534517288208008\n",
      "Validation: Epoch [18], Batch [872/938], Loss: 0.35265815258026123\n",
      "Validation: Epoch [18], Batch [873/938], Loss: 0.39363884925842285\n",
      "Validation: Epoch [18], Batch [874/938], Loss: 0.40436768531799316\n",
      "Validation: Epoch [18], Batch [875/938], Loss: 0.28193166851997375\n",
      "Validation: Epoch [18], Batch [876/938], Loss: 0.2780143916606903\n",
      "Validation: Epoch [18], Batch [877/938], Loss: 0.33163291215896606\n",
      "Validation: Epoch [18], Batch [878/938], Loss: 0.48824650049209595\n",
      "Validation: Epoch [18], Batch [879/938], Loss: 0.6150241494178772\n",
      "Validation: Epoch [18], Batch [880/938], Loss: 0.2479075789451599\n",
      "Validation: Epoch [18], Batch [881/938], Loss: 0.4793500304222107\n",
      "Validation: Epoch [18], Batch [882/938], Loss: 0.3401782214641571\n",
      "Validation: Epoch [18], Batch [883/938], Loss: 0.28347429633140564\n",
      "Validation: Epoch [18], Batch [884/938], Loss: 0.2551606297492981\n",
      "Validation: Epoch [18], Batch [885/938], Loss: 0.40139684081077576\n",
      "Validation: Epoch [18], Batch [886/938], Loss: 0.3252564072608948\n",
      "Validation: Epoch [18], Batch [887/938], Loss: 0.3834376037120819\n",
      "Validation: Epoch [18], Batch [888/938], Loss: 0.5135320425033569\n",
      "Validation: Epoch [18], Batch [889/938], Loss: 0.29609036445617676\n",
      "Validation: Epoch [18], Batch [890/938], Loss: 0.41528409719467163\n",
      "Validation: Epoch [18], Batch [891/938], Loss: 0.5625633001327515\n",
      "Validation: Epoch [18], Batch [892/938], Loss: 0.5477313995361328\n",
      "Validation: Epoch [18], Batch [893/938], Loss: 0.23600488901138306\n",
      "Validation: Epoch [18], Batch [894/938], Loss: 0.26392146944999695\n",
      "Validation: Epoch [18], Batch [895/938], Loss: 0.33436763286590576\n",
      "Validation: Epoch [18], Batch [896/938], Loss: 0.32813066244125366\n",
      "Validation: Epoch [18], Batch [897/938], Loss: 0.42131349444389343\n",
      "Validation: Epoch [18], Batch [898/938], Loss: 0.5746057033538818\n",
      "Validation: Epoch [18], Batch [899/938], Loss: 0.5744998455047607\n",
      "Validation: Epoch [18], Batch [900/938], Loss: 0.3134181797504425\n",
      "Validation: Epoch [18], Batch [901/938], Loss: 0.43672436475753784\n",
      "Validation: Epoch [18], Batch [902/938], Loss: 0.5658120512962341\n",
      "Validation: Epoch [18], Batch [903/938], Loss: 0.4081975519657135\n",
      "Validation: Epoch [18], Batch [904/938], Loss: 0.43213945627212524\n",
      "Validation: Epoch [18], Batch [905/938], Loss: 0.4832308292388916\n",
      "Validation: Epoch [18], Batch [906/938], Loss: 0.4242194890975952\n",
      "Validation: Epoch [18], Batch [907/938], Loss: 0.27642664313316345\n",
      "Validation: Epoch [18], Batch [908/938], Loss: 0.49842140078544617\n",
      "Validation: Epoch [18], Batch [909/938], Loss: 0.3041437566280365\n",
      "Validation: Epoch [18], Batch [910/938], Loss: 0.5080715417861938\n",
      "Validation: Epoch [18], Batch [911/938], Loss: 0.42184221744537354\n",
      "Validation: Epoch [18], Batch [912/938], Loss: 0.4263182282447815\n",
      "Validation: Epoch [18], Batch [913/938], Loss: 0.5315037965774536\n",
      "Validation: Epoch [18], Batch [914/938], Loss: 0.3819907605648041\n",
      "Validation: Epoch [18], Batch [915/938], Loss: 0.2779713571071625\n",
      "Validation: Epoch [18], Batch [916/938], Loss: 0.47370344400405884\n",
      "Validation: Epoch [18], Batch [917/938], Loss: 0.46648797392845154\n",
      "Validation: Epoch [18], Batch [918/938], Loss: 0.5724444389343262\n",
      "Validation: Epoch [18], Batch [919/938], Loss: 0.6160986423492432\n",
      "Validation: Epoch [18], Batch [920/938], Loss: 0.46956950426101685\n",
      "Validation: Epoch [18], Batch [921/938], Loss: 0.4451131224632263\n",
      "Validation: Epoch [18], Batch [922/938], Loss: 0.4142621159553528\n",
      "Validation: Epoch [18], Batch [923/938], Loss: 0.49563068151474\n",
      "Validation: Epoch [18], Batch [924/938], Loss: 0.25186997652053833\n",
      "Validation: Epoch [18], Batch [925/938], Loss: 0.49400460720062256\n",
      "Validation: Epoch [18], Batch [926/938], Loss: 0.2674252390861511\n",
      "Validation: Epoch [18], Batch [927/938], Loss: 0.4814912974834442\n",
      "Validation: Epoch [18], Batch [928/938], Loss: 0.47831082344055176\n",
      "Validation: Epoch [18], Batch [929/938], Loss: 0.5994054079055786\n",
      "Validation: Epoch [18], Batch [930/938], Loss: 0.4191012382507324\n",
      "Validation: Epoch [18], Batch [931/938], Loss: 0.4951084852218628\n",
      "Validation: Epoch [18], Batch [932/938], Loss: 0.5652004480361938\n",
      "Validation: Epoch [18], Batch [933/938], Loss: 0.5395756959915161\n",
      "Validation: Epoch [18], Batch [934/938], Loss: 0.4810534715652466\n",
      "Validation: Epoch [18], Batch [935/938], Loss: 0.3603862226009369\n",
      "Validation: Epoch [18], Batch [936/938], Loss: 0.5357596278190613\n",
      "Validation: Epoch [18], Batch [937/938], Loss: 0.4337788224220276\n",
      "Validation: Epoch [18], Batch [938/938], Loss: 0.21315179765224457\n",
      "Accuracy of test set: 0.8516\n",
      "Train: Epoch [19], Batch [1/938], Loss: 0.4926699995994568\n",
      "Train: Epoch [19], Batch [2/938], Loss: 0.45884209871292114\n",
      "Train: Epoch [19], Batch [3/938], Loss: 0.4907640814781189\n",
      "Train: Epoch [19], Batch [4/938], Loss: 0.43454012274742126\n",
      "Train: Epoch [19], Batch [5/938], Loss: 0.32038751244544983\n",
      "Train: Epoch [19], Batch [6/938], Loss: 0.31897979974746704\n",
      "Train: Epoch [19], Batch [7/938], Loss: 0.388113796710968\n",
      "Train: Epoch [19], Batch [8/938], Loss: 0.39473459124565125\n",
      "Train: Epoch [19], Batch [9/938], Loss: 0.4219408631324768\n",
      "Train: Epoch [19], Batch [10/938], Loss: 0.5168784856796265\n",
      "Train: Epoch [19], Batch [11/938], Loss: 0.471656858921051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [12/938], Loss: 0.5127073526382446\n",
      "Train: Epoch [19], Batch [13/938], Loss: 0.3935019075870514\n",
      "Train: Epoch [19], Batch [14/938], Loss: 0.569129228591919\n",
      "Train: Epoch [19], Batch [15/938], Loss: 0.34535321593284607\n",
      "Train: Epoch [19], Batch [16/938], Loss: 0.6277943849563599\n",
      "Train: Epoch [19], Batch [17/938], Loss: 0.44684869050979614\n",
      "Train: Epoch [19], Batch [18/938], Loss: 0.3585107922554016\n",
      "Train: Epoch [19], Batch [19/938], Loss: 0.38267409801483154\n",
      "Train: Epoch [19], Batch [20/938], Loss: 0.41010475158691406\n",
      "Train: Epoch [19], Batch [21/938], Loss: 0.42448046803474426\n",
      "Train: Epoch [19], Batch [22/938], Loss: 0.37394362688064575\n",
      "Train: Epoch [19], Batch [23/938], Loss: 0.596133828163147\n",
      "Train: Epoch [19], Batch [24/938], Loss: 0.43626868724823\n",
      "Train: Epoch [19], Batch [25/938], Loss: 0.31806665658950806\n",
      "Train: Epoch [19], Batch [26/938], Loss: 0.4458450675010681\n",
      "Train: Epoch [19], Batch [27/938], Loss: 0.5002638101577759\n",
      "Train: Epoch [19], Batch [28/938], Loss: 0.3556089401245117\n",
      "Train: Epoch [19], Batch [29/938], Loss: 0.4787735939025879\n",
      "Train: Epoch [19], Batch [30/938], Loss: 0.5965211391448975\n",
      "Train: Epoch [19], Batch [31/938], Loss: 0.3795129060745239\n",
      "Train: Epoch [19], Batch [32/938], Loss: 0.5189818739891052\n",
      "Train: Epoch [19], Batch [33/938], Loss: 0.4122186303138733\n",
      "Train: Epoch [19], Batch [34/938], Loss: 0.37604188919067383\n",
      "Train: Epoch [19], Batch [35/938], Loss: 0.4304092526435852\n",
      "Train: Epoch [19], Batch [36/938], Loss: 0.4239543676376343\n",
      "Train: Epoch [19], Batch [37/938], Loss: 0.31990179419517517\n",
      "Train: Epoch [19], Batch [38/938], Loss: 0.4845713675022125\n",
      "Train: Epoch [19], Batch [39/938], Loss: 0.259157657623291\n",
      "Train: Epoch [19], Batch [40/938], Loss: 0.4095425009727478\n",
      "Train: Epoch [19], Batch [41/938], Loss: 0.4088369309902191\n",
      "Train: Epoch [19], Batch [42/938], Loss: 0.47041526436805725\n",
      "Train: Epoch [19], Batch [43/938], Loss: 0.39843401312828064\n",
      "Train: Epoch [19], Batch [44/938], Loss: 0.5868635177612305\n",
      "Train: Epoch [19], Batch [45/938], Loss: 0.5191067457199097\n",
      "Train: Epoch [19], Batch [46/938], Loss: 0.5265551209449768\n",
      "Train: Epoch [19], Batch [47/938], Loss: 0.36884039640426636\n",
      "Train: Epoch [19], Batch [48/938], Loss: 0.6627548933029175\n",
      "Train: Epoch [19], Batch [49/938], Loss: 0.624784529209137\n",
      "Train: Epoch [19], Batch [50/938], Loss: 0.4493987560272217\n",
      "Train: Epoch [19], Batch [51/938], Loss: 0.5617966651916504\n",
      "Train: Epoch [19], Batch [52/938], Loss: 0.46661704778671265\n",
      "Train: Epoch [19], Batch [53/938], Loss: 0.43942344188690186\n",
      "Train: Epoch [19], Batch [54/938], Loss: 0.42245566844940186\n",
      "Train: Epoch [19], Batch [55/938], Loss: 0.5179793834686279\n",
      "Train: Epoch [19], Batch [56/938], Loss: 0.4069245457649231\n",
      "Train: Epoch [19], Batch [57/938], Loss: 0.2872486114501953\n",
      "Train: Epoch [19], Batch [58/938], Loss: 0.3734476864337921\n",
      "Train: Epoch [19], Batch [59/938], Loss: 0.5100739002227783\n",
      "Train: Epoch [19], Batch [60/938], Loss: 0.3856314718723297\n",
      "Train: Epoch [19], Batch [61/938], Loss: 0.3281547725200653\n",
      "Train: Epoch [19], Batch [62/938], Loss: 0.5596747994422913\n",
      "Train: Epoch [19], Batch [63/938], Loss: 0.4461902976036072\n",
      "Train: Epoch [19], Batch [64/938], Loss: 0.34759825468063354\n",
      "Train: Epoch [19], Batch [65/938], Loss: 0.31404873728752136\n",
      "Train: Epoch [19], Batch [66/938], Loss: 0.23361322283744812\n",
      "Train: Epoch [19], Batch [67/938], Loss: 0.3947632312774658\n",
      "Train: Epoch [19], Batch [68/938], Loss: 0.42682206630706787\n",
      "Train: Epoch [19], Batch [69/938], Loss: 0.3132440447807312\n",
      "Train: Epoch [19], Batch [70/938], Loss: 0.4571947753429413\n",
      "Train: Epoch [19], Batch [71/938], Loss: 0.30392804741859436\n",
      "Train: Epoch [19], Batch [72/938], Loss: 0.4651116132736206\n",
      "Train: Epoch [19], Batch [73/938], Loss: 0.36940932273864746\n",
      "Train: Epoch [19], Batch [74/938], Loss: 0.6256420612335205\n",
      "Train: Epoch [19], Batch [75/938], Loss: 0.48083361983299255\n",
      "Train: Epoch [19], Batch [76/938], Loss: 0.5166083574295044\n",
      "Train: Epoch [19], Batch [77/938], Loss: 0.39292168617248535\n",
      "Train: Epoch [19], Batch [78/938], Loss: 0.3438878357410431\n",
      "Train: Epoch [19], Batch [79/938], Loss: 0.586496114730835\n",
      "Train: Epoch [19], Batch [80/938], Loss: 0.4756864309310913\n",
      "Train: Epoch [19], Batch [81/938], Loss: 0.32930147647857666\n",
      "Train: Epoch [19], Batch [82/938], Loss: 0.564410388469696\n",
      "Train: Epoch [19], Batch [83/938], Loss: 0.4883788824081421\n",
      "Train: Epoch [19], Batch [84/938], Loss: 0.6650587320327759\n",
      "Train: Epoch [19], Batch [85/938], Loss: 0.46874821186065674\n",
      "Train: Epoch [19], Batch [86/938], Loss: 0.23658405244350433\n",
      "Train: Epoch [19], Batch [87/938], Loss: 0.43858832120895386\n",
      "Train: Epoch [19], Batch [88/938], Loss: 0.4434494078159332\n",
      "Train: Epoch [19], Batch [89/938], Loss: 0.41922077536582947\n",
      "Train: Epoch [19], Batch [90/938], Loss: 0.6403689384460449\n",
      "Train: Epoch [19], Batch [91/938], Loss: 0.34273189306259155\n",
      "Train: Epoch [19], Batch [92/938], Loss: 0.44516146183013916\n",
      "Train: Epoch [19], Batch [93/938], Loss: 0.5108990669250488\n",
      "Train: Epoch [19], Batch [94/938], Loss: 0.4438047409057617\n",
      "Train: Epoch [19], Batch [95/938], Loss: 0.3029116690158844\n",
      "Train: Epoch [19], Batch [96/938], Loss: 0.36715954542160034\n",
      "Train: Epoch [19], Batch [97/938], Loss: 0.41954106092453003\n",
      "Train: Epoch [19], Batch [98/938], Loss: 0.4324144721031189\n",
      "Train: Epoch [19], Batch [99/938], Loss: 0.33248379826545715\n",
      "Train: Epoch [19], Batch [100/938], Loss: 0.39166194200515747\n",
      "Train: Epoch [19], Batch [101/938], Loss: 0.3514094948768616\n",
      "Train: Epoch [19], Batch [102/938], Loss: 0.36430543661117554\n",
      "Train: Epoch [19], Batch [103/938], Loss: 0.5669236183166504\n",
      "Train: Epoch [19], Batch [104/938], Loss: 0.4470822513103485\n",
      "Train: Epoch [19], Batch [105/938], Loss: 0.3487876057624817\n",
      "Train: Epoch [19], Batch [106/938], Loss: 0.27204740047454834\n",
      "Train: Epoch [19], Batch [107/938], Loss: 0.5728211998939514\n",
      "Train: Epoch [19], Batch [108/938], Loss: 0.46934211254119873\n",
      "Train: Epoch [19], Batch [109/938], Loss: 0.2911233901977539\n",
      "Train: Epoch [19], Batch [110/938], Loss: 0.45099887251853943\n",
      "Train: Epoch [19], Batch [111/938], Loss: 0.32300689816474915\n",
      "Train: Epoch [19], Batch [112/938], Loss: 0.5809204578399658\n",
      "Train: Epoch [19], Batch [113/938], Loss: 0.4077911674976349\n",
      "Train: Epoch [19], Batch [114/938], Loss: 0.31181201338768005\n",
      "Train: Epoch [19], Batch [115/938], Loss: 0.4332802891731262\n",
      "Train: Epoch [19], Batch [116/938], Loss: 0.47976449131965637\n",
      "Train: Epoch [19], Batch [117/938], Loss: 0.5746273994445801\n",
      "Train: Epoch [19], Batch [118/938], Loss: 0.349748432636261\n",
      "Train: Epoch [19], Batch [119/938], Loss: 0.49417296051979065\n",
      "Train: Epoch [19], Batch [120/938], Loss: 0.5899306535720825\n",
      "Train: Epoch [19], Batch [121/938], Loss: 0.3877381980419159\n",
      "Train: Epoch [19], Batch [122/938], Loss: 0.37399739027023315\n",
      "Train: Epoch [19], Batch [123/938], Loss: 0.4070384204387665\n",
      "Train: Epoch [19], Batch [124/938], Loss: 0.34064167737960815\n",
      "Train: Epoch [19], Batch [125/938], Loss: 0.5064897537231445\n",
      "Train: Epoch [19], Batch [126/938], Loss: 0.4269241690635681\n",
      "Train: Epoch [19], Batch [127/938], Loss: 0.641870379447937\n",
      "Train: Epoch [19], Batch [128/938], Loss: 0.409939169883728\n",
      "Train: Epoch [19], Batch [129/938], Loss: 0.2970813810825348\n",
      "Train: Epoch [19], Batch [130/938], Loss: 0.2922683358192444\n",
      "Train: Epoch [19], Batch [131/938], Loss: 0.40605223178863525\n",
      "Train: Epoch [19], Batch [132/938], Loss: 0.2946489155292511\n",
      "Train: Epoch [19], Batch [133/938], Loss: 0.5289678573608398\n",
      "Train: Epoch [19], Batch [134/938], Loss: 0.4113105833530426\n",
      "Train: Epoch [19], Batch [135/938], Loss: 0.56990647315979\n",
      "Train: Epoch [19], Batch [136/938], Loss: 0.4014701843261719\n",
      "Train: Epoch [19], Batch [137/938], Loss: 0.3456072211265564\n",
      "Train: Epoch [19], Batch [138/938], Loss: 0.2984883189201355\n",
      "Train: Epoch [19], Batch [139/938], Loss: 0.23524406552314758\n",
      "Train: Epoch [19], Batch [140/938], Loss: 0.26286378502845764\n",
      "Train: Epoch [19], Batch [141/938], Loss: 0.39704129099845886\n",
      "Train: Epoch [19], Batch [142/938], Loss: 0.4287695586681366\n",
      "Train: Epoch [19], Batch [143/938], Loss: 0.33071744441986084\n",
      "Train: Epoch [19], Batch [144/938], Loss: 0.34705835580825806\n",
      "Train: Epoch [19], Batch [145/938], Loss: 0.25289386510849\n",
      "Train: Epoch [19], Batch [146/938], Loss: 0.45348986983299255\n",
      "Train: Epoch [19], Batch [147/938], Loss: 0.5068208575248718\n",
      "Train: Epoch [19], Batch [148/938], Loss: 0.3202744126319885\n",
      "Train: Epoch [19], Batch [149/938], Loss: 0.32678869366645813\n",
      "Train: Epoch [19], Batch [150/938], Loss: 0.5385655164718628\n",
      "Train: Epoch [19], Batch [151/938], Loss: 0.35545092821121216\n",
      "Train: Epoch [19], Batch [152/938], Loss: 0.5101684331893921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [153/938], Loss: 0.49854081869125366\n",
      "Train: Epoch [19], Batch [154/938], Loss: 0.3969983160495758\n",
      "Train: Epoch [19], Batch [155/938], Loss: 0.3900735080242157\n",
      "Train: Epoch [19], Batch [156/938], Loss: 0.3692649006843567\n",
      "Train: Epoch [19], Batch [157/938], Loss: 0.6476025581359863\n",
      "Train: Epoch [19], Batch [158/938], Loss: 0.5203772783279419\n",
      "Train: Epoch [19], Batch [159/938], Loss: 0.5153688192367554\n",
      "Train: Epoch [19], Batch [160/938], Loss: 0.3654334545135498\n",
      "Train: Epoch [19], Batch [161/938], Loss: 0.35127031803131104\n",
      "Train: Epoch [19], Batch [162/938], Loss: 0.5047980546951294\n",
      "Train: Epoch [19], Batch [163/938], Loss: 0.44643229246139526\n",
      "Train: Epoch [19], Batch [164/938], Loss: 0.4346073865890503\n",
      "Train: Epoch [19], Batch [165/938], Loss: 0.5681344270706177\n",
      "Train: Epoch [19], Batch [166/938], Loss: 0.446199893951416\n",
      "Train: Epoch [19], Batch [167/938], Loss: 0.27709633111953735\n",
      "Train: Epoch [19], Batch [168/938], Loss: 0.4699806272983551\n",
      "Train: Epoch [19], Batch [169/938], Loss: 0.3712348937988281\n",
      "Train: Epoch [19], Batch [170/938], Loss: 0.3592381477355957\n",
      "Train: Epoch [19], Batch [171/938], Loss: 0.38055965304374695\n",
      "Train: Epoch [19], Batch [172/938], Loss: 0.5098073482513428\n",
      "Train: Epoch [19], Batch [173/938], Loss: 0.35801273584365845\n",
      "Train: Epoch [19], Batch [174/938], Loss: 0.42929068207740784\n",
      "Train: Epoch [19], Batch [175/938], Loss: 0.43928366899490356\n",
      "Train: Epoch [19], Batch [176/938], Loss: 0.355400025844574\n",
      "Train: Epoch [19], Batch [177/938], Loss: 0.4361226260662079\n",
      "Train: Epoch [19], Batch [178/938], Loss: 0.39632779359817505\n",
      "Train: Epoch [19], Batch [179/938], Loss: 0.45690464973449707\n",
      "Train: Epoch [19], Batch [180/938], Loss: 0.6149718165397644\n",
      "Train: Epoch [19], Batch [181/938], Loss: 0.5349902510643005\n",
      "Train: Epoch [19], Batch [182/938], Loss: 0.29673922061920166\n",
      "Train: Epoch [19], Batch [183/938], Loss: 0.32971030473709106\n",
      "Train: Epoch [19], Batch [184/938], Loss: 0.4317304193973541\n",
      "Train: Epoch [19], Batch [185/938], Loss: 0.2938820719718933\n",
      "Train: Epoch [19], Batch [186/938], Loss: 0.5646908283233643\n",
      "Train: Epoch [19], Batch [187/938], Loss: 0.26677098870277405\n",
      "Train: Epoch [19], Batch [188/938], Loss: 0.4200727939605713\n",
      "Train: Epoch [19], Batch [189/938], Loss: 0.28945231437683105\n",
      "Train: Epoch [19], Batch [190/938], Loss: 0.300594687461853\n",
      "Train: Epoch [19], Batch [191/938], Loss: 0.2899562120437622\n",
      "Train: Epoch [19], Batch [192/938], Loss: 0.3419240713119507\n",
      "Train: Epoch [19], Batch [193/938], Loss: 0.3903970718383789\n",
      "Train: Epoch [19], Batch [194/938], Loss: 0.41061288118362427\n",
      "Train: Epoch [19], Batch [195/938], Loss: 0.40859413146972656\n",
      "Train: Epoch [19], Batch [196/938], Loss: 0.27917560935020447\n",
      "Train: Epoch [19], Batch [197/938], Loss: 0.6208228468894958\n",
      "Train: Epoch [19], Batch [198/938], Loss: 0.3224319517612457\n",
      "Train: Epoch [19], Batch [199/938], Loss: 0.48115304112434387\n",
      "Train: Epoch [19], Batch [200/938], Loss: 0.7409271001815796\n",
      "Train: Epoch [19], Batch [201/938], Loss: 0.3514586091041565\n",
      "Train: Epoch [19], Batch [202/938], Loss: 0.40879833698272705\n",
      "Train: Epoch [19], Batch [203/938], Loss: 0.2862532138824463\n",
      "Train: Epoch [19], Batch [204/938], Loss: 0.41535258293151855\n",
      "Train: Epoch [19], Batch [205/938], Loss: 0.4810187518596649\n",
      "Train: Epoch [19], Batch [206/938], Loss: 0.2583368420600891\n",
      "Train: Epoch [19], Batch [207/938], Loss: 0.41194212436676025\n",
      "Train: Epoch [19], Batch [208/938], Loss: 0.36791670322418213\n",
      "Train: Epoch [19], Batch [209/938], Loss: 0.34356778860092163\n",
      "Train: Epoch [19], Batch [210/938], Loss: 0.5883235335350037\n",
      "Train: Epoch [19], Batch [211/938], Loss: 0.388840913772583\n",
      "Train: Epoch [19], Batch [212/938], Loss: 0.41196775436401367\n",
      "Train: Epoch [19], Batch [213/938], Loss: 0.4161062240600586\n",
      "Train: Epoch [19], Batch [214/938], Loss: 0.34846892952919006\n",
      "Train: Epoch [19], Batch [215/938], Loss: 0.46683835983276367\n",
      "Train: Epoch [19], Batch [216/938], Loss: 0.3403930068016052\n",
      "Train: Epoch [19], Batch [217/938], Loss: 0.4291634261608124\n",
      "Train: Epoch [19], Batch [218/938], Loss: 0.3346799910068512\n",
      "Train: Epoch [19], Batch [219/938], Loss: 0.42049354314804077\n",
      "Train: Epoch [19], Batch [220/938], Loss: 0.3505569398403168\n",
      "Train: Epoch [19], Batch [221/938], Loss: 0.4641605019569397\n",
      "Train: Epoch [19], Batch [222/938], Loss: 0.46308818459510803\n",
      "Train: Epoch [19], Batch [223/938], Loss: 0.45602089166641235\n",
      "Train: Epoch [19], Batch [224/938], Loss: 0.4756951928138733\n",
      "Train: Epoch [19], Batch [225/938], Loss: 0.5208091139793396\n",
      "Train: Epoch [19], Batch [226/938], Loss: 0.21297666430473328\n",
      "Train: Epoch [19], Batch [227/938], Loss: 0.2875000834465027\n",
      "Train: Epoch [19], Batch [228/938], Loss: 0.332441508769989\n",
      "Train: Epoch [19], Batch [229/938], Loss: 0.42342689633369446\n",
      "Train: Epoch [19], Batch [230/938], Loss: 0.30333203077316284\n",
      "Train: Epoch [19], Batch [231/938], Loss: 0.5412235260009766\n",
      "Train: Epoch [19], Batch [232/938], Loss: 0.4116254448890686\n",
      "Train: Epoch [19], Batch [233/938], Loss: 0.2804263234138489\n",
      "Train: Epoch [19], Batch [234/938], Loss: 0.31440550088882446\n",
      "Train: Epoch [19], Batch [235/938], Loss: 0.42782193422317505\n",
      "Train: Epoch [19], Batch [236/938], Loss: 0.4142060875892639\n",
      "Train: Epoch [19], Batch [237/938], Loss: 0.48150867223739624\n",
      "Train: Epoch [19], Batch [238/938], Loss: 0.5362862944602966\n",
      "Train: Epoch [19], Batch [239/938], Loss: 0.4861494302749634\n",
      "Train: Epoch [19], Batch [240/938], Loss: 0.4071391224861145\n",
      "Train: Epoch [19], Batch [241/938], Loss: 0.6784891486167908\n",
      "Train: Epoch [19], Batch [242/938], Loss: 0.6037365198135376\n",
      "Train: Epoch [19], Batch [243/938], Loss: 0.3189646303653717\n",
      "Train: Epoch [19], Batch [244/938], Loss: 0.40431100130081177\n",
      "Train: Epoch [19], Batch [245/938], Loss: 0.3815685212612152\n",
      "Train: Epoch [19], Batch [246/938], Loss: 0.38849663734436035\n",
      "Train: Epoch [19], Batch [247/938], Loss: 0.35811811685562134\n",
      "Train: Epoch [19], Batch [248/938], Loss: 0.2510528564453125\n",
      "Train: Epoch [19], Batch [249/938], Loss: 0.38634446263313293\n",
      "Train: Epoch [19], Batch [250/938], Loss: 0.4186573922634125\n",
      "Train: Epoch [19], Batch [251/938], Loss: 0.45031610131263733\n",
      "Train: Epoch [19], Batch [252/938], Loss: 0.4262648820877075\n",
      "Train: Epoch [19], Batch [253/938], Loss: 0.4656425416469574\n",
      "Train: Epoch [19], Batch [254/938], Loss: 0.2477099597454071\n",
      "Train: Epoch [19], Batch [255/938], Loss: 0.4172278046607971\n",
      "Train: Epoch [19], Batch [256/938], Loss: 0.348849356174469\n",
      "Train: Epoch [19], Batch [257/938], Loss: 0.30440229177474976\n",
      "Train: Epoch [19], Batch [258/938], Loss: 0.45882803201675415\n",
      "Train: Epoch [19], Batch [259/938], Loss: 0.4236993193626404\n",
      "Train: Epoch [19], Batch [260/938], Loss: 0.4848693311214447\n",
      "Train: Epoch [19], Batch [261/938], Loss: 0.38582515716552734\n",
      "Train: Epoch [19], Batch [262/938], Loss: 0.33104512095451355\n",
      "Train: Epoch [19], Batch [263/938], Loss: 0.43775129318237305\n",
      "Train: Epoch [19], Batch [264/938], Loss: 0.5159488916397095\n",
      "Train: Epoch [19], Batch [265/938], Loss: 0.6514252424240112\n",
      "Train: Epoch [19], Batch [266/938], Loss: 0.57032310962677\n",
      "Train: Epoch [19], Batch [267/938], Loss: 0.5213843584060669\n",
      "Train: Epoch [19], Batch [268/938], Loss: 0.30834463238716125\n",
      "Train: Epoch [19], Batch [269/938], Loss: 0.7377601265907288\n",
      "Train: Epoch [19], Batch [270/938], Loss: 0.2937365174293518\n",
      "Train: Epoch [19], Batch [271/938], Loss: 0.3455606698989868\n",
      "Train: Epoch [19], Batch [272/938], Loss: 0.40391409397125244\n",
      "Train: Epoch [19], Batch [273/938], Loss: 0.4407469630241394\n",
      "Train: Epoch [19], Batch [274/938], Loss: 0.624491810798645\n",
      "Train: Epoch [19], Batch [275/938], Loss: 0.4886784553527832\n",
      "Train: Epoch [19], Batch [276/938], Loss: 0.27780866622924805\n",
      "Train: Epoch [19], Batch [277/938], Loss: 0.4671396017074585\n",
      "Train: Epoch [19], Batch [278/938], Loss: 0.4590766429901123\n",
      "Train: Epoch [19], Batch [279/938], Loss: 0.3046879768371582\n",
      "Train: Epoch [19], Batch [280/938], Loss: 0.4257088601589203\n",
      "Train: Epoch [19], Batch [281/938], Loss: 0.43801337480545044\n",
      "Train: Epoch [19], Batch [282/938], Loss: 0.3518788516521454\n",
      "Train: Epoch [19], Batch [283/938], Loss: 0.30840325355529785\n",
      "Train: Epoch [19], Batch [284/938], Loss: 0.27570968866348267\n",
      "Train: Epoch [19], Batch [285/938], Loss: 0.4712138772010803\n",
      "Train: Epoch [19], Batch [286/938], Loss: 0.39451849460601807\n",
      "Train: Epoch [19], Batch [287/938], Loss: 0.34589439630508423\n",
      "Train: Epoch [19], Batch [288/938], Loss: 0.45468980073928833\n",
      "Train: Epoch [19], Batch [289/938], Loss: 0.41303762793540955\n",
      "Train: Epoch [19], Batch [290/938], Loss: 0.3826794922351837\n",
      "Train: Epoch [19], Batch [291/938], Loss: 0.4069697856903076\n",
      "Train: Epoch [19], Batch [292/938], Loss: 0.5181571841239929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [293/938], Loss: 0.45834189653396606\n",
      "Train: Epoch [19], Batch [294/938], Loss: 0.47330835461616516\n",
      "Train: Epoch [19], Batch [295/938], Loss: 0.5360119342803955\n",
      "Train: Epoch [19], Batch [296/938], Loss: 0.6105332374572754\n",
      "Train: Epoch [19], Batch [297/938], Loss: 0.5079419612884521\n",
      "Train: Epoch [19], Batch [298/938], Loss: 0.6005027294158936\n",
      "Train: Epoch [19], Batch [299/938], Loss: 0.33183738589286804\n",
      "Train: Epoch [19], Batch [300/938], Loss: 0.5517377853393555\n",
      "Train: Epoch [19], Batch [301/938], Loss: 0.3982332646846771\n",
      "Train: Epoch [19], Batch [302/938], Loss: 0.6486161947250366\n",
      "Train: Epoch [19], Batch [303/938], Loss: 0.3698997497558594\n",
      "Train: Epoch [19], Batch [304/938], Loss: 0.5398026704788208\n",
      "Train: Epoch [19], Batch [305/938], Loss: 0.4004230797290802\n",
      "Train: Epoch [19], Batch [306/938], Loss: 0.3015535771846771\n",
      "Train: Epoch [19], Batch [307/938], Loss: 0.6278405785560608\n",
      "Train: Epoch [19], Batch [308/938], Loss: 0.27911442518234253\n",
      "Train: Epoch [19], Batch [309/938], Loss: 0.4585723876953125\n",
      "Train: Epoch [19], Batch [310/938], Loss: 0.35742712020874023\n",
      "Train: Epoch [19], Batch [311/938], Loss: 0.30315101146698\n",
      "Train: Epoch [19], Batch [312/938], Loss: 0.3907740116119385\n",
      "Train: Epoch [19], Batch [313/938], Loss: 0.6301372647285461\n",
      "Train: Epoch [19], Batch [314/938], Loss: 0.43272635340690613\n",
      "Train: Epoch [19], Batch [315/938], Loss: 0.5453168749809265\n",
      "Train: Epoch [19], Batch [316/938], Loss: 0.4996693432331085\n",
      "Train: Epoch [19], Batch [317/938], Loss: 0.4001306891441345\n",
      "Train: Epoch [19], Batch [318/938], Loss: 0.31671345233917236\n",
      "Train: Epoch [19], Batch [319/938], Loss: 0.3995914161205292\n",
      "Train: Epoch [19], Batch [320/938], Loss: 0.4107266664505005\n",
      "Train: Epoch [19], Batch [321/938], Loss: 0.5115628838539124\n",
      "Train: Epoch [19], Batch [322/938], Loss: 0.6109182834625244\n",
      "Train: Epoch [19], Batch [323/938], Loss: 0.29188838601112366\n",
      "Train: Epoch [19], Batch [324/938], Loss: 0.4056243896484375\n",
      "Train: Epoch [19], Batch [325/938], Loss: 0.62699294090271\n",
      "Train: Epoch [19], Batch [326/938], Loss: 0.4213082194328308\n",
      "Train: Epoch [19], Batch [327/938], Loss: 0.5882720947265625\n",
      "Train: Epoch [19], Batch [328/938], Loss: 0.5872727036476135\n",
      "Train: Epoch [19], Batch [329/938], Loss: 0.2641122043132782\n",
      "Train: Epoch [19], Batch [330/938], Loss: 0.2991061508655548\n",
      "Train: Epoch [19], Batch [331/938], Loss: 0.37301185727119446\n",
      "Train: Epoch [19], Batch [332/938], Loss: 0.4562038779258728\n",
      "Train: Epoch [19], Batch [333/938], Loss: 0.2799879014492035\n",
      "Train: Epoch [19], Batch [334/938], Loss: 0.45540541410446167\n",
      "Train: Epoch [19], Batch [335/938], Loss: 0.3508266508579254\n",
      "Train: Epoch [19], Batch [336/938], Loss: 0.6473100185394287\n",
      "Train: Epoch [19], Batch [337/938], Loss: 0.46309271454811096\n",
      "Train: Epoch [19], Batch [338/938], Loss: 0.22444073855876923\n",
      "Train: Epoch [19], Batch [339/938], Loss: 0.3663075864315033\n",
      "Train: Epoch [19], Batch [340/938], Loss: 0.529003381729126\n",
      "Train: Epoch [19], Batch [341/938], Loss: 0.5701920390129089\n",
      "Train: Epoch [19], Batch [342/938], Loss: 0.37571072578430176\n",
      "Train: Epoch [19], Batch [343/938], Loss: 0.5270612835884094\n",
      "Train: Epoch [19], Batch [344/938], Loss: 0.22001025080680847\n",
      "Train: Epoch [19], Batch [345/938], Loss: 0.5176167488098145\n",
      "Train: Epoch [19], Batch [346/938], Loss: 0.5397869348526001\n",
      "Train: Epoch [19], Batch [347/938], Loss: 0.374206006526947\n",
      "Train: Epoch [19], Batch [348/938], Loss: 0.37342584133148193\n",
      "Train: Epoch [19], Batch [349/938], Loss: 0.35913097858428955\n",
      "Train: Epoch [19], Batch [350/938], Loss: 0.530385434627533\n",
      "Train: Epoch [19], Batch [351/938], Loss: 0.3182305097579956\n",
      "Train: Epoch [19], Batch [352/938], Loss: 0.5252546072006226\n",
      "Train: Epoch [19], Batch [353/938], Loss: 0.3372010588645935\n",
      "Train: Epoch [19], Batch [354/938], Loss: 0.43118399381637573\n",
      "Train: Epoch [19], Batch [355/938], Loss: 0.3647483289241791\n",
      "Train: Epoch [19], Batch [356/938], Loss: 0.48079147934913635\n",
      "Train: Epoch [19], Batch [357/938], Loss: 0.588874101638794\n",
      "Train: Epoch [19], Batch [358/938], Loss: 0.5835908651351929\n",
      "Train: Epoch [19], Batch [359/938], Loss: 0.3459174931049347\n",
      "Train: Epoch [19], Batch [360/938], Loss: 0.5555295944213867\n",
      "Train: Epoch [19], Batch [361/938], Loss: 0.3557988405227661\n",
      "Train: Epoch [19], Batch [362/938], Loss: 0.5077899098396301\n",
      "Train: Epoch [19], Batch [363/938], Loss: 0.3924412727355957\n",
      "Train: Epoch [19], Batch [364/938], Loss: 0.33973708748817444\n",
      "Train: Epoch [19], Batch [365/938], Loss: 0.5615745782852173\n",
      "Train: Epoch [19], Batch [366/938], Loss: 0.49962517619132996\n",
      "Train: Epoch [19], Batch [367/938], Loss: 0.3227924704551697\n",
      "Train: Epoch [19], Batch [368/938], Loss: 0.5253778696060181\n",
      "Train: Epoch [19], Batch [369/938], Loss: 0.6220873594284058\n",
      "Train: Epoch [19], Batch [370/938], Loss: 0.4528626799583435\n",
      "Train: Epoch [19], Batch [371/938], Loss: 0.27539172768592834\n",
      "Train: Epoch [19], Batch [372/938], Loss: 0.564861536026001\n",
      "Train: Epoch [19], Batch [373/938], Loss: 0.3669291138648987\n",
      "Train: Epoch [19], Batch [374/938], Loss: 0.24627482891082764\n",
      "Train: Epoch [19], Batch [375/938], Loss: 0.38616564869880676\n",
      "Train: Epoch [19], Batch [376/938], Loss: 0.35429245233535767\n",
      "Train: Epoch [19], Batch [377/938], Loss: 0.4799973666667938\n",
      "Train: Epoch [19], Batch [378/938], Loss: 0.3571091890335083\n",
      "Train: Epoch [19], Batch [379/938], Loss: 0.4256497621536255\n",
      "Train: Epoch [19], Batch [380/938], Loss: 0.30452001094818115\n",
      "Train: Epoch [19], Batch [381/938], Loss: 0.4313245415687561\n",
      "Train: Epoch [19], Batch [382/938], Loss: 0.4950597286224365\n",
      "Train: Epoch [19], Batch [383/938], Loss: 0.3705512285232544\n",
      "Train: Epoch [19], Batch [384/938], Loss: 0.4703473448753357\n",
      "Train: Epoch [19], Batch [385/938], Loss: 0.36265793442726135\n",
      "Train: Epoch [19], Batch [386/938], Loss: 0.46381711959838867\n",
      "Train: Epoch [19], Batch [387/938], Loss: 0.26727384328842163\n",
      "Train: Epoch [19], Batch [388/938], Loss: 0.4670964479446411\n",
      "Train: Epoch [19], Batch [389/938], Loss: 0.43765825033187866\n",
      "Train: Epoch [19], Batch [390/938], Loss: 0.46036064624786377\n",
      "Train: Epoch [19], Batch [391/938], Loss: 0.4519377052783966\n",
      "Train: Epoch [19], Batch [392/938], Loss: 0.5368978977203369\n",
      "Train: Epoch [19], Batch [393/938], Loss: 0.349356085062027\n",
      "Train: Epoch [19], Batch [394/938], Loss: 0.5361611843109131\n",
      "Train: Epoch [19], Batch [395/938], Loss: 0.34898197650909424\n",
      "Train: Epoch [19], Batch [396/938], Loss: 0.4298912584781647\n",
      "Train: Epoch [19], Batch [397/938], Loss: 0.5055115818977356\n",
      "Train: Epoch [19], Batch [398/938], Loss: 0.4850623607635498\n",
      "Train: Epoch [19], Batch [399/938], Loss: 0.454862117767334\n",
      "Train: Epoch [19], Batch [400/938], Loss: 0.44832563400268555\n",
      "Train: Epoch [19], Batch [401/938], Loss: 0.353823721408844\n",
      "Train: Epoch [19], Batch [402/938], Loss: 0.2882489562034607\n",
      "Train: Epoch [19], Batch [403/938], Loss: 0.7204836010932922\n",
      "Train: Epoch [19], Batch [404/938], Loss: 0.4294903874397278\n",
      "Train: Epoch [19], Batch [405/938], Loss: 0.5683391094207764\n",
      "Train: Epoch [19], Batch [406/938], Loss: 0.4271436333656311\n",
      "Train: Epoch [19], Batch [407/938], Loss: 0.4316086173057556\n",
      "Train: Epoch [19], Batch [408/938], Loss: 0.4204164743423462\n",
      "Train: Epoch [19], Batch [409/938], Loss: 0.49129217863082886\n",
      "Train: Epoch [19], Batch [410/938], Loss: 0.6724814176559448\n",
      "Train: Epoch [19], Batch [411/938], Loss: 0.45659318566322327\n",
      "Train: Epoch [19], Batch [412/938], Loss: 0.41402167081832886\n",
      "Train: Epoch [19], Batch [413/938], Loss: 0.2864753007888794\n",
      "Train: Epoch [19], Batch [414/938], Loss: 0.4475492537021637\n",
      "Train: Epoch [19], Batch [415/938], Loss: 0.34027987718582153\n",
      "Train: Epoch [19], Batch [416/938], Loss: 0.2831875681877136\n",
      "Train: Epoch [19], Batch [417/938], Loss: 0.33586177229881287\n",
      "Train: Epoch [19], Batch [418/938], Loss: 0.4699976146221161\n",
      "Train: Epoch [19], Batch [419/938], Loss: 0.347647488117218\n",
      "Train: Epoch [19], Batch [420/938], Loss: 0.3290724754333496\n",
      "Train: Epoch [19], Batch [421/938], Loss: 0.3563404083251953\n",
      "Train: Epoch [19], Batch [422/938], Loss: 0.4248490333557129\n",
      "Train: Epoch [19], Batch [423/938], Loss: 0.4353500008583069\n",
      "Train: Epoch [19], Batch [424/938], Loss: 0.2208651602268219\n",
      "Train: Epoch [19], Batch [425/938], Loss: 0.37846577167510986\n",
      "Train: Epoch [19], Batch [426/938], Loss: 0.5881772637367249\n",
      "Train: Epoch [19], Batch [427/938], Loss: 0.32706373929977417\n",
      "Train: Epoch [19], Batch [428/938], Loss: 0.49793028831481934\n",
      "Train: Epoch [19], Batch [429/938], Loss: 0.42850184440612793\n",
      "Train: Epoch [19], Batch [430/938], Loss: 0.4047296345233917\n",
      "Train: Epoch [19], Batch [431/938], Loss: 0.37780219316482544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [432/938], Loss: 0.30794042348861694\n",
      "Train: Epoch [19], Batch [433/938], Loss: 0.4549630880355835\n",
      "Train: Epoch [19], Batch [434/938], Loss: 0.3935059905052185\n",
      "Train: Epoch [19], Batch [435/938], Loss: 0.4019085466861725\n",
      "Train: Epoch [19], Batch [436/938], Loss: 0.36610251665115356\n",
      "Train: Epoch [19], Batch [437/938], Loss: 0.556251049041748\n",
      "Train: Epoch [19], Batch [438/938], Loss: 0.5142786502838135\n",
      "Train: Epoch [19], Batch [439/938], Loss: 0.34918272495269775\n",
      "Train: Epoch [19], Batch [440/938], Loss: 0.4302850067615509\n",
      "Train: Epoch [19], Batch [441/938], Loss: 0.48932644724845886\n",
      "Train: Epoch [19], Batch [442/938], Loss: 0.4805513322353363\n",
      "Train: Epoch [19], Batch [443/938], Loss: 0.3901090919971466\n",
      "Train: Epoch [19], Batch [444/938], Loss: 0.2447451949119568\n",
      "Train: Epoch [19], Batch [445/938], Loss: 0.37053072452545166\n",
      "Train: Epoch [19], Batch [446/938], Loss: 0.27081602811813354\n",
      "Train: Epoch [19], Batch [447/938], Loss: 0.5156029462814331\n",
      "Train: Epoch [19], Batch [448/938], Loss: 0.5555872917175293\n",
      "Train: Epoch [19], Batch [449/938], Loss: 0.3614271283149719\n",
      "Train: Epoch [19], Batch [450/938], Loss: 0.3834090530872345\n",
      "Train: Epoch [19], Batch [451/938], Loss: 0.46980708837509155\n",
      "Train: Epoch [19], Batch [452/938], Loss: 0.34321147203445435\n",
      "Train: Epoch [19], Batch [453/938], Loss: 0.4008384943008423\n",
      "Train: Epoch [19], Batch [454/938], Loss: 0.40389761328697205\n",
      "Train: Epoch [19], Batch [455/938], Loss: 0.46775078773498535\n",
      "Train: Epoch [19], Batch [456/938], Loss: 0.3137514889240265\n",
      "Train: Epoch [19], Batch [457/938], Loss: 0.39671987295150757\n",
      "Train: Epoch [19], Batch [458/938], Loss: 0.6481213569641113\n",
      "Train: Epoch [19], Batch [459/938], Loss: 0.586692750453949\n",
      "Train: Epoch [19], Batch [460/938], Loss: 0.3632146716117859\n",
      "Train: Epoch [19], Batch [461/938], Loss: 0.7364827394485474\n",
      "Train: Epoch [19], Batch [462/938], Loss: 0.5027530789375305\n",
      "Train: Epoch [19], Batch [463/938], Loss: 0.5626651048660278\n",
      "Train: Epoch [19], Batch [464/938], Loss: 0.34707802534103394\n",
      "Train: Epoch [19], Batch [465/938], Loss: 0.4550417363643646\n",
      "Train: Epoch [19], Batch [466/938], Loss: 0.2880045473575592\n",
      "Train: Epoch [19], Batch [467/938], Loss: 0.3582315742969513\n",
      "Train: Epoch [19], Batch [468/938], Loss: 0.5938632488250732\n",
      "Train: Epoch [19], Batch [469/938], Loss: 0.44718602299690247\n",
      "Train: Epoch [19], Batch [470/938], Loss: 0.3624628186225891\n",
      "Train: Epoch [19], Batch [471/938], Loss: 0.24329856038093567\n",
      "Train: Epoch [19], Batch [472/938], Loss: 0.2973300814628601\n",
      "Train: Epoch [19], Batch [473/938], Loss: 0.6191509366035461\n",
      "Train: Epoch [19], Batch [474/938], Loss: 0.4387526512145996\n",
      "Train: Epoch [19], Batch [475/938], Loss: 0.3692765235900879\n",
      "Train: Epoch [19], Batch [476/938], Loss: 0.4588848948478699\n",
      "Train: Epoch [19], Batch [477/938], Loss: 0.24610725045204163\n",
      "Train: Epoch [19], Batch [478/938], Loss: 0.41308194398880005\n",
      "Train: Epoch [19], Batch [479/938], Loss: 0.2974461317062378\n",
      "Train: Epoch [19], Batch [480/938], Loss: 0.40161561965942383\n",
      "Train: Epoch [19], Batch [481/938], Loss: 0.33224666118621826\n",
      "Train: Epoch [19], Batch [482/938], Loss: 0.2855186462402344\n",
      "Train: Epoch [19], Batch [483/938], Loss: 0.36001667380332947\n",
      "Train: Epoch [19], Batch [484/938], Loss: 0.31758472323417664\n",
      "Train: Epoch [19], Batch [485/938], Loss: 0.4466792643070221\n",
      "Train: Epoch [19], Batch [486/938], Loss: 0.45656126737594604\n",
      "Train: Epoch [19], Batch [487/938], Loss: 0.24568158388137817\n",
      "Train: Epoch [19], Batch [488/938], Loss: 0.4803586006164551\n",
      "Train: Epoch [19], Batch [489/938], Loss: 0.35774344205856323\n",
      "Train: Epoch [19], Batch [490/938], Loss: 0.33615148067474365\n",
      "Train: Epoch [19], Batch [491/938], Loss: 0.36294710636138916\n",
      "Train: Epoch [19], Batch [492/938], Loss: 0.4102404713630676\n",
      "Train: Epoch [19], Batch [493/938], Loss: 0.3305308222770691\n",
      "Train: Epoch [19], Batch [494/938], Loss: 0.23126693069934845\n",
      "Train: Epoch [19], Batch [495/938], Loss: 0.3464110493659973\n",
      "Train: Epoch [19], Batch [496/938], Loss: 0.437491238117218\n",
      "Train: Epoch [19], Batch [497/938], Loss: 0.275875449180603\n",
      "Train: Epoch [19], Batch [498/938], Loss: 0.30794501304626465\n",
      "Train: Epoch [19], Batch [499/938], Loss: 0.4314465820789337\n",
      "Train: Epoch [19], Batch [500/938], Loss: 0.35704565048217773\n",
      "Train: Epoch [19], Batch [501/938], Loss: 0.6067037582397461\n",
      "Train: Epoch [19], Batch [502/938], Loss: 0.4004160463809967\n",
      "Train: Epoch [19], Batch [503/938], Loss: 0.20888151228427887\n",
      "Train: Epoch [19], Batch [504/938], Loss: 0.5450098514556885\n",
      "Train: Epoch [19], Batch [505/938], Loss: 0.4356544613838196\n",
      "Train: Epoch [19], Batch [506/938], Loss: 0.3363308012485504\n",
      "Train: Epoch [19], Batch [507/938], Loss: 0.47802984714508057\n",
      "Train: Epoch [19], Batch [508/938], Loss: 0.44179442524909973\n",
      "Train: Epoch [19], Batch [509/938], Loss: 0.4876604378223419\n",
      "Train: Epoch [19], Batch [510/938], Loss: 0.8337434530258179\n",
      "Train: Epoch [19], Batch [511/938], Loss: 0.4916646480560303\n",
      "Train: Epoch [19], Batch [512/938], Loss: 0.2655980587005615\n",
      "Train: Epoch [19], Batch [513/938], Loss: 0.39722365140914917\n",
      "Train: Epoch [19], Batch [514/938], Loss: 0.28290432691574097\n",
      "Train: Epoch [19], Batch [515/938], Loss: 0.41710519790649414\n",
      "Train: Epoch [19], Batch [516/938], Loss: 0.4526391923427582\n",
      "Train: Epoch [19], Batch [517/938], Loss: 0.8009558916091919\n",
      "Train: Epoch [19], Batch [518/938], Loss: 0.35577473044395447\n",
      "Train: Epoch [19], Batch [519/938], Loss: 0.3143177628517151\n",
      "Train: Epoch [19], Batch [520/938], Loss: 0.544432520866394\n",
      "Train: Epoch [19], Batch [521/938], Loss: 0.3199998140335083\n",
      "Train: Epoch [19], Batch [522/938], Loss: 0.23174284398555756\n",
      "Train: Epoch [19], Batch [523/938], Loss: 0.5606957674026489\n",
      "Train: Epoch [19], Batch [524/938], Loss: 0.569230318069458\n",
      "Train: Epoch [19], Batch [525/938], Loss: 0.279167115688324\n",
      "Train: Epoch [19], Batch [526/938], Loss: 0.3362787067890167\n",
      "Train: Epoch [19], Batch [527/938], Loss: 0.19269999861717224\n",
      "Train: Epoch [19], Batch [528/938], Loss: 0.32498985528945923\n",
      "Train: Epoch [19], Batch [529/938], Loss: 0.3600892722606659\n",
      "Train: Epoch [19], Batch [530/938], Loss: 0.30957353115081787\n",
      "Train: Epoch [19], Batch [531/938], Loss: 0.24979998171329498\n",
      "Train: Epoch [19], Batch [532/938], Loss: 0.49738168716430664\n",
      "Train: Epoch [19], Batch [533/938], Loss: 0.41991177201271057\n",
      "Train: Epoch [19], Batch [534/938], Loss: 0.2827627658843994\n",
      "Train: Epoch [19], Batch [535/938], Loss: 0.43207818269729614\n",
      "Train: Epoch [19], Batch [536/938], Loss: 0.5396126508712769\n",
      "Train: Epoch [19], Batch [537/938], Loss: 0.24721959233283997\n",
      "Train: Epoch [19], Batch [538/938], Loss: 0.4114612936973572\n",
      "Train: Epoch [19], Batch [539/938], Loss: 0.4990346431732178\n",
      "Train: Epoch [19], Batch [540/938], Loss: 0.5087324380874634\n",
      "Train: Epoch [19], Batch [541/938], Loss: 0.538550853729248\n",
      "Train: Epoch [19], Batch [542/938], Loss: 0.6536463499069214\n",
      "Train: Epoch [19], Batch [543/938], Loss: 0.6767847537994385\n",
      "Train: Epoch [19], Batch [544/938], Loss: 0.4029655158519745\n",
      "Train: Epoch [19], Batch [545/938], Loss: 0.37787285447120667\n",
      "Train: Epoch [19], Batch [546/938], Loss: 0.5119650363922119\n",
      "Train: Epoch [19], Batch [547/938], Loss: 0.46376654505729675\n",
      "Train: Epoch [19], Batch [548/938], Loss: 0.4349253475666046\n",
      "Train: Epoch [19], Batch [549/938], Loss: 0.5963068008422852\n",
      "Train: Epoch [19], Batch [550/938], Loss: 0.30362409353256226\n",
      "Train: Epoch [19], Batch [551/938], Loss: 0.341004341840744\n",
      "Train: Epoch [19], Batch [552/938], Loss: 0.5024473667144775\n",
      "Train: Epoch [19], Batch [553/938], Loss: 0.4004889130592346\n",
      "Train: Epoch [19], Batch [554/938], Loss: 0.5203172564506531\n",
      "Train: Epoch [19], Batch [555/938], Loss: 0.47381138801574707\n",
      "Train: Epoch [19], Batch [556/938], Loss: 0.4886082708835602\n",
      "Train: Epoch [19], Batch [557/938], Loss: 0.45410770177841187\n",
      "Train: Epoch [19], Batch [558/938], Loss: 0.476752907037735\n",
      "Train: Epoch [19], Batch [559/938], Loss: 0.38226187229156494\n",
      "Train: Epoch [19], Batch [560/938], Loss: 0.4536300301551819\n",
      "Train: Epoch [19], Batch [561/938], Loss: 0.506496250629425\n",
      "Train: Epoch [19], Batch [562/938], Loss: 0.45943140983581543\n",
      "Train: Epoch [19], Batch [563/938], Loss: 0.3854254484176636\n",
      "Train: Epoch [19], Batch [564/938], Loss: 0.4345587193965912\n",
      "Train: Epoch [19], Batch [565/938], Loss: 0.31401771306991577\n",
      "Train: Epoch [19], Batch [566/938], Loss: 0.420115202665329\n",
      "Train: Epoch [19], Batch [567/938], Loss: 0.5930012464523315\n",
      "Train: Epoch [19], Batch [568/938], Loss: 0.37881365418434143\n",
      "Train: Epoch [19], Batch [569/938], Loss: 0.5616320371627808\n",
      "Train: Epoch [19], Batch [570/938], Loss: 0.4614548087120056\n",
      "Train: Epoch [19], Batch [571/938], Loss: 0.40144455432891846\n",
      "Train: Epoch [19], Batch [572/938], Loss: 0.32349613308906555\n",
      "Train: Epoch [19], Batch [573/938], Loss: 0.501128077507019\n",
      "Train: Epoch [19], Batch [574/938], Loss: 0.4614739418029785\n",
      "Train: Epoch [19], Batch [575/938], Loss: 0.5252443552017212\n",
      "Train: Epoch [19], Batch [576/938], Loss: 0.4940014183521271\n",
      "Train: Epoch [19], Batch [577/938], Loss: 0.3078165650367737\n",
      "Train: Epoch [19], Batch [578/938], Loss: 0.28879600763320923\n",
      "Train: Epoch [19], Batch [579/938], Loss: 0.4759100079536438\n",
      "Train: Epoch [19], Batch [580/938], Loss: 0.4468926787376404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [581/938], Loss: 0.36746013164520264\n",
      "Train: Epoch [19], Batch [582/938], Loss: 0.33214008808135986\n",
      "Train: Epoch [19], Batch [583/938], Loss: 0.31454116106033325\n",
      "Train: Epoch [19], Batch [584/938], Loss: 0.3885469138622284\n",
      "Train: Epoch [19], Batch [585/938], Loss: 0.4339340329170227\n",
      "Train: Epoch [19], Batch [586/938], Loss: 0.513673722743988\n",
      "Train: Epoch [19], Batch [587/938], Loss: 0.3855787515640259\n",
      "Train: Epoch [19], Batch [588/938], Loss: 0.40315350890159607\n",
      "Train: Epoch [19], Batch [589/938], Loss: 0.421875923871994\n",
      "Train: Epoch [19], Batch [590/938], Loss: 0.5285507440567017\n",
      "Train: Epoch [19], Batch [591/938], Loss: 0.4615779519081116\n",
      "Train: Epoch [19], Batch [592/938], Loss: 0.5475063920021057\n",
      "Train: Epoch [19], Batch [593/938], Loss: 0.47715502977371216\n",
      "Train: Epoch [19], Batch [594/938], Loss: 0.31076428294181824\n",
      "Train: Epoch [19], Batch [595/938], Loss: 0.3501277267932892\n",
      "Train: Epoch [19], Batch [596/938], Loss: 0.5836980938911438\n",
      "Train: Epoch [19], Batch [597/938], Loss: 0.5401920676231384\n",
      "Train: Epoch [19], Batch [598/938], Loss: 0.3761686682701111\n",
      "Train: Epoch [19], Batch [599/938], Loss: 0.2755695581436157\n",
      "Train: Epoch [19], Batch [600/938], Loss: 0.5155104398727417\n",
      "Train: Epoch [19], Batch [601/938], Loss: 0.484975665807724\n",
      "Train: Epoch [19], Batch [602/938], Loss: 0.3271029591560364\n",
      "Train: Epoch [19], Batch [603/938], Loss: 0.48869264125823975\n",
      "Train: Epoch [19], Batch [604/938], Loss: 0.4493533968925476\n",
      "Train: Epoch [19], Batch [605/938], Loss: 0.4969311058521271\n",
      "Train: Epoch [19], Batch [606/938], Loss: 0.4231811463832855\n",
      "Train: Epoch [19], Batch [607/938], Loss: 0.3545161783695221\n",
      "Train: Epoch [19], Batch [608/938], Loss: 0.41314584016799927\n",
      "Train: Epoch [19], Batch [609/938], Loss: 0.5427589416503906\n",
      "Train: Epoch [19], Batch [610/938], Loss: 0.5039181113243103\n",
      "Train: Epoch [19], Batch [611/938], Loss: 0.42640411853790283\n",
      "Train: Epoch [19], Batch [612/938], Loss: 0.5038577318191528\n",
      "Train: Epoch [19], Batch [613/938], Loss: 0.5539642572402954\n",
      "Train: Epoch [19], Batch [614/938], Loss: 0.6133811473846436\n",
      "Train: Epoch [19], Batch [615/938], Loss: 0.4412805736064911\n",
      "Train: Epoch [19], Batch [616/938], Loss: 0.2857608497142792\n",
      "Train: Epoch [19], Batch [617/938], Loss: 0.37014374136924744\n",
      "Train: Epoch [19], Batch [618/938], Loss: 0.5151586532592773\n",
      "Train: Epoch [19], Batch [619/938], Loss: 0.6112889051437378\n",
      "Train: Epoch [19], Batch [620/938], Loss: 0.3115251958370209\n",
      "Train: Epoch [19], Batch [621/938], Loss: 0.27068957686424255\n",
      "Train: Epoch [19], Batch [622/938], Loss: 0.31100159883499146\n",
      "Train: Epoch [19], Batch [623/938], Loss: 0.449313759803772\n",
      "Train: Epoch [19], Batch [624/938], Loss: 0.43192756175994873\n",
      "Train: Epoch [19], Batch [625/938], Loss: 0.34954071044921875\n",
      "Train: Epoch [19], Batch [626/938], Loss: 0.2958209812641144\n",
      "Train: Epoch [19], Batch [627/938], Loss: 0.4704839587211609\n",
      "Train: Epoch [19], Batch [628/938], Loss: 0.5141602754592896\n",
      "Train: Epoch [19], Batch [629/938], Loss: 0.31397613883018494\n",
      "Train: Epoch [19], Batch [630/938], Loss: 0.42611169815063477\n",
      "Train: Epoch [19], Batch [631/938], Loss: 0.6052417755126953\n",
      "Train: Epoch [19], Batch [632/938], Loss: 0.265987753868103\n",
      "Train: Epoch [19], Batch [633/938], Loss: 0.5311161279678345\n",
      "Train: Epoch [19], Batch [634/938], Loss: 0.4767950773239136\n",
      "Train: Epoch [19], Batch [635/938], Loss: 0.4585898220539093\n",
      "Train: Epoch [19], Batch [636/938], Loss: 0.3550434708595276\n",
      "Train: Epoch [19], Batch [637/938], Loss: 0.4568086564540863\n",
      "Train: Epoch [19], Batch [638/938], Loss: 0.3042612671852112\n",
      "Train: Epoch [19], Batch [639/938], Loss: 0.5344126224517822\n",
      "Train: Epoch [19], Batch [640/938], Loss: 0.36618101596832275\n",
      "Train: Epoch [19], Batch [641/938], Loss: 0.46059495210647583\n",
      "Train: Epoch [19], Batch [642/938], Loss: 0.34714365005493164\n",
      "Train: Epoch [19], Batch [643/938], Loss: 0.4227152168750763\n",
      "Train: Epoch [19], Batch [644/938], Loss: 0.2543873190879822\n",
      "Train: Epoch [19], Batch [645/938], Loss: 0.4836505651473999\n",
      "Train: Epoch [19], Batch [646/938], Loss: 0.39936619997024536\n",
      "Train: Epoch [19], Batch [647/938], Loss: 0.4311671257019043\n",
      "Train: Epoch [19], Batch [648/938], Loss: 0.38252609968185425\n",
      "Train: Epoch [19], Batch [649/938], Loss: 0.4219951927661896\n",
      "Train: Epoch [19], Batch [650/938], Loss: 0.4042832851409912\n",
      "Train: Epoch [19], Batch [651/938], Loss: 0.37413930892944336\n",
      "Train: Epoch [19], Batch [652/938], Loss: 0.4700964689254761\n",
      "Train: Epoch [19], Batch [653/938], Loss: 0.5290433764457703\n",
      "Train: Epoch [19], Batch [654/938], Loss: 0.30270805954933167\n",
      "Train: Epoch [19], Batch [655/938], Loss: 0.3890143632888794\n",
      "Train: Epoch [19], Batch [656/938], Loss: 0.4683232605457306\n",
      "Train: Epoch [19], Batch [657/938], Loss: 0.5531818866729736\n",
      "Train: Epoch [19], Batch [658/938], Loss: 0.35275426506996155\n",
      "Train: Epoch [19], Batch [659/938], Loss: 0.3937690854072571\n",
      "Train: Epoch [19], Batch [660/938], Loss: 0.16009709239006042\n",
      "Train: Epoch [19], Batch [661/938], Loss: 0.5716147422790527\n",
      "Train: Epoch [19], Batch [662/938], Loss: 0.49414533376693726\n",
      "Train: Epoch [19], Batch [663/938], Loss: 0.5223993062973022\n",
      "Train: Epoch [19], Batch [664/938], Loss: 0.42442071437835693\n",
      "Train: Epoch [19], Batch [665/938], Loss: 0.41451820731163025\n",
      "Train: Epoch [19], Batch [666/938], Loss: 0.320603609085083\n",
      "Train: Epoch [19], Batch [667/938], Loss: 0.2888459265232086\n",
      "Train: Epoch [19], Batch [668/938], Loss: 0.3141268491744995\n",
      "Train: Epoch [19], Batch [669/938], Loss: 0.3031069040298462\n",
      "Train: Epoch [19], Batch [670/938], Loss: 0.5590074062347412\n",
      "Train: Epoch [19], Batch [671/938], Loss: 0.5220264196395874\n",
      "Train: Epoch [19], Batch [672/938], Loss: 0.4697875380516052\n",
      "Train: Epoch [19], Batch [673/938], Loss: 0.43786919116973877\n",
      "Train: Epoch [19], Batch [674/938], Loss: 0.7702751755714417\n",
      "Train: Epoch [19], Batch [675/938], Loss: 0.4377386271953583\n",
      "Train: Epoch [19], Batch [676/938], Loss: 0.4893631935119629\n",
      "Train: Epoch [19], Batch [677/938], Loss: 0.4768902361392975\n",
      "Train: Epoch [19], Batch [678/938], Loss: 0.49783068895339966\n",
      "Train: Epoch [19], Batch [679/938], Loss: 0.41382575035095215\n",
      "Train: Epoch [19], Batch [680/938], Loss: 0.2795405983924866\n",
      "Train: Epoch [19], Batch [681/938], Loss: 0.4676983952522278\n",
      "Train: Epoch [19], Batch [682/938], Loss: 0.2154466062784195\n",
      "Train: Epoch [19], Batch [683/938], Loss: 0.32534536719322205\n",
      "Train: Epoch [19], Batch [684/938], Loss: 0.45577162504196167\n",
      "Train: Epoch [19], Batch [685/938], Loss: 0.2809262275695801\n",
      "Train: Epoch [19], Batch [686/938], Loss: 0.3345299959182739\n",
      "Train: Epoch [19], Batch [687/938], Loss: 0.3506869375705719\n",
      "Train: Epoch [19], Batch [688/938], Loss: 0.5503745079040527\n",
      "Train: Epoch [19], Batch [689/938], Loss: 0.4988518953323364\n",
      "Train: Epoch [19], Batch [690/938], Loss: 0.3862352967262268\n",
      "Train: Epoch [19], Batch [691/938], Loss: 0.38540175557136536\n",
      "Train: Epoch [19], Batch [692/938], Loss: 0.30910801887512207\n",
      "Train: Epoch [19], Batch [693/938], Loss: 0.4494132399559021\n",
      "Train: Epoch [19], Batch [694/938], Loss: 0.30695784091949463\n",
      "Train: Epoch [19], Batch [695/938], Loss: 0.2719888687133789\n",
      "Train: Epoch [19], Batch [696/938], Loss: 0.5094791054725647\n",
      "Train: Epoch [19], Batch [697/938], Loss: 0.499247282743454\n",
      "Train: Epoch [19], Batch [698/938], Loss: 0.4364183843135834\n",
      "Train: Epoch [19], Batch [699/938], Loss: 0.39743486046791077\n",
      "Train: Epoch [19], Batch [700/938], Loss: 0.347833514213562\n",
      "Train: Epoch [19], Batch [701/938], Loss: 0.3971315324306488\n",
      "Train: Epoch [19], Batch [702/938], Loss: 0.42434486746788025\n",
      "Train: Epoch [19], Batch [703/938], Loss: 0.4117039740085602\n",
      "Train: Epoch [19], Batch [704/938], Loss: 0.33552607893943787\n",
      "Train: Epoch [19], Batch [705/938], Loss: 0.4031750559806824\n",
      "Train: Epoch [19], Batch [706/938], Loss: 0.3431531488895416\n",
      "Train: Epoch [19], Batch [707/938], Loss: 0.4503500461578369\n",
      "Train: Epoch [19], Batch [708/938], Loss: 0.22624818980693817\n",
      "Train: Epoch [19], Batch [709/938], Loss: 0.36196643114089966\n",
      "Train: Epoch [19], Batch [710/938], Loss: 0.42743080854415894\n",
      "Train: Epoch [19], Batch [711/938], Loss: 0.4203421473503113\n",
      "Train: Epoch [19], Batch [712/938], Loss: 0.4618043303489685\n",
      "Train: Epoch [19], Batch [713/938], Loss: 0.46713948249816895\n",
      "Train: Epoch [19], Batch [714/938], Loss: 0.40439191460609436\n",
      "Train: Epoch [19], Batch [715/938], Loss: 0.5416821837425232\n",
      "Train: Epoch [19], Batch [716/938], Loss: 0.39082807302474976\n",
      "Train: Epoch [19], Batch [717/938], Loss: 0.2017633318901062\n",
      "Train: Epoch [19], Batch [718/938], Loss: 0.4134829640388489\n",
      "Train: Epoch [19], Batch [719/938], Loss: 0.5508487224578857\n",
      "Train: Epoch [19], Batch [720/938], Loss: 0.401577889919281\n",
      "Train: Epoch [19], Batch [721/938], Loss: 0.34835293889045715\n",
      "Train: Epoch [19], Batch [722/938], Loss: 0.3623325824737549\n",
      "Train: Epoch [19], Batch [723/938], Loss: 0.3273600935935974\n",
      "Train: Epoch [19], Batch [724/938], Loss: 0.45534488558769226\n",
      "Train: Epoch [19], Batch [725/938], Loss: 0.5109897255897522\n",
      "Train: Epoch [19], Batch [726/938], Loss: 0.37981390953063965\n",
      "Train: Epoch [19], Batch [727/938], Loss: 0.5647158622741699\n",
      "Train: Epoch [19], Batch [728/938], Loss: 0.3125491440296173\n",
      "Train: Epoch [19], Batch [729/938], Loss: 0.5475934743881226\n",
      "Train: Epoch [19], Batch [730/938], Loss: 0.4464714527130127\n",
      "Train: Epoch [19], Batch [731/938], Loss: 0.3196718692779541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [732/938], Loss: 0.514250636100769\n",
      "Train: Epoch [19], Batch [733/938], Loss: 0.30586618185043335\n",
      "Train: Epoch [19], Batch [734/938], Loss: 0.33569878339767456\n",
      "Train: Epoch [19], Batch [735/938], Loss: 0.30858975648880005\n",
      "Train: Epoch [19], Batch [736/938], Loss: 0.5110405683517456\n",
      "Train: Epoch [19], Batch [737/938], Loss: 0.6201892495155334\n",
      "Train: Epoch [19], Batch [738/938], Loss: 0.2947857081890106\n",
      "Train: Epoch [19], Batch [739/938], Loss: 0.550445020198822\n",
      "Train: Epoch [19], Batch [740/938], Loss: 0.38241082429885864\n",
      "Train: Epoch [19], Batch [741/938], Loss: 0.5624774694442749\n",
      "Train: Epoch [19], Batch [742/938], Loss: 0.22970673441886902\n",
      "Train: Epoch [19], Batch [743/938], Loss: 0.42357468605041504\n",
      "Train: Epoch [19], Batch [744/938], Loss: 0.32226961851119995\n",
      "Train: Epoch [19], Batch [745/938], Loss: 0.27664825320243835\n",
      "Train: Epoch [19], Batch [746/938], Loss: 0.4274922013282776\n",
      "Train: Epoch [19], Batch [747/938], Loss: 0.3657783269882202\n",
      "Train: Epoch [19], Batch [748/938], Loss: 0.4438212215900421\n",
      "Train: Epoch [19], Batch [749/938], Loss: 0.39954304695129395\n",
      "Train: Epoch [19], Batch [750/938], Loss: 0.45124107599258423\n",
      "Train: Epoch [19], Batch [751/938], Loss: 0.6373841166496277\n",
      "Train: Epoch [19], Batch [752/938], Loss: 0.5221280455589294\n",
      "Train: Epoch [19], Batch [753/938], Loss: 0.3925201892852783\n",
      "Train: Epoch [19], Batch [754/938], Loss: 0.5270140767097473\n",
      "Train: Epoch [19], Batch [755/938], Loss: 0.4004807472229004\n",
      "Train: Epoch [19], Batch [756/938], Loss: 0.4205728769302368\n",
      "Train: Epoch [19], Batch [757/938], Loss: 0.3858758211135864\n",
      "Train: Epoch [19], Batch [758/938], Loss: 0.28478264808654785\n",
      "Train: Epoch [19], Batch [759/938], Loss: 0.457038551568985\n",
      "Train: Epoch [19], Batch [760/938], Loss: 0.3893602788448334\n",
      "Train: Epoch [19], Batch [761/938], Loss: 0.38585177063941956\n",
      "Train: Epoch [19], Batch [762/938], Loss: 0.38530102372169495\n",
      "Train: Epoch [19], Batch [763/938], Loss: 0.2983134388923645\n",
      "Train: Epoch [19], Batch [764/938], Loss: 0.5021880865097046\n",
      "Train: Epoch [19], Batch [765/938], Loss: 0.38936734199523926\n",
      "Train: Epoch [19], Batch [766/938], Loss: 0.6529210209846497\n",
      "Train: Epoch [19], Batch [767/938], Loss: 0.34131920337677\n",
      "Train: Epoch [19], Batch [768/938], Loss: 0.4374319016933441\n",
      "Train: Epoch [19], Batch [769/938], Loss: 0.4674302935600281\n",
      "Train: Epoch [19], Batch [770/938], Loss: 0.3972696363925934\n",
      "Train: Epoch [19], Batch [771/938], Loss: 0.417036235332489\n",
      "Train: Epoch [19], Batch [772/938], Loss: 0.3288329839706421\n",
      "Train: Epoch [19], Batch [773/938], Loss: 0.48345667123794556\n",
      "Train: Epoch [19], Batch [774/938], Loss: 0.5649862885475159\n",
      "Train: Epoch [19], Batch [775/938], Loss: 0.46452128887176514\n",
      "Train: Epoch [19], Batch [776/938], Loss: 0.3056532144546509\n",
      "Train: Epoch [19], Batch [777/938], Loss: 0.33728083968162537\n",
      "Train: Epoch [19], Batch [778/938], Loss: 0.33008402585983276\n",
      "Train: Epoch [19], Batch [779/938], Loss: 0.3126218914985657\n",
      "Train: Epoch [19], Batch [780/938], Loss: 0.36221370100975037\n",
      "Train: Epoch [19], Batch [781/938], Loss: 0.33214202523231506\n",
      "Train: Epoch [19], Batch [782/938], Loss: 0.2752520442008972\n",
      "Train: Epoch [19], Batch [783/938], Loss: 0.49509602785110474\n",
      "Train: Epoch [19], Batch [784/938], Loss: 0.363889217376709\n",
      "Train: Epoch [19], Batch [785/938], Loss: 0.4884583652019501\n",
      "Train: Epoch [19], Batch [786/938], Loss: 0.2071843147277832\n",
      "Train: Epoch [19], Batch [787/938], Loss: 0.4875125288963318\n",
      "Train: Epoch [19], Batch [788/938], Loss: 0.5494291186332703\n",
      "Train: Epoch [19], Batch [789/938], Loss: 0.5717847943305969\n",
      "Train: Epoch [19], Batch [790/938], Loss: 0.40369153022766113\n",
      "Train: Epoch [19], Batch [791/938], Loss: 0.2628104090690613\n",
      "Train: Epoch [19], Batch [792/938], Loss: 0.5693490505218506\n",
      "Train: Epoch [19], Batch [793/938], Loss: 0.40904998779296875\n",
      "Train: Epoch [19], Batch [794/938], Loss: 0.3412541151046753\n",
      "Train: Epoch [19], Batch [795/938], Loss: 0.3978698253631592\n",
      "Train: Epoch [19], Batch [796/938], Loss: 0.2876719832420349\n",
      "Train: Epoch [19], Batch [797/938], Loss: 0.4990830719470978\n",
      "Train: Epoch [19], Batch [798/938], Loss: 0.41708898544311523\n",
      "Train: Epoch [19], Batch [799/938], Loss: 0.46119076013565063\n",
      "Train: Epoch [19], Batch [800/938], Loss: 0.3702758550643921\n",
      "Train: Epoch [19], Batch [801/938], Loss: 0.42446643114089966\n",
      "Train: Epoch [19], Batch [802/938], Loss: 0.4182206392288208\n",
      "Train: Epoch [19], Batch [803/938], Loss: 0.4073716402053833\n",
      "Train: Epoch [19], Batch [804/938], Loss: 0.4294545650482178\n",
      "Train: Epoch [19], Batch [805/938], Loss: 0.37515825033187866\n",
      "Train: Epoch [19], Batch [806/938], Loss: 0.28713303804397583\n",
      "Train: Epoch [19], Batch [807/938], Loss: 0.41165539622306824\n",
      "Train: Epoch [19], Batch [808/938], Loss: 0.36686548590660095\n",
      "Train: Epoch [19], Batch [809/938], Loss: 0.2781977653503418\n",
      "Train: Epoch [19], Batch [810/938], Loss: 0.31309300661087036\n",
      "Train: Epoch [19], Batch [811/938], Loss: 0.3557015657424927\n",
      "Train: Epoch [19], Batch [812/938], Loss: 0.4595397412776947\n",
      "Train: Epoch [19], Batch [813/938], Loss: 0.3440522253513336\n",
      "Train: Epoch [19], Batch [814/938], Loss: 0.3663349449634552\n",
      "Train: Epoch [19], Batch [815/938], Loss: 0.5722761154174805\n",
      "Train: Epoch [19], Batch [816/938], Loss: 0.5495994091033936\n",
      "Train: Epoch [19], Batch [817/938], Loss: 0.49843671917915344\n",
      "Train: Epoch [19], Batch [818/938], Loss: 0.2181912660598755\n",
      "Train: Epoch [19], Batch [819/938], Loss: 0.33418887853622437\n",
      "Train: Epoch [19], Batch [820/938], Loss: 0.38241761922836304\n",
      "Train: Epoch [19], Batch [821/938], Loss: 0.32541990280151367\n",
      "Train: Epoch [19], Batch [822/938], Loss: 0.42925703525543213\n",
      "Train: Epoch [19], Batch [823/938], Loss: 0.4552551209926605\n",
      "Train: Epoch [19], Batch [824/938], Loss: 0.35246580839157104\n",
      "Train: Epoch [19], Batch [825/938], Loss: 0.35147812962532043\n",
      "Train: Epoch [19], Batch [826/938], Loss: 0.4950774610042572\n",
      "Train: Epoch [19], Batch [827/938], Loss: 0.3710261583328247\n",
      "Train: Epoch [19], Batch [828/938], Loss: 0.4979664087295532\n",
      "Train: Epoch [19], Batch [829/938], Loss: 0.3621998429298401\n",
      "Train: Epoch [19], Batch [830/938], Loss: 0.5976313352584839\n",
      "Train: Epoch [19], Batch [831/938], Loss: 0.4627476632595062\n",
      "Train: Epoch [19], Batch [832/938], Loss: 0.5280115008354187\n",
      "Train: Epoch [19], Batch [833/938], Loss: 0.6775799989700317\n",
      "Train: Epoch [19], Batch [834/938], Loss: 0.5156105756759644\n",
      "Train: Epoch [19], Batch [835/938], Loss: 0.46343517303466797\n",
      "Train: Epoch [19], Batch [836/938], Loss: 0.2337668240070343\n",
      "Train: Epoch [19], Batch [837/938], Loss: 0.3533962070941925\n",
      "Train: Epoch [19], Batch [838/938], Loss: 0.3727705478668213\n",
      "Train: Epoch [19], Batch [839/938], Loss: 0.4565432071685791\n",
      "Train: Epoch [19], Batch [840/938], Loss: 0.4421112835407257\n",
      "Train: Epoch [19], Batch [841/938], Loss: 0.38060468435287476\n",
      "Train: Epoch [19], Batch [842/938], Loss: 0.39255648851394653\n",
      "Train: Epoch [19], Batch [843/938], Loss: 0.5324851274490356\n",
      "Train: Epoch [19], Batch [844/938], Loss: 0.5495635867118835\n",
      "Train: Epoch [19], Batch [845/938], Loss: 0.45416080951690674\n",
      "Train: Epoch [19], Batch [846/938], Loss: 0.5037083029747009\n",
      "Train: Epoch [19], Batch [847/938], Loss: 0.3889665901660919\n",
      "Train: Epoch [19], Batch [848/938], Loss: 0.38323891162872314\n",
      "Train: Epoch [19], Batch [849/938], Loss: 0.43616533279418945\n",
      "Train: Epoch [19], Batch [850/938], Loss: 0.5018864274024963\n",
      "Train: Epoch [19], Batch [851/938], Loss: 0.37931376695632935\n",
      "Train: Epoch [19], Batch [852/938], Loss: 0.5182790160179138\n",
      "Train: Epoch [19], Batch [853/938], Loss: 0.35426753759384155\n",
      "Train: Epoch [19], Batch [854/938], Loss: 0.37426966428756714\n",
      "Train: Epoch [19], Batch [855/938], Loss: 0.5292156934738159\n",
      "Train: Epoch [19], Batch [856/938], Loss: 0.3194502294063568\n",
      "Train: Epoch [19], Batch [857/938], Loss: 0.34942877292633057\n",
      "Train: Epoch [19], Batch [858/938], Loss: 0.3482781648635864\n",
      "Train: Epoch [19], Batch [859/938], Loss: 0.7043161392211914\n",
      "Train: Epoch [19], Batch [860/938], Loss: 0.2969106137752533\n",
      "Train: Epoch [19], Batch [861/938], Loss: 0.6493426561355591\n",
      "Train: Epoch [19], Batch [862/938], Loss: 0.49143797159194946\n",
      "Train: Epoch [19], Batch [863/938], Loss: 0.5472092628479004\n",
      "Train: Epoch [19], Batch [864/938], Loss: 0.2640616297721863\n",
      "Train: Epoch [19], Batch [865/938], Loss: 0.40166908502578735\n",
      "Train: Epoch [19], Batch [866/938], Loss: 0.41294044256210327\n",
      "Train: Epoch [19], Batch [867/938], Loss: 0.37641483545303345\n",
      "Train: Epoch [19], Batch [868/938], Loss: 0.5192651748657227\n",
      "Train: Epoch [19], Batch [869/938], Loss: 0.28594881296157837\n",
      "Train: Epoch [19], Batch [870/938], Loss: 0.4627496302127838\n",
      "Train: Epoch [19], Batch [871/938], Loss: 0.41966596245765686\n",
      "Train: Epoch [19], Batch [872/938], Loss: 0.4249950647354126\n",
      "Train: Epoch [19], Batch [873/938], Loss: 0.5382446050643921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [19], Batch [874/938], Loss: 0.44426965713500977\n",
      "Train: Epoch [19], Batch [875/938], Loss: 0.5664510726928711\n",
      "Train: Epoch [19], Batch [876/938], Loss: 0.5109896659851074\n",
      "Train: Epoch [19], Batch [877/938], Loss: 0.4185163974761963\n",
      "Train: Epoch [19], Batch [878/938], Loss: 0.3040004372596741\n",
      "Train: Epoch [19], Batch [879/938], Loss: 0.35344019532203674\n",
      "Train: Epoch [19], Batch [880/938], Loss: 0.4543893337249756\n",
      "Train: Epoch [19], Batch [881/938], Loss: 0.4944744408130646\n",
      "Train: Epoch [19], Batch [882/938], Loss: 0.24633096158504486\n",
      "Train: Epoch [19], Batch [883/938], Loss: 0.3450648784637451\n",
      "Train: Epoch [19], Batch [884/938], Loss: 0.3186357021331787\n",
      "Train: Epoch [19], Batch [885/938], Loss: 0.5566154718399048\n",
      "Train: Epoch [19], Batch [886/938], Loss: 0.4728432595729828\n",
      "Train: Epoch [19], Batch [887/938], Loss: 0.46361762285232544\n",
      "Train: Epoch [19], Batch [888/938], Loss: 0.33834338188171387\n",
      "Train: Epoch [19], Batch [889/938], Loss: 0.4349742531776428\n",
      "Train: Epoch [19], Batch [890/938], Loss: 0.3654537498950958\n",
      "Train: Epoch [19], Batch [891/938], Loss: 0.4393412470817566\n",
      "Train: Epoch [19], Batch [892/938], Loss: 0.44247621297836304\n",
      "Train: Epoch [19], Batch [893/938], Loss: 0.5130167603492737\n",
      "Train: Epoch [19], Batch [894/938], Loss: 0.36924242973327637\n",
      "Train: Epoch [19], Batch [895/938], Loss: 0.5597520470619202\n",
      "Train: Epoch [19], Batch [896/938], Loss: 0.458702027797699\n",
      "Train: Epoch [19], Batch [897/938], Loss: 0.30984583497047424\n",
      "Train: Epoch [19], Batch [898/938], Loss: 0.2987324595451355\n",
      "Train: Epoch [19], Batch [899/938], Loss: 0.4870644807815552\n",
      "Train: Epoch [19], Batch [900/938], Loss: 0.4444018602371216\n",
      "Train: Epoch [19], Batch [901/938], Loss: 0.4910129904747009\n",
      "Train: Epoch [19], Batch [902/938], Loss: 0.4845867455005646\n",
      "Train: Epoch [19], Batch [903/938], Loss: 0.3371005058288574\n",
      "Train: Epoch [19], Batch [904/938], Loss: 0.40073853731155396\n",
      "Train: Epoch [19], Batch [905/938], Loss: 0.41860371828079224\n",
      "Train: Epoch [19], Batch [906/938], Loss: 0.30642977356910706\n",
      "Train: Epoch [19], Batch [907/938], Loss: 0.2867701053619385\n",
      "Train: Epoch [19], Batch [908/938], Loss: 0.3080473244190216\n",
      "Train: Epoch [19], Batch [909/938], Loss: 0.517039954662323\n",
      "Train: Epoch [19], Batch [910/938], Loss: 0.44853460788726807\n",
      "Train: Epoch [19], Batch [911/938], Loss: 0.528951108455658\n",
      "Train: Epoch [19], Batch [912/938], Loss: 0.31987759470939636\n",
      "Train: Epoch [19], Batch [913/938], Loss: 0.32176870107650757\n",
      "Train: Epoch [19], Batch [914/938], Loss: 0.2533150017261505\n",
      "Train: Epoch [19], Batch [915/938], Loss: 0.41374626755714417\n",
      "Train: Epoch [19], Batch [916/938], Loss: 0.29938387870788574\n",
      "Train: Epoch [19], Batch [917/938], Loss: 0.4448804259300232\n",
      "Train: Epoch [19], Batch [918/938], Loss: 0.38341131806373596\n",
      "Train: Epoch [19], Batch [919/938], Loss: 0.37779563665390015\n",
      "Train: Epoch [19], Batch [920/938], Loss: 0.46736931800842285\n",
      "Train: Epoch [19], Batch [921/938], Loss: 0.28453001379966736\n",
      "Train: Epoch [19], Batch [922/938], Loss: 0.37866824865341187\n",
      "Train: Epoch [19], Batch [923/938], Loss: 0.4618176221847534\n",
      "Train: Epoch [19], Batch [924/938], Loss: 0.4600605368614197\n",
      "Train: Epoch [19], Batch [925/938], Loss: 0.44348713755607605\n",
      "Train: Epoch [19], Batch [926/938], Loss: 0.42934417724609375\n",
      "Train: Epoch [19], Batch [927/938], Loss: 0.38311779499053955\n",
      "Train: Epoch [19], Batch [928/938], Loss: 0.471098929643631\n",
      "Train: Epoch [19], Batch [929/938], Loss: 0.47075438499450684\n",
      "Train: Epoch [19], Batch [930/938], Loss: 0.574372410774231\n",
      "Train: Epoch [19], Batch [931/938], Loss: 0.6081525087356567\n",
      "Train: Epoch [19], Batch [932/938], Loss: 0.6567710041999817\n",
      "Train: Epoch [19], Batch [933/938], Loss: 0.44822001457214355\n",
      "Train: Epoch [19], Batch [934/938], Loss: 0.42744413018226624\n",
      "Train: Epoch [19], Batch [935/938], Loss: 0.3707091808319092\n",
      "Train: Epoch [19], Batch [936/938], Loss: 0.32731205224990845\n",
      "Train: Epoch [19], Batch [937/938], Loss: 0.45123741030693054\n",
      "Train: Epoch [19], Batch [938/938], Loss: 0.6459665298461914\n",
      "Accuracy of train set: 0.8519\n",
      "Validation: Epoch [19], Batch [1/938], Loss: 0.36313873529434204\n",
      "Validation: Epoch [19], Batch [2/938], Loss: 0.3049938380718231\n",
      "Validation: Epoch [19], Batch [3/938], Loss: 0.44807398319244385\n",
      "Validation: Epoch [19], Batch [4/938], Loss: 0.2694810926914215\n",
      "Validation: Epoch [19], Batch [5/938], Loss: 0.43673086166381836\n",
      "Validation: Epoch [19], Batch [6/938], Loss: 0.5101398229598999\n",
      "Validation: Epoch [19], Batch [7/938], Loss: 0.3433087170124054\n",
      "Validation: Epoch [19], Batch [8/938], Loss: 0.5635503530502319\n",
      "Validation: Epoch [19], Batch [9/938], Loss: 0.2634419798851013\n",
      "Validation: Epoch [19], Batch [10/938], Loss: 0.4659789502620697\n",
      "Validation: Epoch [19], Batch [11/938], Loss: 0.42304062843322754\n",
      "Validation: Epoch [19], Batch [12/938], Loss: 0.4683239161968231\n",
      "Validation: Epoch [19], Batch [13/938], Loss: 0.40414661169052124\n",
      "Validation: Epoch [19], Batch [14/938], Loss: 0.41245752573013306\n",
      "Validation: Epoch [19], Batch [15/938], Loss: 0.25947505235671997\n",
      "Validation: Epoch [19], Batch [16/938], Loss: 0.638823390007019\n",
      "Validation: Epoch [19], Batch [17/938], Loss: 0.26612207293510437\n",
      "Validation: Epoch [19], Batch [18/938], Loss: 0.7662156224250793\n",
      "Validation: Epoch [19], Batch [19/938], Loss: 0.3673982322216034\n",
      "Validation: Epoch [19], Batch [20/938], Loss: 0.2237912118434906\n",
      "Validation: Epoch [19], Batch [21/938], Loss: 0.5580590963363647\n",
      "Validation: Epoch [19], Batch [22/938], Loss: 0.41031748056411743\n",
      "Validation: Epoch [19], Batch [23/938], Loss: 0.5188814401626587\n",
      "Validation: Epoch [19], Batch [24/938], Loss: 0.40963470935821533\n",
      "Validation: Epoch [19], Batch [25/938], Loss: 0.30387169122695923\n",
      "Validation: Epoch [19], Batch [26/938], Loss: 0.2329634428024292\n",
      "Validation: Epoch [19], Batch [27/938], Loss: 0.7012128233909607\n",
      "Validation: Epoch [19], Batch [28/938], Loss: 0.39487048983573914\n",
      "Validation: Epoch [19], Batch [29/938], Loss: 0.23738053441047668\n",
      "Validation: Epoch [19], Batch [30/938], Loss: 0.5429444909095764\n",
      "Validation: Epoch [19], Batch [31/938], Loss: 0.4300636649131775\n",
      "Validation: Epoch [19], Batch [32/938], Loss: 0.4725397825241089\n",
      "Validation: Epoch [19], Batch [33/938], Loss: 0.26209568977355957\n",
      "Validation: Epoch [19], Batch [34/938], Loss: 0.48849183320999146\n",
      "Validation: Epoch [19], Batch [35/938], Loss: 0.4493043124675751\n",
      "Validation: Epoch [19], Batch [36/938], Loss: 0.643652081489563\n",
      "Validation: Epoch [19], Batch [37/938], Loss: 0.4410889744758606\n",
      "Validation: Epoch [19], Batch [38/938], Loss: 0.44122782349586487\n",
      "Validation: Epoch [19], Batch [39/938], Loss: 0.4198874533176422\n",
      "Validation: Epoch [19], Batch [40/938], Loss: 0.4049476385116577\n",
      "Validation: Epoch [19], Batch [41/938], Loss: 0.3973664939403534\n",
      "Validation: Epoch [19], Batch [42/938], Loss: 0.3975180387496948\n",
      "Validation: Epoch [19], Batch [43/938], Loss: 0.39654743671417236\n",
      "Validation: Epoch [19], Batch [44/938], Loss: 0.3092074394226074\n",
      "Validation: Epoch [19], Batch [45/938], Loss: 0.2094971239566803\n",
      "Validation: Epoch [19], Batch [46/938], Loss: 0.43787214159965515\n",
      "Validation: Epoch [19], Batch [47/938], Loss: 0.3373199701309204\n",
      "Validation: Epoch [19], Batch [48/938], Loss: 0.32382315397262573\n",
      "Validation: Epoch [19], Batch [49/938], Loss: 0.3477992117404938\n",
      "Validation: Epoch [19], Batch [50/938], Loss: 0.35176318883895874\n",
      "Validation: Epoch [19], Batch [51/938], Loss: 0.34233996272087097\n",
      "Validation: Epoch [19], Batch [52/938], Loss: 0.3070520758628845\n",
      "Validation: Epoch [19], Batch [53/938], Loss: 0.5009974241256714\n",
      "Validation: Epoch [19], Batch [54/938], Loss: 0.48880499601364136\n",
      "Validation: Epoch [19], Batch [55/938], Loss: 0.39658045768737793\n",
      "Validation: Epoch [19], Batch [56/938], Loss: 0.4517947733402252\n",
      "Validation: Epoch [19], Batch [57/938], Loss: 0.6918437480926514\n",
      "Validation: Epoch [19], Batch [58/938], Loss: 0.34899553656578064\n",
      "Validation: Epoch [19], Batch [59/938], Loss: 0.45323383808135986\n",
      "Validation: Epoch [19], Batch [60/938], Loss: 0.3517068922519684\n",
      "Validation: Epoch [19], Batch [61/938], Loss: 0.4067346155643463\n",
      "Validation: Epoch [19], Batch [62/938], Loss: 0.3340097665786743\n",
      "Validation: Epoch [19], Batch [63/938], Loss: 0.43832045793533325\n",
      "Validation: Epoch [19], Batch [64/938], Loss: 0.5124286413192749\n",
      "Validation: Epoch [19], Batch [65/938], Loss: 0.27986088395118713\n",
      "Validation: Epoch [19], Batch [66/938], Loss: 0.3188730776309967\n",
      "Validation: Epoch [19], Batch [67/938], Loss: 0.4084646701812744\n",
      "Validation: Epoch [19], Batch [68/938], Loss: 0.40908586978912354\n",
      "Validation: Epoch [19], Batch [69/938], Loss: 0.4487403333187103\n",
      "Validation: Epoch [19], Batch [70/938], Loss: 0.39774078130722046\n",
      "Validation: Epoch [19], Batch [71/938], Loss: 0.5503475666046143\n",
      "Validation: Epoch [19], Batch [72/938], Loss: 0.4179612398147583\n",
      "Validation: Epoch [19], Batch [73/938], Loss: 0.43898433446884155\n",
      "Validation: Epoch [19], Batch [74/938], Loss: 0.4326064884662628\n",
      "Validation: Epoch [19], Batch [75/938], Loss: 0.5041331052780151\n",
      "Validation: Epoch [19], Batch [76/938], Loss: 0.5666776895523071\n",
      "Validation: Epoch [19], Batch [77/938], Loss: 0.3527739644050598\n",
      "Validation: Epoch [19], Batch [78/938], Loss: 0.3215887248516083\n",
      "Validation: Epoch [19], Batch [79/938], Loss: 0.27114349603652954\n",
      "Validation: Epoch [19], Batch [80/938], Loss: 0.3140257000923157\n",
      "Validation: Epoch [19], Batch [81/938], Loss: 0.5565640330314636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [82/938], Loss: 0.3933229446411133\n",
      "Validation: Epoch [19], Batch [83/938], Loss: 0.3242168426513672\n",
      "Validation: Epoch [19], Batch [84/938], Loss: 0.49911603331565857\n",
      "Validation: Epoch [19], Batch [85/938], Loss: 0.444278359413147\n",
      "Validation: Epoch [19], Batch [86/938], Loss: 0.3590473532676697\n",
      "Validation: Epoch [19], Batch [87/938], Loss: 0.3149818480014801\n",
      "Validation: Epoch [19], Batch [88/938], Loss: 0.513656735420227\n",
      "Validation: Epoch [19], Batch [89/938], Loss: 0.37882113456726074\n",
      "Validation: Epoch [19], Batch [90/938], Loss: 0.3081326484680176\n",
      "Validation: Epoch [19], Batch [91/938], Loss: 0.2510172128677368\n",
      "Validation: Epoch [19], Batch [92/938], Loss: 0.2786247134208679\n",
      "Validation: Epoch [19], Batch [93/938], Loss: 0.4234386086463928\n",
      "Validation: Epoch [19], Batch [94/938], Loss: 0.34128502011299133\n",
      "Validation: Epoch [19], Batch [95/938], Loss: 0.5615322589874268\n",
      "Validation: Epoch [19], Batch [96/938], Loss: 0.24963662028312683\n",
      "Validation: Epoch [19], Batch [97/938], Loss: 0.6478297114372253\n",
      "Validation: Epoch [19], Batch [98/938], Loss: 0.38608652353286743\n",
      "Validation: Epoch [19], Batch [99/938], Loss: 0.40813151001930237\n",
      "Validation: Epoch [19], Batch [100/938], Loss: 0.2775747776031494\n",
      "Validation: Epoch [19], Batch [101/938], Loss: 0.28376418352127075\n",
      "Validation: Epoch [19], Batch [102/938], Loss: 0.5437278747558594\n",
      "Validation: Epoch [19], Batch [103/938], Loss: 0.573957622051239\n",
      "Validation: Epoch [19], Batch [104/938], Loss: 0.4154292047023773\n",
      "Validation: Epoch [19], Batch [105/938], Loss: 0.30832263827323914\n",
      "Validation: Epoch [19], Batch [106/938], Loss: 0.4126344919204712\n",
      "Validation: Epoch [19], Batch [107/938], Loss: 0.2830618619918823\n",
      "Validation: Epoch [19], Batch [108/938], Loss: 0.3067329227924347\n",
      "Validation: Epoch [19], Batch [109/938], Loss: 0.7075940370559692\n",
      "Validation: Epoch [19], Batch [110/938], Loss: 0.30724024772644043\n",
      "Validation: Epoch [19], Batch [111/938], Loss: 0.3635269105434418\n",
      "Validation: Epoch [19], Batch [112/938], Loss: 0.3971433639526367\n",
      "Validation: Epoch [19], Batch [113/938], Loss: 0.6877012252807617\n",
      "Validation: Epoch [19], Batch [114/938], Loss: 0.43813180923461914\n",
      "Validation: Epoch [19], Batch [115/938], Loss: 0.6058353781700134\n",
      "Validation: Epoch [19], Batch [116/938], Loss: 0.4339745044708252\n",
      "Validation: Epoch [19], Batch [117/938], Loss: 0.397308349609375\n",
      "Validation: Epoch [19], Batch [118/938], Loss: 0.3024205267429352\n",
      "Validation: Epoch [19], Batch [119/938], Loss: 0.6983014345169067\n",
      "Validation: Epoch [19], Batch [120/938], Loss: 0.22990086674690247\n",
      "Validation: Epoch [19], Batch [121/938], Loss: 0.5992729663848877\n",
      "Validation: Epoch [19], Batch [122/938], Loss: 0.45446714758872986\n",
      "Validation: Epoch [19], Batch [123/938], Loss: 0.5134538412094116\n",
      "Validation: Epoch [19], Batch [124/938], Loss: 0.4344485402107239\n",
      "Validation: Epoch [19], Batch [125/938], Loss: 0.45556023716926575\n",
      "Validation: Epoch [19], Batch [126/938], Loss: 0.41206294298171997\n",
      "Validation: Epoch [19], Batch [127/938], Loss: 0.5512286424636841\n",
      "Validation: Epoch [19], Batch [128/938], Loss: 0.4788101315498352\n",
      "Validation: Epoch [19], Batch [129/938], Loss: 0.2105768620967865\n",
      "Validation: Epoch [19], Batch [130/938], Loss: 0.3791356086730957\n",
      "Validation: Epoch [19], Batch [131/938], Loss: 0.44915562868118286\n",
      "Validation: Epoch [19], Batch [132/938], Loss: 0.4829271137714386\n",
      "Validation: Epoch [19], Batch [133/938], Loss: 0.5439702868461609\n",
      "Validation: Epoch [19], Batch [134/938], Loss: 0.2699764370918274\n",
      "Validation: Epoch [19], Batch [135/938], Loss: 0.37778419256210327\n",
      "Validation: Epoch [19], Batch [136/938], Loss: 0.3769383728504181\n",
      "Validation: Epoch [19], Batch [137/938], Loss: 0.43471407890319824\n",
      "Validation: Epoch [19], Batch [138/938], Loss: 0.2863491177558899\n",
      "Validation: Epoch [19], Batch [139/938], Loss: 0.3894546329975128\n",
      "Validation: Epoch [19], Batch [140/938], Loss: 0.37882980704307556\n",
      "Validation: Epoch [19], Batch [141/938], Loss: 0.400360107421875\n",
      "Validation: Epoch [19], Batch [142/938], Loss: 0.38982173800468445\n",
      "Validation: Epoch [19], Batch [143/938], Loss: 0.5457825660705566\n",
      "Validation: Epoch [19], Batch [144/938], Loss: 0.3190199136734009\n",
      "Validation: Epoch [19], Batch [145/938], Loss: 0.31506967544555664\n",
      "Validation: Epoch [19], Batch [146/938], Loss: 0.4578348994255066\n",
      "Validation: Epoch [19], Batch [147/938], Loss: 0.33846694231033325\n",
      "Validation: Epoch [19], Batch [148/938], Loss: 0.40806859731674194\n",
      "Validation: Epoch [19], Batch [149/938], Loss: 0.27534219622612\n",
      "Validation: Epoch [19], Batch [150/938], Loss: 0.29760801792144775\n",
      "Validation: Epoch [19], Batch [151/938], Loss: 0.47222715616226196\n",
      "Validation: Epoch [19], Batch [152/938], Loss: 0.4312794804573059\n",
      "Validation: Epoch [19], Batch [153/938], Loss: 0.5274109244346619\n",
      "Validation: Epoch [19], Batch [154/938], Loss: 0.332058310508728\n",
      "Validation: Epoch [19], Batch [155/938], Loss: 0.4504619240760803\n",
      "Validation: Epoch [19], Batch [156/938], Loss: 0.5791817903518677\n",
      "Validation: Epoch [19], Batch [157/938], Loss: 0.43288570642471313\n",
      "Validation: Epoch [19], Batch [158/938], Loss: 0.4818212389945984\n",
      "Validation: Epoch [19], Batch [159/938], Loss: 0.3781939744949341\n",
      "Validation: Epoch [19], Batch [160/938], Loss: 0.26951634883880615\n",
      "Validation: Epoch [19], Batch [161/938], Loss: 0.44215086102485657\n",
      "Validation: Epoch [19], Batch [162/938], Loss: 0.3576917052268982\n",
      "Validation: Epoch [19], Batch [163/938], Loss: 0.4688950181007385\n",
      "Validation: Epoch [19], Batch [164/938], Loss: 0.42881375551223755\n",
      "Validation: Epoch [19], Batch [165/938], Loss: 0.27726346254348755\n",
      "Validation: Epoch [19], Batch [166/938], Loss: 0.3141225576400757\n",
      "Validation: Epoch [19], Batch [167/938], Loss: 0.549981951713562\n",
      "Validation: Epoch [19], Batch [168/938], Loss: 0.37760210037231445\n",
      "Validation: Epoch [19], Batch [169/938], Loss: 0.5719684958457947\n",
      "Validation: Epoch [19], Batch [170/938], Loss: 0.32856154441833496\n",
      "Validation: Epoch [19], Batch [171/938], Loss: 0.5553503632545471\n",
      "Validation: Epoch [19], Batch [172/938], Loss: 0.32872331142425537\n",
      "Validation: Epoch [19], Batch [173/938], Loss: 0.4474071264266968\n",
      "Validation: Epoch [19], Batch [174/938], Loss: 0.27932867407798767\n",
      "Validation: Epoch [19], Batch [175/938], Loss: 0.4745086431503296\n",
      "Validation: Epoch [19], Batch [176/938], Loss: 0.3751702606678009\n",
      "Validation: Epoch [19], Batch [177/938], Loss: 0.5358060598373413\n",
      "Validation: Epoch [19], Batch [178/938], Loss: 0.3833240568637848\n",
      "Validation: Epoch [19], Batch [179/938], Loss: 0.4173368215560913\n",
      "Validation: Epoch [19], Batch [180/938], Loss: 0.39736175537109375\n",
      "Validation: Epoch [19], Batch [181/938], Loss: 0.2829015552997589\n",
      "Validation: Epoch [19], Batch [182/938], Loss: 0.5550616979598999\n",
      "Validation: Epoch [19], Batch [183/938], Loss: 0.3329717516899109\n",
      "Validation: Epoch [19], Batch [184/938], Loss: 0.32045724987983704\n",
      "Validation: Epoch [19], Batch [185/938], Loss: 0.5776985883712769\n",
      "Validation: Epoch [19], Batch [186/938], Loss: 0.39722052216529846\n",
      "Validation: Epoch [19], Batch [187/938], Loss: 0.4293476343154907\n",
      "Validation: Epoch [19], Batch [188/938], Loss: 0.47122952342033386\n",
      "Validation: Epoch [19], Batch [189/938], Loss: 0.36750802397727966\n",
      "Validation: Epoch [19], Batch [190/938], Loss: 0.36363041400909424\n",
      "Validation: Epoch [19], Batch [191/938], Loss: 0.26601532101631165\n",
      "Validation: Epoch [19], Batch [192/938], Loss: 0.3112545907497406\n",
      "Validation: Epoch [19], Batch [193/938], Loss: 0.18109261989593506\n",
      "Validation: Epoch [19], Batch [194/938], Loss: 0.378537118434906\n",
      "Validation: Epoch [19], Batch [195/938], Loss: 0.2603537440299988\n",
      "Validation: Epoch [19], Batch [196/938], Loss: 0.4378015398979187\n",
      "Validation: Epoch [19], Batch [197/938], Loss: 0.33382701873779297\n",
      "Validation: Epoch [19], Batch [198/938], Loss: 0.4157315492630005\n",
      "Validation: Epoch [19], Batch [199/938], Loss: 0.3474924564361572\n",
      "Validation: Epoch [19], Batch [200/938], Loss: 0.5647341012954712\n",
      "Validation: Epoch [19], Batch [201/938], Loss: 0.3944360017776489\n",
      "Validation: Epoch [19], Batch [202/938], Loss: 0.44998785853385925\n",
      "Validation: Epoch [19], Batch [203/938], Loss: 0.4814434051513672\n",
      "Validation: Epoch [19], Batch [204/938], Loss: 0.42422789335250854\n",
      "Validation: Epoch [19], Batch [205/938], Loss: 0.6490113735198975\n",
      "Validation: Epoch [19], Batch [206/938], Loss: 0.43539169430732727\n",
      "Validation: Epoch [19], Batch [207/938], Loss: 0.44274652004241943\n",
      "Validation: Epoch [19], Batch [208/938], Loss: 0.3649577796459198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [209/938], Loss: 0.28163450956344604\n",
      "Validation: Epoch [19], Batch [210/938], Loss: 0.5361213088035583\n",
      "Validation: Epoch [19], Batch [211/938], Loss: 0.35343873500823975\n",
      "Validation: Epoch [19], Batch [212/938], Loss: 0.3881688714027405\n",
      "Validation: Epoch [19], Batch [213/938], Loss: 0.4198264479637146\n",
      "Validation: Epoch [19], Batch [214/938], Loss: 0.4964042007923126\n",
      "Validation: Epoch [19], Batch [215/938], Loss: 0.3866948187351227\n",
      "Validation: Epoch [19], Batch [216/938], Loss: 0.3247062563896179\n",
      "Validation: Epoch [19], Batch [217/938], Loss: 0.2906981408596039\n",
      "Validation: Epoch [19], Batch [218/938], Loss: 0.5307630300521851\n",
      "Validation: Epoch [19], Batch [219/938], Loss: 0.318178653717041\n",
      "Validation: Epoch [19], Batch [220/938], Loss: 0.3610975742340088\n",
      "Validation: Epoch [19], Batch [221/938], Loss: 0.47728362679481506\n",
      "Validation: Epoch [19], Batch [222/938], Loss: 0.5002708435058594\n",
      "Validation: Epoch [19], Batch [223/938], Loss: 0.3809565007686615\n",
      "Validation: Epoch [19], Batch [224/938], Loss: 0.3765120506286621\n",
      "Validation: Epoch [19], Batch [225/938], Loss: 0.42173394560813904\n",
      "Validation: Epoch [19], Batch [226/938], Loss: 0.32355934381484985\n",
      "Validation: Epoch [19], Batch [227/938], Loss: 0.3951631188392639\n",
      "Validation: Epoch [19], Batch [228/938], Loss: 0.28037115931510925\n",
      "Validation: Epoch [19], Batch [229/938], Loss: 0.4471820890903473\n",
      "Validation: Epoch [19], Batch [230/938], Loss: 0.36227717995643616\n",
      "Validation: Epoch [19], Batch [231/938], Loss: 0.43489977717399597\n",
      "Validation: Epoch [19], Batch [232/938], Loss: 0.3055781126022339\n",
      "Validation: Epoch [19], Batch [233/938], Loss: 0.4171307682991028\n",
      "Validation: Epoch [19], Batch [234/938], Loss: 0.35114994645118713\n",
      "Validation: Epoch [19], Batch [235/938], Loss: 0.49413448572158813\n",
      "Validation: Epoch [19], Batch [236/938], Loss: 0.26566216349601746\n",
      "Validation: Epoch [19], Batch [237/938], Loss: 0.433456152677536\n",
      "Validation: Epoch [19], Batch [238/938], Loss: 0.594234824180603\n",
      "Validation: Epoch [19], Batch [239/938], Loss: 0.32087770104408264\n",
      "Validation: Epoch [19], Batch [240/938], Loss: 0.544018030166626\n",
      "Validation: Epoch [19], Batch [241/938], Loss: 0.29474198818206787\n",
      "Validation: Epoch [19], Batch [242/938], Loss: 0.6773371696472168\n",
      "Validation: Epoch [19], Batch [243/938], Loss: 0.4598481059074402\n",
      "Validation: Epoch [19], Batch [244/938], Loss: 0.4363177418708801\n",
      "Validation: Epoch [19], Batch [245/938], Loss: 0.5227028131484985\n",
      "Validation: Epoch [19], Batch [246/938], Loss: 0.5204633474349976\n",
      "Validation: Epoch [19], Batch [247/938], Loss: 0.28636571764945984\n",
      "Validation: Epoch [19], Batch [248/938], Loss: 0.42948904633522034\n",
      "Validation: Epoch [19], Batch [249/938], Loss: 0.21597370505332947\n",
      "Validation: Epoch [19], Batch [250/938], Loss: 0.3197687268257141\n",
      "Validation: Epoch [19], Batch [251/938], Loss: 0.480141282081604\n",
      "Validation: Epoch [19], Batch [252/938], Loss: 0.3852356970310211\n",
      "Validation: Epoch [19], Batch [253/938], Loss: 0.534991443157196\n",
      "Validation: Epoch [19], Batch [254/938], Loss: 0.4225609302520752\n",
      "Validation: Epoch [19], Batch [255/938], Loss: 0.33485889434814453\n",
      "Validation: Epoch [19], Batch [256/938], Loss: 0.2847464084625244\n",
      "Validation: Epoch [19], Batch [257/938], Loss: 0.3367413282394409\n",
      "Validation: Epoch [19], Batch [258/938], Loss: 0.45226234197616577\n",
      "Validation: Epoch [19], Batch [259/938], Loss: 0.5313364267349243\n",
      "Validation: Epoch [19], Batch [260/938], Loss: 0.2351050078868866\n",
      "Validation: Epoch [19], Batch [261/938], Loss: 0.5568623542785645\n",
      "Validation: Epoch [19], Batch [262/938], Loss: 0.39376381039619446\n",
      "Validation: Epoch [19], Batch [263/938], Loss: 0.4333300292491913\n",
      "Validation: Epoch [19], Batch [264/938], Loss: 0.3345060348510742\n",
      "Validation: Epoch [19], Batch [265/938], Loss: 0.27748942375183105\n",
      "Validation: Epoch [19], Batch [266/938], Loss: 0.3469562828540802\n",
      "Validation: Epoch [19], Batch [267/938], Loss: 0.3669772148132324\n",
      "Validation: Epoch [19], Batch [268/938], Loss: 0.36759108304977417\n",
      "Validation: Epoch [19], Batch [269/938], Loss: 0.4424636662006378\n",
      "Validation: Epoch [19], Batch [270/938], Loss: 0.504388689994812\n",
      "Validation: Epoch [19], Batch [271/938], Loss: 0.46771538257598877\n",
      "Validation: Epoch [19], Batch [272/938], Loss: 0.25326845049858093\n",
      "Validation: Epoch [19], Batch [273/938], Loss: 0.39136895537376404\n",
      "Validation: Epoch [19], Batch [274/938], Loss: 0.5125489234924316\n",
      "Validation: Epoch [19], Batch [275/938], Loss: 0.4258793592453003\n",
      "Validation: Epoch [19], Batch [276/938], Loss: 0.33816033601760864\n",
      "Validation: Epoch [19], Batch [277/938], Loss: 0.42516273260116577\n",
      "Validation: Epoch [19], Batch [278/938], Loss: 0.5773817300796509\n",
      "Validation: Epoch [19], Batch [279/938], Loss: 0.340989351272583\n",
      "Validation: Epoch [19], Batch [280/938], Loss: 0.34752291440963745\n",
      "Validation: Epoch [19], Batch [281/938], Loss: 0.34890681505203247\n",
      "Validation: Epoch [19], Batch [282/938], Loss: 0.5872538685798645\n",
      "Validation: Epoch [19], Batch [283/938], Loss: 0.3946608603000641\n",
      "Validation: Epoch [19], Batch [284/938], Loss: 0.4077651798725128\n",
      "Validation: Epoch [19], Batch [285/938], Loss: 0.37219855189323425\n",
      "Validation: Epoch [19], Batch [286/938], Loss: 0.22938475012779236\n",
      "Validation: Epoch [19], Batch [287/938], Loss: 0.334780216217041\n",
      "Validation: Epoch [19], Batch [288/938], Loss: 0.34603020548820496\n",
      "Validation: Epoch [19], Batch [289/938], Loss: 0.4016930162906647\n",
      "Validation: Epoch [19], Batch [290/938], Loss: 0.5443868041038513\n",
      "Validation: Epoch [19], Batch [291/938], Loss: 0.43834155797958374\n",
      "Validation: Epoch [19], Batch [292/938], Loss: 0.3112352788448334\n",
      "Validation: Epoch [19], Batch [293/938], Loss: 0.34869247674942017\n",
      "Validation: Epoch [19], Batch [294/938], Loss: 0.23535871505737305\n",
      "Validation: Epoch [19], Batch [295/938], Loss: 0.4701809287071228\n",
      "Validation: Epoch [19], Batch [296/938], Loss: 0.6769100427627563\n",
      "Validation: Epoch [19], Batch [297/938], Loss: 0.14725741744041443\n",
      "Validation: Epoch [19], Batch [298/938], Loss: 0.27651870250701904\n",
      "Validation: Epoch [19], Batch [299/938], Loss: 0.35928839445114136\n",
      "Validation: Epoch [19], Batch [300/938], Loss: 0.5716570615768433\n",
      "Validation: Epoch [19], Batch [301/938], Loss: 0.4147190749645233\n",
      "Validation: Epoch [19], Batch [302/938], Loss: 0.5358304977416992\n",
      "Validation: Epoch [19], Batch [303/938], Loss: 0.48357272148132324\n",
      "Validation: Epoch [19], Batch [304/938], Loss: 0.4594455361366272\n",
      "Validation: Epoch [19], Batch [305/938], Loss: 0.4091097414493561\n",
      "Validation: Epoch [19], Batch [306/938], Loss: 0.3772884011268616\n",
      "Validation: Epoch [19], Batch [307/938], Loss: 0.3323928713798523\n",
      "Validation: Epoch [19], Batch [308/938], Loss: 0.4025363326072693\n",
      "Validation: Epoch [19], Batch [309/938], Loss: 0.5850521922111511\n",
      "Validation: Epoch [19], Batch [310/938], Loss: 0.32361212372779846\n",
      "Validation: Epoch [19], Batch [311/938], Loss: 0.3230701684951782\n",
      "Validation: Epoch [19], Batch [312/938], Loss: 0.6412289142608643\n",
      "Validation: Epoch [19], Batch [313/938], Loss: 0.32330217957496643\n",
      "Validation: Epoch [19], Batch [314/938], Loss: 0.27740293741226196\n",
      "Validation: Epoch [19], Batch [315/938], Loss: 0.39434266090393066\n",
      "Validation: Epoch [19], Batch [316/938], Loss: 0.3884564936161041\n",
      "Validation: Epoch [19], Batch [317/938], Loss: 0.3603367805480957\n",
      "Validation: Epoch [19], Batch [318/938], Loss: 0.4910008907318115\n",
      "Validation: Epoch [19], Batch [319/938], Loss: 0.5644460916519165\n",
      "Validation: Epoch [19], Batch [320/938], Loss: 0.3308899402618408\n",
      "Validation: Epoch [19], Batch [321/938], Loss: 0.30205124616622925\n",
      "Validation: Epoch [19], Batch [322/938], Loss: 0.23256558179855347\n",
      "Validation: Epoch [19], Batch [323/938], Loss: 0.3156481683254242\n",
      "Validation: Epoch [19], Batch [324/938], Loss: 0.5090749263763428\n",
      "Validation: Epoch [19], Batch [325/938], Loss: 0.32197105884552\n",
      "Validation: Epoch [19], Batch [326/938], Loss: 0.408512145280838\n",
      "Validation: Epoch [19], Batch [327/938], Loss: 0.41960281133651733\n",
      "Validation: Epoch [19], Batch [328/938], Loss: 0.26033592224121094\n",
      "Validation: Epoch [19], Batch [329/938], Loss: 0.3869520425796509\n",
      "Validation: Epoch [19], Batch [330/938], Loss: 0.5478018522262573\n",
      "Validation: Epoch [19], Batch [331/938], Loss: 0.530145525932312\n",
      "Validation: Epoch [19], Batch [332/938], Loss: 0.356818288564682\n",
      "Validation: Epoch [19], Batch [333/938], Loss: 0.3200916349887848\n",
      "Validation: Epoch [19], Batch [334/938], Loss: 0.4411390423774719\n",
      "Validation: Epoch [19], Batch [335/938], Loss: 0.2785549461841583\n",
      "Validation: Epoch [19], Batch [336/938], Loss: 0.2230934202671051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [337/938], Loss: 0.3253081440925598\n",
      "Validation: Epoch [19], Batch [338/938], Loss: 0.526683509349823\n",
      "Validation: Epoch [19], Batch [339/938], Loss: 0.4115891754627228\n",
      "Validation: Epoch [19], Batch [340/938], Loss: 0.47407278418540955\n",
      "Validation: Epoch [19], Batch [341/938], Loss: 0.5990177392959595\n",
      "Validation: Epoch [19], Batch [342/938], Loss: 0.40488535165786743\n",
      "Validation: Epoch [19], Batch [343/938], Loss: 0.2815723717212677\n",
      "Validation: Epoch [19], Batch [344/938], Loss: 0.3389741778373718\n",
      "Validation: Epoch [19], Batch [345/938], Loss: 0.4214748740196228\n",
      "Validation: Epoch [19], Batch [346/938], Loss: 0.6150293946266174\n",
      "Validation: Epoch [19], Batch [347/938], Loss: 0.34297552704811096\n",
      "Validation: Epoch [19], Batch [348/938], Loss: 0.49198657274246216\n",
      "Validation: Epoch [19], Batch [349/938], Loss: 0.5709913969039917\n",
      "Validation: Epoch [19], Batch [350/938], Loss: 0.33429545164108276\n",
      "Validation: Epoch [19], Batch [351/938], Loss: 0.36833271384239197\n",
      "Validation: Epoch [19], Batch [352/938], Loss: 0.42938679456710815\n",
      "Validation: Epoch [19], Batch [353/938], Loss: 0.48805442452430725\n",
      "Validation: Epoch [19], Batch [354/938], Loss: 0.4576987326145172\n",
      "Validation: Epoch [19], Batch [355/938], Loss: 0.37315720319747925\n",
      "Validation: Epoch [19], Batch [356/938], Loss: 0.30408498644828796\n",
      "Validation: Epoch [19], Batch [357/938], Loss: 0.46428182721138\n",
      "Validation: Epoch [19], Batch [358/938], Loss: 0.331300288438797\n",
      "Validation: Epoch [19], Batch [359/938], Loss: 0.48790183663368225\n",
      "Validation: Epoch [19], Batch [360/938], Loss: 0.38308268785476685\n",
      "Validation: Epoch [19], Batch [361/938], Loss: 0.4046264886856079\n",
      "Validation: Epoch [19], Batch [362/938], Loss: 0.4080444276332855\n",
      "Validation: Epoch [19], Batch [363/938], Loss: 0.1853080540895462\n",
      "Validation: Epoch [19], Batch [364/938], Loss: 0.7992637157440186\n",
      "Validation: Epoch [19], Batch [365/938], Loss: 0.374174565076828\n",
      "Validation: Epoch [19], Batch [366/938], Loss: 0.3346971869468689\n",
      "Validation: Epoch [19], Batch [367/938], Loss: 0.35477057099342346\n",
      "Validation: Epoch [19], Batch [368/938], Loss: 0.36886030435562134\n",
      "Validation: Epoch [19], Batch [369/938], Loss: 0.5006990432739258\n",
      "Validation: Epoch [19], Batch [370/938], Loss: 0.6116293668746948\n",
      "Validation: Epoch [19], Batch [371/938], Loss: 0.6154736280441284\n",
      "Validation: Epoch [19], Batch [372/938], Loss: 0.2851274907588959\n",
      "Validation: Epoch [19], Batch [373/938], Loss: 0.5094190239906311\n",
      "Validation: Epoch [19], Batch [374/938], Loss: 0.3981652855873108\n",
      "Validation: Epoch [19], Batch [375/938], Loss: 0.4491037130355835\n",
      "Validation: Epoch [19], Batch [376/938], Loss: 0.389212042093277\n",
      "Validation: Epoch [19], Batch [377/938], Loss: 0.3843500018119812\n",
      "Validation: Epoch [19], Batch [378/938], Loss: 0.3545050621032715\n",
      "Validation: Epoch [19], Batch [379/938], Loss: 0.41043466329574585\n",
      "Validation: Epoch [19], Batch [380/938], Loss: 0.3901379108428955\n",
      "Validation: Epoch [19], Batch [381/938], Loss: 0.3175802230834961\n",
      "Validation: Epoch [19], Batch [382/938], Loss: 0.5789720416069031\n",
      "Validation: Epoch [19], Batch [383/938], Loss: 0.7035133838653564\n",
      "Validation: Epoch [19], Batch [384/938], Loss: 0.4212495982646942\n",
      "Validation: Epoch [19], Batch [385/938], Loss: 0.41272979974746704\n",
      "Validation: Epoch [19], Batch [386/938], Loss: 0.4008399546146393\n",
      "Validation: Epoch [19], Batch [387/938], Loss: 0.4947124123573303\n",
      "Validation: Epoch [19], Batch [388/938], Loss: 0.30796754360198975\n",
      "Validation: Epoch [19], Batch [389/938], Loss: 0.2879672944545746\n",
      "Validation: Epoch [19], Batch [390/938], Loss: 0.45918187499046326\n",
      "Validation: Epoch [19], Batch [391/938], Loss: 0.6496732831001282\n",
      "Validation: Epoch [19], Batch [392/938], Loss: 0.2918032705783844\n",
      "Validation: Epoch [19], Batch [393/938], Loss: 0.45148926973342896\n",
      "Validation: Epoch [19], Batch [394/938], Loss: 0.5648477077484131\n",
      "Validation: Epoch [19], Batch [395/938], Loss: 0.35632628202438354\n",
      "Validation: Epoch [19], Batch [396/938], Loss: 0.37121790647506714\n",
      "Validation: Epoch [19], Batch [397/938], Loss: 0.29308125376701355\n",
      "Validation: Epoch [19], Batch [398/938], Loss: 0.3436320126056671\n",
      "Validation: Epoch [19], Batch [399/938], Loss: 0.37711000442504883\n",
      "Validation: Epoch [19], Batch [400/938], Loss: 0.28803545236587524\n",
      "Validation: Epoch [19], Batch [401/938], Loss: 0.658808708190918\n",
      "Validation: Epoch [19], Batch [402/938], Loss: 0.41665682196617126\n",
      "Validation: Epoch [19], Batch [403/938], Loss: 0.3022904396057129\n",
      "Validation: Epoch [19], Batch [404/938], Loss: 0.5599325299263\n",
      "Validation: Epoch [19], Batch [405/938], Loss: 0.4685961902141571\n",
      "Validation: Epoch [19], Batch [406/938], Loss: 0.35252243280410767\n",
      "Validation: Epoch [19], Batch [407/938], Loss: 0.3842965066432953\n",
      "Validation: Epoch [19], Batch [408/938], Loss: 0.32865840196609497\n",
      "Validation: Epoch [19], Batch [409/938], Loss: 0.31787511706352234\n",
      "Validation: Epoch [19], Batch [410/938], Loss: 0.49930310249328613\n",
      "Validation: Epoch [19], Batch [411/938], Loss: 0.4014742970466614\n",
      "Validation: Epoch [19], Batch [412/938], Loss: 0.42265909910202026\n",
      "Validation: Epoch [19], Batch [413/938], Loss: 0.30631762742996216\n",
      "Validation: Epoch [19], Batch [414/938], Loss: 0.5303919911384583\n",
      "Validation: Epoch [19], Batch [415/938], Loss: 0.43387579917907715\n",
      "Validation: Epoch [19], Batch [416/938], Loss: 0.45013153553009033\n",
      "Validation: Epoch [19], Batch [417/938], Loss: 0.32482653856277466\n",
      "Validation: Epoch [19], Batch [418/938], Loss: 0.3921216130256653\n",
      "Validation: Epoch [19], Batch [419/938], Loss: 0.43318602442741394\n",
      "Validation: Epoch [19], Batch [420/938], Loss: 0.3835452198982239\n",
      "Validation: Epoch [19], Batch [421/938], Loss: 0.30886855721473694\n",
      "Validation: Epoch [19], Batch [422/938], Loss: 0.48749905824661255\n",
      "Validation: Epoch [19], Batch [423/938], Loss: 0.46918919682502747\n",
      "Validation: Epoch [19], Batch [424/938], Loss: 0.3952459394931793\n",
      "Validation: Epoch [19], Batch [425/938], Loss: 0.4053094983100891\n",
      "Validation: Epoch [19], Batch [426/938], Loss: 0.30797529220581055\n",
      "Validation: Epoch [19], Batch [427/938], Loss: 0.5282797813415527\n",
      "Validation: Epoch [19], Batch [428/938], Loss: 0.4209793508052826\n",
      "Validation: Epoch [19], Batch [429/938], Loss: 0.4443840980529785\n",
      "Validation: Epoch [19], Batch [430/938], Loss: 0.3192431330680847\n",
      "Validation: Epoch [19], Batch [431/938], Loss: 0.31637391448020935\n",
      "Validation: Epoch [19], Batch [432/938], Loss: 0.3288412392139435\n",
      "Validation: Epoch [19], Batch [433/938], Loss: 0.4400296211242676\n",
      "Validation: Epoch [19], Batch [434/938], Loss: 0.5891101956367493\n",
      "Validation: Epoch [19], Batch [435/938], Loss: 0.5602507591247559\n",
      "Validation: Epoch [19], Batch [436/938], Loss: 0.23383477330207825\n",
      "Validation: Epoch [19], Batch [437/938], Loss: 0.4025004208087921\n",
      "Validation: Epoch [19], Batch [438/938], Loss: 0.4093307852745056\n",
      "Validation: Epoch [19], Batch [439/938], Loss: 0.34631600975990295\n",
      "Validation: Epoch [19], Batch [440/938], Loss: 0.45357275009155273\n",
      "Validation: Epoch [19], Batch [441/938], Loss: 0.36594152450561523\n",
      "Validation: Epoch [19], Batch [442/938], Loss: 0.27364087104797363\n",
      "Validation: Epoch [19], Batch [443/938], Loss: 0.2721264958381653\n",
      "Validation: Epoch [19], Batch [444/938], Loss: 0.3366202116012573\n",
      "Validation: Epoch [19], Batch [445/938], Loss: 0.35288459062576294\n",
      "Validation: Epoch [19], Batch [446/938], Loss: 0.5453251004219055\n",
      "Validation: Epoch [19], Batch [447/938], Loss: 0.2884737551212311\n",
      "Validation: Epoch [19], Batch [448/938], Loss: 0.4023703932762146\n",
      "Validation: Epoch [19], Batch [449/938], Loss: 0.40402883291244507\n",
      "Validation: Epoch [19], Batch [450/938], Loss: 0.4505784809589386\n",
      "Validation: Epoch [19], Batch [451/938], Loss: 0.47407734394073486\n",
      "Validation: Epoch [19], Batch [452/938], Loss: 0.4975796639919281\n",
      "Validation: Epoch [19], Batch [453/938], Loss: 0.4215301275253296\n",
      "Validation: Epoch [19], Batch [454/938], Loss: 0.5498707294464111\n",
      "Validation: Epoch [19], Batch [455/938], Loss: 0.5828928351402283\n",
      "Validation: Epoch [19], Batch [456/938], Loss: 0.8155462145805359\n",
      "Validation: Epoch [19], Batch [457/938], Loss: 0.4707101285457611\n",
      "Validation: Epoch [19], Batch [458/938], Loss: 0.23720112442970276\n",
      "Validation: Epoch [19], Batch [459/938], Loss: 0.46653851866722107\n",
      "Validation: Epoch [19], Batch [460/938], Loss: 0.3623333275318146\n",
      "Validation: Epoch [19], Batch [461/938], Loss: 0.3701803386211395\n",
      "Validation: Epoch [19], Batch [462/938], Loss: 0.6564352512359619\n",
      "Validation: Epoch [19], Batch [463/938], Loss: 0.453183650970459\n",
      "Validation: Epoch [19], Batch [464/938], Loss: 0.32931455969810486\n",
      "Validation: Epoch [19], Batch [465/938], Loss: 0.518758237361908\n",
      "Validation: Epoch [19], Batch [466/938], Loss: 0.4737176299095154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [467/938], Loss: 0.3598881959915161\n",
      "Validation: Epoch [19], Batch [468/938], Loss: 0.3150475025177002\n",
      "Validation: Epoch [19], Batch [469/938], Loss: 0.41378483176231384\n",
      "Validation: Epoch [19], Batch [470/938], Loss: 0.2888793349266052\n",
      "Validation: Epoch [19], Batch [471/938], Loss: 0.31948819756507874\n",
      "Validation: Epoch [19], Batch [472/938], Loss: 0.6904494762420654\n",
      "Validation: Epoch [19], Batch [473/938], Loss: 0.35940295457839966\n",
      "Validation: Epoch [19], Batch [474/938], Loss: 0.3433772623538971\n",
      "Validation: Epoch [19], Batch [475/938], Loss: 0.5595828294754028\n",
      "Validation: Epoch [19], Batch [476/938], Loss: 0.2979710102081299\n",
      "Validation: Epoch [19], Batch [477/938], Loss: 0.49054041504859924\n",
      "Validation: Epoch [19], Batch [478/938], Loss: 0.4192342758178711\n",
      "Validation: Epoch [19], Batch [479/938], Loss: 0.34484297037124634\n",
      "Validation: Epoch [19], Batch [480/938], Loss: 0.33502453565597534\n",
      "Validation: Epoch [19], Batch [481/938], Loss: 0.3408752679824829\n",
      "Validation: Epoch [19], Batch [482/938], Loss: 0.5161574482917786\n",
      "Validation: Epoch [19], Batch [483/938], Loss: 0.34351980686187744\n",
      "Validation: Epoch [19], Batch [484/938], Loss: 0.3695402443408966\n",
      "Validation: Epoch [19], Batch [485/938], Loss: 0.468982458114624\n",
      "Validation: Epoch [19], Batch [486/938], Loss: 0.4864298701286316\n",
      "Validation: Epoch [19], Batch [487/938], Loss: 0.3681551218032837\n",
      "Validation: Epoch [19], Batch [488/938], Loss: 0.3018658757209778\n",
      "Validation: Epoch [19], Batch [489/938], Loss: 0.3671700954437256\n",
      "Validation: Epoch [19], Batch [490/938], Loss: 0.2716487944126129\n",
      "Validation: Epoch [19], Batch [491/938], Loss: 0.4244847893714905\n",
      "Validation: Epoch [19], Batch [492/938], Loss: 0.4954705238342285\n",
      "Validation: Epoch [19], Batch [493/938], Loss: 0.26347798109054565\n",
      "Validation: Epoch [19], Batch [494/938], Loss: 0.36003029346466064\n",
      "Validation: Epoch [19], Batch [495/938], Loss: 0.42779427766799927\n",
      "Validation: Epoch [19], Batch [496/938], Loss: 0.4438678026199341\n",
      "Validation: Epoch [19], Batch [497/938], Loss: 0.5652320384979248\n",
      "Validation: Epoch [19], Batch [498/938], Loss: 0.38431409001350403\n",
      "Validation: Epoch [19], Batch [499/938], Loss: 0.45983174443244934\n",
      "Validation: Epoch [19], Batch [500/938], Loss: 0.29435306787490845\n",
      "Validation: Epoch [19], Batch [501/938], Loss: 0.30771172046661377\n",
      "Validation: Epoch [19], Batch [502/938], Loss: 0.49439963698387146\n",
      "Validation: Epoch [19], Batch [503/938], Loss: 0.3516032099723816\n",
      "Validation: Epoch [19], Batch [504/938], Loss: 0.3951619863510132\n",
      "Validation: Epoch [19], Batch [505/938], Loss: 0.3465358018875122\n",
      "Validation: Epoch [19], Batch [506/938], Loss: 0.4301479756832123\n",
      "Validation: Epoch [19], Batch [507/938], Loss: 0.3558349013328552\n",
      "Validation: Epoch [19], Batch [508/938], Loss: 0.23445947468280792\n",
      "Validation: Epoch [19], Batch [509/938], Loss: 0.30004745721817017\n",
      "Validation: Epoch [19], Batch [510/938], Loss: 0.5736997127532959\n",
      "Validation: Epoch [19], Batch [511/938], Loss: 0.3398565948009491\n",
      "Validation: Epoch [19], Batch [512/938], Loss: 0.341732382774353\n",
      "Validation: Epoch [19], Batch [513/938], Loss: 0.48042580485343933\n",
      "Validation: Epoch [19], Batch [514/938], Loss: 0.3582494258880615\n",
      "Validation: Epoch [19], Batch [515/938], Loss: 0.4338591396808624\n",
      "Validation: Epoch [19], Batch [516/938], Loss: 0.3861525058746338\n",
      "Validation: Epoch [19], Batch [517/938], Loss: 0.34691113233566284\n",
      "Validation: Epoch [19], Batch [518/938], Loss: 0.3465949594974518\n",
      "Validation: Epoch [19], Batch [519/938], Loss: 0.27896344661712646\n",
      "Validation: Epoch [19], Batch [520/938], Loss: 0.4116629958152771\n",
      "Validation: Epoch [19], Batch [521/938], Loss: 0.3200500011444092\n",
      "Validation: Epoch [19], Batch [522/938], Loss: 0.3285962641239166\n",
      "Validation: Epoch [19], Batch [523/938], Loss: 0.5548446774482727\n",
      "Validation: Epoch [19], Batch [524/938], Loss: 0.2576901912689209\n",
      "Validation: Epoch [19], Batch [525/938], Loss: 0.6716117858886719\n",
      "Validation: Epoch [19], Batch [526/938], Loss: 0.36487194895744324\n",
      "Validation: Epoch [19], Batch [527/938], Loss: 0.3514863848686218\n",
      "Validation: Epoch [19], Batch [528/938], Loss: 0.4494403302669525\n",
      "Validation: Epoch [19], Batch [529/938], Loss: 0.42676979303359985\n",
      "Validation: Epoch [19], Batch [530/938], Loss: 0.501679539680481\n",
      "Validation: Epoch [19], Batch [531/938], Loss: 0.37276706099510193\n",
      "Validation: Epoch [19], Batch [532/938], Loss: 0.5170265436172485\n",
      "Validation: Epoch [19], Batch [533/938], Loss: 0.4455465078353882\n",
      "Validation: Epoch [19], Batch [534/938], Loss: 0.28046053647994995\n",
      "Validation: Epoch [19], Batch [535/938], Loss: 0.2205788791179657\n",
      "Validation: Epoch [19], Batch [536/938], Loss: 0.39282459020614624\n",
      "Validation: Epoch [19], Batch [537/938], Loss: 0.5130641460418701\n",
      "Validation: Epoch [19], Batch [538/938], Loss: 0.3046601414680481\n",
      "Validation: Epoch [19], Batch [539/938], Loss: 0.45881542563438416\n",
      "Validation: Epoch [19], Batch [540/938], Loss: 0.37237733602523804\n",
      "Validation: Epoch [19], Batch [541/938], Loss: 0.2954326272010803\n",
      "Validation: Epoch [19], Batch [542/938], Loss: 0.34690892696380615\n",
      "Validation: Epoch [19], Batch [543/938], Loss: 0.4684429168701172\n",
      "Validation: Epoch [19], Batch [544/938], Loss: 0.29080092906951904\n",
      "Validation: Epoch [19], Batch [545/938], Loss: 0.62436443567276\n",
      "Validation: Epoch [19], Batch [546/938], Loss: 0.5044549703598022\n",
      "Validation: Epoch [19], Batch [547/938], Loss: 0.6134604215621948\n",
      "Validation: Epoch [19], Batch [548/938], Loss: 0.611638069152832\n",
      "Validation: Epoch [19], Batch [549/938], Loss: 0.3177596628665924\n",
      "Validation: Epoch [19], Batch [550/938], Loss: 0.4370541572570801\n",
      "Validation: Epoch [19], Batch [551/938], Loss: 0.4072664976119995\n",
      "Validation: Epoch [19], Batch [552/938], Loss: 0.35118430852890015\n",
      "Validation: Epoch [19], Batch [553/938], Loss: 0.31794363260269165\n",
      "Validation: Epoch [19], Batch [554/938], Loss: 0.44444525241851807\n",
      "Validation: Epoch [19], Batch [555/938], Loss: 0.4223529100418091\n",
      "Validation: Epoch [19], Batch [556/938], Loss: 0.6032506227493286\n",
      "Validation: Epoch [19], Batch [557/938], Loss: 0.5203072428703308\n",
      "Validation: Epoch [19], Batch [558/938], Loss: 0.3531706929206848\n",
      "Validation: Epoch [19], Batch [559/938], Loss: 0.3760761618614197\n",
      "Validation: Epoch [19], Batch [560/938], Loss: 0.39991986751556396\n",
      "Validation: Epoch [19], Batch [561/938], Loss: 0.3352355360984802\n",
      "Validation: Epoch [19], Batch [562/938], Loss: 0.40237194299697876\n",
      "Validation: Epoch [19], Batch [563/938], Loss: 0.4057096838951111\n",
      "Validation: Epoch [19], Batch [564/938], Loss: 0.4111292362213135\n",
      "Validation: Epoch [19], Batch [565/938], Loss: 0.40099361538887024\n",
      "Validation: Epoch [19], Batch [566/938], Loss: 0.5373082160949707\n",
      "Validation: Epoch [19], Batch [567/938], Loss: 0.27066999673843384\n",
      "Validation: Epoch [19], Batch [568/938], Loss: 0.48141542077064514\n",
      "Validation: Epoch [19], Batch [569/938], Loss: 0.41082313656806946\n",
      "Validation: Epoch [19], Batch [570/938], Loss: 0.26325124502182007\n",
      "Validation: Epoch [19], Batch [571/938], Loss: 0.4082227647304535\n",
      "Validation: Epoch [19], Batch [572/938], Loss: 0.4319505989551544\n",
      "Validation: Epoch [19], Batch [573/938], Loss: 0.3401144742965698\n",
      "Validation: Epoch [19], Batch [574/938], Loss: 0.44588541984558105\n",
      "Validation: Epoch [19], Batch [575/938], Loss: 0.3232352137565613\n",
      "Validation: Epoch [19], Batch [576/938], Loss: 0.3559560775756836\n",
      "Validation: Epoch [19], Batch [577/938], Loss: 0.3935272693634033\n",
      "Validation: Epoch [19], Batch [578/938], Loss: 0.5472956299781799\n",
      "Validation: Epoch [19], Batch [579/938], Loss: 0.36735764145851135\n",
      "Validation: Epoch [19], Batch [580/938], Loss: 0.21039941906929016\n",
      "Validation: Epoch [19], Batch [581/938], Loss: 0.3393326997756958\n",
      "Validation: Epoch [19], Batch [582/938], Loss: 0.540703296661377\n",
      "Validation: Epoch [19], Batch [583/938], Loss: 0.5761815309524536\n",
      "Validation: Epoch [19], Batch [584/938], Loss: 0.4319271445274353\n",
      "Validation: Epoch [19], Batch [585/938], Loss: 0.2848077118396759\n",
      "Validation: Epoch [19], Batch [586/938], Loss: 0.3483860492706299\n",
      "Validation: Epoch [19], Batch [587/938], Loss: 0.3761821389198303\n",
      "Validation: Epoch [19], Batch [588/938], Loss: 0.38726866245269775\n",
      "Validation: Epoch [19], Batch [589/938], Loss: 0.4226275682449341\n",
      "Validation: Epoch [19], Batch [590/938], Loss: 0.37343448400497437\n",
      "Validation: Epoch [19], Batch [591/938], Loss: 0.3374939262866974\n",
      "Validation: Epoch [19], Batch [592/938], Loss: 0.3407650291919708\n",
      "Validation: Epoch [19], Batch [593/938], Loss: 0.43059682846069336\n",
      "Validation: Epoch [19], Batch [594/938], Loss: 0.4341977834701538\n",
      "Validation: Epoch [19], Batch [595/938], Loss: 0.4225755035877228\n",
      "Validation: Epoch [19], Batch [596/938], Loss: 0.5260419845581055\n",
      "Validation: Epoch [19], Batch [597/938], Loss: 0.32319143414497375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [598/938], Loss: 0.35744091868400574\n",
      "Validation: Epoch [19], Batch [599/938], Loss: 0.24291327595710754\n",
      "Validation: Epoch [19], Batch [600/938], Loss: 0.46685880422592163\n",
      "Validation: Epoch [19], Batch [601/938], Loss: 0.4548671841621399\n",
      "Validation: Epoch [19], Batch [602/938], Loss: 0.7053219079971313\n",
      "Validation: Epoch [19], Batch [603/938], Loss: 0.5482448935508728\n",
      "Validation: Epoch [19], Batch [604/938], Loss: 0.40295979380607605\n",
      "Validation: Epoch [19], Batch [605/938], Loss: 0.29714253544807434\n",
      "Validation: Epoch [19], Batch [606/938], Loss: 0.2930569052696228\n",
      "Validation: Epoch [19], Batch [607/938], Loss: 0.4080270528793335\n",
      "Validation: Epoch [19], Batch [608/938], Loss: 0.30976495146751404\n",
      "Validation: Epoch [19], Batch [609/938], Loss: 0.47066885232925415\n",
      "Validation: Epoch [19], Batch [610/938], Loss: 0.4529893100261688\n",
      "Validation: Epoch [19], Batch [611/938], Loss: 0.36467504501342773\n",
      "Validation: Epoch [19], Batch [612/938], Loss: 0.6705148816108704\n",
      "Validation: Epoch [19], Batch [613/938], Loss: 0.33848053216934204\n",
      "Validation: Epoch [19], Batch [614/938], Loss: 0.5217350721359253\n",
      "Validation: Epoch [19], Batch [615/938], Loss: 0.49659648537635803\n",
      "Validation: Epoch [19], Batch [616/938], Loss: 0.35768967866897583\n",
      "Validation: Epoch [19], Batch [617/938], Loss: 0.5146276950836182\n",
      "Validation: Epoch [19], Batch [618/938], Loss: 0.3717864155769348\n",
      "Validation: Epoch [19], Batch [619/938], Loss: 0.5261749625205994\n",
      "Validation: Epoch [19], Batch [620/938], Loss: 0.4664544463157654\n",
      "Validation: Epoch [19], Batch [621/938], Loss: 0.38071709871292114\n",
      "Validation: Epoch [19], Batch [622/938], Loss: 0.4263622760772705\n",
      "Validation: Epoch [19], Batch [623/938], Loss: 0.36988648772239685\n",
      "Validation: Epoch [19], Batch [624/938], Loss: 0.29702824354171753\n",
      "Validation: Epoch [19], Batch [625/938], Loss: 0.329336553812027\n",
      "Validation: Epoch [19], Batch [626/938], Loss: 0.26232263445854187\n",
      "Validation: Epoch [19], Batch [627/938], Loss: 0.386192262172699\n",
      "Validation: Epoch [19], Batch [628/938], Loss: 0.27627378702163696\n",
      "Validation: Epoch [19], Batch [629/938], Loss: 0.3427259922027588\n",
      "Validation: Epoch [19], Batch [630/938], Loss: 0.49635860323905945\n",
      "Validation: Epoch [19], Batch [631/938], Loss: 0.4319339096546173\n",
      "Validation: Epoch [19], Batch [632/938], Loss: 0.3158162236213684\n",
      "Validation: Epoch [19], Batch [633/938], Loss: 0.44388246536254883\n",
      "Validation: Epoch [19], Batch [634/938], Loss: 0.42817553877830505\n",
      "Validation: Epoch [19], Batch [635/938], Loss: 0.4480896294116974\n",
      "Validation: Epoch [19], Batch [636/938], Loss: 0.4750857651233673\n",
      "Validation: Epoch [19], Batch [637/938], Loss: 0.40155869722366333\n",
      "Validation: Epoch [19], Batch [638/938], Loss: 0.6870112419128418\n",
      "Validation: Epoch [19], Batch [639/938], Loss: 0.4092416763305664\n",
      "Validation: Epoch [19], Batch [640/938], Loss: 0.5188800692558289\n",
      "Validation: Epoch [19], Batch [641/938], Loss: 0.6042813062667847\n",
      "Validation: Epoch [19], Batch [642/938], Loss: 0.33877670764923096\n",
      "Validation: Epoch [19], Batch [643/938], Loss: 0.3338230848312378\n",
      "Validation: Epoch [19], Batch [644/938], Loss: 0.3128778338432312\n",
      "Validation: Epoch [19], Batch [645/938], Loss: 0.4271964132785797\n",
      "Validation: Epoch [19], Batch [646/938], Loss: 0.4448079466819763\n",
      "Validation: Epoch [19], Batch [647/938], Loss: 0.2880835235118866\n",
      "Validation: Epoch [19], Batch [648/938], Loss: 0.4424223303794861\n",
      "Validation: Epoch [19], Batch [649/938], Loss: 0.5059448480606079\n",
      "Validation: Epoch [19], Batch [650/938], Loss: 0.3328348398208618\n",
      "Validation: Epoch [19], Batch [651/938], Loss: 0.45558106899261475\n",
      "Validation: Epoch [19], Batch [652/938], Loss: 0.5256324410438538\n",
      "Validation: Epoch [19], Batch [653/938], Loss: 0.493378221988678\n",
      "Validation: Epoch [19], Batch [654/938], Loss: 0.3469744026660919\n",
      "Validation: Epoch [19], Batch [655/938], Loss: 0.4246543049812317\n",
      "Validation: Epoch [19], Batch [656/938], Loss: 0.474481463432312\n",
      "Validation: Epoch [19], Batch [657/938], Loss: 0.5587838292121887\n",
      "Validation: Epoch [19], Batch [658/938], Loss: 0.35015252232551575\n",
      "Validation: Epoch [19], Batch [659/938], Loss: 0.433285117149353\n",
      "Validation: Epoch [19], Batch [660/938], Loss: 0.4859021306037903\n",
      "Validation: Epoch [19], Batch [661/938], Loss: 0.38603824377059937\n",
      "Validation: Epoch [19], Batch [662/938], Loss: 0.33052951097488403\n",
      "Validation: Epoch [19], Batch [663/938], Loss: 0.32274991273880005\n",
      "Validation: Epoch [19], Batch [664/938], Loss: 0.6051703691482544\n",
      "Validation: Epoch [19], Batch [665/938], Loss: 0.35856038331985474\n",
      "Validation: Epoch [19], Batch [666/938], Loss: 0.49739009141921997\n",
      "Validation: Epoch [19], Batch [667/938], Loss: 0.6921307444572449\n",
      "Validation: Epoch [19], Batch [668/938], Loss: 0.4732317328453064\n",
      "Validation: Epoch [19], Batch [669/938], Loss: 0.38430142402648926\n",
      "Validation: Epoch [19], Batch [670/938], Loss: 0.3966003954410553\n",
      "Validation: Epoch [19], Batch [671/938], Loss: 0.45763543248176575\n",
      "Validation: Epoch [19], Batch [672/938], Loss: 0.3480480909347534\n",
      "Validation: Epoch [19], Batch [673/938], Loss: 0.26519447565078735\n",
      "Validation: Epoch [19], Batch [674/938], Loss: 0.4269281029701233\n",
      "Validation: Epoch [19], Batch [675/938], Loss: 0.3807295858860016\n",
      "Validation: Epoch [19], Batch [676/938], Loss: 0.4575585722923279\n",
      "Validation: Epoch [19], Batch [677/938], Loss: 0.3209976553916931\n",
      "Validation: Epoch [19], Batch [678/938], Loss: 0.4839027225971222\n",
      "Validation: Epoch [19], Batch [679/938], Loss: 0.45589205622673035\n",
      "Validation: Epoch [19], Batch [680/938], Loss: 0.4996034801006317\n",
      "Validation: Epoch [19], Batch [681/938], Loss: 0.31385675072669983\n",
      "Validation: Epoch [19], Batch [682/938], Loss: 0.38958054780960083\n",
      "Validation: Epoch [19], Batch [683/938], Loss: 0.5032269954681396\n",
      "Validation: Epoch [19], Batch [684/938], Loss: 0.28616228699684143\n",
      "Validation: Epoch [19], Batch [685/938], Loss: 0.3907044231891632\n",
      "Validation: Epoch [19], Batch [686/938], Loss: 0.6518366932868958\n",
      "Validation: Epoch [19], Batch [687/938], Loss: 0.36814063787460327\n",
      "Validation: Epoch [19], Batch [688/938], Loss: 0.42533960938453674\n",
      "Validation: Epoch [19], Batch [689/938], Loss: 0.4058345556259155\n",
      "Validation: Epoch [19], Batch [690/938], Loss: 0.37001264095306396\n",
      "Validation: Epoch [19], Batch [691/938], Loss: 0.3710005283355713\n",
      "Validation: Epoch [19], Batch [692/938], Loss: 0.34069201350212097\n",
      "Validation: Epoch [19], Batch [693/938], Loss: 0.5025579929351807\n",
      "Validation: Epoch [19], Batch [694/938], Loss: 0.6624513268470764\n",
      "Validation: Epoch [19], Batch [695/938], Loss: 0.49875956773757935\n",
      "Validation: Epoch [19], Batch [696/938], Loss: 0.4039345383644104\n",
      "Validation: Epoch [19], Batch [697/938], Loss: 0.4688941240310669\n",
      "Validation: Epoch [19], Batch [698/938], Loss: 0.352743923664093\n",
      "Validation: Epoch [19], Batch [699/938], Loss: 0.41027724742889404\n",
      "Validation: Epoch [19], Batch [700/938], Loss: 0.371686726808548\n",
      "Validation: Epoch [19], Batch [701/938], Loss: 0.2832523584365845\n",
      "Validation: Epoch [19], Batch [702/938], Loss: 0.4095768332481384\n",
      "Validation: Epoch [19], Batch [703/938], Loss: 0.3962875008583069\n",
      "Validation: Epoch [19], Batch [704/938], Loss: 0.37146466970443726\n",
      "Validation: Epoch [19], Batch [705/938], Loss: 0.3679088354110718\n",
      "Validation: Epoch [19], Batch [706/938], Loss: 0.3289148807525635\n",
      "Validation: Epoch [19], Batch [707/938], Loss: 0.3010300397872925\n",
      "Validation: Epoch [19], Batch [708/938], Loss: 0.2877117693424225\n",
      "Validation: Epoch [19], Batch [709/938], Loss: 0.7357412576675415\n",
      "Validation: Epoch [19], Batch [710/938], Loss: 0.3783358335494995\n",
      "Validation: Epoch [19], Batch [711/938], Loss: 0.4099006652832031\n",
      "Validation: Epoch [19], Batch [712/938], Loss: 0.4204202890396118\n",
      "Validation: Epoch [19], Batch [713/938], Loss: 0.38023465871810913\n",
      "Validation: Epoch [19], Batch [714/938], Loss: 0.23768436908721924\n",
      "Validation: Epoch [19], Batch [715/938], Loss: 0.2752907872200012\n",
      "Validation: Epoch [19], Batch [716/938], Loss: 0.2727052569389343\n",
      "Validation: Epoch [19], Batch [717/938], Loss: 0.35788458585739136\n",
      "Validation: Epoch [19], Batch [718/938], Loss: 0.340378999710083\n",
      "Validation: Epoch [19], Batch [719/938], Loss: 0.48414626717567444\n",
      "Validation: Epoch [19], Batch [720/938], Loss: 0.36667317152023315\n",
      "Validation: Epoch [19], Batch [721/938], Loss: 0.4552183151245117\n",
      "Validation: Epoch [19], Batch [722/938], Loss: 0.4339730143547058\n",
      "Validation: Epoch [19], Batch [723/938], Loss: 0.32318583130836487\n",
      "Validation: Epoch [19], Batch [724/938], Loss: 0.5448418855667114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [725/938], Loss: 0.295009970664978\n",
      "Validation: Epoch [19], Batch [726/938], Loss: 0.5377102494239807\n",
      "Validation: Epoch [19], Batch [727/938], Loss: 0.3836899995803833\n",
      "Validation: Epoch [19], Batch [728/938], Loss: 0.25386565923690796\n",
      "Validation: Epoch [19], Batch [729/938], Loss: 0.40013495087623596\n",
      "Validation: Epoch [19], Batch [730/938], Loss: 0.4807446599006653\n",
      "Validation: Epoch [19], Batch [731/938], Loss: 0.5825055837631226\n",
      "Validation: Epoch [19], Batch [732/938], Loss: 0.4095207452774048\n",
      "Validation: Epoch [19], Batch [733/938], Loss: 0.2824314534664154\n",
      "Validation: Epoch [19], Batch [734/938], Loss: 0.20419995486736298\n",
      "Validation: Epoch [19], Batch [735/938], Loss: 0.368652880191803\n",
      "Validation: Epoch [19], Batch [736/938], Loss: 0.45745447278022766\n",
      "Validation: Epoch [19], Batch [737/938], Loss: 0.39373213052749634\n",
      "Validation: Epoch [19], Batch [738/938], Loss: 0.38842666149139404\n",
      "Validation: Epoch [19], Batch [739/938], Loss: 0.5090696811676025\n",
      "Validation: Epoch [19], Batch [740/938], Loss: 0.4396149218082428\n",
      "Validation: Epoch [19], Batch [741/938], Loss: 0.3058512806892395\n",
      "Validation: Epoch [19], Batch [742/938], Loss: 0.332747220993042\n",
      "Validation: Epoch [19], Batch [743/938], Loss: 0.41674214601516724\n",
      "Validation: Epoch [19], Batch [744/938], Loss: 0.4613739252090454\n",
      "Validation: Epoch [19], Batch [745/938], Loss: 0.38848376274108887\n",
      "Validation: Epoch [19], Batch [746/938], Loss: 0.3557969927787781\n",
      "Validation: Epoch [19], Batch [747/938], Loss: 0.3426680564880371\n",
      "Validation: Epoch [19], Batch [748/938], Loss: 0.3527011275291443\n",
      "Validation: Epoch [19], Batch [749/938], Loss: 0.45873475074768066\n",
      "Validation: Epoch [19], Batch [750/938], Loss: 0.5490758419036865\n",
      "Validation: Epoch [19], Batch [751/938], Loss: 0.372967392206192\n",
      "Validation: Epoch [19], Batch [752/938], Loss: 0.5553979873657227\n",
      "Validation: Epoch [19], Batch [753/938], Loss: 0.4521041214466095\n",
      "Validation: Epoch [19], Batch [754/938], Loss: 0.5161515474319458\n",
      "Validation: Epoch [19], Batch [755/938], Loss: 0.4461742639541626\n",
      "Validation: Epoch [19], Batch [756/938], Loss: 0.5515271425247192\n",
      "Validation: Epoch [19], Batch [757/938], Loss: 0.6230831146240234\n",
      "Validation: Epoch [19], Batch [758/938], Loss: 0.22935521602630615\n",
      "Validation: Epoch [19], Batch [759/938], Loss: 0.35855796933174133\n",
      "Validation: Epoch [19], Batch [760/938], Loss: 0.29410088062286377\n",
      "Validation: Epoch [19], Batch [761/938], Loss: 0.3219778537750244\n",
      "Validation: Epoch [19], Batch [762/938], Loss: 0.4064353108406067\n",
      "Validation: Epoch [19], Batch [763/938], Loss: 0.4863471984863281\n",
      "Validation: Epoch [19], Batch [764/938], Loss: 0.32011592388153076\n",
      "Validation: Epoch [19], Batch [765/938], Loss: 0.4560028910636902\n",
      "Validation: Epoch [19], Batch [766/938], Loss: 0.3382900059223175\n",
      "Validation: Epoch [19], Batch [767/938], Loss: 0.44100573658943176\n",
      "Validation: Epoch [19], Batch [768/938], Loss: 0.3663735091686249\n",
      "Validation: Epoch [19], Batch [769/938], Loss: 0.28345224261283875\n",
      "Validation: Epoch [19], Batch [770/938], Loss: 0.4413949251174927\n",
      "Validation: Epoch [19], Batch [771/938], Loss: 0.32134246826171875\n",
      "Validation: Epoch [19], Batch [772/938], Loss: 0.2294512689113617\n",
      "Validation: Epoch [19], Batch [773/938], Loss: 0.4874959886074066\n",
      "Validation: Epoch [19], Batch [774/938], Loss: 0.4652983546257019\n",
      "Validation: Epoch [19], Batch [775/938], Loss: 0.4730488955974579\n",
      "Validation: Epoch [19], Batch [776/938], Loss: 0.5426532030105591\n",
      "Validation: Epoch [19], Batch [777/938], Loss: 0.5115528702735901\n",
      "Validation: Epoch [19], Batch [778/938], Loss: 0.28726232051849365\n",
      "Validation: Epoch [19], Batch [779/938], Loss: 0.33804187178611755\n",
      "Validation: Epoch [19], Batch [780/938], Loss: 0.4038170576095581\n",
      "Validation: Epoch [19], Batch [781/938], Loss: 0.38204479217529297\n",
      "Validation: Epoch [19], Batch [782/938], Loss: 0.33848559856414795\n",
      "Validation: Epoch [19], Batch [783/938], Loss: 0.6045079231262207\n",
      "Validation: Epoch [19], Batch [784/938], Loss: 0.578971266746521\n",
      "Validation: Epoch [19], Batch [785/938], Loss: 0.24812066555023193\n",
      "Validation: Epoch [19], Batch [786/938], Loss: 0.29121193289756775\n",
      "Validation: Epoch [19], Batch [787/938], Loss: 0.3655157685279846\n",
      "Validation: Epoch [19], Batch [788/938], Loss: 0.4033825099468231\n",
      "Validation: Epoch [19], Batch [789/938], Loss: 0.35054725408554077\n",
      "Validation: Epoch [19], Batch [790/938], Loss: 0.3755195736885071\n",
      "Validation: Epoch [19], Batch [791/938], Loss: 0.32878100872039795\n",
      "Validation: Epoch [19], Batch [792/938], Loss: 0.3260647654533386\n",
      "Validation: Epoch [19], Batch [793/938], Loss: 0.3454838991165161\n",
      "Validation: Epoch [19], Batch [794/938], Loss: 0.3954940736293793\n",
      "Validation: Epoch [19], Batch [795/938], Loss: 0.5874565839767456\n",
      "Validation: Epoch [19], Batch [796/938], Loss: 0.4041481912136078\n",
      "Validation: Epoch [19], Batch [797/938], Loss: 0.27642345428466797\n",
      "Validation: Epoch [19], Batch [798/938], Loss: 0.34692102670669556\n",
      "Validation: Epoch [19], Batch [799/938], Loss: 0.359636127948761\n",
      "Validation: Epoch [19], Batch [800/938], Loss: 0.5218229293823242\n",
      "Validation: Epoch [19], Batch [801/938], Loss: 0.4749726951122284\n",
      "Validation: Epoch [19], Batch [802/938], Loss: 0.39763695001602173\n",
      "Validation: Epoch [19], Batch [803/938], Loss: 0.41327333450317383\n",
      "Validation: Epoch [19], Batch [804/938], Loss: 0.515271782875061\n",
      "Validation: Epoch [19], Batch [805/938], Loss: 0.2962701916694641\n",
      "Validation: Epoch [19], Batch [806/938], Loss: 0.4823265075683594\n",
      "Validation: Epoch [19], Batch [807/938], Loss: 0.26075249910354614\n",
      "Validation: Epoch [19], Batch [808/938], Loss: 0.3681698441505432\n",
      "Validation: Epoch [19], Batch [809/938], Loss: 0.34115004539489746\n",
      "Validation: Epoch [19], Batch [810/938], Loss: 0.2645660638809204\n",
      "Validation: Epoch [19], Batch [811/938], Loss: 0.5482611656188965\n",
      "Validation: Epoch [19], Batch [812/938], Loss: 0.49856895208358765\n",
      "Validation: Epoch [19], Batch [813/938], Loss: 0.47254982590675354\n",
      "Validation: Epoch [19], Batch [814/938], Loss: 0.6156767010688782\n",
      "Validation: Epoch [19], Batch [815/938], Loss: 0.3860955834388733\n",
      "Validation: Epoch [19], Batch [816/938], Loss: 0.4666324555873871\n",
      "Validation: Epoch [19], Batch [817/938], Loss: 0.495236337184906\n",
      "Validation: Epoch [19], Batch [818/938], Loss: 0.30775517225265503\n",
      "Validation: Epoch [19], Batch [819/938], Loss: 0.3431398868560791\n",
      "Validation: Epoch [19], Batch [820/938], Loss: 0.2712966501712799\n",
      "Validation: Epoch [19], Batch [821/938], Loss: 0.3335406184196472\n",
      "Validation: Epoch [19], Batch [822/938], Loss: 0.32203564047813416\n",
      "Validation: Epoch [19], Batch [823/938], Loss: 0.3415699601173401\n",
      "Validation: Epoch [19], Batch [824/938], Loss: 0.37632569670677185\n",
      "Validation: Epoch [19], Batch [825/938], Loss: 0.2997731566429138\n",
      "Validation: Epoch [19], Batch [826/938], Loss: 0.5422244071960449\n",
      "Validation: Epoch [19], Batch [827/938], Loss: 0.2895645499229431\n",
      "Validation: Epoch [19], Batch [828/938], Loss: 0.39559048414230347\n",
      "Validation: Epoch [19], Batch [829/938], Loss: 0.4116070866584778\n",
      "Validation: Epoch [19], Batch [830/938], Loss: 0.5684909820556641\n",
      "Validation: Epoch [19], Batch [831/938], Loss: 0.3340190351009369\n",
      "Validation: Epoch [19], Batch [832/938], Loss: 0.6552902460098267\n",
      "Validation: Epoch [19], Batch [833/938], Loss: 0.361215740442276\n",
      "Validation: Epoch [19], Batch [834/938], Loss: 0.5792504549026489\n",
      "Validation: Epoch [19], Batch [835/938], Loss: 0.33456212282180786\n",
      "Validation: Epoch [19], Batch [836/938], Loss: 0.3553638756275177\n",
      "Validation: Epoch [19], Batch [837/938], Loss: 0.4727441966533661\n",
      "Validation: Epoch [19], Batch [838/938], Loss: 0.2910500764846802\n",
      "Validation: Epoch [19], Batch [839/938], Loss: 0.3006912171840668\n",
      "Validation: Epoch [19], Batch [840/938], Loss: 0.36349934339523315\n",
      "Validation: Epoch [19], Batch [841/938], Loss: 0.3994656801223755\n",
      "Validation: Epoch [19], Batch [842/938], Loss: 0.5306488275527954\n",
      "Validation: Epoch [19], Batch [843/938], Loss: 0.39553701877593994\n",
      "Validation: Epoch [19], Batch [844/938], Loss: 0.47167444229125977\n",
      "Validation: Epoch [19], Batch [845/938], Loss: 0.309991717338562\n",
      "Validation: Epoch [19], Batch [846/938], Loss: 0.27050670981407166\n",
      "Validation: Epoch [19], Batch [847/938], Loss: 0.5485562086105347\n",
      "Validation: Epoch [19], Batch [848/938], Loss: 0.3138931393623352\n",
      "Validation: Epoch [19], Batch [849/938], Loss: 0.3886062204837799\n",
      "Validation: Epoch [19], Batch [850/938], Loss: 0.4946475625038147\n",
      "Validation: Epoch [19], Batch [851/938], Loss: 0.3408825993537903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [19], Batch [852/938], Loss: 0.4454483389854431\n",
      "Validation: Epoch [19], Batch [853/938], Loss: 0.4446725845336914\n",
      "Validation: Epoch [19], Batch [854/938], Loss: 0.5529430508613586\n",
      "Validation: Epoch [19], Batch [855/938], Loss: 0.4432538151741028\n",
      "Validation: Epoch [19], Batch [856/938], Loss: 0.4413193464279175\n",
      "Validation: Epoch [19], Batch [857/938], Loss: 0.5766785144805908\n",
      "Validation: Epoch [19], Batch [858/938], Loss: 0.5214521884918213\n",
      "Validation: Epoch [19], Batch [859/938], Loss: 0.2757893204689026\n",
      "Validation: Epoch [19], Batch [860/938], Loss: 0.3880274295806885\n",
      "Validation: Epoch [19], Batch [861/938], Loss: 0.31723901629447937\n",
      "Validation: Epoch [19], Batch [862/938], Loss: 0.4117923676967621\n",
      "Validation: Epoch [19], Batch [863/938], Loss: 0.3522062301635742\n",
      "Validation: Epoch [19], Batch [864/938], Loss: 0.22903504967689514\n",
      "Validation: Epoch [19], Batch [865/938], Loss: 0.35783693194389343\n",
      "Validation: Epoch [19], Batch [866/938], Loss: 0.36476969718933105\n",
      "Validation: Epoch [19], Batch [867/938], Loss: 0.4681738615036011\n",
      "Validation: Epoch [19], Batch [868/938], Loss: 0.3359566032886505\n",
      "Validation: Epoch [19], Batch [869/938], Loss: 0.5813616514205933\n",
      "Validation: Epoch [19], Batch [870/938], Loss: 0.3597305119037628\n",
      "Validation: Epoch [19], Batch [871/938], Loss: 0.40455734729766846\n",
      "Validation: Epoch [19], Batch [872/938], Loss: 0.46826091408729553\n",
      "Validation: Epoch [19], Batch [873/938], Loss: 0.38344448804855347\n",
      "Validation: Epoch [19], Batch [874/938], Loss: 0.3922516107559204\n",
      "Validation: Epoch [19], Batch [875/938], Loss: 0.25808361172676086\n",
      "Validation: Epoch [19], Batch [876/938], Loss: 0.4488142728805542\n",
      "Validation: Epoch [19], Batch [877/938], Loss: 0.4211871027946472\n",
      "Validation: Epoch [19], Batch [878/938], Loss: 0.3345777988433838\n",
      "Validation: Epoch [19], Batch [879/938], Loss: 0.2287357747554779\n",
      "Validation: Epoch [19], Batch [880/938], Loss: 0.4362414479255676\n",
      "Validation: Epoch [19], Batch [881/938], Loss: 0.6510672569274902\n",
      "Validation: Epoch [19], Batch [882/938], Loss: 0.40770629048347473\n",
      "Validation: Epoch [19], Batch [883/938], Loss: 0.5256941318511963\n",
      "Validation: Epoch [19], Batch [884/938], Loss: 0.36057841777801514\n",
      "Validation: Epoch [19], Batch [885/938], Loss: 0.3774539530277252\n",
      "Validation: Epoch [19], Batch [886/938], Loss: 0.3711918890476227\n",
      "Validation: Epoch [19], Batch [887/938], Loss: 0.39847543835639954\n",
      "Validation: Epoch [19], Batch [888/938], Loss: 0.4655049443244934\n",
      "Validation: Epoch [19], Batch [889/938], Loss: 0.4798125624656677\n",
      "Validation: Epoch [19], Batch [890/938], Loss: 0.5524056553840637\n",
      "Validation: Epoch [19], Batch [891/938], Loss: 0.639445960521698\n",
      "Validation: Epoch [19], Batch [892/938], Loss: 0.44926220178604126\n",
      "Validation: Epoch [19], Batch [893/938], Loss: 0.4716241955757141\n",
      "Validation: Epoch [19], Batch [894/938], Loss: 0.5260859131813049\n",
      "Validation: Epoch [19], Batch [895/938], Loss: 0.31405338644981384\n",
      "Validation: Epoch [19], Batch [896/938], Loss: 0.31602421402931213\n",
      "Validation: Epoch [19], Batch [897/938], Loss: 0.40295296907424927\n",
      "Validation: Epoch [19], Batch [898/938], Loss: 0.46332821249961853\n",
      "Validation: Epoch [19], Batch [899/938], Loss: 0.3939972221851349\n",
      "Validation: Epoch [19], Batch [900/938], Loss: 0.50146484375\n",
      "Validation: Epoch [19], Batch [901/938], Loss: 0.4780552089214325\n",
      "Validation: Epoch [19], Batch [902/938], Loss: 0.38704749941825867\n",
      "Validation: Epoch [19], Batch [903/938], Loss: 0.4175981283187866\n",
      "Validation: Epoch [19], Batch [904/938], Loss: 0.3333730697631836\n",
      "Validation: Epoch [19], Batch [905/938], Loss: 0.4923691153526306\n",
      "Validation: Epoch [19], Batch [906/938], Loss: 0.5171936750411987\n",
      "Validation: Epoch [19], Batch [907/938], Loss: 0.5948606133460999\n",
      "Validation: Epoch [19], Batch [908/938], Loss: 0.32407569885253906\n",
      "Validation: Epoch [19], Batch [909/938], Loss: 0.36675405502319336\n",
      "Validation: Epoch [19], Batch [910/938], Loss: 0.3204748034477234\n",
      "Validation: Epoch [19], Batch [911/938], Loss: 0.49153053760528564\n",
      "Validation: Epoch [19], Batch [912/938], Loss: 0.4910609722137451\n",
      "Validation: Epoch [19], Batch [913/938], Loss: 0.5333170890808105\n",
      "Validation: Epoch [19], Batch [914/938], Loss: 0.5268275737762451\n",
      "Validation: Epoch [19], Batch [915/938], Loss: 0.46156492829322815\n",
      "Validation: Epoch [19], Batch [916/938], Loss: 0.32516103982925415\n",
      "Validation: Epoch [19], Batch [917/938], Loss: 0.40828272700309753\n",
      "Validation: Epoch [19], Batch [918/938], Loss: 0.560100257396698\n",
      "Validation: Epoch [19], Batch [919/938], Loss: 0.2862117290496826\n",
      "Validation: Epoch [19], Batch [920/938], Loss: 0.43365973234176636\n",
      "Validation: Epoch [19], Batch [921/938], Loss: 0.3500467538833618\n",
      "Validation: Epoch [19], Batch [922/938], Loss: 0.2680087685585022\n",
      "Validation: Epoch [19], Batch [923/938], Loss: 0.28023070096969604\n",
      "Validation: Epoch [19], Batch [924/938], Loss: 0.4278903007507324\n",
      "Validation: Epoch [19], Batch [925/938], Loss: 0.3468776345252991\n",
      "Validation: Epoch [19], Batch [926/938], Loss: 0.28974786400794983\n",
      "Validation: Epoch [19], Batch [927/938], Loss: 0.39091333746910095\n",
      "Validation: Epoch [19], Batch [928/938], Loss: 0.3948235511779785\n",
      "Validation: Epoch [19], Batch [929/938], Loss: 0.36277779936790466\n",
      "Validation: Epoch [19], Batch [930/938], Loss: 0.47047775983810425\n",
      "Validation: Epoch [19], Batch [931/938], Loss: 0.2451515793800354\n",
      "Validation: Epoch [19], Batch [932/938], Loss: 0.3360618054866791\n",
      "Validation: Epoch [19], Batch [933/938], Loss: 0.37429553270339966\n",
      "Validation: Epoch [19], Batch [934/938], Loss: 0.3550106883049011\n",
      "Validation: Epoch [19], Batch [935/938], Loss: 0.473734050989151\n",
      "Validation: Epoch [19], Batch [936/938], Loss: 0.4291505813598633\n",
      "Validation: Epoch [19], Batch [937/938], Loss: 0.25444552302360535\n",
      "Validation: Epoch [19], Batch [938/938], Loss: 0.6005171537399292\n",
      "Accuracy of test set: 0.8577333333333333\n",
      "Train: Epoch [20], Batch [1/938], Loss: 0.5221894383430481\n",
      "Train: Epoch [20], Batch [2/938], Loss: 0.3458978533744812\n",
      "Train: Epoch [20], Batch [3/938], Loss: 0.4822186231613159\n",
      "Train: Epoch [20], Batch [4/938], Loss: 0.35343462228775024\n",
      "Train: Epoch [20], Batch [5/938], Loss: 0.405620276927948\n",
      "Train: Epoch [20], Batch [6/938], Loss: 0.33381569385528564\n",
      "Train: Epoch [20], Batch [7/938], Loss: 0.3874494731426239\n",
      "Train: Epoch [20], Batch [8/938], Loss: 0.4114128351211548\n",
      "Train: Epoch [20], Batch [9/938], Loss: 0.43472546339035034\n",
      "Train: Epoch [20], Batch [10/938], Loss: 0.3211182951927185\n",
      "Train: Epoch [20], Batch [11/938], Loss: 0.4323505759239197\n",
      "Train: Epoch [20], Batch [12/938], Loss: 0.4928995370864868\n",
      "Train: Epoch [20], Batch [13/938], Loss: 0.40572381019592285\n",
      "Train: Epoch [20], Batch [14/938], Loss: 0.4168938398361206\n",
      "Train: Epoch [20], Batch [15/938], Loss: 0.219108447432518\n",
      "Train: Epoch [20], Batch [16/938], Loss: 0.42556023597717285\n",
      "Train: Epoch [20], Batch [17/938], Loss: 0.49790453910827637\n",
      "Train: Epoch [20], Batch [18/938], Loss: 0.3024855852127075\n",
      "Train: Epoch [20], Batch [19/938], Loss: 0.3558698892593384\n",
      "Train: Epoch [20], Batch [20/938], Loss: 0.40607860684394836\n",
      "Train: Epoch [20], Batch [21/938], Loss: 0.40676945447921753\n",
      "Train: Epoch [20], Batch [22/938], Loss: 0.3015296161174774\n",
      "Train: Epoch [20], Batch [23/938], Loss: 0.258122980594635\n",
      "Train: Epoch [20], Batch [24/938], Loss: 0.3834923803806305\n",
      "Train: Epoch [20], Batch [25/938], Loss: 0.4256786108016968\n",
      "Train: Epoch [20], Batch [26/938], Loss: 0.33397364616394043\n",
      "Train: Epoch [20], Batch [27/938], Loss: 0.2924492657184601\n",
      "Train: Epoch [20], Batch [28/938], Loss: 0.28750425577163696\n",
      "Train: Epoch [20], Batch [29/938], Loss: 0.43982869386672974\n",
      "Train: Epoch [20], Batch [30/938], Loss: 0.4670652747154236\n",
      "Train: Epoch [20], Batch [31/938], Loss: 0.43142396211624146\n",
      "Train: Epoch [20], Batch [32/938], Loss: 0.42139750719070435\n",
      "Train: Epoch [20], Batch [33/938], Loss: 0.4310653805732727\n",
      "Train: Epoch [20], Batch [34/938], Loss: 0.4109523296356201\n",
      "Train: Epoch [20], Batch [35/938], Loss: 0.2358376383781433\n",
      "Train: Epoch [20], Batch [36/938], Loss: 0.3565303087234497\n",
      "Train: Epoch [20], Batch [37/938], Loss: 0.42199504375457764\n",
      "Train: Epoch [20], Batch [38/938], Loss: 0.33292147517204285\n",
      "Train: Epoch [20], Batch [39/938], Loss: 0.46302181482315063\n",
      "Train: Epoch [20], Batch [40/938], Loss: 0.3995288908481598\n",
      "Train: Epoch [20], Batch [41/938], Loss: 0.36225757002830505\n",
      "Train: Epoch [20], Batch [42/938], Loss: 0.5248246788978577\n",
      "Train: Epoch [20], Batch [43/938], Loss: 0.3756721019744873\n",
      "Train: Epoch [20], Batch [44/938], Loss: 0.4577893018722534\n",
      "Train: Epoch [20], Batch [45/938], Loss: 0.4740388095378876\n",
      "Train: Epoch [20], Batch [46/938], Loss: 0.35796892642974854\n",
      "Train: Epoch [20], Batch [47/938], Loss: 0.38489556312561035\n",
      "Train: Epoch [20], Batch [48/938], Loss: 0.42744871973991394\n",
      "Train: Epoch [20], Batch [49/938], Loss: 0.7441186904907227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [50/938], Loss: 0.32008692622184753\n",
      "Train: Epoch [20], Batch [51/938], Loss: 0.6278122663497925\n",
      "Train: Epoch [20], Batch [52/938], Loss: 0.30416339635849\n",
      "Train: Epoch [20], Batch [53/938], Loss: 0.4014536738395691\n",
      "Train: Epoch [20], Batch [54/938], Loss: 0.27435749769210815\n",
      "Train: Epoch [20], Batch [55/938], Loss: 0.529204249382019\n",
      "Train: Epoch [20], Batch [56/938], Loss: 0.4264621436595917\n",
      "Train: Epoch [20], Batch [57/938], Loss: 0.3682475984096527\n",
      "Train: Epoch [20], Batch [58/938], Loss: 0.26924991607666016\n",
      "Train: Epoch [20], Batch [59/938], Loss: 0.36978423595428467\n",
      "Train: Epoch [20], Batch [60/938], Loss: 0.36716902256011963\n",
      "Train: Epoch [20], Batch [61/938], Loss: 0.2883206903934479\n",
      "Train: Epoch [20], Batch [62/938], Loss: 0.5854591727256775\n",
      "Train: Epoch [20], Batch [63/938], Loss: 0.4945932924747467\n",
      "Train: Epoch [20], Batch [64/938], Loss: 0.33926790952682495\n",
      "Train: Epoch [20], Batch [65/938], Loss: 0.4833288788795471\n",
      "Train: Epoch [20], Batch [66/938], Loss: 0.4417201280593872\n",
      "Train: Epoch [20], Batch [67/938], Loss: 0.3639400601387024\n",
      "Train: Epoch [20], Batch [68/938], Loss: 0.6275190114974976\n",
      "Train: Epoch [20], Batch [69/938], Loss: 0.4789924621582031\n",
      "Train: Epoch [20], Batch [70/938], Loss: 0.3058004677295685\n",
      "Train: Epoch [20], Batch [71/938], Loss: 0.45623311400413513\n",
      "Train: Epoch [20], Batch [72/938], Loss: 0.30903559923171997\n",
      "Train: Epoch [20], Batch [73/938], Loss: 0.44953471422195435\n",
      "Train: Epoch [20], Batch [74/938], Loss: 0.5731605291366577\n",
      "Train: Epoch [20], Batch [75/938], Loss: 0.3328976631164551\n",
      "Train: Epoch [20], Batch [76/938], Loss: 0.3639824092388153\n",
      "Train: Epoch [20], Batch [77/938], Loss: 0.3333556354045868\n",
      "Train: Epoch [20], Batch [78/938], Loss: 0.3496227264404297\n",
      "Train: Epoch [20], Batch [79/938], Loss: 0.4092104732990265\n",
      "Train: Epoch [20], Batch [80/938], Loss: 0.40192246437072754\n",
      "Train: Epoch [20], Batch [81/938], Loss: 0.4060978591442108\n",
      "Train: Epoch [20], Batch [82/938], Loss: 0.43146491050720215\n",
      "Train: Epoch [20], Batch [83/938], Loss: 0.3972843885421753\n",
      "Train: Epoch [20], Batch [84/938], Loss: 0.39150384068489075\n",
      "Train: Epoch [20], Batch [85/938], Loss: 0.324850469827652\n",
      "Train: Epoch [20], Batch [86/938], Loss: 0.41389042139053345\n",
      "Train: Epoch [20], Batch [87/938], Loss: 0.352540522813797\n",
      "Train: Epoch [20], Batch [88/938], Loss: 0.385261207818985\n",
      "Train: Epoch [20], Batch [89/938], Loss: 0.3433237373828888\n",
      "Train: Epoch [20], Batch [90/938], Loss: 0.21789106726646423\n",
      "Train: Epoch [20], Batch [91/938], Loss: 0.35683488845825195\n",
      "Train: Epoch [20], Batch [92/938], Loss: 0.2899426817893982\n",
      "Train: Epoch [20], Batch [93/938], Loss: 0.5047462582588196\n",
      "Train: Epoch [20], Batch [94/938], Loss: 0.5032919645309448\n",
      "Train: Epoch [20], Batch [95/938], Loss: 0.43898889422416687\n",
      "Train: Epoch [20], Batch [96/938], Loss: 0.2759494483470917\n",
      "Train: Epoch [20], Batch [97/938], Loss: 0.4119618237018585\n",
      "Train: Epoch [20], Batch [98/938], Loss: 0.3182019293308258\n",
      "Train: Epoch [20], Batch [99/938], Loss: 0.31901416182518005\n",
      "Train: Epoch [20], Batch [100/938], Loss: 0.3156428933143616\n",
      "Train: Epoch [20], Batch [101/938], Loss: 0.3737635016441345\n",
      "Train: Epoch [20], Batch [102/938], Loss: 0.4708578586578369\n",
      "Train: Epoch [20], Batch [103/938], Loss: 0.35305023193359375\n",
      "Train: Epoch [20], Batch [104/938], Loss: 0.29532429575920105\n",
      "Train: Epoch [20], Batch [105/938], Loss: 0.34991174936294556\n",
      "Train: Epoch [20], Batch [106/938], Loss: 0.440038800239563\n",
      "Train: Epoch [20], Batch [107/938], Loss: 0.4145720899105072\n",
      "Train: Epoch [20], Batch [108/938], Loss: 0.3929487466812134\n",
      "Train: Epoch [20], Batch [109/938], Loss: 0.3536316156387329\n",
      "Train: Epoch [20], Batch [110/938], Loss: 0.35442620515823364\n",
      "Train: Epoch [20], Batch [111/938], Loss: 0.4003469944000244\n",
      "Train: Epoch [20], Batch [112/938], Loss: 0.25697949528694153\n",
      "Train: Epoch [20], Batch [113/938], Loss: 0.5925653576850891\n",
      "Train: Epoch [20], Batch [114/938], Loss: 0.6295328140258789\n",
      "Train: Epoch [20], Batch [115/938], Loss: 0.6661447286605835\n",
      "Train: Epoch [20], Batch [116/938], Loss: 0.39721816778182983\n",
      "Train: Epoch [20], Batch [117/938], Loss: 0.45248109102249146\n",
      "Train: Epoch [20], Batch [118/938], Loss: 0.4714524745941162\n",
      "Train: Epoch [20], Batch [119/938], Loss: 0.5023089647293091\n",
      "Train: Epoch [20], Batch [120/938], Loss: 0.4034383296966553\n",
      "Train: Epoch [20], Batch [121/938], Loss: 0.36628100275993347\n",
      "Train: Epoch [20], Batch [122/938], Loss: 0.2749878466129303\n",
      "Train: Epoch [20], Batch [123/938], Loss: 0.49349355697631836\n",
      "Train: Epoch [20], Batch [124/938], Loss: 0.4142719507217407\n",
      "Train: Epoch [20], Batch [125/938], Loss: 0.31263646483421326\n",
      "Train: Epoch [20], Batch [126/938], Loss: 0.5813270807266235\n",
      "Train: Epoch [20], Batch [127/938], Loss: 0.34586650133132935\n",
      "Train: Epoch [20], Batch [128/938], Loss: 0.48779571056365967\n",
      "Train: Epoch [20], Batch [129/938], Loss: 0.6235677003860474\n",
      "Train: Epoch [20], Batch [130/938], Loss: 0.47810813784599304\n",
      "Train: Epoch [20], Batch [131/938], Loss: 0.40413758158683777\n",
      "Train: Epoch [20], Batch [132/938], Loss: 0.48189085721969604\n",
      "Train: Epoch [20], Batch [133/938], Loss: 0.32765254378318787\n",
      "Train: Epoch [20], Batch [134/938], Loss: 0.5121245384216309\n",
      "Train: Epoch [20], Batch [135/938], Loss: 0.4825201630592346\n",
      "Train: Epoch [20], Batch [136/938], Loss: 0.38320958614349365\n",
      "Train: Epoch [20], Batch [137/938], Loss: 0.2709555923938751\n",
      "Train: Epoch [20], Batch [138/938], Loss: 0.33948785066604614\n",
      "Train: Epoch [20], Batch [139/938], Loss: 0.43542414903640747\n",
      "Train: Epoch [20], Batch [140/938], Loss: 0.5017696022987366\n",
      "Train: Epoch [20], Batch [141/938], Loss: 0.34851309657096863\n",
      "Train: Epoch [20], Batch [142/938], Loss: 0.49528804421424866\n",
      "Train: Epoch [20], Batch [143/938], Loss: 0.5553514957427979\n",
      "Train: Epoch [20], Batch [144/938], Loss: 0.44331520795822144\n",
      "Train: Epoch [20], Batch [145/938], Loss: 0.4742510914802551\n",
      "Train: Epoch [20], Batch [146/938], Loss: 0.4953072667121887\n",
      "Train: Epoch [20], Batch [147/938], Loss: 0.6016272306442261\n",
      "Train: Epoch [20], Batch [148/938], Loss: 0.47937440872192383\n",
      "Train: Epoch [20], Batch [149/938], Loss: 0.43875569105148315\n",
      "Train: Epoch [20], Batch [150/938], Loss: 0.3997073173522949\n",
      "Train: Epoch [20], Batch [151/938], Loss: 0.3050103783607483\n",
      "Train: Epoch [20], Batch [152/938], Loss: 0.3910014033317566\n",
      "Train: Epoch [20], Batch [153/938], Loss: 0.4599517583847046\n",
      "Train: Epoch [20], Batch [154/938], Loss: 0.3715939521789551\n",
      "Train: Epoch [20], Batch [155/938], Loss: 0.2730615735054016\n",
      "Train: Epoch [20], Batch [156/938], Loss: 0.4391191303730011\n",
      "Train: Epoch [20], Batch [157/938], Loss: 0.5675938129425049\n",
      "Train: Epoch [20], Batch [158/938], Loss: 0.3837433457374573\n",
      "Train: Epoch [20], Batch [159/938], Loss: 0.3050116300582886\n",
      "Train: Epoch [20], Batch [160/938], Loss: 0.4745514690876007\n",
      "Train: Epoch [20], Batch [161/938], Loss: 0.3713826537132263\n",
      "Train: Epoch [20], Batch [162/938], Loss: 0.29224610328674316\n",
      "Train: Epoch [20], Batch [163/938], Loss: 0.40923818945884705\n",
      "Train: Epoch [20], Batch [164/938], Loss: 0.5071806907653809\n",
      "Train: Epoch [20], Batch [165/938], Loss: 0.26026397943496704\n",
      "Train: Epoch [20], Batch [166/938], Loss: 0.30505800247192383\n",
      "Train: Epoch [20], Batch [167/938], Loss: 0.5966742038726807\n",
      "Train: Epoch [20], Batch [168/938], Loss: 0.44293853640556335\n",
      "Train: Epoch [20], Batch [169/938], Loss: 0.43575870990753174\n",
      "Train: Epoch [20], Batch [170/938], Loss: 0.4128325581550598\n",
      "Train: Epoch [20], Batch [171/938], Loss: 0.42345258593559265\n",
      "Train: Epoch [20], Batch [172/938], Loss: 0.4962961971759796\n",
      "Train: Epoch [20], Batch [173/938], Loss: 0.44459953904151917\n",
      "Train: Epoch [20], Batch [174/938], Loss: 0.560745894908905\n",
      "Train: Epoch [20], Batch [175/938], Loss: 0.594174861907959\n",
      "Train: Epoch [20], Batch [176/938], Loss: 0.36715567111968994\n",
      "Train: Epoch [20], Batch [177/938], Loss: 0.44375938177108765\n",
      "Train: Epoch [20], Batch [178/938], Loss: 0.463867723941803\n",
      "Train: Epoch [20], Batch [179/938], Loss: 0.5066731572151184\n",
      "Train: Epoch [20], Batch [180/938], Loss: 0.3150908648967743\n",
      "Train: Epoch [20], Batch [181/938], Loss: 0.3266865611076355\n",
      "Train: Epoch [20], Batch [182/938], Loss: 0.48892202973365784\n",
      "Train: Epoch [20], Batch [183/938], Loss: 0.6840866208076477\n",
      "Train: Epoch [20], Batch [184/938], Loss: 0.2513048052787781\n",
      "Train: Epoch [20], Batch [185/938], Loss: 0.3530436158180237\n",
      "Train: Epoch [20], Batch [186/938], Loss: 0.31033068895339966\n",
      "Train: Epoch [20], Batch [187/938], Loss: 0.3304467797279358\n",
      "Train: Epoch [20], Batch [188/938], Loss: 0.37985050678253174\n",
      "Train: Epoch [20], Batch [189/938], Loss: 0.2641538977622986\n",
      "Train: Epoch [20], Batch [190/938], Loss: 0.4851566553115845\n",
      "Train: Epoch [20], Batch [191/938], Loss: 0.3166103959083557\n",
      "Train: Epoch [20], Batch [192/938], Loss: 0.4804576337337494\n",
      "Train: Epoch [20], Batch [193/938], Loss: 0.5152149200439453\n",
      "Train: Epoch [20], Batch [194/938], Loss: 0.39020806550979614\n",
      "Train: Epoch [20], Batch [195/938], Loss: 0.5056949853897095\n",
      "Train: Epoch [20], Batch [196/938], Loss: 0.29115617275238037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [197/938], Loss: 0.27251365780830383\n",
      "Train: Epoch [20], Batch [198/938], Loss: 0.3522167205810547\n",
      "Train: Epoch [20], Batch [199/938], Loss: 0.4175655245780945\n",
      "Train: Epoch [20], Batch [200/938], Loss: 0.5019826889038086\n",
      "Train: Epoch [20], Batch [201/938], Loss: 0.4413292706012726\n",
      "Train: Epoch [20], Batch [202/938], Loss: 0.29915592074394226\n",
      "Train: Epoch [20], Batch [203/938], Loss: 0.31142717599868774\n",
      "Train: Epoch [20], Batch [204/938], Loss: 0.2942335307598114\n",
      "Train: Epoch [20], Batch [205/938], Loss: 0.44818150997161865\n",
      "Train: Epoch [20], Batch [206/938], Loss: 0.386648952960968\n",
      "Train: Epoch [20], Batch [207/938], Loss: 0.47068214416503906\n",
      "Train: Epoch [20], Batch [208/938], Loss: 0.30769234895706177\n",
      "Train: Epoch [20], Batch [209/938], Loss: 0.41326290369033813\n",
      "Train: Epoch [20], Batch [210/938], Loss: 0.5149633884429932\n",
      "Train: Epoch [20], Batch [211/938], Loss: 0.4484097361564636\n",
      "Train: Epoch [20], Batch [212/938], Loss: 0.33742600679397583\n",
      "Train: Epoch [20], Batch [213/938], Loss: 0.5086002349853516\n",
      "Train: Epoch [20], Batch [214/938], Loss: 0.23742717504501343\n",
      "Train: Epoch [20], Batch [215/938], Loss: 0.30903977155685425\n",
      "Train: Epoch [20], Batch [216/938], Loss: 0.46254515647888184\n",
      "Train: Epoch [20], Batch [217/938], Loss: 0.4151073098182678\n",
      "Train: Epoch [20], Batch [218/938], Loss: 0.31343525648117065\n",
      "Train: Epoch [20], Batch [219/938], Loss: 0.44929787516593933\n",
      "Train: Epoch [20], Batch [220/938], Loss: 0.3197184205055237\n",
      "Train: Epoch [20], Batch [221/938], Loss: 0.4180704355239868\n",
      "Train: Epoch [20], Batch [222/938], Loss: 0.6664970517158508\n",
      "Train: Epoch [20], Batch [223/938], Loss: 0.6198920011520386\n",
      "Train: Epoch [20], Batch [224/938], Loss: 0.39718157052993774\n",
      "Train: Epoch [20], Batch [225/938], Loss: 0.44508084654808044\n",
      "Train: Epoch [20], Batch [226/938], Loss: 0.4885154068470001\n",
      "Train: Epoch [20], Batch [227/938], Loss: 0.34125977754592896\n",
      "Train: Epoch [20], Batch [228/938], Loss: 0.30065006017684937\n",
      "Train: Epoch [20], Batch [229/938], Loss: 0.47911158204078674\n",
      "Train: Epoch [20], Batch [230/938], Loss: 0.4597567617893219\n",
      "Train: Epoch [20], Batch [231/938], Loss: 0.29673928022384644\n",
      "Train: Epoch [20], Batch [232/938], Loss: 0.33791565895080566\n",
      "Train: Epoch [20], Batch [233/938], Loss: 0.36489740014076233\n",
      "Train: Epoch [20], Batch [234/938], Loss: 0.5597629547119141\n",
      "Train: Epoch [20], Batch [235/938], Loss: 0.32699668407440186\n",
      "Train: Epoch [20], Batch [236/938], Loss: 0.31133559346199036\n",
      "Train: Epoch [20], Batch [237/938], Loss: 0.6361273527145386\n",
      "Train: Epoch [20], Batch [238/938], Loss: 0.32167860865592957\n",
      "Train: Epoch [20], Batch [239/938], Loss: 0.4032406806945801\n",
      "Train: Epoch [20], Batch [240/938], Loss: 0.30253300070762634\n",
      "Train: Epoch [20], Batch [241/938], Loss: 0.3584176301956177\n",
      "Train: Epoch [20], Batch [242/938], Loss: 0.3297474980354309\n",
      "Train: Epoch [20], Batch [243/938], Loss: 0.2725951373577118\n",
      "Train: Epoch [20], Batch [244/938], Loss: 0.3575880527496338\n",
      "Train: Epoch [20], Batch [245/938], Loss: 0.2991337180137634\n",
      "Train: Epoch [20], Batch [246/938], Loss: 0.38878363370895386\n",
      "Train: Epoch [20], Batch [247/938], Loss: 0.4249681830406189\n",
      "Train: Epoch [20], Batch [248/938], Loss: 0.5381914377212524\n",
      "Train: Epoch [20], Batch [249/938], Loss: 0.37382882833480835\n",
      "Train: Epoch [20], Batch [250/938], Loss: 0.4645744562149048\n",
      "Train: Epoch [20], Batch [251/938], Loss: 0.3394800126552582\n",
      "Train: Epoch [20], Batch [252/938], Loss: 0.46218493580818176\n",
      "Train: Epoch [20], Batch [253/938], Loss: 0.35229456424713135\n",
      "Train: Epoch [20], Batch [254/938], Loss: 0.2776743769645691\n",
      "Train: Epoch [20], Batch [255/938], Loss: 0.21636070311069489\n",
      "Train: Epoch [20], Batch [256/938], Loss: 0.3170190453529358\n",
      "Train: Epoch [20], Batch [257/938], Loss: 0.2841542661190033\n",
      "Train: Epoch [20], Batch [258/938], Loss: 0.4848959147930145\n",
      "Train: Epoch [20], Batch [259/938], Loss: 0.32556667923927307\n",
      "Train: Epoch [20], Batch [260/938], Loss: 0.4665805697441101\n",
      "Train: Epoch [20], Batch [261/938], Loss: 0.43381959199905396\n",
      "Train: Epoch [20], Batch [262/938], Loss: 0.3207159638404846\n",
      "Train: Epoch [20], Batch [263/938], Loss: 0.3746148645877838\n",
      "Train: Epoch [20], Batch [264/938], Loss: 0.323112428188324\n",
      "Train: Epoch [20], Batch [265/938], Loss: 0.5151522159576416\n",
      "Train: Epoch [20], Batch [266/938], Loss: 0.41374337673187256\n",
      "Train: Epoch [20], Batch [267/938], Loss: 0.37178048491477966\n",
      "Train: Epoch [20], Batch [268/938], Loss: 0.4610024094581604\n",
      "Train: Epoch [20], Batch [269/938], Loss: 0.3359835743904114\n",
      "Train: Epoch [20], Batch [270/938], Loss: 0.3525232672691345\n",
      "Train: Epoch [20], Batch [271/938], Loss: 0.44275981187820435\n",
      "Train: Epoch [20], Batch [272/938], Loss: 0.3643510341644287\n",
      "Train: Epoch [20], Batch [273/938], Loss: 0.5364384651184082\n",
      "Train: Epoch [20], Batch [274/938], Loss: 0.4033673405647278\n",
      "Train: Epoch [20], Batch [275/938], Loss: 0.37518930435180664\n",
      "Train: Epoch [20], Batch [276/938], Loss: 0.5727195739746094\n",
      "Train: Epoch [20], Batch [277/938], Loss: 0.43773606419563293\n",
      "Train: Epoch [20], Batch [278/938], Loss: 0.4824048578739166\n",
      "Train: Epoch [20], Batch [279/938], Loss: 0.2800266742706299\n",
      "Train: Epoch [20], Batch [280/938], Loss: 0.4713160991668701\n",
      "Train: Epoch [20], Batch [281/938], Loss: 0.30782538652420044\n",
      "Train: Epoch [20], Batch [282/938], Loss: 0.3743968904018402\n",
      "Train: Epoch [20], Batch [283/938], Loss: 0.4504055380821228\n",
      "Train: Epoch [20], Batch [284/938], Loss: 0.38907381892204285\n",
      "Train: Epoch [20], Batch [285/938], Loss: 0.241064190864563\n",
      "Train: Epoch [20], Batch [286/938], Loss: 0.46705108880996704\n",
      "Train: Epoch [20], Batch [287/938], Loss: 0.4044387638568878\n",
      "Train: Epoch [20], Batch [288/938], Loss: 0.3755451440811157\n",
      "Train: Epoch [20], Batch [289/938], Loss: 0.3157654106616974\n",
      "Train: Epoch [20], Batch [290/938], Loss: 0.8282943964004517\n",
      "Train: Epoch [20], Batch [291/938], Loss: 0.3883912265300751\n",
      "Train: Epoch [20], Batch [292/938], Loss: 0.37303125858306885\n",
      "Train: Epoch [20], Batch [293/938], Loss: 0.5072184205055237\n",
      "Train: Epoch [20], Batch [294/938], Loss: 0.40519285202026367\n",
      "Train: Epoch [20], Batch [295/938], Loss: 0.5353520512580872\n",
      "Train: Epoch [20], Batch [296/938], Loss: 0.5567196011543274\n",
      "Train: Epoch [20], Batch [297/938], Loss: 0.5398614406585693\n",
      "Train: Epoch [20], Batch [298/938], Loss: 0.27601882815361023\n",
      "Train: Epoch [20], Batch [299/938], Loss: 0.44650983810424805\n",
      "Train: Epoch [20], Batch [300/938], Loss: 0.4306472837924957\n",
      "Train: Epoch [20], Batch [301/938], Loss: 0.41775327920913696\n",
      "Train: Epoch [20], Batch [302/938], Loss: 0.3395060896873474\n",
      "Train: Epoch [20], Batch [303/938], Loss: 0.4718608260154724\n",
      "Train: Epoch [20], Batch [304/938], Loss: 0.41637712717056274\n",
      "Train: Epoch [20], Batch [305/938], Loss: 0.33783602714538574\n",
      "Train: Epoch [20], Batch [306/938], Loss: 0.2385524958372116\n",
      "Train: Epoch [20], Batch [307/938], Loss: 0.22478298842906952\n",
      "Train: Epoch [20], Batch [308/938], Loss: 0.2037666141986847\n",
      "Train: Epoch [20], Batch [309/938], Loss: 0.7016522288322449\n",
      "Train: Epoch [20], Batch [310/938], Loss: 0.31901371479034424\n",
      "Train: Epoch [20], Batch [311/938], Loss: 0.48786622285842896\n",
      "Train: Epoch [20], Batch [312/938], Loss: 0.4439411163330078\n",
      "Train: Epoch [20], Batch [313/938], Loss: 0.476767361164093\n",
      "Train: Epoch [20], Batch [314/938], Loss: 0.3767503499984741\n",
      "Train: Epoch [20], Batch [315/938], Loss: 0.44022336602211\n",
      "Train: Epoch [20], Batch [316/938], Loss: 0.38632625341415405\n",
      "Train: Epoch [20], Batch [317/938], Loss: 0.43382424116134644\n",
      "Train: Epoch [20], Batch [318/938], Loss: 0.32664328813552856\n",
      "Train: Epoch [20], Batch [319/938], Loss: 0.5575778484344482\n",
      "Train: Epoch [20], Batch [320/938], Loss: 0.4280930757522583\n",
      "Train: Epoch [20], Batch [321/938], Loss: 0.5075416564941406\n",
      "Train: Epoch [20], Batch [322/938], Loss: 0.2872251272201538\n",
      "Train: Epoch [20], Batch [323/938], Loss: 0.3809362053871155\n",
      "Train: Epoch [20], Batch [324/938], Loss: 0.5480537414550781\n",
      "Train: Epoch [20], Batch [325/938], Loss: 0.3743504285812378\n",
      "Train: Epoch [20], Batch [326/938], Loss: 0.43561267852783203\n",
      "Train: Epoch [20], Batch [327/938], Loss: 0.4289420247077942\n",
      "Train: Epoch [20], Batch [328/938], Loss: 0.37501800060272217\n",
      "Train: Epoch [20], Batch [329/938], Loss: 0.46475929021835327\n",
      "Train: Epoch [20], Batch [330/938], Loss: 0.32344192266464233\n",
      "Train: Epoch [20], Batch [331/938], Loss: 0.25072383880615234\n",
      "Train: Epoch [20], Batch [332/938], Loss: 0.4695010185241699\n",
      "Train: Epoch [20], Batch [333/938], Loss: 0.5137274861335754\n",
      "Train: Epoch [20], Batch [334/938], Loss: 0.48702818155288696\n",
      "Train: Epoch [20], Batch [335/938], Loss: 0.3961561918258667\n",
      "Train: Epoch [20], Batch [336/938], Loss: 0.6272099018096924\n",
      "Train: Epoch [20], Batch [337/938], Loss: 0.6896476149559021\n",
      "Train: Epoch [20], Batch [338/938], Loss: 0.4747559428215027\n",
      "Train: Epoch [20], Batch [339/938], Loss: 0.4348158836364746\n",
      "Train: Epoch [20], Batch [340/938], Loss: 0.45728936791419983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [341/938], Loss: 0.37945955991744995\n",
      "Train: Epoch [20], Batch [342/938], Loss: 0.2787647843360901\n",
      "Train: Epoch [20], Batch [343/938], Loss: 0.5680469274520874\n",
      "Train: Epoch [20], Batch [344/938], Loss: 0.5198147296905518\n",
      "Train: Epoch [20], Batch [345/938], Loss: 0.47217875719070435\n",
      "Train: Epoch [20], Batch [346/938], Loss: 0.43059229850769043\n",
      "Train: Epoch [20], Batch [347/938], Loss: 0.41710036993026733\n",
      "Train: Epoch [20], Batch [348/938], Loss: 0.5600124001502991\n",
      "Train: Epoch [20], Batch [349/938], Loss: 0.38232937455177307\n",
      "Train: Epoch [20], Batch [350/938], Loss: 0.3037736117839813\n",
      "Train: Epoch [20], Batch [351/938], Loss: 0.6073302030563354\n",
      "Train: Epoch [20], Batch [352/938], Loss: 0.3114627003669739\n",
      "Train: Epoch [20], Batch [353/938], Loss: 0.37092283368110657\n",
      "Train: Epoch [20], Batch [354/938], Loss: 0.40342044830322266\n",
      "Train: Epoch [20], Batch [355/938], Loss: 0.7043316960334778\n",
      "Train: Epoch [20], Batch [356/938], Loss: 0.4232400059700012\n",
      "Train: Epoch [20], Batch [357/938], Loss: 0.42410045862197876\n",
      "Train: Epoch [20], Batch [358/938], Loss: 0.3032487630844116\n",
      "Train: Epoch [20], Batch [359/938], Loss: 0.4496758282184601\n",
      "Train: Epoch [20], Batch [360/938], Loss: 0.23768135905265808\n",
      "Train: Epoch [20], Batch [361/938], Loss: 0.44456273317337036\n",
      "Train: Epoch [20], Batch [362/938], Loss: 0.4275909662246704\n",
      "Train: Epoch [20], Batch [363/938], Loss: 0.395306795835495\n",
      "Train: Epoch [20], Batch [364/938], Loss: 0.3664206862449646\n",
      "Train: Epoch [20], Batch [365/938], Loss: 0.34181803464889526\n",
      "Train: Epoch [20], Batch [366/938], Loss: 0.6389854550361633\n",
      "Train: Epoch [20], Batch [367/938], Loss: 0.2813801169395447\n",
      "Train: Epoch [20], Batch [368/938], Loss: 0.3516257405281067\n",
      "Train: Epoch [20], Batch [369/938], Loss: 0.3813691735267639\n",
      "Train: Epoch [20], Batch [370/938], Loss: 0.259462833404541\n",
      "Train: Epoch [20], Batch [371/938], Loss: 0.3129710257053375\n",
      "Train: Epoch [20], Batch [372/938], Loss: 0.45998018980026245\n",
      "Train: Epoch [20], Batch [373/938], Loss: 0.38820815086364746\n",
      "Train: Epoch [20], Batch [374/938], Loss: 0.5699474811553955\n",
      "Train: Epoch [20], Batch [375/938], Loss: 0.25165706872940063\n",
      "Train: Epoch [20], Batch [376/938], Loss: 0.3612712323665619\n",
      "Train: Epoch [20], Batch [377/938], Loss: 0.27559614181518555\n",
      "Train: Epoch [20], Batch [378/938], Loss: 0.41271454095840454\n",
      "Train: Epoch [20], Batch [379/938], Loss: 0.34946054220199585\n",
      "Train: Epoch [20], Batch [380/938], Loss: 0.3856089115142822\n",
      "Train: Epoch [20], Batch [381/938], Loss: 0.3225434124469757\n",
      "Train: Epoch [20], Batch [382/938], Loss: 0.44119614362716675\n",
      "Train: Epoch [20], Batch [383/938], Loss: 0.29306602478027344\n",
      "Train: Epoch [20], Batch [384/938], Loss: 0.5651239156723022\n",
      "Train: Epoch [20], Batch [385/938], Loss: 0.3733801245689392\n",
      "Train: Epoch [20], Batch [386/938], Loss: 0.36778926849365234\n",
      "Train: Epoch [20], Batch [387/938], Loss: 0.30859506130218506\n",
      "Train: Epoch [20], Batch [388/938], Loss: 0.43136119842529297\n",
      "Train: Epoch [20], Batch [389/938], Loss: 0.41358301043510437\n",
      "Train: Epoch [20], Batch [390/938], Loss: 0.26425886154174805\n",
      "Train: Epoch [20], Batch [391/938], Loss: 0.4233602285385132\n",
      "Train: Epoch [20], Batch [392/938], Loss: 0.5093825459480286\n",
      "Train: Epoch [20], Batch [393/938], Loss: 0.46673792600631714\n",
      "Train: Epoch [20], Batch [394/938], Loss: 0.2723642587661743\n",
      "Train: Epoch [20], Batch [395/938], Loss: 0.2763740122318268\n",
      "Train: Epoch [20], Batch [396/938], Loss: 0.398823618888855\n",
      "Train: Epoch [20], Batch [397/938], Loss: 0.4201357066631317\n",
      "Train: Epoch [20], Batch [398/938], Loss: 0.3408224880695343\n",
      "Train: Epoch [20], Batch [399/938], Loss: 0.35385072231292725\n",
      "Train: Epoch [20], Batch [400/938], Loss: 0.43446606397628784\n",
      "Train: Epoch [20], Batch [401/938], Loss: 0.4744594991207123\n",
      "Train: Epoch [20], Batch [402/938], Loss: 0.31399208307266235\n",
      "Train: Epoch [20], Batch [403/938], Loss: 0.35830727219581604\n",
      "Train: Epoch [20], Batch [404/938], Loss: 0.3567155599594116\n",
      "Train: Epoch [20], Batch [405/938], Loss: 0.5474193096160889\n",
      "Train: Epoch [20], Batch [406/938], Loss: 0.21698734164237976\n",
      "Train: Epoch [20], Batch [407/938], Loss: 0.29419630765914917\n",
      "Train: Epoch [20], Batch [408/938], Loss: 0.3565450608730316\n",
      "Train: Epoch [20], Batch [409/938], Loss: 0.3809947371482849\n",
      "Train: Epoch [20], Batch [410/938], Loss: 0.7113045454025269\n",
      "Train: Epoch [20], Batch [411/938], Loss: 0.5711948871612549\n",
      "Train: Epoch [20], Batch [412/938], Loss: 0.5704459547996521\n",
      "Train: Epoch [20], Batch [413/938], Loss: 0.40907278656959534\n",
      "Train: Epoch [20], Batch [414/938], Loss: 0.5295346975326538\n",
      "Train: Epoch [20], Batch [415/938], Loss: 0.40936392545700073\n",
      "Train: Epoch [20], Batch [416/938], Loss: 0.4634294807910919\n",
      "Train: Epoch [20], Batch [417/938], Loss: 0.5400785207748413\n",
      "Train: Epoch [20], Batch [418/938], Loss: 0.35514095425605774\n",
      "Train: Epoch [20], Batch [419/938], Loss: 0.39084160327911377\n",
      "Train: Epoch [20], Batch [420/938], Loss: 0.4560078978538513\n",
      "Train: Epoch [20], Batch [421/938], Loss: 0.2993190884590149\n",
      "Train: Epoch [20], Batch [422/938], Loss: 0.5034538507461548\n",
      "Train: Epoch [20], Batch [423/938], Loss: 0.343193382024765\n",
      "Train: Epoch [20], Batch [424/938], Loss: 0.30691617727279663\n",
      "Train: Epoch [20], Batch [425/938], Loss: 0.3777272403240204\n",
      "Train: Epoch [20], Batch [426/938], Loss: 0.305900901556015\n",
      "Train: Epoch [20], Batch [427/938], Loss: 0.35044652223587036\n",
      "Train: Epoch [20], Batch [428/938], Loss: 0.3796302378177643\n",
      "Train: Epoch [20], Batch [429/938], Loss: 0.5739110112190247\n",
      "Train: Epoch [20], Batch [430/938], Loss: 0.3038887679576874\n",
      "Train: Epoch [20], Batch [431/938], Loss: 0.49963295459747314\n",
      "Train: Epoch [20], Batch [432/938], Loss: 0.37932562828063965\n",
      "Train: Epoch [20], Batch [433/938], Loss: 0.49021124839782715\n",
      "Train: Epoch [20], Batch [434/938], Loss: 0.3771739900112152\n",
      "Train: Epoch [20], Batch [435/938], Loss: 0.6254866123199463\n",
      "Train: Epoch [20], Batch [436/938], Loss: 0.36417821049690247\n",
      "Train: Epoch [20], Batch [437/938], Loss: 0.22803489863872528\n",
      "Train: Epoch [20], Batch [438/938], Loss: 0.3229216933250427\n",
      "Train: Epoch [20], Batch [439/938], Loss: 0.36124134063720703\n",
      "Train: Epoch [20], Batch [440/938], Loss: 0.5437635183334351\n",
      "Train: Epoch [20], Batch [441/938], Loss: 0.381570041179657\n",
      "Train: Epoch [20], Batch [442/938], Loss: 0.4320847988128662\n",
      "Train: Epoch [20], Batch [443/938], Loss: 0.41582247614860535\n",
      "Train: Epoch [20], Batch [444/938], Loss: 0.6290873289108276\n",
      "Train: Epoch [20], Batch [445/938], Loss: 0.7433987855911255\n",
      "Train: Epoch [20], Batch [446/938], Loss: 0.5021258592605591\n",
      "Train: Epoch [20], Batch [447/938], Loss: 0.4654862582683563\n",
      "Train: Epoch [20], Batch [448/938], Loss: 0.2284093201160431\n",
      "Train: Epoch [20], Batch [449/938], Loss: 0.3418334126472473\n",
      "Train: Epoch [20], Batch [450/938], Loss: 0.3854907155036926\n",
      "Train: Epoch [20], Batch [451/938], Loss: 0.41191592812538147\n",
      "Train: Epoch [20], Batch [452/938], Loss: 0.3195644021034241\n",
      "Train: Epoch [20], Batch [453/938], Loss: 0.4748542010784149\n",
      "Train: Epoch [20], Batch [454/938], Loss: 0.4762466251850128\n",
      "Train: Epoch [20], Batch [455/938], Loss: 0.4554515480995178\n",
      "Train: Epoch [20], Batch [456/938], Loss: 0.3737551271915436\n",
      "Train: Epoch [20], Batch [457/938], Loss: 0.3585941791534424\n",
      "Train: Epoch [20], Batch [458/938], Loss: 0.47400403022766113\n",
      "Train: Epoch [20], Batch [459/938], Loss: 0.40042561292648315\n",
      "Train: Epoch [20], Batch [460/938], Loss: 0.5892881751060486\n",
      "Train: Epoch [20], Batch [461/938], Loss: 0.7801230549812317\n",
      "Train: Epoch [20], Batch [462/938], Loss: 0.5943396091461182\n",
      "Train: Epoch [20], Batch [463/938], Loss: 0.523769199848175\n",
      "Train: Epoch [20], Batch [464/938], Loss: 0.2847941517829895\n",
      "Train: Epoch [20], Batch [465/938], Loss: 0.32090699672698975\n",
      "Train: Epoch [20], Batch [466/938], Loss: 0.5607137680053711\n",
      "Train: Epoch [20], Batch [467/938], Loss: 0.29354172945022583\n",
      "Train: Epoch [20], Batch [468/938], Loss: 0.43024468421936035\n",
      "Train: Epoch [20], Batch [469/938], Loss: 0.7657997608184814\n",
      "Train: Epoch [20], Batch [470/938], Loss: 0.30009907484054565\n",
      "Train: Epoch [20], Batch [471/938], Loss: 0.2900063991546631\n",
      "Train: Epoch [20], Batch [472/938], Loss: 0.3592291474342346\n",
      "Train: Epoch [20], Batch [473/938], Loss: 0.4097781777381897\n",
      "Train: Epoch [20], Batch [474/938], Loss: 0.4671623408794403\n",
      "Train: Epoch [20], Batch [475/938], Loss: 0.3332345485687256\n",
      "Train: Epoch [20], Batch [476/938], Loss: 0.43734192848205566\n",
      "Train: Epoch [20], Batch [477/938], Loss: 0.43272385001182556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [478/938], Loss: 0.4307442903518677\n",
      "Train: Epoch [20], Batch [479/938], Loss: 0.5620251297950745\n",
      "Train: Epoch [20], Batch [480/938], Loss: 0.3937220573425293\n",
      "Train: Epoch [20], Batch [481/938], Loss: 0.3367154002189636\n",
      "Train: Epoch [20], Batch [482/938], Loss: 0.357028067111969\n",
      "Train: Epoch [20], Batch [483/938], Loss: 0.468502938747406\n",
      "Train: Epoch [20], Batch [484/938], Loss: 0.3323554992675781\n",
      "Train: Epoch [20], Batch [485/938], Loss: 0.3579074740409851\n",
      "Train: Epoch [20], Batch [486/938], Loss: 0.39922523498535156\n",
      "Train: Epoch [20], Batch [487/938], Loss: 0.3676111400127411\n",
      "Train: Epoch [20], Batch [488/938], Loss: 0.4270642399787903\n",
      "Train: Epoch [20], Batch [489/938], Loss: 0.2612890899181366\n",
      "Train: Epoch [20], Batch [490/938], Loss: 0.3749935030937195\n",
      "Train: Epoch [20], Batch [491/938], Loss: 0.4415292739868164\n",
      "Train: Epoch [20], Batch [492/938], Loss: 0.5647655725479126\n",
      "Train: Epoch [20], Batch [493/938], Loss: 0.5349239110946655\n",
      "Train: Epoch [20], Batch [494/938], Loss: 0.464252233505249\n",
      "Train: Epoch [20], Batch [495/938], Loss: 0.6688693761825562\n",
      "Train: Epoch [20], Batch [496/938], Loss: 0.6181092858314514\n",
      "Train: Epoch [20], Batch [497/938], Loss: 0.45670390129089355\n",
      "Train: Epoch [20], Batch [498/938], Loss: 0.4330695867538452\n",
      "Train: Epoch [20], Batch [499/938], Loss: 0.5241271257400513\n",
      "Train: Epoch [20], Batch [500/938], Loss: 0.5304980874061584\n",
      "Train: Epoch [20], Batch [501/938], Loss: 0.3116704225540161\n",
      "Train: Epoch [20], Batch [502/938], Loss: 0.3896368741989136\n",
      "Train: Epoch [20], Batch [503/938], Loss: 0.29953157901763916\n",
      "Train: Epoch [20], Batch [504/938], Loss: 0.6704434156417847\n",
      "Train: Epoch [20], Batch [505/938], Loss: 0.27255356311798096\n",
      "Train: Epoch [20], Batch [506/938], Loss: 0.3624921441078186\n",
      "Train: Epoch [20], Batch [507/938], Loss: 0.32703033089637756\n",
      "Train: Epoch [20], Batch [508/938], Loss: 0.507691502571106\n",
      "Train: Epoch [20], Batch [509/938], Loss: 0.5344252586364746\n",
      "Train: Epoch [20], Batch [510/938], Loss: 0.5117244720458984\n",
      "Train: Epoch [20], Batch [511/938], Loss: 0.36282843351364136\n",
      "Train: Epoch [20], Batch [512/938], Loss: 0.4055179953575134\n",
      "Train: Epoch [20], Batch [513/938], Loss: 0.34712475538253784\n",
      "Train: Epoch [20], Batch [514/938], Loss: 0.44760727882385254\n",
      "Train: Epoch [20], Batch [515/938], Loss: 0.37648600339889526\n",
      "Train: Epoch [20], Batch [516/938], Loss: 0.3745834231376648\n",
      "Train: Epoch [20], Batch [517/938], Loss: 0.4562854766845703\n",
      "Train: Epoch [20], Batch [518/938], Loss: 0.6445808410644531\n",
      "Train: Epoch [20], Batch [519/938], Loss: 0.39237526059150696\n",
      "Train: Epoch [20], Batch [520/938], Loss: 0.3585210144519806\n",
      "Train: Epoch [20], Batch [521/938], Loss: 0.4133839011192322\n",
      "Train: Epoch [20], Batch [522/938], Loss: 0.37523993849754333\n",
      "Train: Epoch [20], Batch [523/938], Loss: 0.45767447352409363\n",
      "Train: Epoch [20], Batch [524/938], Loss: 0.3700829744338989\n",
      "Train: Epoch [20], Batch [525/938], Loss: 0.3124381899833679\n",
      "Train: Epoch [20], Batch [526/938], Loss: 0.2997089624404907\n",
      "Train: Epoch [20], Batch [527/938], Loss: 0.6008402109146118\n",
      "Train: Epoch [20], Batch [528/938], Loss: 0.2647532820701599\n",
      "Train: Epoch [20], Batch [529/938], Loss: 0.5239680409431458\n",
      "Train: Epoch [20], Batch [530/938], Loss: 0.4309728145599365\n",
      "Train: Epoch [20], Batch [531/938], Loss: 0.42643943428993225\n",
      "Train: Epoch [20], Batch [532/938], Loss: 0.3207578659057617\n",
      "Train: Epoch [20], Batch [533/938], Loss: 0.32093456387519836\n",
      "Train: Epoch [20], Batch [534/938], Loss: 0.25421467423439026\n",
      "Train: Epoch [20], Batch [535/938], Loss: 0.46714726090431213\n",
      "Train: Epoch [20], Batch [536/938], Loss: 0.22525176405906677\n",
      "Train: Epoch [20], Batch [537/938], Loss: 0.34517261385917664\n",
      "Train: Epoch [20], Batch [538/938], Loss: 0.35465356707572937\n",
      "Train: Epoch [20], Batch [539/938], Loss: 0.3835185766220093\n",
      "Train: Epoch [20], Batch [540/938], Loss: 0.30655744671821594\n",
      "Train: Epoch [20], Batch [541/938], Loss: 0.25594326853752136\n",
      "Train: Epoch [20], Batch [542/938], Loss: 0.5065089464187622\n",
      "Train: Epoch [20], Batch [543/938], Loss: 0.4329555332660675\n",
      "Train: Epoch [20], Batch [544/938], Loss: 0.2408977895975113\n",
      "Train: Epoch [20], Batch [545/938], Loss: 0.2721700370311737\n",
      "Train: Epoch [20], Batch [546/938], Loss: 0.20593121647834778\n",
      "Train: Epoch [20], Batch [547/938], Loss: 0.35308682918548584\n",
      "Train: Epoch [20], Batch [548/938], Loss: 0.5419353246688843\n",
      "Train: Epoch [20], Batch [549/938], Loss: 0.6625035405158997\n",
      "Train: Epoch [20], Batch [550/938], Loss: 0.3650170564651489\n",
      "Train: Epoch [20], Batch [551/938], Loss: 0.37573081254959106\n",
      "Train: Epoch [20], Batch [552/938], Loss: 0.5413471460342407\n",
      "Train: Epoch [20], Batch [553/938], Loss: 0.3442932069301605\n",
      "Train: Epoch [20], Batch [554/938], Loss: 0.6002825498580933\n",
      "Train: Epoch [20], Batch [555/938], Loss: 0.4822686016559601\n",
      "Train: Epoch [20], Batch [556/938], Loss: 0.49045583605766296\n",
      "Train: Epoch [20], Batch [557/938], Loss: 0.44409263134002686\n",
      "Train: Epoch [20], Batch [558/938], Loss: 0.40358543395996094\n",
      "Train: Epoch [20], Batch [559/938], Loss: 0.3465885519981384\n",
      "Train: Epoch [20], Batch [560/938], Loss: 0.3605640232563019\n",
      "Train: Epoch [20], Batch [561/938], Loss: 0.4445464611053467\n",
      "Train: Epoch [20], Batch [562/938], Loss: 0.41583251953125\n",
      "Train: Epoch [20], Batch [563/938], Loss: 0.45951831340789795\n",
      "Train: Epoch [20], Batch [564/938], Loss: 0.38745296001434326\n",
      "Train: Epoch [20], Batch [565/938], Loss: 0.40472373366355896\n",
      "Train: Epoch [20], Batch [566/938], Loss: 0.30132511258125305\n",
      "Train: Epoch [20], Batch [567/938], Loss: 0.23807501792907715\n",
      "Train: Epoch [20], Batch [568/938], Loss: 0.26814839243888855\n",
      "Train: Epoch [20], Batch [569/938], Loss: 0.38296762108802795\n",
      "Train: Epoch [20], Batch [570/938], Loss: 0.44630295038223267\n",
      "Train: Epoch [20], Batch [571/938], Loss: 0.5074208974838257\n",
      "Train: Epoch [20], Batch [572/938], Loss: 0.33846503496170044\n",
      "Train: Epoch [20], Batch [573/938], Loss: 0.3173726499080658\n",
      "Train: Epoch [20], Batch [574/938], Loss: 0.19911982119083405\n",
      "Train: Epoch [20], Batch [575/938], Loss: 0.3846941590309143\n",
      "Train: Epoch [20], Batch [576/938], Loss: 0.44613662362098694\n",
      "Train: Epoch [20], Batch [577/938], Loss: 0.2973998785018921\n",
      "Train: Epoch [20], Batch [578/938], Loss: 0.47988080978393555\n",
      "Train: Epoch [20], Batch [579/938], Loss: 0.4258444607257843\n",
      "Train: Epoch [20], Batch [580/938], Loss: 0.2760531008243561\n",
      "Train: Epoch [20], Batch [581/938], Loss: 0.39856091141700745\n",
      "Train: Epoch [20], Batch [582/938], Loss: 0.6404255628585815\n",
      "Train: Epoch [20], Batch [583/938], Loss: 0.5619345307350159\n",
      "Train: Epoch [20], Batch [584/938], Loss: 0.4778907299041748\n",
      "Train: Epoch [20], Batch [585/938], Loss: 0.18341869115829468\n",
      "Train: Epoch [20], Batch [586/938], Loss: 0.4374929964542389\n",
      "Train: Epoch [20], Batch [587/938], Loss: 0.4472960829734802\n",
      "Train: Epoch [20], Batch [588/938], Loss: 0.25342971086502075\n",
      "Train: Epoch [20], Batch [589/938], Loss: 0.4932575523853302\n",
      "Train: Epoch [20], Batch [590/938], Loss: 0.3421843647956848\n",
      "Train: Epoch [20], Batch [591/938], Loss: 0.24803921580314636\n",
      "Train: Epoch [20], Batch [592/938], Loss: 0.518562912940979\n",
      "Train: Epoch [20], Batch [593/938], Loss: 0.4871675968170166\n",
      "Train: Epoch [20], Batch [594/938], Loss: 0.4614773988723755\n",
      "Train: Epoch [20], Batch [595/938], Loss: 0.3520768880844116\n",
      "Train: Epoch [20], Batch [596/938], Loss: 0.4738248586654663\n",
      "Train: Epoch [20], Batch [597/938], Loss: 0.321000874042511\n",
      "Train: Epoch [20], Batch [598/938], Loss: 0.3977651000022888\n",
      "Train: Epoch [20], Batch [599/938], Loss: 0.4181240200996399\n",
      "Train: Epoch [20], Batch [600/938], Loss: 0.308401495218277\n",
      "Train: Epoch [20], Batch [601/938], Loss: 0.36460602283477783\n",
      "Train: Epoch [20], Batch [602/938], Loss: 0.5110732316970825\n",
      "Train: Epoch [20], Batch [603/938], Loss: 0.4056059420108795\n",
      "Train: Epoch [20], Batch [604/938], Loss: 0.5943465232849121\n",
      "Train: Epoch [20], Batch [605/938], Loss: 0.4023989737033844\n",
      "Train: Epoch [20], Batch [606/938], Loss: 0.5228177309036255\n",
      "Train: Epoch [20], Batch [607/938], Loss: 0.39087027311325073\n",
      "Train: Epoch [20], Batch [608/938], Loss: 0.29817354679107666\n",
      "Train: Epoch [20], Batch [609/938], Loss: 0.4350300431251526\n",
      "Train: Epoch [20], Batch [610/938], Loss: 0.36899465322494507\n",
      "Train: Epoch [20], Batch [611/938], Loss: 0.5065683126449585\n",
      "Train: Epoch [20], Batch [612/938], Loss: 0.3656173050403595\n",
      "Train: Epoch [20], Batch [613/938], Loss: 0.3350369930267334\n",
      "Train: Epoch [20], Batch [614/938], Loss: 0.7735426425933838\n",
      "Train: Epoch [20], Batch [615/938], Loss: 0.3107449412345886\n",
      "Train: Epoch [20], Batch [616/938], Loss: 0.7289416790008545\n",
      "Train: Epoch [20], Batch [617/938], Loss: 0.5317724943161011\n",
      "Train: Epoch [20], Batch [618/938], Loss: 0.37414857745170593\n",
      "Train: Epoch [20], Batch [619/938], Loss: 0.509490966796875\n",
      "Train: Epoch [20], Batch [620/938], Loss: 0.1928628534078598\n",
      "Train: Epoch [20], Batch [621/938], Loss: 0.494361013174057\n",
      "Train: Epoch [20], Batch [622/938], Loss: 0.2254943698644638\n",
      "Train: Epoch [20], Batch [623/938], Loss: 0.42410561442375183\n",
      "Train: Epoch [20], Batch [624/938], Loss: 0.35095763206481934\n",
      "Train: Epoch [20], Batch [625/938], Loss: 0.7071430683135986\n",
      "Train: Epoch [20], Batch [626/938], Loss: 0.4694095253944397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [627/938], Loss: 0.3789950907230377\n",
      "Train: Epoch [20], Batch [628/938], Loss: 0.6737002730369568\n",
      "Train: Epoch [20], Batch [629/938], Loss: 0.36613044142723083\n",
      "Train: Epoch [20], Batch [630/938], Loss: 0.5158650875091553\n",
      "Train: Epoch [20], Batch [631/938], Loss: 0.3779318928718567\n",
      "Train: Epoch [20], Batch [632/938], Loss: 0.28968703746795654\n",
      "Train: Epoch [20], Batch [633/938], Loss: 0.4867706596851349\n",
      "Train: Epoch [20], Batch [634/938], Loss: 0.5268217325210571\n",
      "Train: Epoch [20], Batch [635/938], Loss: 0.19150258600711823\n",
      "Train: Epoch [20], Batch [636/938], Loss: 0.6406757831573486\n",
      "Train: Epoch [20], Batch [637/938], Loss: 0.45947837829589844\n",
      "Train: Epoch [20], Batch [638/938], Loss: 0.7493960857391357\n",
      "Train: Epoch [20], Batch [639/938], Loss: 0.3259221613407135\n",
      "Train: Epoch [20], Batch [640/938], Loss: 0.3689950108528137\n",
      "Train: Epoch [20], Batch [641/938], Loss: 0.46756863594055176\n",
      "Train: Epoch [20], Batch [642/938], Loss: 0.3646565079689026\n",
      "Train: Epoch [20], Batch [643/938], Loss: 0.46020156145095825\n",
      "Train: Epoch [20], Batch [644/938], Loss: 0.5299549698829651\n",
      "Train: Epoch [20], Batch [645/938], Loss: 0.36332273483276367\n",
      "Train: Epoch [20], Batch [646/938], Loss: 0.48448649048805237\n",
      "Train: Epoch [20], Batch [647/938], Loss: 0.4347420632839203\n",
      "Train: Epoch [20], Batch [648/938], Loss: 0.4503459334373474\n",
      "Train: Epoch [20], Batch [649/938], Loss: 0.42264312505722046\n",
      "Train: Epoch [20], Batch [650/938], Loss: 0.3329978883266449\n",
      "Train: Epoch [20], Batch [651/938], Loss: 0.526913583278656\n",
      "Train: Epoch [20], Batch [652/938], Loss: 0.4689800441265106\n",
      "Train: Epoch [20], Batch [653/938], Loss: 0.4758439362049103\n",
      "Train: Epoch [20], Batch [654/938], Loss: 0.48734351992607117\n",
      "Train: Epoch [20], Batch [655/938], Loss: 0.4557979702949524\n",
      "Train: Epoch [20], Batch [656/938], Loss: 0.4472225606441498\n",
      "Train: Epoch [20], Batch [657/938], Loss: 0.3044416308403015\n",
      "Train: Epoch [20], Batch [658/938], Loss: 0.4114501476287842\n",
      "Train: Epoch [20], Batch [659/938], Loss: 0.3707510530948639\n",
      "Train: Epoch [20], Batch [660/938], Loss: 0.5773543119430542\n",
      "Train: Epoch [20], Batch [661/938], Loss: 0.38023874163627625\n",
      "Train: Epoch [20], Batch [662/938], Loss: 0.4776420295238495\n",
      "Train: Epoch [20], Batch [663/938], Loss: 0.4657216966152191\n",
      "Train: Epoch [20], Batch [664/938], Loss: 0.3631777763366699\n",
      "Train: Epoch [20], Batch [665/938], Loss: 0.4246971011161804\n",
      "Train: Epoch [20], Batch [666/938], Loss: 0.28228625655174255\n",
      "Train: Epoch [20], Batch [667/938], Loss: 0.40610387921333313\n",
      "Train: Epoch [20], Batch [668/938], Loss: 0.3814288377761841\n",
      "Train: Epoch [20], Batch [669/938], Loss: 0.4471542239189148\n",
      "Train: Epoch [20], Batch [670/938], Loss: 0.5800443887710571\n",
      "Train: Epoch [20], Batch [671/938], Loss: 0.4079446792602539\n",
      "Train: Epoch [20], Batch [672/938], Loss: 0.33979105949401855\n",
      "Train: Epoch [20], Batch [673/938], Loss: 0.3863047659397125\n",
      "Train: Epoch [20], Batch [674/938], Loss: 0.2806486189365387\n",
      "Train: Epoch [20], Batch [675/938], Loss: 0.38296937942504883\n",
      "Train: Epoch [20], Batch [676/938], Loss: 0.6203113198280334\n",
      "Train: Epoch [20], Batch [677/938], Loss: 0.5628238916397095\n",
      "Train: Epoch [20], Batch [678/938], Loss: 0.5375216603279114\n",
      "Train: Epoch [20], Batch [679/938], Loss: 0.6044825315475464\n",
      "Train: Epoch [20], Batch [680/938], Loss: 0.3691638708114624\n",
      "Train: Epoch [20], Batch [681/938], Loss: 0.2881050407886505\n",
      "Train: Epoch [20], Batch [682/938], Loss: 0.4326097369194031\n",
      "Train: Epoch [20], Batch [683/938], Loss: 0.35686829686164856\n",
      "Train: Epoch [20], Batch [684/938], Loss: 0.5154560804367065\n",
      "Train: Epoch [20], Batch [685/938], Loss: 0.5367431640625\n",
      "Train: Epoch [20], Batch [686/938], Loss: 0.41987496614456177\n",
      "Train: Epoch [20], Batch [687/938], Loss: 0.4963386654853821\n",
      "Train: Epoch [20], Batch [688/938], Loss: 0.5182600617408752\n",
      "Train: Epoch [20], Batch [689/938], Loss: 0.34407737851142883\n",
      "Train: Epoch [20], Batch [690/938], Loss: 0.34652963280677795\n",
      "Train: Epoch [20], Batch [691/938], Loss: 0.28118646144866943\n",
      "Train: Epoch [20], Batch [692/938], Loss: 0.25780510902404785\n",
      "Train: Epoch [20], Batch [693/938], Loss: 0.3425357937812805\n",
      "Train: Epoch [20], Batch [694/938], Loss: 0.22458919882774353\n",
      "Train: Epoch [20], Batch [695/938], Loss: 0.24108785390853882\n",
      "Train: Epoch [20], Batch [696/938], Loss: 0.23563987016677856\n",
      "Train: Epoch [20], Batch [697/938], Loss: 0.36057764291763306\n",
      "Train: Epoch [20], Batch [698/938], Loss: 0.4904490113258362\n",
      "Train: Epoch [20], Batch [699/938], Loss: 0.27924320101737976\n",
      "Train: Epoch [20], Batch [700/938], Loss: 0.40623268485069275\n",
      "Train: Epoch [20], Batch [701/938], Loss: 0.3239861726760864\n",
      "Train: Epoch [20], Batch [702/938], Loss: 0.3109036684036255\n",
      "Train: Epoch [20], Batch [703/938], Loss: 0.4554091989994049\n",
      "Train: Epoch [20], Batch [704/938], Loss: 0.32703229784965515\n",
      "Train: Epoch [20], Batch [705/938], Loss: 0.37968915700912476\n",
      "Train: Epoch [20], Batch [706/938], Loss: 0.36895453929901123\n",
      "Train: Epoch [20], Batch [707/938], Loss: 0.4933921694755554\n",
      "Train: Epoch [20], Batch [708/938], Loss: 0.40822160243988037\n",
      "Train: Epoch [20], Batch [709/938], Loss: 0.6492886543273926\n",
      "Train: Epoch [20], Batch [710/938], Loss: 0.4207397997379303\n",
      "Train: Epoch [20], Batch [711/938], Loss: 0.5483675003051758\n",
      "Train: Epoch [20], Batch [712/938], Loss: 0.3916320204734802\n",
      "Train: Epoch [20], Batch [713/938], Loss: 0.40774720907211304\n",
      "Train: Epoch [20], Batch [714/938], Loss: 0.23195719718933105\n",
      "Train: Epoch [20], Batch [715/938], Loss: 0.443702757358551\n",
      "Train: Epoch [20], Batch [716/938], Loss: 0.4872252345085144\n",
      "Train: Epoch [20], Batch [717/938], Loss: 0.3779655396938324\n",
      "Train: Epoch [20], Batch [718/938], Loss: 0.2931608557701111\n",
      "Train: Epoch [20], Batch [719/938], Loss: 0.44089025259017944\n",
      "Train: Epoch [20], Batch [720/938], Loss: 0.48166853189468384\n",
      "Train: Epoch [20], Batch [721/938], Loss: 0.37788426876068115\n",
      "Train: Epoch [20], Batch [722/938], Loss: 0.39648735523223877\n",
      "Train: Epoch [20], Batch [723/938], Loss: 0.26639941334724426\n",
      "Train: Epoch [20], Batch [724/938], Loss: 0.43863487243652344\n",
      "Train: Epoch [20], Batch [725/938], Loss: 0.3791353106498718\n",
      "Train: Epoch [20], Batch [726/938], Loss: 0.41885775327682495\n",
      "Train: Epoch [20], Batch [727/938], Loss: 0.4426301121711731\n",
      "Train: Epoch [20], Batch [728/938], Loss: 0.29541873931884766\n",
      "Train: Epoch [20], Batch [729/938], Loss: 0.3977518677711487\n",
      "Train: Epoch [20], Batch [730/938], Loss: 0.35792869329452515\n",
      "Train: Epoch [20], Batch [731/938], Loss: 0.4068523049354553\n",
      "Train: Epoch [20], Batch [732/938], Loss: 0.44963324069976807\n",
      "Train: Epoch [20], Batch [733/938], Loss: 0.3855001628398895\n",
      "Train: Epoch [20], Batch [734/938], Loss: 0.4278855323791504\n",
      "Train: Epoch [20], Batch [735/938], Loss: 0.5656396150588989\n",
      "Train: Epoch [20], Batch [736/938], Loss: 0.38704735040664673\n",
      "Train: Epoch [20], Batch [737/938], Loss: 0.425209641456604\n",
      "Train: Epoch [20], Batch [738/938], Loss: 0.4398820400238037\n",
      "Train: Epoch [20], Batch [739/938], Loss: 0.3414454162120819\n",
      "Train: Epoch [20], Batch [740/938], Loss: 0.2920539379119873\n",
      "Train: Epoch [20], Batch [741/938], Loss: 0.5750733017921448\n",
      "Train: Epoch [20], Batch [742/938], Loss: 0.3827226459980011\n",
      "Train: Epoch [20], Batch [743/938], Loss: 0.3731672763824463\n",
      "Train: Epoch [20], Batch [744/938], Loss: 0.37036117911338806\n",
      "Train: Epoch [20], Batch [745/938], Loss: 0.43369460105895996\n",
      "Train: Epoch [20], Batch [746/938], Loss: 0.5479252934455872\n",
      "Train: Epoch [20], Batch [747/938], Loss: 0.4966859519481659\n",
      "Train: Epoch [20], Batch [748/938], Loss: 0.4418866038322449\n",
      "Train: Epoch [20], Batch [749/938], Loss: 0.6157853007316589\n",
      "Train: Epoch [20], Batch [750/938], Loss: 0.2434307187795639\n",
      "Train: Epoch [20], Batch [751/938], Loss: 0.3372010290622711\n",
      "Train: Epoch [20], Batch [752/938], Loss: 0.5990773439407349\n",
      "Train: Epoch [20], Batch [753/938], Loss: 0.34511443972587585\n",
      "Train: Epoch [20], Batch [754/938], Loss: 0.41198813915252686\n",
      "Train: Epoch [20], Batch [755/938], Loss: 0.3981255888938904\n",
      "Train: Epoch [20], Batch [756/938], Loss: 0.31428390741348267\n",
      "Train: Epoch [20], Batch [757/938], Loss: 0.4259340763092041\n",
      "Train: Epoch [20], Batch [758/938], Loss: 0.37735715508461\n",
      "Train: Epoch [20], Batch [759/938], Loss: 0.5878872871398926\n",
      "Train: Epoch [20], Batch [760/938], Loss: 0.4601534307003021\n",
      "Train: Epoch [20], Batch [761/938], Loss: 0.5518558025360107\n",
      "Train: Epoch [20], Batch [762/938], Loss: 0.3782217502593994\n",
      "Train: Epoch [20], Batch [763/938], Loss: 0.4108104705810547\n",
      "Train: Epoch [20], Batch [764/938], Loss: 0.2885480225086212\n",
      "Train: Epoch [20], Batch [765/938], Loss: 0.32771220803260803\n",
      "Train: Epoch [20], Batch [766/938], Loss: 0.536173403263092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [767/938], Loss: 0.5110585689544678\n",
      "Train: Epoch [20], Batch [768/938], Loss: 0.3160771429538727\n",
      "Train: Epoch [20], Batch [769/938], Loss: 0.5748698711395264\n",
      "Train: Epoch [20], Batch [770/938], Loss: 0.5859394669532776\n",
      "Train: Epoch [20], Batch [771/938], Loss: 0.506524920463562\n",
      "Train: Epoch [20], Batch [772/938], Loss: 0.36376720666885376\n",
      "Train: Epoch [20], Batch [773/938], Loss: 0.21569529175758362\n",
      "Train: Epoch [20], Batch [774/938], Loss: 0.33614227175712585\n",
      "Train: Epoch [20], Batch [775/938], Loss: 0.36916905641555786\n",
      "Train: Epoch [20], Batch [776/938], Loss: 0.4011208117008209\n",
      "Train: Epoch [20], Batch [777/938], Loss: 0.3687777519226074\n",
      "Train: Epoch [20], Batch [778/938], Loss: 0.5716292858123779\n",
      "Train: Epoch [20], Batch [779/938], Loss: 0.34631264209747314\n",
      "Train: Epoch [20], Batch [780/938], Loss: 0.37123143672943115\n",
      "Train: Epoch [20], Batch [781/938], Loss: 0.35232779383659363\n",
      "Train: Epoch [20], Batch [782/938], Loss: 0.2796996235847473\n",
      "Train: Epoch [20], Batch [783/938], Loss: 0.3645288646221161\n",
      "Train: Epoch [20], Batch [784/938], Loss: 0.4610516130924225\n",
      "Train: Epoch [20], Batch [785/938], Loss: 0.4128265082836151\n",
      "Train: Epoch [20], Batch [786/938], Loss: 0.4424951374530792\n",
      "Train: Epoch [20], Batch [787/938], Loss: 0.3697279095649719\n",
      "Train: Epoch [20], Batch [788/938], Loss: 0.2181597203016281\n",
      "Train: Epoch [20], Batch [789/938], Loss: 0.41962915658950806\n",
      "Train: Epoch [20], Batch [790/938], Loss: 0.27893269062042236\n",
      "Train: Epoch [20], Batch [791/938], Loss: 0.43760764598846436\n",
      "Train: Epoch [20], Batch [792/938], Loss: 0.4752606749534607\n",
      "Train: Epoch [20], Batch [793/938], Loss: 0.5002813935279846\n",
      "Train: Epoch [20], Batch [794/938], Loss: 0.7383856177330017\n",
      "Train: Epoch [20], Batch [795/938], Loss: 0.4597623348236084\n",
      "Train: Epoch [20], Batch [796/938], Loss: 0.4262012541294098\n",
      "Train: Epoch [20], Batch [797/938], Loss: 0.4666360020637512\n",
      "Train: Epoch [20], Batch [798/938], Loss: 0.2963944673538208\n",
      "Train: Epoch [20], Batch [799/938], Loss: 0.5093675851821899\n",
      "Train: Epoch [20], Batch [800/938], Loss: 0.34367984533309937\n",
      "Train: Epoch [20], Batch [801/938], Loss: 0.36776697635650635\n",
      "Train: Epoch [20], Batch [802/938], Loss: 0.4574144780635834\n",
      "Train: Epoch [20], Batch [803/938], Loss: 0.3553077280521393\n",
      "Train: Epoch [20], Batch [804/938], Loss: 0.37746942043304443\n",
      "Train: Epoch [20], Batch [805/938], Loss: 0.5808038711547852\n",
      "Train: Epoch [20], Batch [806/938], Loss: 0.4639504551887512\n",
      "Train: Epoch [20], Batch [807/938], Loss: 0.31791290640830994\n",
      "Train: Epoch [20], Batch [808/938], Loss: 0.5704953074455261\n",
      "Train: Epoch [20], Batch [809/938], Loss: 0.5462590456008911\n",
      "Train: Epoch [20], Batch [810/938], Loss: 0.34236449003219604\n",
      "Train: Epoch [20], Batch [811/938], Loss: 0.19449689984321594\n",
      "Train: Epoch [20], Batch [812/938], Loss: 0.33052191138267517\n",
      "Train: Epoch [20], Batch [813/938], Loss: 0.3917590379714966\n",
      "Train: Epoch [20], Batch [814/938], Loss: 0.402543306350708\n",
      "Train: Epoch [20], Batch [815/938], Loss: 0.3107268214225769\n",
      "Train: Epoch [20], Batch [816/938], Loss: 0.3378908634185791\n",
      "Train: Epoch [20], Batch [817/938], Loss: 0.338879257440567\n",
      "Train: Epoch [20], Batch [818/938], Loss: 0.3607706129550934\n",
      "Train: Epoch [20], Batch [819/938], Loss: 0.3477814197540283\n",
      "Train: Epoch [20], Batch [820/938], Loss: 0.3123610019683838\n",
      "Train: Epoch [20], Batch [821/938], Loss: 0.3825299143791199\n",
      "Train: Epoch [20], Batch [822/938], Loss: 0.43710264563560486\n",
      "Train: Epoch [20], Batch [823/938], Loss: 0.5796694159507751\n",
      "Train: Epoch [20], Batch [824/938], Loss: 0.2756364643573761\n",
      "Train: Epoch [20], Batch [825/938], Loss: 0.3201717138290405\n",
      "Train: Epoch [20], Batch [826/938], Loss: 0.5278855562210083\n",
      "Train: Epoch [20], Batch [827/938], Loss: 0.3276281952857971\n",
      "Train: Epoch [20], Batch [828/938], Loss: 0.5115851759910583\n",
      "Train: Epoch [20], Batch [829/938], Loss: 0.47500279545783997\n",
      "Train: Epoch [20], Batch [830/938], Loss: 0.2570597231388092\n",
      "Train: Epoch [20], Batch [831/938], Loss: 0.3325030207633972\n",
      "Train: Epoch [20], Batch [832/938], Loss: 0.2617563009262085\n",
      "Train: Epoch [20], Batch [833/938], Loss: 0.3807138204574585\n",
      "Train: Epoch [20], Batch [834/938], Loss: 0.444511353969574\n",
      "Train: Epoch [20], Batch [835/938], Loss: 0.49820664525032043\n",
      "Train: Epoch [20], Batch [836/938], Loss: 0.44987475872039795\n",
      "Train: Epoch [20], Batch [837/938], Loss: 0.5475128293037415\n",
      "Train: Epoch [20], Batch [838/938], Loss: 0.5374857187271118\n",
      "Train: Epoch [20], Batch [839/938], Loss: 0.5510232448577881\n",
      "Train: Epoch [20], Batch [840/938], Loss: 0.43550464510917664\n",
      "Train: Epoch [20], Batch [841/938], Loss: 0.400798499584198\n",
      "Train: Epoch [20], Batch [842/938], Loss: 0.46722912788391113\n",
      "Train: Epoch [20], Batch [843/938], Loss: 0.291900098323822\n",
      "Train: Epoch [20], Batch [844/938], Loss: 0.30673760175704956\n",
      "Train: Epoch [20], Batch [845/938], Loss: 0.6079870462417603\n",
      "Train: Epoch [20], Batch [846/938], Loss: 0.42904776334762573\n",
      "Train: Epoch [20], Batch [847/938], Loss: 0.44093048572540283\n",
      "Train: Epoch [20], Batch [848/938], Loss: 0.44122037291526794\n",
      "Train: Epoch [20], Batch [849/938], Loss: 0.4537520110607147\n",
      "Train: Epoch [20], Batch [850/938], Loss: 0.37021490931510925\n",
      "Train: Epoch [20], Batch [851/938], Loss: 0.39255136251449585\n",
      "Train: Epoch [20], Batch [852/938], Loss: 0.30068135261535645\n",
      "Train: Epoch [20], Batch [853/938], Loss: 0.28846532106399536\n",
      "Train: Epoch [20], Batch [854/938], Loss: 0.4268307685852051\n",
      "Train: Epoch [20], Batch [855/938], Loss: 0.532649576663971\n",
      "Train: Epoch [20], Batch [856/938], Loss: 0.3961995244026184\n",
      "Train: Epoch [20], Batch [857/938], Loss: 0.48789525032043457\n",
      "Train: Epoch [20], Batch [858/938], Loss: 0.47083964943885803\n",
      "Train: Epoch [20], Batch [859/938], Loss: 0.42800548672676086\n",
      "Train: Epoch [20], Batch [860/938], Loss: 0.31006988883018494\n",
      "Train: Epoch [20], Batch [861/938], Loss: 0.32172611355781555\n",
      "Train: Epoch [20], Batch [862/938], Loss: 0.43498873710632324\n",
      "Train: Epoch [20], Batch [863/938], Loss: 0.26751357316970825\n",
      "Train: Epoch [20], Batch [864/938], Loss: 0.32561594247817993\n",
      "Train: Epoch [20], Batch [865/938], Loss: 0.4689411520957947\n",
      "Train: Epoch [20], Batch [866/938], Loss: 0.4240773022174835\n",
      "Train: Epoch [20], Batch [867/938], Loss: 0.6392501592636108\n",
      "Train: Epoch [20], Batch [868/938], Loss: 0.3681446611881256\n",
      "Train: Epoch [20], Batch [869/938], Loss: 0.4124305844306946\n",
      "Train: Epoch [20], Batch [870/938], Loss: 0.5050264596939087\n",
      "Train: Epoch [20], Batch [871/938], Loss: 0.5255810022354126\n",
      "Train: Epoch [20], Batch [872/938], Loss: 0.4963020384311676\n",
      "Train: Epoch [20], Batch [873/938], Loss: 0.279669851064682\n",
      "Train: Epoch [20], Batch [874/938], Loss: 0.23897507786750793\n",
      "Train: Epoch [20], Batch [875/938], Loss: 0.36799168586730957\n",
      "Train: Epoch [20], Batch [876/938], Loss: 0.367694616317749\n",
      "Train: Epoch [20], Batch [877/938], Loss: 0.4220806062221527\n",
      "Train: Epoch [20], Batch [878/938], Loss: 0.6711206436157227\n",
      "Train: Epoch [20], Batch [879/938], Loss: 0.47529733180999756\n",
      "Train: Epoch [20], Batch [880/938], Loss: 0.29067522287368774\n",
      "Train: Epoch [20], Batch [881/938], Loss: 0.4035537838935852\n",
      "Train: Epoch [20], Batch [882/938], Loss: 0.5569461584091187\n",
      "Train: Epoch [20], Batch [883/938], Loss: 0.38354289531707764\n",
      "Train: Epoch [20], Batch [884/938], Loss: 0.41318202018737793\n",
      "Train: Epoch [20], Batch [885/938], Loss: 0.459449827671051\n",
      "Train: Epoch [20], Batch [886/938], Loss: 0.5108689069747925\n",
      "Train: Epoch [20], Batch [887/938], Loss: 0.38429445028305054\n",
      "Train: Epoch [20], Batch [888/938], Loss: 0.32272934913635254\n",
      "Train: Epoch [20], Batch [889/938], Loss: 0.4233146905899048\n",
      "Train: Epoch [20], Batch [890/938], Loss: 0.4285695552825928\n",
      "Train: Epoch [20], Batch [891/938], Loss: 0.49199897050857544\n",
      "Train: Epoch [20], Batch [892/938], Loss: 0.40732771158218384\n",
      "Train: Epoch [20], Batch [893/938], Loss: 0.34552353620529175\n",
      "Train: Epoch [20], Batch [894/938], Loss: 0.5233893394470215\n",
      "Train: Epoch [20], Batch [895/938], Loss: 0.2327045053243637\n",
      "Train: Epoch [20], Batch [896/938], Loss: 0.6073364019393921\n",
      "Train: Epoch [20], Batch [897/938], Loss: 0.4743120074272156\n",
      "Train: Epoch [20], Batch [898/938], Loss: 0.42257267236709595\n",
      "Train: Epoch [20], Batch [899/938], Loss: 0.2743568420410156\n",
      "Train: Epoch [20], Batch [900/938], Loss: 0.434832900762558\n",
      "Train: Epoch [20], Batch [901/938], Loss: 0.18815812468528748\n",
      "Train: Epoch [20], Batch [902/938], Loss: 0.47756409645080566\n",
      "Train: Epoch [20], Batch [903/938], Loss: 0.4040873050689697\n",
      "Train: Epoch [20], Batch [904/938], Loss: 0.28140556812286377\n",
      "Train: Epoch [20], Batch [905/938], Loss: 0.5374449491500854\n",
      "Train: Epoch [20], Batch [906/938], Loss: 0.4925078749656677\n",
      "Train: Epoch [20], Batch [907/938], Loss: 0.4088391959667206\n",
      "Train: Epoch [20], Batch [908/938], Loss: 0.3548748791217804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [20], Batch [909/938], Loss: 0.5637871623039246\n",
      "Train: Epoch [20], Batch [910/938], Loss: 0.3089413046836853\n",
      "Train: Epoch [20], Batch [911/938], Loss: 0.48719581961631775\n",
      "Train: Epoch [20], Batch [912/938], Loss: 0.49572354555130005\n",
      "Train: Epoch [20], Batch [913/938], Loss: 0.38964423537254333\n",
      "Train: Epoch [20], Batch [914/938], Loss: 0.5329674482345581\n",
      "Train: Epoch [20], Batch [915/938], Loss: 0.6626001596450806\n",
      "Train: Epoch [20], Batch [916/938], Loss: 0.40877658128738403\n",
      "Train: Epoch [20], Batch [917/938], Loss: 0.30794408917427063\n",
      "Train: Epoch [20], Batch [918/938], Loss: 0.37186384201049805\n",
      "Train: Epoch [20], Batch [919/938], Loss: 0.5884014368057251\n",
      "Train: Epoch [20], Batch [920/938], Loss: 0.6128408312797546\n",
      "Train: Epoch [20], Batch [921/938], Loss: 0.4581294655799866\n",
      "Train: Epoch [20], Batch [922/938], Loss: 0.6108555793762207\n",
      "Train: Epoch [20], Batch [923/938], Loss: 0.48415088653564453\n",
      "Train: Epoch [20], Batch [924/938], Loss: 0.3570863604545593\n",
      "Train: Epoch [20], Batch [925/938], Loss: 0.4083845019340515\n",
      "Train: Epoch [20], Batch [926/938], Loss: 0.3543441593647003\n",
      "Train: Epoch [20], Batch [927/938], Loss: 0.37148919701576233\n",
      "Train: Epoch [20], Batch [928/938], Loss: 0.37344875931739807\n",
      "Train: Epoch [20], Batch [929/938], Loss: 0.3017168343067169\n",
      "Train: Epoch [20], Batch [930/938], Loss: 0.5514257550239563\n",
      "Train: Epoch [20], Batch [931/938], Loss: 0.5225998163223267\n",
      "Train: Epoch [20], Batch [932/938], Loss: 0.5202623605728149\n",
      "Train: Epoch [20], Batch [933/938], Loss: 0.29635608196258545\n",
      "Train: Epoch [20], Batch [934/938], Loss: 0.3685041666030884\n",
      "Train: Epoch [20], Batch [935/938], Loss: 0.4009702205657959\n",
      "Train: Epoch [20], Batch [936/938], Loss: 0.28922149538993835\n",
      "Train: Epoch [20], Batch [937/938], Loss: 0.47188928723335266\n",
      "Train: Epoch [20], Batch [938/938], Loss: 0.5379839539527893\n",
      "Accuracy of train set: 0.8553\n",
      "Validation: Epoch [20], Batch [1/938], Loss: 0.4062349200248718\n",
      "Validation: Epoch [20], Batch [2/938], Loss: 0.5255022048950195\n",
      "Validation: Epoch [20], Batch [3/938], Loss: 0.4767022728919983\n",
      "Validation: Epoch [20], Batch [4/938], Loss: 0.23683133721351624\n",
      "Validation: Epoch [20], Batch [5/938], Loss: 0.25188231468200684\n",
      "Validation: Epoch [20], Batch [6/938], Loss: 0.2172587513923645\n",
      "Validation: Epoch [20], Batch [7/938], Loss: 0.47034019231796265\n",
      "Validation: Epoch [20], Batch [8/938], Loss: 0.36863386631011963\n",
      "Validation: Epoch [20], Batch [9/938], Loss: 0.368283212184906\n",
      "Validation: Epoch [20], Batch [10/938], Loss: 0.34347960352897644\n",
      "Validation: Epoch [20], Batch [11/938], Loss: 0.4488622546195984\n",
      "Validation: Epoch [20], Batch [12/938], Loss: 0.38088133931159973\n",
      "Validation: Epoch [20], Batch [13/938], Loss: 0.3913794755935669\n",
      "Validation: Epoch [20], Batch [14/938], Loss: 0.341190904378891\n",
      "Validation: Epoch [20], Batch [15/938], Loss: 0.30971744656562805\n",
      "Validation: Epoch [20], Batch [16/938], Loss: 0.3861553966999054\n",
      "Validation: Epoch [20], Batch [17/938], Loss: 0.4434780478477478\n",
      "Validation: Epoch [20], Batch [18/938], Loss: 0.45552974939346313\n",
      "Validation: Epoch [20], Batch [19/938], Loss: 0.4016910195350647\n",
      "Validation: Epoch [20], Batch [20/938], Loss: 0.4572412967681885\n",
      "Validation: Epoch [20], Batch [21/938], Loss: 0.2710231840610504\n",
      "Validation: Epoch [20], Batch [22/938], Loss: 0.5067664980888367\n",
      "Validation: Epoch [20], Batch [23/938], Loss: 0.24210773408412933\n",
      "Validation: Epoch [20], Batch [24/938], Loss: 0.5645653009414673\n",
      "Validation: Epoch [20], Batch [25/938], Loss: 0.5441302061080933\n",
      "Validation: Epoch [20], Batch [26/938], Loss: 0.4431836009025574\n",
      "Validation: Epoch [20], Batch [27/938], Loss: 0.4339987635612488\n",
      "Validation: Epoch [20], Batch [28/938], Loss: 0.4387575387954712\n",
      "Validation: Epoch [20], Batch [29/938], Loss: 0.29201748967170715\n",
      "Validation: Epoch [20], Batch [30/938], Loss: 0.48698103427886963\n",
      "Validation: Epoch [20], Batch [31/938], Loss: 0.24547481536865234\n",
      "Validation: Epoch [20], Batch [32/938], Loss: 0.27536314725875854\n",
      "Validation: Epoch [20], Batch [33/938], Loss: 0.3809894323348999\n",
      "Validation: Epoch [20], Batch [34/938], Loss: 0.3847145438194275\n",
      "Validation: Epoch [20], Batch [35/938], Loss: 0.6878648996353149\n",
      "Validation: Epoch [20], Batch [36/938], Loss: 0.23390744626522064\n",
      "Validation: Epoch [20], Batch [37/938], Loss: 0.3081875145435333\n",
      "Validation: Epoch [20], Batch [38/938], Loss: 0.2548891305923462\n",
      "Validation: Epoch [20], Batch [39/938], Loss: 0.4947149157524109\n",
      "Validation: Epoch [20], Batch [40/938], Loss: 0.3857496976852417\n",
      "Validation: Epoch [20], Batch [41/938], Loss: 0.36369138956069946\n",
      "Validation: Epoch [20], Batch [42/938], Loss: 0.5055441856384277\n",
      "Validation: Epoch [20], Batch [43/938], Loss: 0.42390885949134827\n",
      "Validation: Epoch [20], Batch [44/938], Loss: 0.5224006175994873\n",
      "Validation: Epoch [20], Batch [45/938], Loss: 0.39753997325897217\n",
      "Validation: Epoch [20], Batch [46/938], Loss: 0.45352867245674133\n",
      "Validation: Epoch [20], Batch [47/938], Loss: 0.5278801918029785\n",
      "Validation: Epoch [20], Batch [48/938], Loss: 0.27257537841796875\n",
      "Validation: Epoch [20], Batch [49/938], Loss: 0.31929343938827515\n",
      "Validation: Epoch [20], Batch [50/938], Loss: 0.362213134765625\n",
      "Validation: Epoch [20], Batch [51/938], Loss: 0.42819085717201233\n",
      "Validation: Epoch [20], Batch [52/938], Loss: 0.35518378019332886\n",
      "Validation: Epoch [20], Batch [53/938], Loss: 0.430139422416687\n",
      "Validation: Epoch [20], Batch [54/938], Loss: 0.5590620040893555\n",
      "Validation: Epoch [20], Batch [55/938], Loss: 0.4532175660133362\n",
      "Validation: Epoch [20], Batch [56/938], Loss: 0.6034415364265442\n",
      "Validation: Epoch [20], Batch [57/938], Loss: 0.27426451444625854\n",
      "Validation: Epoch [20], Batch [58/938], Loss: 0.35645848512649536\n",
      "Validation: Epoch [20], Batch [59/938], Loss: 0.3576818108558655\n",
      "Validation: Epoch [20], Batch [60/938], Loss: 0.32136598229408264\n",
      "Validation: Epoch [20], Batch [61/938], Loss: 0.3112976551055908\n",
      "Validation: Epoch [20], Batch [62/938], Loss: 0.54656982421875\n",
      "Validation: Epoch [20], Batch [63/938], Loss: 0.49097228050231934\n",
      "Validation: Epoch [20], Batch [64/938], Loss: 0.3274122476577759\n",
      "Validation: Epoch [20], Batch [65/938], Loss: 0.3919208347797394\n",
      "Validation: Epoch [20], Batch [66/938], Loss: 0.547607421875\n",
      "Validation: Epoch [20], Batch [67/938], Loss: 0.30148693919181824\n",
      "Validation: Epoch [20], Batch [68/938], Loss: 0.39035969972610474\n",
      "Validation: Epoch [20], Batch [69/938], Loss: 0.39547908306121826\n",
      "Validation: Epoch [20], Batch [70/938], Loss: 0.4668525159358978\n",
      "Validation: Epoch [20], Batch [71/938], Loss: 0.27966073155403137\n",
      "Validation: Epoch [20], Batch [72/938], Loss: 0.6024621725082397\n",
      "Validation: Epoch [20], Batch [73/938], Loss: 0.44106829166412354\n",
      "Validation: Epoch [20], Batch [74/938], Loss: 0.6110471487045288\n",
      "Validation: Epoch [20], Batch [75/938], Loss: 0.4203125238418579\n",
      "Validation: Epoch [20], Batch [76/938], Loss: 0.5032658576965332\n",
      "Validation: Epoch [20], Batch [77/938], Loss: 0.46931761503219604\n",
      "Validation: Epoch [20], Batch [78/938], Loss: 0.36255785822868347\n",
      "Validation: Epoch [20], Batch [79/938], Loss: 0.3072932958602905\n",
      "Validation: Epoch [20], Batch [80/938], Loss: 0.4224610924720764\n",
      "Validation: Epoch [20], Batch [81/938], Loss: 0.5715757608413696\n",
      "Validation: Epoch [20], Batch [82/938], Loss: 0.5130383968353271\n",
      "Validation: Epoch [20], Batch [83/938], Loss: 0.7227553129196167\n",
      "Validation: Epoch [20], Batch [84/938], Loss: 0.49259713292121887\n",
      "Validation: Epoch [20], Batch [85/938], Loss: 0.33637234568595886\n",
      "Validation: Epoch [20], Batch [86/938], Loss: 0.18975752592086792\n",
      "Validation: Epoch [20], Batch [87/938], Loss: 0.400637149810791\n",
      "Validation: Epoch [20], Batch [88/938], Loss: 0.4560108184814453\n",
      "Validation: Epoch [20], Batch [89/938], Loss: 0.6248971223831177\n",
      "Validation: Epoch [20], Batch [90/938], Loss: 0.4008083939552307\n",
      "Validation: Epoch [20], Batch [91/938], Loss: 0.5152866244316101\n",
      "Validation: Epoch [20], Batch [92/938], Loss: 0.4961663484573364\n",
      "Validation: Epoch [20], Batch [93/938], Loss: 0.5370348691940308\n",
      "Validation: Epoch [20], Batch [94/938], Loss: 0.4936603605747223\n",
      "Validation: Epoch [20], Batch [95/938], Loss: 0.3740864396095276\n",
      "Validation: Epoch [20], Batch [96/938], Loss: 0.5727529525756836\n",
      "Validation: Epoch [20], Batch [97/938], Loss: 0.5007474422454834\n",
      "Validation: Epoch [20], Batch [98/938], Loss: 0.3346753716468811\n",
      "Validation: Epoch [20], Batch [99/938], Loss: 0.458509624004364\n",
      "Validation: Epoch [20], Batch [100/938], Loss: 0.4281659424304962\n",
      "Validation: Epoch [20], Batch [101/938], Loss: 0.4926873445510864\n",
      "Validation: Epoch [20], Batch [102/938], Loss: 0.37649479508399963\n",
      "Validation: Epoch [20], Batch [103/938], Loss: 0.5662754774093628\n",
      "Validation: Epoch [20], Batch [104/938], Loss: 0.3755565285682678\n",
      "Validation: Epoch [20], Batch [105/938], Loss: 0.6314423084259033\n",
      "Validation: Epoch [20], Batch [106/938], Loss: 0.4206003248691559\n",
      "Validation: Epoch [20], Batch [107/938], Loss: 0.48196130990982056\n",
      "Validation: Epoch [20], Batch [108/938], Loss: 0.603349506855011\n",
      "Validation: Epoch [20], Batch [109/938], Loss: 0.43686649203300476\n",
      "Validation: Epoch [20], Batch [110/938], Loss: 0.34760046005249023\n",
      "Validation: Epoch [20], Batch [111/938], Loss: 0.43913203477859497\n",
      "Validation: Epoch [20], Batch [112/938], Loss: 0.34415385127067566\n",
      "Validation: Epoch [20], Batch [113/938], Loss: 0.6057186126708984\n",
      "Validation: Epoch [20], Batch [114/938], Loss: 0.306564599275589\n",
      "Validation: Epoch [20], Batch [115/938], Loss: 0.46478742361068726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [116/938], Loss: 0.3442855775356293\n",
      "Validation: Epoch [20], Batch [117/938], Loss: 0.2920772135257721\n",
      "Validation: Epoch [20], Batch [118/938], Loss: 0.4340924918651581\n",
      "Validation: Epoch [20], Batch [119/938], Loss: 0.41027989983558655\n",
      "Validation: Epoch [20], Batch [120/938], Loss: 0.5651770830154419\n",
      "Validation: Epoch [20], Batch [121/938], Loss: 0.6051286458969116\n",
      "Validation: Epoch [20], Batch [122/938], Loss: 0.4482232332229614\n",
      "Validation: Epoch [20], Batch [123/938], Loss: 0.3758945167064667\n",
      "Validation: Epoch [20], Batch [124/938], Loss: 0.2758311331272125\n",
      "Validation: Epoch [20], Batch [125/938], Loss: 0.29380038380622864\n",
      "Validation: Epoch [20], Batch [126/938], Loss: 0.46889814734458923\n",
      "Validation: Epoch [20], Batch [127/938], Loss: 0.3981323540210724\n",
      "Validation: Epoch [20], Batch [128/938], Loss: 0.47490450739860535\n",
      "Validation: Epoch [20], Batch [129/938], Loss: 0.5823091268539429\n",
      "Validation: Epoch [20], Batch [130/938], Loss: 0.3670649230480194\n",
      "Validation: Epoch [20], Batch [131/938], Loss: 0.31858938932418823\n",
      "Validation: Epoch [20], Batch [132/938], Loss: 0.3994489014148712\n",
      "Validation: Epoch [20], Batch [133/938], Loss: 0.47933822870254517\n",
      "Validation: Epoch [20], Batch [134/938], Loss: 0.4198925495147705\n",
      "Validation: Epoch [20], Batch [135/938], Loss: 0.4821798503398895\n",
      "Validation: Epoch [20], Batch [136/938], Loss: 0.26462480425834656\n",
      "Validation: Epoch [20], Batch [137/938], Loss: 0.3413620591163635\n",
      "Validation: Epoch [20], Batch [138/938], Loss: 0.37291768193244934\n",
      "Validation: Epoch [20], Batch [139/938], Loss: 0.30308493971824646\n",
      "Validation: Epoch [20], Batch [140/938], Loss: 0.2897098958492279\n",
      "Validation: Epoch [20], Batch [141/938], Loss: 0.2522127330303192\n",
      "Validation: Epoch [20], Batch [142/938], Loss: 0.3959448039531708\n",
      "Validation: Epoch [20], Batch [143/938], Loss: 0.3827279210090637\n",
      "Validation: Epoch [20], Batch [144/938], Loss: 0.4545906186103821\n",
      "Validation: Epoch [20], Batch [145/938], Loss: 0.31819409132003784\n",
      "Validation: Epoch [20], Batch [146/938], Loss: 0.3733886480331421\n",
      "Validation: Epoch [20], Batch [147/938], Loss: 0.45330822467803955\n",
      "Validation: Epoch [20], Batch [148/938], Loss: 0.37694472074508667\n",
      "Validation: Epoch [20], Batch [149/938], Loss: 0.3508193790912628\n",
      "Validation: Epoch [20], Batch [150/938], Loss: 0.6541644930839539\n",
      "Validation: Epoch [20], Batch [151/938], Loss: 0.3013067841529846\n",
      "Validation: Epoch [20], Batch [152/938], Loss: 0.299383282661438\n",
      "Validation: Epoch [20], Batch [153/938], Loss: 0.4388318657875061\n",
      "Validation: Epoch [20], Batch [154/938], Loss: 0.6019735932350159\n",
      "Validation: Epoch [20], Batch [155/938], Loss: 0.5819907784461975\n",
      "Validation: Epoch [20], Batch [156/938], Loss: 0.49653372168540955\n",
      "Validation: Epoch [20], Batch [157/938], Loss: 0.5404272675514221\n",
      "Validation: Epoch [20], Batch [158/938], Loss: 0.29689013957977295\n",
      "Validation: Epoch [20], Batch [159/938], Loss: 0.29084038734436035\n",
      "Validation: Epoch [20], Batch [160/938], Loss: 0.46297329664230347\n",
      "Validation: Epoch [20], Batch [161/938], Loss: 0.44158029556274414\n",
      "Validation: Epoch [20], Batch [162/938], Loss: 0.4038577079772949\n",
      "Validation: Epoch [20], Batch [163/938], Loss: 0.444988489151001\n",
      "Validation: Epoch [20], Batch [164/938], Loss: 0.3092667758464813\n",
      "Validation: Epoch [20], Batch [165/938], Loss: 0.24940000474452972\n",
      "Validation: Epoch [20], Batch [166/938], Loss: 0.34117835760116577\n",
      "Validation: Epoch [20], Batch [167/938], Loss: 0.26967954635620117\n",
      "Validation: Epoch [20], Batch [168/938], Loss: 0.3671966791152954\n",
      "Validation: Epoch [20], Batch [169/938], Loss: 0.3537856638431549\n",
      "Validation: Epoch [20], Batch [170/938], Loss: 0.4944612979888916\n",
      "Validation: Epoch [20], Batch [171/938], Loss: 0.37407898902893066\n",
      "Validation: Epoch [20], Batch [172/938], Loss: 0.4762611389160156\n",
      "Validation: Epoch [20], Batch [173/938], Loss: 0.3004472553730011\n",
      "Validation: Epoch [20], Batch [174/938], Loss: 0.3401227593421936\n",
      "Validation: Epoch [20], Batch [175/938], Loss: 0.4433460533618927\n",
      "Validation: Epoch [20], Batch [176/938], Loss: 0.5336934924125671\n",
      "Validation: Epoch [20], Batch [177/938], Loss: 0.6076384782791138\n",
      "Validation: Epoch [20], Batch [178/938], Loss: 0.4726669192314148\n",
      "Validation: Epoch [20], Batch [179/938], Loss: 0.47597604990005493\n",
      "Validation: Epoch [20], Batch [180/938], Loss: 0.2623850107192993\n",
      "Validation: Epoch [20], Batch [181/938], Loss: 0.4126555621623993\n",
      "Validation: Epoch [20], Batch [182/938], Loss: 0.4350574314594269\n",
      "Validation: Epoch [20], Batch [183/938], Loss: 0.47846049070358276\n",
      "Validation: Epoch [20], Batch [184/938], Loss: 0.4991489350795746\n",
      "Validation: Epoch [20], Batch [185/938], Loss: 0.4528430104255676\n",
      "Validation: Epoch [20], Batch [186/938], Loss: 0.37043240666389465\n",
      "Validation: Epoch [20], Batch [187/938], Loss: 0.5362318754196167\n",
      "Validation: Epoch [20], Batch [188/938], Loss: 0.3212464451789856\n",
      "Validation: Epoch [20], Batch [189/938], Loss: 0.3666730523109436\n",
      "Validation: Epoch [20], Batch [190/938], Loss: 0.4383426904678345\n",
      "Validation: Epoch [20], Batch [191/938], Loss: 0.45133906602859497\n",
      "Validation: Epoch [20], Batch [192/938], Loss: 0.45412755012512207\n",
      "Validation: Epoch [20], Batch [193/938], Loss: 0.6042250990867615\n",
      "Validation: Epoch [20], Batch [194/938], Loss: 0.666462779045105\n",
      "Validation: Epoch [20], Batch [195/938], Loss: 0.4138440489768982\n",
      "Validation: Epoch [20], Batch [196/938], Loss: 0.37958645820617676\n",
      "Validation: Epoch [20], Batch [197/938], Loss: 0.38805174827575684\n",
      "Validation: Epoch [20], Batch [198/938], Loss: 0.39213329553604126\n",
      "Validation: Epoch [20], Batch [199/938], Loss: 0.4342898726463318\n",
      "Validation: Epoch [20], Batch [200/938], Loss: 0.371479868888855\n",
      "Validation: Epoch [20], Batch [201/938], Loss: 0.4295796751976013\n",
      "Validation: Epoch [20], Batch [202/938], Loss: 0.4404871165752411\n",
      "Validation: Epoch [20], Batch [203/938], Loss: 0.37168368697166443\n",
      "Validation: Epoch [20], Batch [204/938], Loss: 0.44599995017051697\n",
      "Validation: Epoch [20], Batch [205/938], Loss: 0.23052969574928284\n",
      "Validation: Epoch [20], Batch [206/938], Loss: 0.44352373480796814\n",
      "Validation: Epoch [20], Batch [207/938], Loss: 0.42948004603385925\n",
      "Validation: Epoch [20], Batch [208/938], Loss: 0.49471500515937805\n",
      "Validation: Epoch [20], Batch [209/938], Loss: 0.5157404541969299\n",
      "Validation: Epoch [20], Batch [210/938], Loss: 0.47085076570510864\n",
      "Validation: Epoch [20], Batch [211/938], Loss: 0.4394475817680359\n",
      "Validation: Epoch [20], Batch [212/938], Loss: 0.33674174547195435\n",
      "Validation: Epoch [20], Batch [213/938], Loss: 0.3424265384674072\n",
      "Validation: Epoch [20], Batch [214/938], Loss: 0.23945054411888123\n",
      "Validation: Epoch [20], Batch [215/938], Loss: 0.3546561300754547\n",
      "Validation: Epoch [20], Batch [216/938], Loss: 0.5643290281295776\n",
      "Validation: Epoch [20], Batch [217/938], Loss: 0.34591662883758545\n",
      "Validation: Epoch [20], Batch [218/938], Loss: 0.4393577575683594\n",
      "Validation: Epoch [20], Batch [219/938], Loss: 0.426742285490036\n",
      "Validation: Epoch [20], Batch [220/938], Loss: 0.41945669054985046\n",
      "Validation: Epoch [20], Batch [221/938], Loss: 0.3265853822231293\n",
      "Validation: Epoch [20], Batch [222/938], Loss: 0.636911153793335\n",
      "Validation: Epoch [20], Batch [223/938], Loss: 0.343697726726532\n",
      "Validation: Epoch [20], Batch [224/938], Loss: 0.5229935646057129\n",
      "Validation: Epoch [20], Batch [225/938], Loss: 0.5126828551292419\n",
      "Validation: Epoch [20], Batch [226/938], Loss: 0.6301971673965454\n",
      "Validation: Epoch [20], Batch [227/938], Loss: 0.46374109387397766\n",
      "Validation: Epoch [20], Batch [228/938], Loss: 0.5408569574356079\n",
      "Validation: Epoch [20], Batch [229/938], Loss: 0.45815756916999817\n",
      "Validation: Epoch [20], Batch [230/938], Loss: 0.48608487844467163\n",
      "Validation: Epoch [20], Batch [231/938], Loss: 0.6287686824798584\n",
      "Validation: Epoch [20], Batch [232/938], Loss: 0.30932849645614624\n",
      "Validation: Epoch [20], Batch [233/938], Loss: 0.4313865005970001\n",
      "Validation: Epoch [20], Batch [234/938], Loss: 0.32350367307662964\n",
      "Validation: Epoch [20], Batch [235/938], Loss: 0.4026920795440674\n",
      "Validation: Epoch [20], Batch [236/938], Loss: 0.2752922773361206\n",
      "Validation: Epoch [20], Batch [237/938], Loss: 0.34288299083709717\n",
      "Validation: Epoch [20], Batch [238/938], Loss: 0.4640257954597473\n",
      "Validation: Epoch [20], Batch [239/938], Loss: 0.27871641516685486\n",
      "Validation: Epoch [20], Batch [240/938], Loss: 0.3392100930213928\n",
      "Validation: Epoch [20], Batch [241/938], Loss: 0.4822399914264679\n",
      "Validation: Epoch [20], Batch [242/938], Loss: 0.3994843065738678\n",
      "Validation: Epoch [20], Batch [243/938], Loss: 0.4465845227241516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [244/938], Loss: 0.5284050703048706\n",
      "Validation: Epoch [20], Batch [245/938], Loss: 0.3165024518966675\n",
      "Validation: Epoch [20], Batch [246/938], Loss: 0.5767392516136169\n",
      "Validation: Epoch [20], Batch [247/938], Loss: 0.4105680584907532\n",
      "Validation: Epoch [20], Batch [248/938], Loss: 0.3033858835697174\n",
      "Validation: Epoch [20], Batch [249/938], Loss: 0.4128580391407013\n",
      "Validation: Epoch [20], Batch [250/938], Loss: 0.4440649151802063\n",
      "Validation: Epoch [20], Batch [251/938], Loss: 0.3773697018623352\n",
      "Validation: Epoch [20], Batch [252/938], Loss: 0.387821763753891\n",
      "Validation: Epoch [20], Batch [253/938], Loss: 0.5617940425872803\n",
      "Validation: Epoch [20], Batch [254/938], Loss: 0.46465805172920227\n",
      "Validation: Epoch [20], Batch [255/938], Loss: 0.39829304814338684\n",
      "Validation: Epoch [20], Batch [256/938], Loss: 0.224496528506279\n",
      "Validation: Epoch [20], Batch [257/938], Loss: 0.3800070881843567\n",
      "Validation: Epoch [20], Batch [258/938], Loss: 0.5066893100738525\n",
      "Validation: Epoch [20], Batch [259/938], Loss: 0.2828550934791565\n",
      "Validation: Epoch [20], Batch [260/938], Loss: 0.49775001406669617\n",
      "Validation: Epoch [20], Batch [261/938], Loss: 0.31108176708221436\n",
      "Validation: Epoch [20], Batch [262/938], Loss: 0.36497288942337036\n",
      "Validation: Epoch [20], Batch [263/938], Loss: 0.47067952156066895\n",
      "Validation: Epoch [20], Batch [264/938], Loss: 0.27765053510665894\n",
      "Validation: Epoch [20], Batch [265/938], Loss: 0.48130491375923157\n",
      "Validation: Epoch [20], Batch [266/938], Loss: 0.4772878885269165\n",
      "Validation: Epoch [20], Batch [267/938], Loss: 0.4476449489593506\n",
      "Validation: Epoch [20], Batch [268/938], Loss: 0.3311430513858795\n",
      "Validation: Epoch [20], Batch [269/938], Loss: 0.27478745579719543\n",
      "Validation: Epoch [20], Batch [270/938], Loss: 0.3228432536125183\n",
      "Validation: Epoch [20], Batch [271/938], Loss: 0.47309184074401855\n",
      "Validation: Epoch [20], Batch [272/938], Loss: 0.36837977170944214\n",
      "Validation: Epoch [20], Batch [273/938], Loss: 0.2902850806713104\n",
      "Validation: Epoch [20], Batch [274/938], Loss: 0.41789355874061584\n",
      "Validation: Epoch [20], Batch [275/938], Loss: 0.43912625312805176\n",
      "Validation: Epoch [20], Batch [276/938], Loss: 0.39665207266807556\n",
      "Validation: Epoch [20], Batch [277/938], Loss: 0.4311266243457794\n",
      "Validation: Epoch [20], Batch [278/938], Loss: 0.46816229820251465\n",
      "Validation: Epoch [20], Batch [279/938], Loss: 0.34634384512901306\n",
      "Validation: Epoch [20], Batch [280/938], Loss: 0.359939843416214\n",
      "Validation: Epoch [20], Batch [281/938], Loss: 0.28655174374580383\n",
      "Validation: Epoch [20], Batch [282/938], Loss: 0.5065931081771851\n",
      "Validation: Epoch [20], Batch [283/938], Loss: 0.5534496307373047\n",
      "Validation: Epoch [20], Batch [284/938], Loss: 0.5012955665588379\n",
      "Validation: Epoch [20], Batch [285/938], Loss: 0.6903976202011108\n",
      "Validation: Epoch [20], Batch [286/938], Loss: 0.3537072539329529\n",
      "Validation: Epoch [20], Batch [287/938], Loss: 0.3595365881919861\n",
      "Validation: Epoch [20], Batch [288/938], Loss: 0.4260725975036621\n",
      "Validation: Epoch [20], Batch [289/938], Loss: 0.4959844648838043\n",
      "Validation: Epoch [20], Batch [290/938], Loss: 0.2106732726097107\n",
      "Validation: Epoch [20], Batch [291/938], Loss: 0.5368003845214844\n",
      "Validation: Epoch [20], Batch [292/938], Loss: 0.514919638633728\n",
      "Validation: Epoch [20], Batch [293/938], Loss: 0.45814189314842224\n",
      "Validation: Epoch [20], Batch [294/938], Loss: 0.38638561964035034\n",
      "Validation: Epoch [20], Batch [295/938], Loss: 0.3676849603652954\n",
      "Validation: Epoch [20], Batch [296/938], Loss: 0.3023756146430969\n",
      "Validation: Epoch [20], Batch [297/938], Loss: 0.3939107656478882\n",
      "Validation: Epoch [20], Batch [298/938], Loss: 0.572594404220581\n",
      "Validation: Epoch [20], Batch [299/938], Loss: 0.28741079568862915\n",
      "Validation: Epoch [20], Batch [300/938], Loss: 0.543563723564148\n",
      "Validation: Epoch [20], Batch [301/938], Loss: 0.2621830403804779\n",
      "Validation: Epoch [20], Batch [302/938], Loss: 0.3794279992580414\n",
      "Validation: Epoch [20], Batch [303/938], Loss: 0.3845672607421875\n",
      "Validation: Epoch [20], Batch [304/938], Loss: 0.4018182158470154\n",
      "Validation: Epoch [20], Batch [305/938], Loss: 0.3988800644874573\n",
      "Validation: Epoch [20], Batch [306/938], Loss: 0.5190966725349426\n",
      "Validation: Epoch [20], Batch [307/938], Loss: 0.45542094111442566\n",
      "Validation: Epoch [20], Batch [308/938], Loss: 0.4154210388660431\n",
      "Validation: Epoch [20], Batch [309/938], Loss: 0.4119469225406647\n",
      "Validation: Epoch [20], Batch [310/938], Loss: 0.4776742458343506\n",
      "Validation: Epoch [20], Batch [311/938], Loss: 0.3901764452457428\n",
      "Validation: Epoch [20], Batch [312/938], Loss: 0.5785930752754211\n",
      "Validation: Epoch [20], Batch [313/938], Loss: 0.2687986493110657\n",
      "Validation: Epoch [20], Batch [314/938], Loss: 0.3690902590751648\n",
      "Validation: Epoch [20], Batch [315/938], Loss: 0.48458462953567505\n",
      "Validation: Epoch [20], Batch [316/938], Loss: 0.357310950756073\n",
      "Validation: Epoch [20], Batch [317/938], Loss: 0.3218018412590027\n",
      "Validation: Epoch [20], Batch [318/938], Loss: 0.39748698472976685\n",
      "Validation: Epoch [20], Batch [319/938], Loss: 0.5101318955421448\n",
      "Validation: Epoch [20], Batch [320/938], Loss: 0.41800421476364136\n",
      "Validation: Epoch [20], Batch [321/938], Loss: 0.6250449419021606\n",
      "Validation: Epoch [20], Batch [322/938], Loss: 0.2722894549369812\n",
      "Validation: Epoch [20], Batch [323/938], Loss: 0.5418457984924316\n",
      "Validation: Epoch [20], Batch [324/938], Loss: 0.339699923992157\n",
      "Validation: Epoch [20], Batch [325/938], Loss: 0.30757516622543335\n",
      "Validation: Epoch [20], Batch [326/938], Loss: 0.33766797184944153\n",
      "Validation: Epoch [20], Batch [327/938], Loss: 0.6053460836410522\n",
      "Validation: Epoch [20], Batch [328/938], Loss: 0.3529612421989441\n",
      "Validation: Epoch [20], Batch [329/938], Loss: 0.339229017496109\n",
      "Validation: Epoch [20], Batch [330/938], Loss: 0.38253796100616455\n",
      "Validation: Epoch [20], Batch [331/938], Loss: 0.4526856243610382\n",
      "Validation: Epoch [20], Batch [332/938], Loss: 0.5256417393684387\n",
      "Validation: Epoch [20], Batch [333/938], Loss: 0.5375600457191467\n",
      "Validation: Epoch [20], Batch [334/938], Loss: 0.36643123626708984\n",
      "Validation: Epoch [20], Batch [335/938], Loss: 0.541563868522644\n",
      "Validation: Epoch [20], Batch [336/938], Loss: 0.25507479906082153\n",
      "Validation: Epoch [20], Batch [337/938], Loss: 0.3357163667678833\n",
      "Validation: Epoch [20], Batch [338/938], Loss: 0.2829791009426117\n",
      "Validation: Epoch [20], Batch [339/938], Loss: 0.3562399744987488\n",
      "Validation: Epoch [20], Batch [340/938], Loss: 0.446792334318161\n",
      "Validation: Epoch [20], Batch [341/938], Loss: 0.3750165104866028\n",
      "Validation: Epoch [20], Batch [342/938], Loss: 0.2999404966831207\n",
      "Validation: Epoch [20], Batch [343/938], Loss: 0.20757444202899933\n",
      "Validation: Epoch [20], Batch [344/938], Loss: 0.3446488678455353\n",
      "Validation: Epoch [20], Batch [345/938], Loss: 0.40153467655181885\n",
      "Validation: Epoch [20], Batch [346/938], Loss: 0.3433779180049896\n",
      "Validation: Epoch [20], Batch [347/938], Loss: 0.21695037186145782\n",
      "Validation: Epoch [20], Batch [348/938], Loss: 0.4514397382736206\n",
      "Validation: Epoch [20], Batch [349/938], Loss: 0.2914780378341675\n",
      "Validation: Epoch [20], Batch [350/938], Loss: 0.38358449935913086\n",
      "Validation: Epoch [20], Batch [351/938], Loss: 0.43842265009880066\n",
      "Validation: Epoch [20], Batch [352/938], Loss: 0.45221421122550964\n",
      "Validation: Epoch [20], Batch [353/938], Loss: 0.36880573630332947\n",
      "Validation: Epoch [20], Batch [354/938], Loss: 0.20175868272781372\n",
      "Validation: Epoch [20], Batch [355/938], Loss: 0.2750168442726135\n",
      "Validation: Epoch [20], Batch [356/938], Loss: 0.30970364809036255\n",
      "Validation: Epoch [20], Batch [357/938], Loss: 0.48823288083076477\n",
      "Validation: Epoch [20], Batch [358/938], Loss: 0.4956939220428467\n",
      "Validation: Epoch [20], Batch [359/938], Loss: 0.44262683391571045\n",
      "Validation: Epoch [20], Batch [360/938], Loss: 0.42558553814888\n",
      "Validation: Epoch [20], Batch [361/938], Loss: 0.3619692325592041\n",
      "Validation: Epoch [20], Batch [362/938], Loss: 0.43296658992767334\n",
      "Validation: Epoch [20], Batch [363/938], Loss: 0.3117333948612213\n",
      "Validation: Epoch [20], Batch [364/938], Loss: 0.3367539644241333\n",
      "Validation: Epoch [20], Batch [365/938], Loss: 0.29474714398384094\n",
      "Validation: Epoch [20], Batch [366/938], Loss: 0.5900441408157349\n",
      "Validation: Epoch [20], Batch [367/938], Loss: 0.4179369807243347\n",
      "Validation: Epoch [20], Batch [368/938], Loss: 0.3047611117362976\n",
      "Validation: Epoch [20], Batch [369/938], Loss: 0.38048067688941956\n",
      "Validation: Epoch [20], Batch [370/938], Loss: 0.3935748040676117\n",
      "Validation: Epoch [20], Batch [371/938], Loss: 0.3604276180267334\n",
      "Validation: Epoch [20], Batch [372/938], Loss: 0.3496646285057068\n",
      "Validation: Epoch [20], Batch [373/938], Loss: 0.7013078927993774\n",
      "Validation: Epoch [20], Batch [374/938], Loss: 0.4443117082118988\n",
      "Validation: Epoch [20], Batch [375/938], Loss: 0.41787442564964294\n",
      "Validation: Epoch [20], Batch [376/938], Loss: 0.4409899115562439\n",
      "Validation: Epoch [20], Batch [377/938], Loss: 0.3791106641292572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [378/938], Loss: 0.5008379220962524\n",
      "Validation: Epoch [20], Batch [379/938], Loss: 0.35132157802581787\n",
      "Validation: Epoch [20], Batch [380/938], Loss: 0.2862315773963928\n",
      "Validation: Epoch [20], Batch [381/938], Loss: 0.45621243119239807\n",
      "Validation: Epoch [20], Batch [382/938], Loss: 0.45435407757759094\n",
      "Validation: Epoch [20], Batch [383/938], Loss: 0.48025065660476685\n",
      "Validation: Epoch [20], Batch [384/938], Loss: 0.532526969909668\n",
      "Validation: Epoch [20], Batch [385/938], Loss: 0.49487733840942383\n",
      "Validation: Epoch [20], Batch [386/938], Loss: 0.3291853070259094\n",
      "Validation: Epoch [20], Batch [387/938], Loss: 0.34322094917297363\n",
      "Validation: Epoch [20], Batch [388/938], Loss: 0.37475091218948364\n",
      "Validation: Epoch [20], Batch [389/938], Loss: 0.4183337986469269\n",
      "Validation: Epoch [20], Batch [390/938], Loss: 0.38285523653030396\n",
      "Validation: Epoch [20], Batch [391/938], Loss: 0.329498291015625\n",
      "Validation: Epoch [20], Batch [392/938], Loss: 0.371242880821228\n",
      "Validation: Epoch [20], Batch [393/938], Loss: 0.43539494276046753\n",
      "Validation: Epoch [20], Batch [394/938], Loss: 0.3826521337032318\n",
      "Validation: Epoch [20], Batch [395/938], Loss: 0.5113860368728638\n",
      "Validation: Epoch [20], Batch [396/938], Loss: 0.2915484309196472\n",
      "Validation: Epoch [20], Batch [397/938], Loss: 0.3911288380622864\n",
      "Validation: Epoch [20], Batch [398/938], Loss: 0.3885113596916199\n",
      "Validation: Epoch [20], Batch [399/938], Loss: 0.35793429613113403\n",
      "Validation: Epoch [20], Batch [400/938], Loss: 0.3274189531803131\n",
      "Validation: Epoch [20], Batch [401/938], Loss: 0.42827415466308594\n",
      "Validation: Epoch [20], Batch [402/938], Loss: 0.23282989859580994\n",
      "Validation: Epoch [20], Batch [403/938], Loss: 0.3450216054916382\n",
      "Validation: Epoch [20], Batch [404/938], Loss: 0.40831637382507324\n",
      "Validation: Epoch [20], Batch [405/938], Loss: 0.46086645126342773\n",
      "Validation: Epoch [20], Batch [406/938], Loss: 0.2788351774215698\n",
      "Validation: Epoch [20], Batch [407/938], Loss: 0.43086060881614685\n",
      "Validation: Epoch [20], Batch [408/938], Loss: 0.2914021611213684\n",
      "Validation: Epoch [20], Batch [409/938], Loss: 0.40748411417007446\n",
      "Validation: Epoch [20], Batch [410/938], Loss: 0.4138292074203491\n",
      "Validation: Epoch [20], Batch [411/938], Loss: 0.7078787088394165\n",
      "Validation: Epoch [20], Batch [412/938], Loss: 0.5100056529045105\n",
      "Validation: Epoch [20], Batch [413/938], Loss: 0.3764643669128418\n",
      "Validation: Epoch [20], Batch [414/938], Loss: 0.46474480628967285\n",
      "Validation: Epoch [20], Batch [415/938], Loss: 0.5452865958213806\n",
      "Validation: Epoch [20], Batch [416/938], Loss: 0.5929045677185059\n",
      "Validation: Epoch [20], Batch [417/938], Loss: 0.5814979076385498\n",
      "Validation: Epoch [20], Batch [418/938], Loss: 0.4827890396118164\n",
      "Validation: Epoch [20], Batch [419/938], Loss: 0.3772008419036865\n",
      "Validation: Epoch [20], Batch [420/938], Loss: 0.4597364068031311\n",
      "Validation: Epoch [20], Batch [421/938], Loss: 0.3345119059085846\n",
      "Validation: Epoch [20], Batch [422/938], Loss: 0.38948383927345276\n",
      "Validation: Epoch [20], Batch [423/938], Loss: 0.5222423076629639\n",
      "Validation: Epoch [20], Batch [424/938], Loss: 0.6581275463104248\n",
      "Validation: Epoch [20], Batch [425/938], Loss: 0.5510718822479248\n",
      "Validation: Epoch [20], Batch [426/938], Loss: 0.4996687173843384\n",
      "Validation: Epoch [20], Batch [427/938], Loss: 0.2557394206523895\n",
      "Validation: Epoch [20], Batch [428/938], Loss: 0.3630410432815552\n",
      "Validation: Epoch [20], Batch [429/938], Loss: 0.4383593797683716\n",
      "Validation: Epoch [20], Batch [430/938], Loss: 0.35841262340545654\n",
      "Validation: Epoch [20], Batch [431/938], Loss: 0.5052595138549805\n",
      "Validation: Epoch [20], Batch [432/938], Loss: 0.49618107080459595\n",
      "Validation: Epoch [20], Batch [433/938], Loss: 0.4450356960296631\n",
      "Validation: Epoch [20], Batch [434/938], Loss: 0.26094260811805725\n",
      "Validation: Epoch [20], Batch [435/938], Loss: 0.3331534266471863\n",
      "Validation: Epoch [20], Batch [436/938], Loss: 0.27016618847846985\n",
      "Validation: Epoch [20], Batch [437/938], Loss: 0.3745186924934387\n",
      "Validation: Epoch [20], Batch [438/938], Loss: 0.5405633449554443\n",
      "Validation: Epoch [20], Batch [439/938], Loss: 0.4499610960483551\n",
      "Validation: Epoch [20], Batch [440/938], Loss: 0.3817734718322754\n",
      "Validation: Epoch [20], Batch [441/938], Loss: 0.3749017119407654\n",
      "Validation: Epoch [20], Batch [442/938], Loss: 0.34472009539604187\n",
      "Validation: Epoch [20], Batch [443/938], Loss: 0.3456011712551117\n",
      "Validation: Epoch [20], Batch [444/938], Loss: 0.35681167244911194\n",
      "Validation: Epoch [20], Batch [445/938], Loss: 0.3358858525753021\n",
      "Validation: Epoch [20], Batch [446/938], Loss: 0.5615898370742798\n",
      "Validation: Epoch [20], Batch [447/938], Loss: 0.5049046277999878\n",
      "Validation: Epoch [20], Batch [448/938], Loss: 0.4821361005306244\n",
      "Validation: Epoch [20], Batch [449/938], Loss: 0.266107976436615\n",
      "Validation: Epoch [20], Batch [450/938], Loss: 0.39991262555122375\n",
      "Validation: Epoch [20], Batch [451/938], Loss: 0.6587848663330078\n",
      "Validation: Epoch [20], Batch [452/938], Loss: 0.5309086441993713\n",
      "Validation: Epoch [20], Batch [453/938], Loss: 0.4498046934604645\n",
      "Validation: Epoch [20], Batch [454/938], Loss: 0.3318254053592682\n",
      "Validation: Epoch [20], Batch [455/938], Loss: 0.4558112621307373\n",
      "Validation: Epoch [20], Batch [456/938], Loss: 0.4218388497829437\n",
      "Validation: Epoch [20], Batch [457/938], Loss: 0.4408019781112671\n",
      "Validation: Epoch [20], Batch [458/938], Loss: 0.41680753231048584\n",
      "Validation: Epoch [20], Batch [459/938], Loss: 0.4376862347126007\n",
      "Validation: Epoch [20], Batch [460/938], Loss: 0.4211595058441162\n",
      "Validation: Epoch [20], Batch [461/938], Loss: 0.3366636633872986\n",
      "Validation: Epoch [20], Batch [462/938], Loss: 0.45487910509109497\n",
      "Validation: Epoch [20], Batch [463/938], Loss: 0.46059489250183105\n",
      "Validation: Epoch [20], Batch [464/938], Loss: 0.5207974314689636\n",
      "Validation: Epoch [20], Batch [465/938], Loss: 0.46441197395324707\n",
      "Validation: Epoch [20], Batch [466/938], Loss: 0.46212446689605713\n",
      "Validation: Epoch [20], Batch [467/938], Loss: 0.4159347712993622\n",
      "Validation: Epoch [20], Batch [468/938], Loss: 0.3755764365196228\n",
      "Validation: Epoch [20], Batch [469/938], Loss: 0.4274938702583313\n",
      "Validation: Epoch [20], Batch [470/938], Loss: 0.248134046792984\n",
      "Validation: Epoch [20], Batch [471/938], Loss: 0.3014010190963745\n",
      "Validation: Epoch [20], Batch [472/938], Loss: 0.3257618248462677\n",
      "Validation: Epoch [20], Batch [473/938], Loss: 0.22735252976417542\n",
      "Validation: Epoch [20], Batch [474/938], Loss: 0.47101011872291565\n",
      "Validation: Epoch [20], Batch [475/938], Loss: 0.5164496898651123\n",
      "Validation: Epoch [20], Batch [476/938], Loss: 0.3208461105823517\n",
      "Validation: Epoch [20], Batch [477/938], Loss: 0.34024778008461\n",
      "Validation: Epoch [20], Batch [478/938], Loss: 0.3830615282058716\n",
      "Validation: Epoch [20], Batch [479/938], Loss: 0.5240565538406372\n",
      "Validation: Epoch [20], Batch [480/938], Loss: 0.6107174158096313\n",
      "Validation: Epoch [20], Batch [481/938], Loss: 0.4635363519191742\n",
      "Validation: Epoch [20], Batch [482/938], Loss: 0.2565084397792816\n",
      "Validation: Epoch [20], Batch [483/938], Loss: 0.27835071086883545\n",
      "Validation: Epoch [20], Batch [484/938], Loss: 0.4210073947906494\n",
      "Validation: Epoch [20], Batch [485/938], Loss: 0.32771220803260803\n",
      "Validation: Epoch [20], Batch [486/938], Loss: 0.47890275716781616\n",
      "Validation: Epoch [20], Batch [487/938], Loss: 0.3682573735713959\n",
      "Validation: Epoch [20], Batch [488/938], Loss: 0.38021907210350037\n",
      "Validation: Epoch [20], Batch [489/938], Loss: 0.1637578010559082\n",
      "Validation: Epoch [20], Batch [490/938], Loss: 0.3338163495063782\n",
      "Validation: Epoch [20], Batch [491/938], Loss: 0.353380411863327\n",
      "Validation: Epoch [20], Batch [492/938], Loss: 0.36477893590927124\n",
      "Validation: Epoch [20], Batch [493/938], Loss: 0.43258556723594666\n",
      "Validation: Epoch [20], Batch [494/938], Loss: 0.4180212616920471\n",
      "Validation: Epoch [20], Batch [495/938], Loss: 0.3874397575855255\n",
      "Validation: Epoch [20], Batch [496/938], Loss: 0.3623272776603699\n",
      "Validation: Epoch [20], Batch [497/938], Loss: 0.6500139236450195\n",
      "Validation: Epoch [20], Batch [498/938], Loss: 0.6079901456832886\n",
      "Validation: Epoch [20], Batch [499/938], Loss: 0.31473925709724426\n",
      "Validation: Epoch [20], Batch [500/938], Loss: 0.47921091318130493\n",
      "Validation: Epoch [20], Batch [501/938], Loss: 0.47569897770881653\n",
      "Validation: Epoch [20], Batch [502/938], Loss: 0.38453733921051025\n",
      "Validation: Epoch [20], Batch [503/938], Loss: 0.4262658953666687\n",
      "Validation: Epoch [20], Batch [504/938], Loss: 0.19323337078094482\n",
      "Validation: Epoch [20], Batch [505/938], Loss: 0.27714914083480835\n",
      "Validation: Epoch [20], Batch [506/938], Loss: 0.77888023853302\n",
      "Validation: Epoch [20], Batch [507/938], Loss: 0.5578655004501343\n",
      "Validation: Epoch [20], Batch [508/938], Loss: 0.5130358934402466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [509/938], Loss: 0.41987138986587524\n",
      "Validation: Epoch [20], Batch [510/938], Loss: 0.4705694913864136\n",
      "Validation: Epoch [20], Batch [511/938], Loss: 0.30073994398117065\n",
      "Validation: Epoch [20], Batch [512/938], Loss: 0.31428712606430054\n",
      "Validation: Epoch [20], Batch [513/938], Loss: 0.3695724308490753\n",
      "Validation: Epoch [20], Batch [514/938], Loss: 0.46164000034332275\n",
      "Validation: Epoch [20], Batch [515/938], Loss: 0.3664184510707855\n",
      "Validation: Epoch [20], Batch [516/938], Loss: 0.3697722256183624\n",
      "Validation: Epoch [20], Batch [517/938], Loss: 0.46805915236473083\n",
      "Validation: Epoch [20], Batch [518/938], Loss: 0.5037477016448975\n",
      "Validation: Epoch [20], Batch [519/938], Loss: 0.5476511716842651\n",
      "Validation: Epoch [20], Batch [520/938], Loss: 0.3177158236503601\n",
      "Validation: Epoch [20], Batch [521/938], Loss: 0.37234586477279663\n",
      "Validation: Epoch [20], Batch [522/938], Loss: 0.27786532044410706\n",
      "Validation: Epoch [20], Batch [523/938], Loss: 0.3806092441082001\n",
      "Validation: Epoch [20], Batch [524/938], Loss: 0.39753663539886475\n",
      "Validation: Epoch [20], Batch [525/938], Loss: 0.2773761749267578\n",
      "Validation: Epoch [20], Batch [526/938], Loss: 0.2962469160556793\n",
      "Validation: Epoch [20], Batch [527/938], Loss: 0.7089155316352844\n",
      "Validation: Epoch [20], Batch [528/938], Loss: 0.30940788984298706\n",
      "Validation: Epoch [20], Batch [529/938], Loss: 0.32729339599609375\n",
      "Validation: Epoch [20], Batch [530/938], Loss: 0.3676108121871948\n",
      "Validation: Epoch [20], Batch [531/938], Loss: 0.446231484413147\n",
      "Validation: Epoch [20], Batch [532/938], Loss: 0.4742857813835144\n",
      "Validation: Epoch [20], Batch [533/938], Loss: 0.5406299829483032\n",
      "Validation: Epoch [20], Batch [534/938], Loss: 0.44277822971343994\n",
      "Validation: Epoch [20], Batch [535/938], Loss: 0.449332058429718\n",
      "Validation: Epoch [20], Batch [536/938], Loss: 0.3445570468902588\n",
      "Validation: Epoch [20], Batch [537/938], Loss: 0.3872489035129547\n",
      "Validation: Epoch [20], Batch [538/938], Loss: 0.41299712657928467\n",
      "Validation: Epoch [20], Batch [539/938], Loss: 0.44289594888687134\n",
      "Validation: Epoch [20], Batch [540/938], Loss: 0.4909347891807556\n",
      "Validation: Epoch [20], Batch [541/938], Loss: 0.509665846824646\n",
      "Validation: Epoch [20], Batch [542/938], Loss: 0.2629796862602234\n",
      "Validation: Epoch [20], Batch [543/938], Loss: 0.47028684616088867\n",
      "Validation: Epoch [20], Batch [544/938], Loss: 0.3890162706375122\n",
      "Validation: Epoch [20], Batch [545/938], Loss: 0.4198165535926819\n",
      "Validation: Epoch [20], Batch [546/938], Loss: 0.4740541875362396\n",
      "Validation: Epoch [20], Batch [547/938], Loss: 0.49843519926071167\n",
      "Validation: Epoch [20], Batch [548/938], Loss: 0.4744374752044678\n",
      "Validation: Epoch [20], Batch [549/938], Loss: 0.3283817768096924\n",
      "Validation: Epoch [20], Batch [550/938], Loss: 0.47979849576950073\n",
      "Validation: Epoch [20], Batch [551/938], Loss: 0.16900227963924408\n",
      "Validation: Epoch [20], Batch [552/938], Loss: 0.41145047545433044\n",
      "Validation: Epoch [20], Batch [553/938], Loss: 0.29353541135787964\n",
      "Validation: Epoch [20], Batch [554/938], Loss: 0.4718206226825714\n",
      "Validation: Epoch [20], Batch [555/938], Loss: 0.48666152358055115\n",
      "Validation: Epoch [20], Batch [556/938], Loss: 0.7077386379241943\n",
      "Validation: Epoch [20], Batch [557/938], Loss: 0.4474871754646301\n",
      "Validation: Epoch [20], Batch [558/938], Loss: 0.322013258934021\n",
      "Validation: Epoch [20], Batch [559/938], Loss: 0.4743760824203491\n",
      "Validation: Epoch [20], Batch [560/938], Loss: 0.5264120101928711\n",
      "Validation: Epoch [20], Batch [561/938], Loss: 0.3707793653011322\n",
      "Validation: Epoch [20], Batch [562/938], Loss: 0.4557068943977356\n",
      "Validation: Epoch [20], Batch [563/938], Loss: 0.343176007270813\n",
      "Validation: Epoch [20], Batch [564/938], Loss: 0.40225479006767273\n",
      "Validation: Epoch [20], Batch [565/938], Loss: 0.3447166681289673\n",
      "Validation: Epoch [20], Batch [566/938], Loss: 0.4225652813911438\n",
      "Validation: Epoch [20], Batch [567/938], Loss: 0.3717191815376282\n",
      "Validation: Epoch [20], Batch [568/938], Loss: 0.5283185243606567\n",
      "Validation: Epoch [20], Batch [569/938], Loss: 0.5786144733428955\n",
      "Validation: Epoch [20], Batch [570/938], Loss: 0.31856590509414673\n",
      "Validation: Epoch [20], Batch [571/938], Loss: 0.3606554865837097\n",
      "Validation: Epoch [20], Batch [572/938], Loss: 0.3960859775543213\n",
      "Validation: Epoch [20], Batch [573/938], Loss: 0.4476252496242523\n",
      "Validation: Epoch [20], Batch [574/938], Loss: 0.35308051109313965\n",
      "Validation: Epoch [20], Batch [575/938], Loss: 0.49067172408103943\n",
      "Validation: Epoch [20], Batch [576/938], Loss: 0.31623438000679016\n",
      "Validation: Epoch [20], Batch [577/938], Loss: 0.3902064859867096\n",
      "Validation: Epoch [20], Batch [578/938], Loss: 0.4313513934612274\n",
      "Validation: Epoch [20], Batch [579/938], Loss: 0.6735166311264038\n",
      "Validation: Epoch [20], Batch [580/938], Loss: 0.38344433903694153\n",
      "Validation: Epoch [20], Batch [581/938], Loss: 0.4402446150779724\n",
      "Validation: Epoch [20], Batch [582/938], Loss: 0.4486767053604126\n",
      "Validation: Epoch [20], Batch [583/938], Loss: 0.5430244207382202\n",
      "Validation: Epoch [20], Batch [584/938], Loss: 0.38533520698547363\n",
      "Validation: Epoch [20], Batch [585/938], Loss: 0.4243747591972351\n",
      "Validation: Epoch [20], Batch [586/938], Loss: 0.4336945116519928\n",
      "Validation: Epoch [20], Batch [587/938], Loss: 0.33492839336395264\n",
      "Validation: Epoch [20], Batch [588/938], Loss: 0.24031537771224976\n",
      "Validation: Epoch [20], Batch [589/938], Loss: 0.2856079936027527\n",
      "Validation: Epoch [20], Batch [590/938], Loss: 0.4769172668457031\n",
      "Validation: Epoch [20], Batch [591/938], Loss: 0.5407721996307373\n",
      "Validation: Epoch [20], Batch [592/938], Loss: 0.2978072762489319\n",
      "Validation: Epoch [20], Batch [593/938], Loss: 0.28357309103012085\n",
      "Validation: Epoch [20], Batch [594/938], Loss: 0.3310988247394562\n",
      "Validation: Epoch [20], Batch [595/938], Loss: 0.33848488330841064\n",
      "Validation: Epoch [20], Batch [596/938], Loss: 0.5198518633842468\n",
      "Validation: Epoch [20], Batch [597/938], Loss: 0.4341535270214081\n",
      "Validation: Epoch [20], Batch [598/938], Loss: 0.4482104480266571\n",
      "Validation: Epoch [20], Batch [599/938], Loss: 0.45448100566864014\n",
      "Validation: Epoch [20], Batch [600/938], Loss: 0.3627331852912903\n",
      "Validation: Epoch [20], Batch [601/938], Loss: 0.3428700566291809\n",
      "Validation: Epoch [20], Batch [602/938], Loss: 0.44411593675613403\n",
      "Validation: Epoch [20], Batch [603/938], Loss: 0.36101582646369934\n",
      "Validation: Epoch [20], Batch [604/938], Loss: 0.37826013565063477\n",
      "Validation: Epoch [20], Batch [605/938], Loss: 0.4878084063529968\n",
      "Validation: Epoch [20], Batch [606/938], Loss: 0.43042904138565063\n",
      "Validation: Epoch [20], Batch [607/938], Loss: 0.43981799483299255\n",
      "Validation: Epoch [20], Batch [608/938], Loss: 0.37988221645355225\n",
      "Validation: Epoch [20], Batch [609/938], Loss: 0.3413792848587036\n",
      "Validation: Epoch [20], Batch [610/938], Loss: 0.2834651470184326\n",
      "Validation: Epoch [20], Batch [611/938], Loss: 0.4914364814758301\n",
      "Validation: Epoch [20], Batch [612/938], Loss: 0.48483115434646606\n",
      "Validation: Epoch [20], Batch [613/938], Loss: 0.39430999755859375\n",
      "Validation: Epoch [20], Batch [614/938], Loss: 0.5312102437019348\n",
      "Validation: Epoch [20], Batch [615/938], Loss: 0.49305856227874756\n",
      "Validation: Epoch [20], Batch [616/938], Loss: 0.544048547744751\n",
      "Validation: Epoch [20], Batch [617/938], Loss: 0.5317940711975098\n",
      "Validation: Epoch [20], Batch [618/938], Loss: 0.38267165422439575\n",
      "Validation: Epoch [20], Batch [619/938], Loss: 0.5046594142913818\n",
      "Validation: Epoch [20], Batch [620/938], Loss: 0.5136958956718445\n",
      "Validation: Epoch [20], Batch [621/938], Loss: 0.5883413553237915\n",
      "Validation: Epoch [20], Batch [622/938], Loss: 0.4848027527332306\n",
      "Validation: Epoch [20], Batch [623/938], Loss: 0.459583044052124\n",
      "Validation: Epoch [20], Batch [624/938], Loss: 0.3469540476799011\n",
      "Validation: Epoch [20], Batch [625/938], Loss: 0.3237661123275757\n",
      "Validation: Epoch [20], Batch [626/938], Loss: 0.3844306170940399\n",
      "Validation: Epoch [20], Batch [627/938], Loss: 0.45719292759895325\n",
      "Validation: Epoch [20], Batch [628/938], Loss: 0.3121785521507263\n",
      "Validation: Epoch [20], Batch [629/938], Loss: 0.20481766760349274\n",
      "Validation: Epoch [20], Batch [630/938], Loss: 0.27286070585250854\n",
      "Validation: Epoch [20], Batch [631/938], Loss: 0.301350861787796\n",
      "Validation: Epoch [20], Batch [632/938], Loss: 0.5477129220962524\n",
      "Validation: Epoch [20], Batch [633/938], Loss: 0.2601633667945862\n",
      "Validation: Epoch [20], Batch [634/938], Loss: 0.4787483215332031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [635/938], Loss: 0.42914366722106934\n",
      "Validation: Epoch [20], Batch [636/938], Loss: 0.384979248046875\n",
      "Validation: Epoch [20], Batch [637/938], Loss: 0.6416131258010864\n",
      "Validation: Epoch [20], Batch [638/938], Loss: 0.3859180808067322\n",
      "Validation: Epoch [20], Batch [639/938], Loss: 0.4338231384754181\n",
      "Validation: Epoch [20], Batch [640/938], Loss: 0.39918386936187744\n",
      "Validation: Epoch [20], Batch [641/938], Loss: 0.31568360328674316\n",
      "Validation: Epoch [20], Batch [642/938], Loss: 0.455771267414093\n",
      "Validation: Epoch [20], Batch [643/938], Loss: 0.4607495069503784\n",
      "Validation: Epoch [20], Batch [644/938], Loss: 0.5653226375579834\n",
      "Validation: Epoch [20], Batch [645/938], Loss: 0.31177857518196106\n",
      "Validation: Epoch [20], Batch [646/938], Loss: 0.45890143513679504\n",
      "Validation: Epoch [20], Batch [647/938], Loss: 0.3129463791847229\n",
      "Validation: Epoch [20], Batch [648/938], Loss: 0.5384401679039001\n",
      "Validation: Epoch [20], Batch [649/938], Loss: 0.29723620414733887\n",
      "Validation: Epoch [20], Batch [650/938], Loss: 0.38358330726623535\n",
      "Validation: Epoch [20], Batch [651/938], Loss: 0.431793749332428\n",
      "Validation: Epoch [20], Batch [652/938], Loss: 0.3332694470882416\n",
      "Validation: Epoch [20], Batch [653/938], Loss: 0.2745653986930847\n",
      "Validation: Epoch [20], Batch [654/938], Loss: 0.4246968626976013\n",
      "Validation: Epoch [20], Batch [655/938], Loss: 0.5413397550582886\n",
      "Validation: Epoch [20], Batch [656/938], Loss: 0.43208497762680054\n",
      "Validation: Epoch [20], Batch [657/938], Loss: 0.4311988949775696\n",
      "Validation: Epoch [20], Batch [658/938], Loss: 0.4688090682029724\n",
      "Validation: Epoch [20], Batch [659/938], Loss: 0.43390387296676636\n",
      "Validation: Epoch [20], Batch [660/938], Loss: 0.41148120164871216\n",
      "Validation: Epoch [20], Batch [661/938], Loss: 0.41417986154556274\n",
      "Validation: Epoch [20], Batch [662/938], Loss: 0.33742713928222656\n",
      "Validation: Epoch [20], Batch [663/938], Loss: 0.5951803922653198\n",
      "Validation: Epoch [20], Batch [664/938], Loss: 0.5911651849746704\n",
      "Validation: Epoch [20], Batch [665/938], Loss: 0.30420589447021484\n",
      "Validation: Epoch [20], Batch [666/938], Loss: 0.3211129307746887\n",
      "Validation: Epoch [20], Batch [667/938], Loss: 0.4039803147315979\n",
      "Validation: Epoch [20], Batch [668/938], Loss: 0.3606143593788147\n",
      "Validation: Epoch [20], Batch [669/938], Loss: 0.40116551518440247\n",
      "Validation: Epoch [20], Batch [670/938], Loss: 0.4344692528247833\n",
      "Validation: Epoch [20], Batch [671/938], Loss: 0.35367459058761597\n",
      "Validation: Epoch [20], Batch [672/938], Loss: 0.6164944171905518\n",
      "Validation: Epoch [20], Batch [673/938], Loss: 0.3539314270019531\n",
      "Validation: Epoch [20], Batch [674/938], Loss: 0.39179620146751404\n",
      "Validation: Epoch [20], Batch [675/938], Loss: 0.39860495924949646\n",
      "Validation: Epoch [20], Batch [676/938], Loss: 0.5178283452987671\n",
      "Validation: Epoch [20], Batch [677/938], Loss: 0.2890442907810211\n",
      "Validation: Epoch [20], Batch [678/938], Loss: 0.4480273127555847\n",
      "Validation: Epoch [20], Batch [679/938], Loss: 0.5931400060653687\n",
      "Validation: Epoch [20], Batch [680/938], Loss: 0.5881341099739075\n",
      "Validation: Epoch [20], Batch [681/938], Loss: 0.4309966564178467\n",
      "Validation: Epoch [20], Batch [682/938], Loss: 0.398513525724411\n",
      "Validation: Epoch [20], Batch [683/938], Loss: 0.3443101644515991\n",
      "Validation: Epoch [20], Batch [684/938], Loss: 0.4983179569244385\n",
      "Validation: Epoch [20], Batch [685/938], Loss: 0.27451178431510925\n",
      "Validation: Epoch [20], Batch [686/938], Loss: 0.6429943442344666\n",
      "Validation: Epoch [20], Batch [687/938], Loss: 0.3403450548648834\n",
      "Validation: Epoch [20], Batch [688/938], Loss: 0.47194594144821167\n",
      "Validation: Epoch [20], Batch [689/938], Loss: 0.2955952286720276\n",
      "Validation: Epoch [20], Batch [690/938], Loss: 0.9184739589691162\n",
      "Validation: Epoch [20], Batch [691/938], Loss: 0.30159538984298706\n",
      "Validation: Epoch [20], Batch [692/938], Loss: 0.2791699767112732\n",
      "Validation: Epoch [20], Batch [693/938], Loss: 0.4007111191749573\n",
      "Validation: Epoch [20], Batch [694/938], Loss: 0.465106338262558\n",
      "Validation: Epoch [20], Batch [695/938], Loss: 0.2745518088340759\n",
      "Validation: Epoch [20], Batch [696/938], Loss: 0.37400317192077637\n",
      "Validation: Epoch [20], Batch [697/938], Loss: 0.22226092219352722\n",
      "Validation: Epoch [20], Batch [698/938], Loss: 0.4636887311935425\n",
      "Validation: Epoch [20], Batch [699/938], Loss: 0.4695860743522644\n",
      "Validation: Epoch [20], Batch [700/938], Loss: 0.49281275272369385\n",
      "Validation: Epoch [20], Batch [701/938], Loss: 0.4698154926300049\n",
      "Validation: Epoch [20], Batch [702/938], Loss: 0.5465399622917175\n",
      "Validation: Epoch [20], Batch [703/938], Loss: 0.30706965923309326\n",
      "Validation: Epoch [20], Batch [704/938], Loss: 0.48063579201698303\n",
      "Validation: Epoch [20], Batch [705/938], Loss: 0.3752589821815491\n",
      "Validation: Epoch [20], Batch [706/938], Loss: 0.404338538646698\n",
      "Validation: Epoch [20], Batch [707/938], Loss: 0.5278396606445312\n",
      "Validation: Epoch [20], Batch [708/938], Loss: 0.255092591047287\n",
      "Validation: Epoch [20], Batch [709/938], Loss: 0.3277624845504761\n",
      "Validation: Epoch [20], Batch [710/938], Loss: 0.3727581799030304\n",
      "Validation: Epoch [20], Batch [711/938], Loss: 0.4516047239303589\n",
      "Validation: Epoch [20], Batch [712/938], Loss: 0.35551559925079346\n",
      "Validation: Epoch [20], Batch [713/938], Loss: 0.42600882053375244\n",
      "Validation: Epoch [20], Batch [714/938], Loss: 0.4837748408317566\n",
      "Validation: Epoch [20], Batch [715/938], Loss: 0.32875150442123413\n",
      "Validation: Epoch [20], Batch [716/938], Loss: 0.4089414179325104\n",
      "Validation: Epoch [20], Batch [717/938], Loss: 0.6023363471031189\n",
      "Validation: Epoch [20], Batch [718/938], Loss: 0.2539190649986267\n",
      "Validation: Epoch [20], Batch [719/938], Loss: 0.44405853748321533\n",
      "Validation: Epoch [20], Batch [720/938], Loss: 0.37095242738723755\n",
      "Validation: Epoch [20], Batch [721/938], Loss: 0.351646363735199\n",
      "Validation: Epoch [20], Batch [722/938], Loss: 0.4272513687610626\n",
      "Validation: Epoch [20], Batch [723/938], Loss: 0.349770724773407\n",
      "Validation: Epoch [20], Batch [724/938], Loss: 0.3832739591598511\n",
      "Validation: Epoch [20], Batch [725/938], Loss: 0.4698326885700226\n",
      "Validation: Epoch [20], Batch [726/938], Loss: 0.4840851128101349\n",
      "Validation: Epoch [20], Batch [727/938], Loss: 0.3946893811225891\n",
      "Validation: Epoch [20], Batch [728/938], Loss: 0.36146876215934753\n",
      "Validation: Epoch [20], Batch [729/938], Loss: 0.5190959572792053\n",
      "Validation: Epoch [20], Batch [730/938], Loss: 0.42451348900794983\n",
      "Validation: Epoch [20], Batch [731/938], Loss: 0.38240715861320496\n",
      "Validation: Epoch [20], Batch [732/938], Loss: 0.3910019099712372\n",
      "Validation: Epoch [20], Batch [733/938], Loss: 0.40246468782424927\n",
      "Validation: Epoch [20], Batch [734/938], Loss: 0.4213941991329193\n",
      "Validation: Epoch [20], Batch [735/938], Loss: 0.327863872051239\n",
      "Validation: Epoch [20], Batch [736/938], Loss: 0.2712840735912323\n",
      "Validation: Epoch [20], Batch [737/938], Loss: 0.6949025392532349\n",
      "Validation: Epoch [20], Batch [738/938], Loss: 0.34805139899253845\n",
      "Validation: Epoch [20], Batch [739/938], Loss: 0.4848775267601013\n",
      "Validation: Epoch [20], Batch [740/938], Loss: 0.38476669788360596\n",
      "Validation: Epoch [20], Batch [741/938], Loss: 0.5008962750434875\n",
      "Validation: Epoch [20], Batch [742/938], Loss: 0.4477164149284363\n",
      "Validation: Epoch [20], Batch [743/938], Loss: 0.383648157119751\n",
      "Validation: Epoch [20], Batch [744/938], Loss: 0.3519156575202942\n",
      "Validation: Epoch [20], Batch [745/938], Loss: 0.4176590144634247\n",
      "Validation: Epoch [20], Batch [746/938], Loss: 0.4134523868560791\n",
      "Validation: Epoch [20], Batch [747/938], Loss: 0.34292441606521606\n",
      "Validation: Epoch [20], Batch [748/938], Loss: 0.581115186214447\n",
      "Validation: Epoch [20], Batch [749/938], Loss: 0.5678622722625732\n",
      "Validation: Epoch [20], Batch [750/938], Loss: 0.7968930006027222\n",
      "Validation: Epoch [20], Batch [751/938], Loss: 0.4397414028644562\n",
      "Validation: Epoch [20], Batch [752/938], Loss: 0.5114258527755737\n",
      "Validation: Epoch [20], Batch [753/938], Loss: 0.48590636253356934\n",
      "Validation: Epoch [20], Batch [754/938], Loss: 0.3730350732803345\n",
      "Validation: Epoch [20], Batch [755/938], Loss: 0.6312915086746216\n",
      "Validation: Epoch [20], Batch [756/938], Loss: 0.37452036142349243\n",
      "Validation: Epoch [20], Batch [757/938], Loss: 0.6961072683334351\n",
      "Validation: Epoch [20], Batch [758/938], Loss: 0.6524484157562256\n",
      "Validation: Epoch [20], Batch [759/938], Loss: 0.3694189488887787\n",
      "Validation: Epoch [20], Batch [760/938], Loss: 0.44236212968826294\n",
      "Validation: Epoch [20], Batch [761/938], Loss: 0.3628692626953125\n",
      "Validation: Epoch [20], Batch [762/938], Loss: 0.3760761022567749\n",
      "Validation: Epoch [20], Batch [763/938], Loss: 0.4842923879623413\n",
      "Validation: Epoch [20], Batch [764/938], Loss: 0.6029039621353149\n",
      "Validation: Epoch [20], Batch [765/938], Loss: 0.29284197092056274\n",
      "Validation: Epoch [20], Batch [766/938], Loss: 0.43941378593444824\n",
      "Validation: Epoch [20], Batch [767/938], Loss: 0.5119526386260986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [768/938], Loss: 0.5874719619750977\n",
      "Validation: Epoch [20], Batch [769/938], Loss: 0.5346418023109436\n",
      "Validation: Epoch [20], Batch [770/938], Loss: 0.2951147258281708\n",
      "Validation: Epoch [20], Batch [771/938], Loss: 0.3459325134754181\n",
      "Validation: Epoch [20], Batch [772/938], Loss: 0.31134480237960815\n",
      "Validation: Epoch [20], Batch [773/938], Loss: 0.5232883095741272\n",
      "Validation: Epoch [20], Batch [774/938], Loss: 0.2680526375770569\n",
      "Validation: Epoch [20], Batch [775/938], Loss: 0.27863359451293945\n",
      "Validation: Epoch [20], Batch [776/938], Loss: 0.39926838874816895\n",
      "Validation: Epoch [20], Batch [777/938], Loss: 0.445262610912323\n",
      "Validation: Epoch [20], Batch [778/938], Loss: 0.5554835796356201\n",
      "Validation: Epoch [20], Batch [779/938], Loss: 0.5416043996810913\n",
      "Validation: Epoch [20], Batch [780/938], Loss: 0.4741719365119934\n",
      "Validation: Epoch [20], Batch [781/938], Loss: 0.5184588432312012\n",
      "Validation: Epoch [20], Batch [782/938], Loss: 0.4311544895172119\n",
      "Validation: Epoch [20], Batch [783/938], Loss: 0.40109413862228394\n",
      "Validation: Epoch [20], Batch [784/938], Loss: 0.4373629093170166\n",
      "Validation: Epoch [20], Batch [785/938], Loss: 0.4117348790168762\n",
      "Validation: Epoch [20], Batch [786/938], Loss: 0.47385677695274353\n",
      "Validation: Epoch [20], Batch [787/938], Loss: 0.5027406215667725\n",
      "Validation: Epoch [20], Batch [788/938], Loss: 0.5122795104980469\n",
      "Validation: Epoch [20], Batch [789/938], Loss: 0.4628704786300659\n",
      "Validation: Epoch [20], Batch [790/938], Loss: 0.47370225191116333\n",
      "Validation: Epoch [20], Batch [791/938], Loss: 0.2634150981903076\n",
      "Validation: Epoch [20], Batch [792/938], Loss: 0.47994738817214966\n",
      "Validation: Epoch [20], Batch [793/938], Loss: 0.5034667253494263\n",
      "Validation: Epoch [20], Batch [794/938], Loss: 0.33078837394714355\n",
      "Validation: Epoch [20], Batch [795/938], Loss: 0.46551263332366943\n",
      "Validation: Epoch [20], Batch [796/938], Loss: 0.19503317773342133\n",
      "Validation: Epoch [20], Batch [797/938], Loss: 0.3509509861469269\n",
      "Validation: Epoch [20], Batch [798/938], Loss: 0.40721744298934937\n",
      "Validation: Epoch [20], Batch [799/938], Loss: 0.5526946783065796\n",
      "Validation: Epoch [20], Batch [800/938], Loss: 0.4276773929595947\n",
      "Validation: Epoch [20], Batch [801/938], Loss: 0.4737124443054199\n",
      "Validation: Epoch [20], Batch [802/938], Loss: 0.3494330644607544\n",
      "Validation: Epoch [20], Batch [803/938], Loss: 0.3773682713508606\n",
      "Validation: Epoch [20], Batch [804/938], Loss: 0.38747620582580566\n",
      "Validation: Epoch [20], Batch [805/938], Loss: 0.45886847376823425\n",
      "Validation: Epoch [20], Batch [806/938], Loss: 0.36646127700805664\n",
      "Validation: Epoch [20], Batch [807/938], Loss: 0.3477693796157837\n",
      "Validation: Epoch [20], Batch [808/938], Loss: 0.6401373147964478\n",
      "Validation: Epoch [20], Batch [809/938], Loss: 0.3277361989021301\n",
      "Validation: Epoch [20], Batch [810/938], Loss: 0.34973448514938354\n",
      "Validation: Epoch [20], Batch [811/938], Loss: 0.3732232451438904\n",
      "Validation: Epoch [20], Batch [812/938], Loss: 0.4691084921360016\n",
      "Validation: Epoch [20], Batch [813/938], Loss: 0.49579373002052307\n",
      "Validation: Epoch [20], Batch [814/938], Loss: 0.42233705520629883\n",
      "Validation: Epoch [20], Batch [815/938], Loss: 0.8879276514053345\n",
      "Validation: Epoch [20], Batch [816/938], Loss: 0.2634277939796448\n",
      "Validation: Epoch [20], Batch [817/938], Loss: 0.41017669439315796\n",
      "Validation: Epoch [20], Batch [818/938], Loss: 0.34836962819099426\n",
      "Validation: Epoch [20], Batch [819/938], Loss: 0.34629231691360474\n",
      "Validation: Epoch [20], Batch [820/938], Loss: 0.4947405755519867\n",
      "Validation: Epoch [20], Batch [821/938], Loss: 0.30661144852638245\n",
      "Validation: Epoch [20], Batch [822/938], Loss: 0.46998676657676697\n",
      "Validation: Epoch [20], Batch [823/938], Loss: 0.34139513969421387\n",
      "Validation: Epoch [20], Batch [824/938], Loss: 0.4089687764644623\n",
      "Validation: Epoch [20], Batch [825/938], Loss: 0.36027467250823975\n",
      "Validation: Epoch [20], Batch [826/938], Loss: 0.3857612907886505\n",
      "Validation: Epoch [20], Batch [827/938], Loss: 0.4050554037094116\n",
      "Validation: Epoch [20], Batch [828/938], Loss: 0.3071158826351166\n",
      "Validation: Epoch [20], Batch [829/938], Loss: 0.3565613627433777\n",
      "Validation: Epoch [20], Batch [830/938], Loss: 0.35729488730430603\n",
      "Validation: Epoch [20], Batch [831/938], Loss: 0.30833059549331665\n",
      "Validation: Epoch [20], Batch [832/938], Loss: 0.3565874695777893\n",
      "Validation: Epoch [20], Batch [833/938], Loss: 0.5444302558898926\n",
      "Validation: Epoch [20], Batch [834/938], Loss: 0.38334760069847107\n",
      "Validation: Epoch [20], Batch [835/938], Loss: 0.5264244675636292\n",
      "Validation: Epoch [20], Batch [836/938], Loss: 0.3741796612739563\n",
      "Validation: Epoch [20], Batch [837/938], Loss: 0.33027350902557373\n",
      "Validation: Epoch [20], Batch [838/938], Loss: 0.4207134246826172\n",
      "Validation: Epoch [20], Batch [839/938], Loss: 0.35574331879615784\n",
      "Validation: Epoch [20], Batch [840/938], Loss: 0.5350252985954285\n",
      "Validation: Epoch [20], Batch [841/938], Loss: 0.4172804057598114\n",
      "Validation: Epoch [20], Batch [842/938], Loss: 0.3936847746372223\n",
      "Validation: Epoch [20], Batch [843/938], Loss: 0.20723867416381836\n",
      "Validation: Epoch [20], Batch [844/938], Loss: 0.4546184539794922\n",
      "Validation: Epoch [20], Batch [845/938], Loss: 0.3373241424560547\n",
      "Validation: Epoch [20], Batch [846/938], Loss: 0.3312913775444031\n",
      "Validation: Epoch [20], Batch [847/938], Loss: 0.24504998326301575\n",
      "Validation: Epoch [20], Batch [848/938], Loss: 0.39685434103012085\n",
      "Validation: Epoch [20], Batch [849/938], Loss: 0.44377243518829346\n",
      "Validation: Epoch [20], Batch [850/938], Loss: 0.32323509454727173\n",
      "Validation: Epoch [20], Batch [851/938], Loss: 0.30969879031181335\n",
      "Validation: Epoch [20], Batch [852/938], Loss: 0.317365825176239\n",
      "Validation: Epoch [20], Batch [853/938], Loss: 0.24116113781929016\n",
      "Validation: Epoch [20], Batch [854/938], Loss: 0.4542602002620697\n",
      "Validation: Epoch [20], Batch [855/938], Loss: 0.3618243336677551\n",
      "Validation: Epoch [20], Batch [856/938], Loss: 0.4699450135231018\n",
      "Validation: Epoch [20], Batch [857/938], Loss: 0.4568037688732147\n",
      "Validation: Epoch [20], Batch [858/938], Loss: 0.30896568298339844\n",
      "Validation: Epoch [20], Batch [859/938], Loss: 0.5180095434188843\n",
      "Validation: Epoch [20], Batch [860/938], Loss: 0.5800324082374573\n",
      "Validation: Epoch [20], Batch [861/938], Loss: 0.5072126984596252\n",
      "Validation: Epoch [20], Batch [862/938], Loss: 0.4280316233634949\n",
      "Validation: Epoch [20], Batch [863/938], Loss: 0.4049162268638611\n",
      "Validation: Epoch [20], Batch [864/938], Loss: 0.2763024568557739\n",
      "Validation: Epoch [20], Batch [865/938], Loss: 0.4617522060871124\n",
      "Validation: Epoch [20], Batch [866/938], Loss: 0.3311745822429657\n",
      "Validation: Epoch [20], Batch [867/938], Loss: 0.39715248346328735\n",
      "Validation: Epoch [20], Batch [868/938], Loss: 0.2809075713157654\n",
      "Validation: Epoch [20], Batch [869/938], Loss: 0.3355349898338318\n",
      "Validation: Epoch [20], Batch [870/938], Loss: 0.30220070481300354\n",
      "Validation: Epoch [20], Batch [871/938], Loss: 0.538537859916687\n",
      "Validation: Epoch [20], Batch [872/938], Loss: 0.3002192974090576\n",
      "Validation: Epoch [20], Batch [873/938], Loss: 0.4684637188911438\n",
      "Validation: Epoch [20], Batch [874/938], Loss: 0.34998074173927307\n",
      "Validation: Epoch [20], Batch [875/938], Loss: 0.3793220520019531\n",
      "Validation: Epoch [20], Batch [876/938], Loss: 0.24885323643684387\n",
      "Validation: Epoch [20], Batch [877/938], Loss: 0.37764135003089905\n",
      "Validation: Epoch [20], Batch [878/938], Loss: 0.328590989112854\n",
      "Validation: Epoch [20], Batch [879/938], Loss: 0.3874880075454712\n",
      "Validation: Epoch [20], Batch [880/938], Loss: 0.5693726539611816\n",
      "Validation: Epoch [20], Batch [881/938], Loss: 0.336587131023407\n",
      "Validation: Epoch [20], Batch [882/938], Loss: 0.30126136541366577\n",
      "Validation: Epoch [20], Batch [883/938], Loss: 0.35213956236839294\n",
      "Validation: Epoch [20], Batch [884/938], Loss: 0.2824521064758301\n",
      "Validation: Epoch [20], Batch [885/938], Loss: 0.565293550491333\n",
      "Validation: Epoch [20], Batch [886/938], Loss: 0.450041800737381\n",
      "Validation: Epoch [20], Batch [887/938], Loss: 0.4821019768714905\n",
      "Validation: Epoch [20], Batch [888/938], Loss: 0.40852707624435425\n",
      "Validation: Epoch [20], Batch [889/938], Loss: 0.4747156500816345\n",
      "Validation: Epoch [20], Batch [890/938], Loss: 0.41469454765319824\n",
      "Validation: Epoch [20], Batch [891/938], Loss: 0.5609034299850464\n",
      "Validation: Epoch [20], Batch [892/938], Loss: 0.43498778343200684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [20], Batch [893/938], Loss: 0.39210808277130127\n",
      "Validation: Epoch [20], Batch [894/938], Loss: 0.40735968947410583\n",
      "Validation: Epoch [20], Batch [895/938], Loss: 0.43419817090034485\n",
      "Validation: Epoch [20], Batch [896/938], Loss: 0.3236438035964966\n",
      "Validation: Epoch [20], Batch [897/938], Loss: 0.3072435259819031\n",
      "Validation: Epoch [20], Batch [898/938], Loss: 0.597626805305481\n",
      "Validation: Epoch [20], Batch [899/938], Loss: 0.4055638909339905\n",
      "Validation: Epoch [20], Batch [900/938], Loss: 0.4179284870624542\n",
      "Validation: Epoch [20], Batch [901/938], Loss: 0.2603585124015808\n",
      "Validation: Epoch [20], Batch [902/938], Loss: 0.35094794631004333\n",
      "Validation: Epoch [20], Batch [903/938], Loss: 0.5157433152198792\n",
      "Validation: Epoch [20], Batch [904/938], Loss: 0.4660271406173706\n",
      "Validation: Epoch [20], Batch [905/938], Loss: 0.42118388414382935\n",
      "Validation: Epoch [20], Batch [906/938], Loss: 0.17616575956344604\n",
      "Validation: Epoch [20], Batch [907/938], Loss: 0.4079039692878723\n",
      "Validation: Epoch [20], Batch [908/938], Loss: 0.3566668629646301\n",
      "Validation: Epoch [20], Batch [909/938], Loss: 0.3834031820297241\n",
      "Validation: Epoch [20], Batch [910/938], Loss: 0.24375060200691223\n",
      "Validation: Epoch [20], Batch [911/938], Loss: 0.38400521874427795\n",
      "Validation: Epoch [20], Batch [912/938], Loss: 0.3064844310283661\n",
      "Validation: Epoch [20], Batch [913/938], Loss: 0.39259904623031616\n",
      "Validation: Epoch [20], Batch [914/938], Loss: 0.671623945236206\n",
      "Validation: Epoch [20], Batch [915/938], Loss: 0.41951948404312134\n",
      "Validation: Epoch [20], Batch [916/938], Loss: 0.33763474225997925\n",
      "Validation: Epoch [20], Batch [917/938], Loss: 0.3306668996810913\n",
      "Validation: Epoch [20], Batch [918/938], Loss: 0.4016346335411072\n",
      "Validation: Epoch [20], Batch [919/938], Loss: 0.3634755611419678\n",
      "Validation: Epoch [20], Batch [920/938], Loss: 0.4306960999965668\n",
      "Validation: Epoch [20], Batch [921/938], Loss: 0.21371014416217804\n",
      "Validation: Epoch [20], Batch [922/938], Loss: 0.28070199489593506\n",
      "Validation: Epoch [20], Batch [923/938], Loss: 0.3738275468349457\n",
      "Validation: Epoch [20], Batch [924/938], Loss: 0.4188272953033447\n",
      "Validation: Epoch [20], Batch [925/938], Loss: 0.21522584557533264\n",
      "Validation: Epoch [20], Batch [926/938], Loss: 0.4230770468711853\n",
      "Validation: Epoch [20], Batch [927/938], Loss: 0.5774219632148743\n",
      "Validation: Epoch [20], Batch [928/938], Loss: 0.30703750252723694\n",
      "Validation: Epoch [20], Batch [929/938], Loss: 0.221787691116333\n",
      "Validation: Epoch [20], Batch [930/938], Loss: 0.2935951352119446\n",
      "Validation: Epoch [20], Batch [931/938], Loss: 0.5476542115211487\n",
      "Validation: Epoch [20], Batch [932/938], Loss: 0.4987856149673462\n",
      "Validation: Epoch [20], Batch [933/938], Loss: 0.3511990010738373\n",
      "Validation: Epoch [20], Batch [934/938], Loss: 0.35987430810928345\n",
      "Validation: Epoch [20], Batch [935/938], Loss: 0.3362269699573517\n",
      "Validation: Epoch [20], Batch [936/938], Loss: 0.3440203368663788\n",
      "Validation: Epoch [20], Batch [937/938], Loss: 0.46219226717948914\n",
      "Validation: Epoch [20], Batch [938/938], Loss: 0.281078577041626\n",
      "Accuracy of test set: 0.8543333333333333\n",
      "Train: Epoch [21], Batch [1/938], Loss: 0.33662769198417664\n",
      "Train: Epoch [21], Batch [2/938], Loss: 0.549956738948822\n",
      "Train: Epoch [21], Batch [3/938], Loss: 0.29290515184402466\n",
      "Train: Epoch [21], Batch [4/938], Loss: 0.2648969292640686\n",
      "Train: Epoch [21], Batch [5/938], Loss: 0.3094944357872009\n",
      "Train: Epoch [21], Batch [6/938], Loss: 0.5171921253204346\n",
      "Train: Epoch [21], Batch [7/938], Loss: 0.23933614790439606\n",
      "Train: Epoch [21], Batch [8/938], Loss: 0.34914782643318176\n",
      "Train: Epoch [21], Batch [9/938], Loss: 0.3795366883277893\n",
      "Train: Epoch [21], Batch [10/938], Loss: 0.46062204241752625\n",
      "Train: Epoch [21], Batch [11/938], Loss: 0.4875863790512085\n",
      "Train: Epoch [21], Batch [12/938], Loss: 0.5897474884986877\n",
      "Train: Epoch [21], Batch [13/938], Loss: 0.3043208718299866\n",
      "Train: Epoch [21], Batch [14/938], Loss: 0.5441175699234009\n",
      "Train: Epoch [21], Batch [15/938], Loss: 0.31331974267959595\n",
      "Train: Epoch [21], Batch [16/938], Loss: 0.4636826813220978\n",
      "Train: Epoch [21], Batch [17/938], Loss: 0.4456603527069092\n",
      "Train: Epoch [21], Batch [18/938], Loss: 0.3647534251213074\n",
      "Train: Epoch [21], Batch [19/938], Loss: 0.36507779359817505\n",
      "Train: Epoch [21], Batch [20/938], Loss: 0.6887241005897522\n",
      "Train: Epoch [21], Batch [21/938], Loss: 0.4461389183998108\n",
      "Train: Epoch [21], Batch [22/938], Loss: 0.3189968764781952\n",
      "Train: Epoch [21], Batch [23/938], Loss: 0.31543609499931335\n",
      "Train: Epoch [21], Batch [24/938], Loss: 0.4801895022392273\n",
      "Train: Epoch [21], Batch [25/938], Loss: 0.36347293853759766\n",
      "Train: Epoch [21], Batch [26/938], Loss: 0.3483111560344696\n",
      "Train: Epoch [21], Batch [27/938], Loss: 0.4639950096607208\n",
      "Train: Epoch [21], Batch [28/938], Loss: 0.2662176489830017\n",
      "Train: Epoch [21], Batch [29/938], Loss: 0.3664313554763794\n",
      "Train: Epoch [21], Batch [30/938], Loss: 0.5906007885932922\n",
      "Train: Epoch [21], Batch [31/938], Loss: 0.48167574405670166\n",
      "Train: Epoch [21], Batch [32/938], Loss: 0.3983008861541748\n",
      "Train: Epoch [21], Batch [33/938], Loss: 0.3384266793727875\n",
      "Train: Epoch [21], Batch [34/938], Loss: 0.470382958650589\n",
      "Train: Epoch [21], Batch [35/938], Loss: 0.4659106731414795\n",
      "Train: Epoch [21], Batch [36/938], Loss: 0.39956530928611755\n",
      "Train: Epoch [21], Batch [37/938], Loss: 0.2510172426700592\n",
      "Train: Epoch [21], Batch [38/938], Loss: 0.22503629326820374\n",
      "Train: Epoch [21], Batch [39/938], Loss: 0.3365132808685303\n",
      "Train: Epoch [21], Batch [40/938], Loss: 0.39692002534866333\n",
      "Train: Epoch [21], Batch [41/938], Loss: 0.46281588077545166\n",
      "Train: Epoch [21], Batch [42/938], Loss: 0.3495892286300659\n",
      "Train: Epoch [21], Batch [43/938], Loss: 0.3334857225418091\n",
      "Train: Epoch [21], Batch [44/938], Loss: 0.5004996061325073\n",
      "Train: Epoch [21], Batch [45/938], Loss: 0.39858585596084595\n",
      "Train: Epoch [21], Batch [46/938], Loss: 0.375795841217041\n",
      "Train: Epoch [21], Batch [47/938], Loss: 0.6490154266357422\n",
      "Train: Epoch [21], Batch [48/938], Loss: 0.2882186770439148\n",
      "Train: Epoch [21], Batch [49/938], Loss: 0.24528434872627258\n",
      "Train: Epoch [21], Batch [50/938], Loss: 0.34185585379600525\n",
      "Train: Epoch [21], Batch [51/938], Loss: 0.5276139974594116\n",
      "Train: Epoch [21], Batch [52/938], Loss: 0.37066152691841125\n",
      "Train: Epoch [21], Batch [53/938], Loss: 0.5130033493041992\n",
      "Train: Epoch [21], Batch [54/938], Loss: 0.3653477430343628\n",
      "Train: Epoch [21], Batch [55/938], Loss: 0.3396834135055542\n",
      "Train: Epoch [21], Batch [56/938], Loss: 0.45315876603126526\n",
      "Train: Epoch [21], Batch [57/938], Loss: 0.4627045691013336\n",
      "Train: Epoch [21], Batch [58/938], Loss: 0.5435339212417603\n",
      "Train: Epoch [21], Batch [59/938], Loss: 0.5082433819770813\n",
      "Train: Epoch [21], Batch [60/938], Loss: 0.5079315900802612\n",
      "Train: Epoch [21], Batch [61/938], Loss: 0.3437764346599579\n",
      "Train: Epoch [21], Batch [62/938], Loss: 0.38640812039375305\n",
      "Train: Epoch [21], Batch [63/938], Loss: 0.3192744851112366\n",
      "Train: Epoch [21], Batch [64/938], Loss: 0.28437501192092896\n",
      "Train: Epoch [21], Batch [65/938], Loss: 0.4125264286994934\n",
      "Train: Epoch [21], Batch [66/938], Loss: 0.5646162629127502\n",
      "Train: Epoch [21], Batch [67/938], Loss: 0.30517861247062683\n",
      "Train: Epoch [21], Batch [68/938], Loss: 0.3590022623538971\n",
      "Train: Epoch [21], Batch [69/938], Loss: 0.7173689603805542\n",
      "Train: Epoch [21], Batch [70/938], Loss: 0.5240512490272522\n",
      "Train: Epoch [21], Batch [71/938], Loss: 0.401241272687912\n",
      "Train: Epoch [21], Batch [72/938], Loss: 0.3846217095851898\n",
      "Train: Epoch [21], Batch [73/938], Loss: 0.278331995010376\n",
      "Train: Epoch [21], Batch [74/938], Loss: 0.2610490918159485\n",
      "Train: Epoch [21], Batch [75/938], Loss: 0.41980159282684326\n",
      "Train: Epoch [21], Batch [76/938], Loss: 0.3250129520893097\n",
      "Train: Epoch [21], Batch [77/938], Loss: 0.5984775424003601\n",
      "Train: Epoch [21], Batch [78/938], Loss: 0.2771550416946411\n",
      "Train: Epoch [21], Batch [79/938], Loss: 0.36790531873703003\n",
      "Train: Epoch [21], Batch [80/938], Loss: 0.5191390514373779\n",
      "Train: Epoch [21], Batch [81/938], Loss: 0.41697049140930176\n",
      "Train: Epoch [21], Batch [82/938], Loss: 0.3391157388687134\n",
      "Train: Epoch [21], Batch [83/938], Loss: 0.4311768412590027\n",
      "Train: Epoch [21], Batch [84/938], Loss: 0.25794854760169983\n",
      "Train: Epoch [21], Batch [85/938], Loss: 0.4372996985912323\n",
      "Train: Epoch [21], Batch [86/938], Loss: 0.32140421867370605\n",
      "Train: Epoch [21], Batch [87/938], Loss: 0.4272750914096832\n",
      "Train: Epoch [21], Batch [88/938], Loss: 0.33678632974624634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [21], Batch [89/938], Loss: 0.4309883713722229\n",
      "Train: Epoch [21], Batch [90/938], Loss: 0.34148138761520386\n",
      "Train: Epoch [21], Batch [91/938], Loss: 0.30453240871429443\n",
      "Train: Epoch [21], Batch [92/938], Loss: 0.4316042959690094\n",
      "Train: Epoch [21], Batch [93/938], Loss: 0.5414143800735474\n",
      "Train: Epoch [21], Batch [94/938], Loss: 0.43758121132850647\n",
      "Train: Epoch [21], Batch [95/938], Loss: 0.6463631391525269\n",
      "Train: Epoch [21], Batch [96/938], Loss: 0.5628495216369629\n",
      "Train: Epoch [21], Batch [97/938], Loss: 0.4461135268211365\n",
      "Train: Epoch [21], Batch [98/938], Loss: 0.39962705969810486\n",
      "Train: Epoch [21], Batch [99/938], Loss: 0.34630072116851807\n",
      "Train: Epoch [21], Batch [100/938], Loss: 0.5878304243087769\n",
      "Train: Epoch [21], Batch [101/938], Loss: 0.5444279313087463\n",
      "Train: Epoch [21], Batch [102/938], Loss: 0.30904465913772583\n",
      "Train: Epoch [21], Batch [103/938], Loss: 0.23263034224510193\n",
      "Train: Epoch [21], Batch [104/938], Loss: 0.2793862819671631\n",
      "Train: Epoch [21], Batch [105/938], Loss: 0.3697282671928406\n",
      "Train: Epoch [21], Batch [106/938], Loss: 0.3166075348854065\n",
      "Train: Epoch [21], Batch [107/938], Loss: 0.42528069019317627\n",
      "Train: Epoch [21], Batch [108/938], Loss: 0.37277230620384216\n",
      "Train: Epoch [21], Batch [109/938], Loss: 0.4182063043117523\n",
      "Train: Epoch [21], Batch [110/938], Loss: 0.36507439613342285\n",
      "Train: Epoch [21], Batch [111/938], Loss: 0.5042145252227783\n",
      "Train: Epoch [21], Batch [112/938], Loss: 0.5617129802703857\n",
      "Train: Epoch [21], Batch [113/938], Loss: 0.3531285524368286\n",
      "Train: Epoch [21], Batch [114/938], Loss: 0.5300431251525879\n",
      "Train: Epoch [21], Batch [115/938], Loss: 0.39747095108032227\n",
      "Train: Epoch [21], Batch [116/938], Loss: 0.5462710857391357\n",
      "Train: Epoch [21], Batch [117/938], Loss: 0.3983822166919708\n",
      "Train: Epoch [21], Batch [118/938], Loss: 0.4102599620819092\n",
      "Train: Epoch [21], Batch [119/938], Loss: 0.41663721203804016\n",
      "Train: Epoch [21], Batch [120/938], Loss: 0.26491522789001465\n",
      "Train: Epoch [21], Batch [121/938], Loss: 0.4797428250312805\n",
      "Train: Epoch [21], Batch [122/938], Loss: 0.35551053285598755\n",
      "Train: Epoch [21], Batch [123/938], Loss: 0.5394582748413086\n",
      "Train: Epoch [21], Batch [124/938], Loss: 0.32613620162010193\n",
      "Train: Epoch [21], Batch [125/938], Loss: 0.38524770736694336\n",
      "Train: Epoch [21], Batch [126/938], Loss: 0.26261934638023376\n",
      "Train: Epoch [21], Batch [127/938], Loss: 0.48537302017211914\n",
      "Train: Epoch [21], Batch [128/938], Loss: 0.32352322340011597\n",
      "Train: Epoch [21], Batch [129/938], Loss: 0.5258183479309082\n",
      "Train: Epoch [21], Batch [130/938], Loss: 0.3917214274406433\n",
      "Train: Epoch [21], Batch [131/938], Loss: 0.5783063173294067\n",
      "Train: Epoch [21], Batch [132/938], Loss: 0.3789054751396179\n",
      "Train: Epoch [21], Batch [133/938], Loss: 0.39637982845306396\n",
      "Train: Epoch [21], Batch [134/938], Loss: 0.4181486666202545\n",
      "Train: Epoch [21], Batch [135/938], Loss: 0.3263673782348633\n",
      "Train: Epoch [21], Batch [136/938], Loss: 0.28220224380493164\n",
      "Train: Epoch [21], Batch [137/938], Loss: 0.28831833600997925\n",
      "Train: Epoch [21], Batch [138/938], Loss: 0.380492627620697\n",
      "Train: Epoch [21], Batch [139/938], Loss: 0.4569275379180908\n",
      "Train: Epoch [21], Batch [140/938], Loss: 0.4396653175354004\n",
      "Train: Epoch [21], Batch [141/938], Loss: 0.3303086757659912\n",
      "Train: Epoch [21], Batch [142/938], Loss: 0.41429322957992554\n",
      "Train: Epoch [21], Batch [143/938], Loss: 0.42068007588386536\n",
      "Train: Epoch [21], Batch [144/938], Loss: 0.674616813659668\n",
      "Train: Epoch [21], Batch [145/938], Loss: 0.49162277579307556\n",
      "Train: Epoch [21], Batch [146/938], Loss: 0.5965154767036438\n",
      "Train: Epoch [21], Batch [147/938], Loss: 0.40340954065322876\n",
      "Train: Epoch [21], Batch [148/938], Loss: 0.2582380473613739\n",
      "Train: Epoch [21], Batch [149/938], Loss: 0.3647312521934509\n",
      "Train: Epoch [21], Batch [150/938], Loss: 0.46733546257019043\n",
      "Train: Epoch [21], Batch [151/938], Loss: 0.40187299251556396\n",
      "Train: Epoch [21], Batch [152/938], Loss: 0.3466041088104248\n",
      "Train: Epoch [21], Batch [153/938], Loss: 0.31938400864601135\n",
      "Train: Epoch [21], Batch [154/938], Loss: 0.3787347376346588\n",
      "Train: Epoch [21], Batch [155/938], Loss: 0.5835343599319458\n",
      "Train: Epoch [21], Batch [156/938], Loss: 0.5126683712005615\n",
      "Train: Epoch [21], Batch [157/938], Loss: 0.34146326780319214\n",
      "Train: Epoch [21], Batch [158/938], Loss: 0.27544063329696655\n",
      "Train: Epoch [21], Batch [159/938], Loss: 0.3070782423019409\n",
      "Train: Epoch [21], Batch [160/938], Loss: 0.3749990463256836\n",
      "Train: Epoch [21], Batch [161/938], Loss: 0.4053002893924713\n",
      "Train: Epoch [21], Batch [162/938], Loss: 0.3411935865879059\n",
      "Train: Epoch [21], Batch [163/938], Loss: 0.31025660037994385\n",
      "Train: Epoch [21], Batch [164/938], Loss: 0.4503720998764038\n",
      "Train: Epoch [21], Batch [165/938], Loss: 0.4921289086341858\n",
      "Train: Epoch [21], Batch [166/938], Loss: 0.37801694869995117\n",
      "Train: Epoch [21], Batch [167/938], Loss: 0.3247046172618866\n",
      "Train: Epoch [21], Batch [168/938], Loss: 0.3250476121902466\n",
      "Train: Epoch [21], Batch [169/938], Loss: 0.41141799092292786\n",
      "Train: Epoch [21], Batch [170/938], Loss: 0.4196074604988098\n",
      "Train: Epoch [21], Batch [171/938], Loss: 0.4208053946495056\n",
      "Train: Epoch [21], Batch [172/938], Loss: 0.3633055090904236\n",
      "Train: Epoch [21], Batch [173/938], Loss: 0.3802584409713745\n",
      "Train: Epoch [21], Batch [174/938], Loss: 0.2903880476951599\n",
      "Train: Epoch [21], Batch [175/938], Loss: 0.5438405871391296\n",
      "Train: Epoch [21], Batch [176/938], Loss: 0.41151899099349976\n",
      "Train: Epoch [21], Batch [177/938], Loss: 0.21993552148342133\n",
      "Train: Epoch [21], Batch [178/938], Loss: 0.38843482732772827\n",
      "Train: Epoch [21], Batch [179/938], Loss: 0.5594708919525146\n",
      "Train: Epoch [21], Batch [180/938], Loss: 0.30880415439605713\n",
      "Train: Epoch [21], Batch [181/938], Loss: 0.4076034128665924\n",
      "Train: Epoch [21], Batch [182/938], Loss: 0.3652874231338501\n",
      "Train: Epoch [21], Batch [183/938], Loss: 0.6035282611846924\n",
      "Train: Epoch [21], Batch [184/938], Loss: 0.3808876872062683\n",
      "Train: Epoch [21], Batch [185/938], Loss: 0.5195525884628296\n",
      "Train: Epoch [21], Batch [186/938], Loss: 0.37288808822631836\n",
      "Train: Epoch [21], Batch [187/938], Loss: 0.4156501293182373\n",
      "Train: Epoch [21], Batch [188/938], Loss: 0.5052114725112915\n",
      "Train: Epoch [21], Batch [189/938], Loss: 0.29419413208961487\n",
      "Train: Epoch [21], Batch [190/938], Loss: 0.28213152289390564\n",
      "Train: Epoch [21], Batch [191/938], Loss: 0.39059579372406006\n",
      "Train: Epoch [21], Batch [192/938], Loss: 0.2504669427871704\n",
      "Train: Epoch [21], Batch [193/938], Loss: 0.3996778726577759\n",
      "Train: Epoch [21], Batch [194/938], Loss: 0.44034960865974426\n",
      "Train: Epoch [21], Batch [195/938], Loss: 0.38313770294189453\n",
      "Train: Epoch [21], Batch [196/938], Loss: 0.46744465827941895\n",
      "Train: Epoch [21], Batch [197/938], Loss: 0.25449010729789734\n",
      "Train: Epoch [21], Batch [198/938], Loss: 0.2932202219963074\n",
      "Train: Epoch [21], Batch [199/938], Loss: 0.38144567608833313\n",
      "Train: Epoch [21], Batch [200/938], Loss: 0.3543609380722046\n",
      "Train: Epoch [21], Batch [201/938], Loss: 0.46360448002815247\n",
      "Train: Epoch [21], Batch [202/938], Loss: 0.4956887662410736\n",
      "Train: Epoch [21], Batch [203/938], Loss: 0.4787682294845581\n",
      "Train: Epoch [21], Batch [204/938], Loss: 0.4006274342536926\n",
      "Train: Epoch [21], Batch [205/938], Loss: 0.2892470955848694\n",
      "Train: Epoch [21], Batch [206/938], Loss: 0.3379286825656891\n",
      "Train: Epoch [21], Batch [207/938], Loss: 0.2999687194824219\n",
      "Train: Epoch [21], Batch [208/938], Loss: 0.3612668812274933\n",
      "Train: Epoch [21], Batch [209/938], Loss: 0.46699610352516174\n",
      "Train: Epoch [21], Batch [210/938], Loss: 0.4773235023021698\n",
      "Train: Epoch [21], Batch [211/938], Loss: 0.40809720754623413\n",
      "Train: Epoch [21], Batch [212/938], Loss: 0.5422203540802002\n",
      "Train: Epoch [21], Batch [213/938], Loss: 0.4072009027004242\n",
      "Train: Epoch [21], Batch [214/938], Loss: 0.3609100580215454\n",
      "Train: Epoch [21], Batch [215/938], Loss: 0.5483649373054504\n",
      "Train: Epoch [21], Batch [216/938], Loss: 0.6471135020256042\n",
      "Train: Epoch [21], Batch [217/938], Loss: 0.5436667203903198\n",
      "Train: Epoch [21], Batch [218/938], Loss: 0.22835896909236908\n",
      "Train: Epoch [21], Batch [219/938], Loss: 0.44448021054267883\n",
      "Train: Epoch [21], Batch [220/938], Loss: 0.5359225869178772\n",
      "Train: Epoch [21], Batch [221/938], Loss: 0.34717756509780884\n",
      "Train: Epoch [21], Batch [222/938], Loss: 0.5475785732269287\n",
      "Train: Epoch [21], Batch [223/938], Loss: 0.4792981743812561\n",
      "Train: Epoch [21], Batch [224/938], Loss: 0.2380332052707672\n",
      "Train: Epoch [21], Batch [225/938], Loss: 0.39081913232803345\n",
      "Train: Epoch [21], Batch [226/938], Loss: 0.40175190567970276\n",
      "Train: Epoch [21], Batch [227/938], Loss: 0.5627151727676392\n",
      "Train: Epoch [21], Batch [228/938], Loss: 0.4772741198539734\n",
      "Train: Epoch [21], Batch [229/938], Loss: 0.2448379099369049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [21], Batch [230/938], Loss: 0.3806040287017822\n",
      "Train: Epoch [21], Batch [231/938], Loss: 0.31591641902923584\n",
      "Train: Epoch [21], Batch [232/938], Loss: 0.3506765365600586\n",
      "Train: Epoch [21], Batch [233/938], Loss: 0.4075404107570648\n",
      "Train: Epoch [21], Batch [234/938], Loss: 0.32102373242378235\n",
      "Train: Epoch [21], Batch [235/938], Loss: 0.3985446095466614\n",
      "Train: Epoch [21], Batch [236/938], Loss: 0.5108091831207275\n",
      "Train: Epoch [21], Batch [237/938], Loss: 0.4552548825740814\n",
      "Train: Epoch [21], Batch [238/938], Loss: 0.5623528957366943\n",
      "Train: Epoch [21], Batch [239/938], Loss: 0.37063899636268616\n",
      "Train: Epoch [21], Batch [240/938], Loss: 0.40542611479759216\n",
      "Train: Epoch [21], Batch [241/938], Loss: 0.3700239062309265\n",
      "Train: Epoch [21], Batch [242/938], Loss: 0.386277437210083\n",
      "Train: Epoch [21], Batch [243/938], Loss: 0.3501777946949005\n",
      "Train: Epoch [21], Batch [244/938], Loss: 0.3703232705593109\n",
      "Train: Epoch [21], Batch [245/938], Loss: 0.16424120962619781\n",
      "Train: Epoch [21], Batch [246/938], Loss: 0.4010317921638489\n",
      "Train: Epoch [21], Batch [247/938], Loss: 0.329102098941803\n",
      "Train: Epoch [21], Batch [248/938], Loss: 0.5338528156280518\n",
      "Train: Epoch [21], Batch [249/938], Loss: 0.322147935628891\n",
      "Train: Epoch [21], Batch [250/938], Loss: 0.44374948740005493\n",
      "Train: Epoch [21], Batch [251/938], Loss: 0.31095024943351746\n",
      "Train: Epoch [21], Batch [252/938], Loss: 0.42107200622558594\n",
      "Train: Epoch [21], Batch [253/938], Loss: 0.5841038823127747\n",
      "Train: Epoch [21], Batch [254/938], Loss: 0.3151715397834778\n",
      "Train: Epoch [21], Batch [255/938], Loss: 0.3941406011581421\n",
      "Train: Epoch [21], Batch [256/938], Loss: 0.31898224353790283\n",
      "Train: Epoch [21], Batch [257/938], Loss: 0.23363694548606873\n",
      "Train: Epoch [21], Batch [258/938], Loss: 0.5098601579666138\n",
      "Train: Epoch [21], Batch [259/938], Loss: 0.4127357006072998\n",
      "Train: Epoch [21], Batch [260/938], Loss: 0.3736821711063385\n",
      "Train: Epoch [21], Batch [261/938], Loss: 0.4103361964225769\n",
      "Train: Epoch [21], Batch [262/938], Loss: 0.6267037391662598\n",
      "Train: Epoch [21], Batch [263/938], Loss: 0.35615432262420654\n",
      "Train: Epoch [21], Batch [264/938], Loss: 0.3881635069847107\n",
      "Train: Epoch [21], Batch [265/938], Loss: 0.2900562286376953\n",
      "Train: Epoch [21], Batch [266/938], Loss: 0.32951223850250244\n",
      "Train: Epoch [21], Batch [267/938], Loss: 0.30057293176651\n",
      "Train: Epoch [21], Batch [268/938], Loss: 0.31146833300590515\n",
      "Train: Epoch [21], Batch [269/938], Loss: 0.30747315287590027\n",
      "Train: Epoch [21], Batch [270/938], Loss: 0.36532285809516907\n",
      "Train: Epoch [21], Batch [271/938], Loss: 0.35159197449684143\n",
      "Train: Epoch [21], Batch [272/938], Loss: 0.27315133810043335\n",
      "Train: Epoch [21], Batch [273/938], Loss: 0.45543810725212097\n",
      "Train: Epoch [21], Batch [274/938], Loss: 0.4138716459274292\n",
      "Train: Epoch [21], Batch [275/938], Loss: 0.2515650987625122\n",
      "Train: Epoch [21], Batch [276/938], Loss: 0.3766867518424988\n",
      "Train: Epoch [21], Batch [277/938], Loss: 0.3577975630760193\n",
      "Train: Epoch [21], Batch [278/938], Loss: 0.4210730791091919\n",
      "Train: Epoch [21], Batch [279/938], Loss: 0.28092557191848755\n",
      "Train: Epoch [21], Batch [280/938], Loss: 0.3890444338321686\n",
      "Train: Epoch [21], Batch [281/938], Loss: 0.2232203483581543\n",
      "Train: Epoch [21], Batch [282/938], Loss: 0.33121079206466675\n",
      "Train: Epoch [21], Batch [283/938], Loss: 0.42869603633880615\n",
      "Train: Epoch [21], Batch [284/938], Loss: 0.44891852140426636\n",
      "Train: Epoch [21], Batch [285/938], Loss: 0.3566187024116516\n",
      "Train: Epoch [21], Batch [286/938], Loss: 0.4080467224121094\n",
      "Train: Epoch [21], Batch [287/938], Loss: 0.34593665599823\n",
      "Train: Epoch [21], Batch [288/938], Loss: 0.451820969581604\n",
      "Train: Epoch [21], Batch [289/938], Loss: 0.27667510509490967\n",
      "Train: Epoch [21], Batch [290/938], Loss: 0.4375322759151459\n",
      "Train: Epoch [21], Batch [291/938], Loss: 0.21503683924674988\n",
      "Train: Epoch [21], Batch [292/938], Loss: 0.36660581827163696\n",
      "Train: Epoch [21], Batch [293/938], Loss: 0.5234474539756775\n",
      "Train: Epoch [21], Batch [294/938], Loss: 0.3721486032009125\n",
      "Train: Epoch [21], Batch [295/938], Loss: 0.35748958587646484\n",
      "Train: Epoch [21], Batch [296/938], Loss: 0.4665389657020569\n",
      "Train: Epoch [21], Batch [297/938], Loss: 0.34490662813186646\n",
      "Train: Epoch [21], Batch [298/938], Loss: 0.4384565055370331\n",
      "Train: Epoch [21], Batch [299/938], Loss: 0.45519453287124634\n",
      "Train: Epoch [21], Batch [300/938], Loss: 0.32023918628692627\n",
      "Train: Epoch [21], Batch [301/938], Loss: 0.4873857796192169\n",
      "Train: Epoch [21], Batch [302/938], Loss: 0.5218187570571899\n",
      "Train: Epoch [21], Batch [303/938], Loss: 0.5554056167602539\n",
      "Train: Epoch [21], Batch [304/938], Loss: 0.3383886218070984\n",
      "Train: Epoch [21], Batch [305/938], Loss: 0.5519300103187561\n",
      "Train: Epoch [21], Batch [306/938], Loss: 0.46713727712631226\n",
      "Train: Epoch [21], Batch [307/938], Loss: 0.3166782557964325\n",
      "Train: Epoch [21], Batch [308/938], Loss: 0.40237921476364136\n",
      "Train: Epoch [21], Batch [309/938], Loss: 0.42758285999298096\n",
      "Train: Epoch [21], Batch [310/938], Loss: 0.3348385691642761\n",
      "Train: Epoch [21], Batch [311/938], Loss: 0.4825358986854553\n",
      "Train: Epoch [21], Batch [312/938], Loss: 0.4320327639579773\n",
      "Train: Epoch [21], Batch [313/938], Loss: 0.4681793451309204\n",
      "Train: Epoch [21], Batch [314/938], Loss: 0.6973875761032104\n",
      "Train: Epoch [21], Batch [315/938], Loss: 0.5014101266860962\n",
      "Train: Epoch [21], Batch [316/938], Loss: 0.23331284523010254\n",
      "Train: Epoch [21], Batch [317/938], Loss: 0.381172239780426\n",
      "Train: Epoch [21], Batch [318/938], Loss: 0.28472286462783813\n",
      "Train: Epoch [21], Batch [319/938], Loss: 0.43717366456985474\n",
      "Train: Epoch [21], Batch [320/938], Loss: 0.38220566511154175\n",
      "Train: Epoch [21], Batch [321/938], Loss: 0.5034812688827515\n",
      "Train: Epoch [21], Batch [322/938], Loss: 0.3771386742591858\n",
      "Train: Epoch [21], Batch [323/938], Loss: 0.2379789650440216\n",
      "Train: Epoch [21], Batch [324/938], Loss: 0.4188919961452484\n",
      "Train: Epoch [21], Batch [325/938], Loss: 0.38314515352249146\n",
      "Train: Epoch [21], Batch [326/938], Loss: 0.5659287571907043\n",
      "Train: Epoch [21], Batch [327/938], Loss: 0.32329627871513367\n",
      "Train: Epoch [21], Batch [328/938], Loss: 0.4401512145996094\n",
      "Train: Epoch [21], Batch [329/938], Loss: 0.6369501352310181\n",
      "Train: Epoch [21], Batch [330/938], Loss: 0.3739909827709198\n",
      "Train: Epoch [21], Batch [331/938], Loss: 0.48936137557029724\n",
      "Train: Epoch [21], Batch [332/938], Loss: 0.5651445388793945\n",
      "Train: Epoch [21], Batch [333/938], Loss: 0.4156060814857483\n",
      "Train: Epoch [21], Batch [334/938], Loss: 0.5515011548995972\n",
      "Train: Epoch [21], Batch [335/938], Loss: 0.5575987100601196\n",
      "Train: Epoch [21], Batch [336/938], Loss: 0.3936367332935333\n",
      "Train: Epoch [21], Batch [337/938], Loss: 0.3208339810371399\n",
      "Train: Epoch [21], Batch [338/938], Loss: 0.4261776804924011\n",
      "Train: Epoch [21], Batch [339/938], Loss: 0.3203957974910736\n",
      "Train: Epoch [21], Batch [340/938], Loss: 0.4650554358959198\n",
      "Train: Epoch [21], Batch [341/938], Loss: 0.460132360458374\n",
      "Train: Epoch [21], Batch [342/938], Loss: 0.415039598941803\n",
      "Train: Epoch [21], Batch [343/938], Loss: 0.588725209236145\n",
      "Train: Epoch [21], Batch [344/938], Loss: 0.3144489526748657\n",
      "Train: Epoch [21], Batch [345/938], Loss: 0.4956137537956238\n",
      "Train: Epoch [21], Batch [346/938], Loss: 0.5137625932693481\n",
      "Train: Epoch [21], Batch [347/938], Loss: 0.34661194682121277\n",
      "Train: Epoch [21], Batch [348/938], Loss: 0.4659121036529541\n",
      "Train: Epoch [21], Batch [349/938], Loss: 0.5428435206413269\n",
      "Train: Epoch [21], Batch [350/938], Loss: 0.37950873374938965\n",
      "Train: Epoch [21], Batch [351/938], Loss: 0.2541532516479492\n",
      "Train: Epoch [21], Batch [352/938], Loss: 0.27570557594299316\n",
      "Train: Epoch [21], Batch [353/938], Loss: 0.41433584690093994\n",
      "Train: Epoch [21], Batch [354/938], Loss: 0.29555171728134155\n",
      "Train: Epoch [21], Batch [355/938], Loss: 0.38511714339256287\n",
      "Train: Epoch [21], Batch [356/938], Loss: 0.6426942944526672\n",
      "Train: Epoch [21], Batch [357/938], Loss: 0.4338245987892151\n",
      "Train: Epoch [21], Batch [358/938], Loss: 0.4578680992126465\n",
      "Train: Epoch [21], Batch [359/938], Loss: 0.4410250186920166\n",
      "Train: Epoch [21], Batch [360/938], Loss: 0.5394909381866455\n",
      "Train: Epoch [21], Batch [361/938], Loss: 0.5578093528747559\n",
      "Train: Epoch [21], Batch [362/938], Loss: 0.26929453015327454\n",
      "Train: Epoch [21], Batch [363/938], Loss: 0.4765779376029968\n",
      "Train: Epoch [21], Batch [364/938], Loss: 0.36918386816978455\n",
      "Train: Epoch [21], Batch [365/938], Loss: 0.34759533405303955\n",
      "Train: Epoch [21], Batch [366/938], Loss: 0.33405160903930664\n",
      "Train: Epoch [21], Batch [367/938], Loss: 0.31374257802963257\n",
      "Train: Epoch [21], Batch [368/938], Loss: 0.38199514150619507\n",
      "Train: Epoch [21], Batch [369/938], Loss: 0.4193982481956482\n",
      "Train: Epoch [21], Batch [370/938], Loss: 0.3743782043457031\n",
      "Train: Epoch [21], Batch [371/938], Loss: 0.39402204751968384\n",
      "Train: Epoch [21], Batch [372/938], Loss: 0.48166394233703613\n",
      "Train: Epoch [21], Batch [373/938], Loss: 0.29002493619918823\n",
      "Train: Epoch [21], Batch [374/938], Loss: 0.36038416624069214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [21], Batch [375/938], Loss: 0.4351945221424103\n",
      "Train: Epoch [21], Batch [376/938], Loss: 0.24245363473892212\n",
      "Train: Epoch [21], Batch [377/938], Loss: 0.3828260600566864\n",
      "Train: Epoch [21], Batch [378/938], Loss: 0.19409029185771942\n",
      "Train: Epoch [21], Batch [379/938], Loss: 0.49165335297584534\n",
      "Train: Epoch [21], Batch [380/938], Loss: 0.30460017919540405\n",
      "Train: Epoch [21], Batch [381/938], Loss: 0.47965696454048157\n",
      "Train: Epoch [21], Batch [382/938], Loss: 0.6083235740661621\n",
      "Train: Epoch [21], Batch [383/938], Loss: 0.23977819085121155\n",
      "Train: Epoch [21], Batch [384/938], Loss: 0.34148699045181274\n",
      "Train: Epoch [21], Batch [385/938], Loss: 0.48279377818107605\n",
      "Train: Epoch [21], Batch [386/938], Loss: 0.49361687898635864\n",
      "Train: Epoch [21], Batch [387/938], Loss: 0.3223215341567993\n",
      "Train: Epoch [21], Batch [388/938], Loss: 0.5342361927032471\n",
      "Train: Epoch [21], Batch [389/938], Loss: 0.37746596336364746\n",
      "Train: Epoch [21], Batch [390/938], Loss: 0.32293325662612915\n",
      "Train: Epoch [21], Batch [391/938], Loss: 0.5632989406585693\n",
      "Train: Epoch [21], Batch [392/938], Loss: 0.395815372467041\n",
      "Train: Epoch [21], Batch [393/938], Loss: 0.3764711320400238\n",
      "Train: Epoch [21], Batch [394/938], Loss: 0.3517872095108032\n",
      "Train: Epoch [21], Batch [395/938], Loss: 0.3676747679710388\n",
      "Train: Epoch [21], Batch [396/938], Loss: 0.36586159467697144\n",
      "Train: Epoch [21], Batch [397/938], Loss: 0.48174750804901123\n",
      "Train: Epoch [21], Batch [398/938], Loss: 0.5207890272140503\n",
      "Train: Epoch [21], Batch [399/938], Loss: 0.46195486187934875\n",
      "Train: Epoch [21], Batch [400/938], Loss: 0.5290753841400146\n",
      "Train: Epoch [21], Batch [401/938], Loss: 0.4785118103027344\n",
      "Train: Epoch [21], Batch [402/938], Loss: 0.3485802710056305\n",
      "Train: Epoch [21], Batch [403/938], Loss: 0.4031336307525635\n",
      "Train: Epoch [21], Batch [404/938], Loss: 0.3185533285140991\n",
      "Train: Epoch [21], Batch [405/938], Loss: 0.30300629138946533\n",
      "Train: Epoch [21], Batch [406/938], Loss: 0.3657933473587036\n",
      "Train: Epoch [21], Batch [407/938], Loss: 0.42244163155555725\n",
      "Train: Epoch [21], Batch [408/938], Loss: 0.567064106464386\n",
      "Train: Epoch [21], Batch [409/938], Loss: 0.43401038646698\n",
      "Train: Epoch [21], Batch [410/938], Loss: 0.31352365016937256\n",
      "Train: Epoch [21], Batch [411/938], Loss: 0.39393699169158936\n",
      "Train: Epoch [21], Batch [412/938], Loss: 0.33833327889442444\n",
      "Train: Epoch [21], Batch [413/938], Loss: 0.5079865455627441\n",
      "Train: Epoch [21], Batch [414/938], Loss: 0.4661504328250885\n",
      "Train: Epoch [21], Batch [415/938], Loss: 0.5088942050933838\n",
      "Train: Epoch [21], Batch [416/938], Loss: 0.4592670798301697\n",
      "Train: Epoch [21], Batch [417/938], Loss: 0.3282143473625183\n",
      "Train: Epoch [21], Batch [418/938], Loss: 0.4442109763622284\n",
      "Train: Epoch [21], Batch [419/938], Loss: 0.42491310834884644\n",
      "Train: Epoch [21], Batch [420/938], Loss: 0.3739408850669861\n",
      "Train: Epoch [21], Batch [421/938], Loss: 0.3802279233932495\n",
      "Train: Epoch [21], Batch [422/938], Loss: 0.3363633155822754\n",
      "Train: Epoch [21], Batch [423/938], Loss: 0.3513832092285156\n",
      "Train: Epoch [21], Batch [424/938], Loss: 0.559381902217865\n",
      "Train: Epoch [21], Batch [425/938], Loss: 0.493405818939209\n",
      "Train: Epoch [21], Batch [426/938], Loss: 0.30181920528411865\n",
      "Train: Epoch [21], Batch [427/938], Loss: 0.5898765921592712\n",
      "Train: Epoch [21], Batch [428/938], Loss: 0.36226069927215576\n",
      "Train: Epoch [21], Batch [429/938], Loss: 0.484089195728302\n",
      "Train: Epoch [21], Batch [430/938], Loss: 0.493916779756546\n",
      "Train: Epoch [21], Batch [431/938], Loss: 0.6852703094482422\n",
      "Train: Epoch [21], Batch [432/938], Loss: 0.3200446367263794\n",
      "Train: Epoch [21], Batch [433/938], Loss: 0.33454278111457825\n",
      "Train: Epoch [21], Batch [434/938], Loss: 0.43576356768608093\n",
      "Train: Epoch [21], Batch [435/938], Loss: 0.34186363220214844\n",
      "Train: Epoch [21], Batch [436/938], Loss: 0.2954041361808777\n",
      "Train: Epoch [21], Batch [437/938], Loss: 0.5150550603866577\n",
      "Train: Epoch [21], Batch [438/938], Loss: 0.451461523771286\n",
      "Train: Epoch [21], Batch [439/938], Loss: 0.33597230911254883\n",
      "Train: Epoch [21], Batch [440/938], Loss: 0.33592820167541504\n",
      "Train: Epoch [21], Batch [441/938], Loss: 0.4518338739871979\n",
      "Train: Epoch [21], Batch [442/938], Loss: 0.3564685583114624\n",
      "Train: Epoch [21], Batch [443/938], Loss: 0.47605085372924805\n",
      "Train: Epoch [21], Batch [444/938], Loss: 0.5143638849258423\n",
      "Train: Epoch [21], Batch [445/938], Loss: 0.4142643213272095\n",
      "Train: Epoch [21], Batch [446/938], Loss: 0.28599023818969727\n",
      "Train: Epoch [21], Batch [447/938], Loss: 0.45359212160110474\n",
      "Train: Epoch [21], Batch [448/938], Loss: 0.428701251745224\n",
      "Train: Epoch [21], Batch [449/938], Loss: 0.32479310035705566\n",
      "Train: Epoch [21], Batch [450/938], Loss: 0.3453063666820526\n",
      "Train: Epoch [21], Batch [451/938], Loss: 0.47312983870506287\n",
      "Train: Epoch [21], Batch [452/938], Loss: 0.46610042452812195\n",
      "Train: Epoch [21], Batch [453/938], Loss: 0.36883023381233215\n",
      "Train: Epoch [21], Batch [454/938], Loss: 0.4232475459575653\n",
      "Train: Epoch [21], Batch [455/938], Loss: 0.33194905519485474\n",
      "Train: Epoch [21], Batch [456/938], Loss: 0.3244740664958954\n",
      "Train: Epoch [21], Batch [457/938], Loss: 0.4096236824989319\n",
      "Train: Epoch [21], Batch [458/938], Loss: 0.3411113917827606\n",
      "Train: Epoch [21], Batch [459/938], Loss: 0.4956768751144409\n",
      "Train: Epoch [21], Batch [460/938], Loss: 0.4331032633781433\n",
      "Train: Epoch [21], Batch [461/938], Loss: 0.3438307046890259\n",
      "Train: Epoch [21], Batch [462/938], Loss: 0.20343288779258728\n",
      "Train: Epoch [21], Batch [463/938], Loss: 0.45115014910697937\n",
      "Train: Epoch [21], Batch [464/938], Loss: 0.3781348466873169\n",
      "Train: Epoch [21], Batch [465/938], Loss: 0.48338669538497925\n",
      "Train: Epoch [21], Batch [466/938], Loss: 0.5268387794494629\n",
      "Train: Epoch [21], Batch [467/938], Loss: 0.36633050441741943\n",
      "Train: Epoch [21], Batch [468/938], Loss: 0.36708611249923706\n",
      "Train: Epoch [21], Batch [469/938], Loss: 0.4153124690055847\n",
      "Train: Epoch [21], Batch [470/938], Loss: 0.44891199469566345\n",
      "Train: Epoch [21], Batch [471/938], Loss: 0.42167699337005615\n",
      "Train: Epoch [21], Batch [472/938], Loss: 0.446925550699234\n",
      "Train: Epoch [21], Batch [473/938], Loss: 0.3818686008453369\n",
      "Train: Epoch [21], Batch [474/938], Loss: 0.48086678981781006\n",
      "Train: Epoch [21], Batch [475/938], Loss: 0.5078111886978149\n",
      "Train: Epoch [21], Batch [476/938], Loss: 0.4911772608757019\n",
      "Train: Epoch [21], Batch [477/938], Loss: 0.22624732553958893\n",
      "Train: Epoch [21], Batch [478/938], Loss: 0.44191208481788635\n",
      "Train: Epoch [21], Batch [479/938], Loss: 0.46218207478523254\n",
      "Train: Epoch [21], Batch [480/938], Loss: 0.3315871059894562\n",
      "Train: Epoch [21], Batch [481/938], Loss: 0.2975567877292633\n",
      "Train: Epoch [21], Batch [482/938], Loss: 0.39685019850730896\n",
      "Train: Epoch [21], Batch [483/938], Loss: 0.3888395428657532\n",
      "Train: Epoch [21], Batch [484/938], Loss: 0.46540209650993347\n",
      "Train: Epoch [21], Batch [485/938], Loss: 0.37944796681404114\n",
      "Train: Epoch [21], Batch [486/938], Loss: 0.35252487659454346\n",
      "Train: Epoch [21], Batch [487/938], Loss: 0.3919990658760071\n",
      "Train: Epoch [21], Batch [488/938], Loss: 0.5390501022338867\n",
      "Train: Epoch [21], Batch [489/938], Loss: 0.5299756526947021\n",
      "Train: Epoch [21], Batch [490/938], Loss: 0.34393930435180664\n",
      "Train: Epoch [21], Batch [491/938], Loss: 0.38495853543281555\n",
      "Train: Epoch [21], Batch [492/938], Loss: 0.3648727834224701\n",
      "Train: Epoch [21], Batch [493/938], Loss: 0.5212780237197876\n",
      "Train: Epoch [21], Batch [494/938], Loss: 0.42165297269821167\n",
      "Train: Epoch [21], Batch [495/938], Loss: 0.3475712835788727\n",
      "Train: Epoch [21], Batch [496/938], Loss: 0.5787486433982849\n",
      "Train: Epoch [21], Batch [497/938], Loss: 0.4665462076663971\n",
      "Train: Epoch [21], Batch [498/938], Loss: 0.4109840989112854\n",
      "Train: Epoch [21], Batch [499/938], Loss: 0.5258570313453674\n",
      "Train: Epoch [21], Batch [500/938], Loss: 0.3772994875907898\n",
      "Train: Epoch [21], Batch [501/938], Loss: 0.4799606502056122\n",
      "Train: Epoch [21], Batch [502/938], Loss: 0.44336920976638794\n",
      "Train: Epoch [21], Batch [503/938], Loss: 0.24996989965438843\n",
      "Train: Epoch [21], Batch [504/938], Loss: 0.5725983381271362\n",
      "Train: Epoch [21], Batch [505/938], Loss: 0.3381028473377228\n",
      "Train: Epoch [21], Batch [506/938], Loss: 0.5904642343521118\n",
      "Train: Epoch [21], Batch [507/938], Loss: 0.2854204475879669\n",
      "Train: Epoch [21], Batch [508/938], Loss: 0.3301731050014496\n",
      "Train: Epoch [21], Batch [509/938], Loss: 0.5069683790206909\n",
      "Train: Epoch [21], Batch [510/938], Loss: 0.3837733864784241\n",
      "Train: Epoch [21], Batch [511/938], Loss: 0.2697691321372986\n",
      "Train: Epoch [21], Batch [512/938], Loss: 0.41339802742004395\n",
      "Train: Epoch [21], Batch [513/938], Loss: 0.3693513870239258\n",
      "Train: Epoch [21], Batch [514/938], Loss: 0.3113879859447479\n",
      "Train: Epoch [21], Batch [515/938], Loss: 0.2647402286529541\n",
      "Train: Epoch [21], Batch [516/938], Loss: 0.36205777525901794\n",
      "Train: Epoch [21], Batch [517/938], Loss: 0.5583022832870483\n",
      "Train: Epoch [21], Batch [518/938], Loss: 0.40178924798965454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [21], Batch [519/938], Loss: 0.42983824014663696\n",
      "Train: Epoch [21], Batch [520/938], Loss: 0.29628124833106995\n",
      "Train: Epoch [21], Batch [521/938], Loss: 0.3729589879512787\n",
      "Train: Epoch [21], Batch [522/938], Loss: 0.36897504329681396\n",
      "Train: Epoch [21], Batch [523/938], Loss: 0.4308096766471863\n",
      "Train: Epoch [21], Batch [524/938], Loss: 0.3561748266220093\n",
      "Train: Epoch [21], Batch [525/938], Loss: 0.256504088640213\n",
      "Train: Epoch [21], Batch [526/938], Loss: 0.34660208225250244\n",
      "Train: Epoch [21], Batch [527/938], Loss: 0.42653408646583557\n",
      "Train: Epoch [21], Batch [528/938], Loss: 0.30029797554016113\n",
      "Train: Epoch [21], Batch [529/938], Loss: 0.23049475252628326\n",
      "Train: Epoch [21], Batch [530/938], Loss: 0.6573514938354492\n",
      "Train: Epoch [21], Batch [531/938], Loss: 0.37402817606925964\n",
      "Train: Epoch [21], Batch [532/938], Loss: 0.3685108721256256\n",
      "Train: Epoch [21], Batch [533/938], Loss: 0.3680498003959656\n",
      "Train: Epoch [21], Batch [534/938], Loss: 0.33327338099479675\n",
      "Train: Epoch [21], Batch [535/938], Loss: 0.24464038014411926\n",
      "Train: Epoch [21], Batch [536/938], Loss: 0.3719172179698944\n",
      "Train: Epoch [21], Batch [537/938], Loss: 0.3525184988975525\n",
      "Train: Epoch [21], Batch [538/938], Loss: 0.5735365152359009\n",
      "Train: Epoch [21], Batch [539/938], Loss: 0.7666555643081665\n",
      "Train: Epoch [21], Batch [540/938], Loss: 0.4397757053375244\n",
      "Train: Epoch [21], Batch [541/938], Loss: 0.2716463506221771\n",
      "Train: Epoch [21], Batch [542/938], Loss: 0.419644296169281\n",
      "Train: Epoch [21], Batch [543/938], Loss: 0.468484103679657\n",
      "Train: Epoch [21], Batch [544/938], Loss: 0.607369065284729\n",
      "Train: Epoch [21], Batch [545/938], Loss: 0.2679186463356018\n",
      "Train: Epoch [21], Batch [546/938], Loss: 0.24290920794010162\n",
      "Train: Epoch [21], Batch [547/938], Loss: 0.2898291349411011\n",
      "Train: Epoch [21], Batch [548/938], Loss: 0.25613874197006226\n",
      "Train: Epoch [21], Batch [549/938], Loss: 0.40739521384239197\n",
      "Train: Epoch [21], Batch [550/938], Loss: 0.3176512122154236\n",
      "Train: Epoch [21], Batch [551/938], Loss: 0.428592711687088\n",
      "Train: Epoch [21], Batch [552/938], Loss: 0.31742316484451294\n",
      "Train: Epoch [21], Batch [553/938], Loss: 0.4330436885356903\n",
      "Train: Epoch [21], Batch [554/938], Loss: 0.3264918029308319\n",
      "Train: Epoch [21], Batch [555/938], Loss: 0.4908570647239685\n",
      "Train: Epoch [21], Batch [556/938], Loss: 0.27244600653648376\n",
      "Train: Epoch [21], Batch [557/938], Loss: 0.3719467520713806\n",
      "Train: Epoch [21], Batch [558/938], Loss: 0.27573397755622864\n",
      "Train: Epoch [21], Batch [559/938], Loss: 0.421602725982666\n",
      "Train: Epoch [21], Batch [560/938], Loss: 0.46260523796081543\n",
      "Train: Epoch [21], Batch [561/938], Loss: 0.38397297263145447\n",
      "Train: Epoch [21], Batch [562/938], Loss: 0.5848028659820557\n",
      "Train: Epoch [21], Batch [563/938], Loss: 0.3635684847831726\n",
      "Train: Epoch [21], Batch [564/938], Loss: 0.24618570506572723\n",
      "Train: Epoch [21], Batch [565/938], Loss: 0.39763393998146057\n",
      "Train: Epoch [21], Batch [566/938], Loss: 0.3089742958545685\n",
      "Train: Epoch [21], Batch [567/938], Loss: 0.5005382895469666\n",
      "Train: Epoch [21], Batch [568/938], Loss: 0.49455779790878296\n",
      "Train: Epoch [21], Batch [569/938], Loss: 0.3084763288497925\n",
      "Train: Epoch [21], Batch [570/938], Loss: 0.485893189907074\n",
      "Train: Epoch [21], Batch [571/938], Loss: 0.44740030169487\n",
      "Train: Epoch [21], Batch [572/938], Loss: 0.23877623677253723\n",
      "Train: Epoch [21], Batch [573/938], Loss: 0.3860422968864441\n",
      "Train: Epoch [21], Batch [574/938], Loss: 0.2772599458694458\n",
      "Train: Epoch [21], Batch [575/938], Loss: 0.5624633431434631\n",
      "Train: Epoch [21], Batch [576/938], Loss: 0.32958781719207764\n",
      "Train: Epoch [21], Batch [577/938], Loss: 0.35966289043426514\n",
      "Train: Epoch [21], Batch [578/938], Loss: 0.3943348824977875\n",
      "Train: Epoch [21], Batch [579/938], Loss: 0.383650541305542\n",
      "Train: Epoch [21], Batch [580/938], Loss: 0.5361000895500183\n",
      "Train: Epoch [21], Batch [581/938], Loss: 0.35220134258270264\n",
      "Train: Epoch [21], Batch [582/938], Loss: 0.37417072057724\n",
      "Train: Epoch [21], Batch [583/938], Loss: 0.21652862429618835\n",
      "Train: Epoch [21], Batch [584/938], Loss: 0.44766777753829956\n",
      "Train: Epoch [21], Batch [585/938], Loss: 0.48221462965011597\n",
      "Train: Epoch [21], Batch [586/938], Loss: 0.3136870861053467\n",
      "Train: Epoch [21], Batch [587/938], Loss: 0.35325098037719727\n",
      "Train: Epoch [21], Batch [588/938], Loss: 0.34266364574432373\n",
      "Train: Epoch [21], Batch [589/938], Loss: 0.3427325487136841\n",
      "Train: Epoch [21], Batch [590/938], Loss: 0.4192435145378113\n",
      "Train: Epoch [21], Batch [591/938], Loss: 0.5615036487579346\n",
      "Train: Epoch [21], Batch [592/938], Loss: 0.45085081458091736\n",
      "Train: Epoch [21], Batch [593/938], Loss: 0.7687416076660156\n",
      "Train: Epoch [21], Batch [594/938], Loss: 0.5032913088798523\n",
      "Train: Epoch [21], Batch [595/938], Loss: 0.34934887290000916\n",
      "Train: Epoch [21], Batch [596/938], Loss: 0.38861197233200073\n",
      "Train: Epoch [21], Batch [597/938], Loss: 0.5220340490341187\n",
      "Train: Epoch [21], Batch [598/938], Loss: 0.5466213822364807\n",
      "Train: Epoch [21], Batch [599/938], Loss: 0.2856898307800293\n",
      "Train: Epoch [21], Batch [600/938], Loss: 0.6214724779129028\n",
      "Train: Epoch [21], Batch [601/938], Loss: 0.34996360540390015\n",
      "Train: Epoch [21], Batch [602/938], Loss: 0.4517752528190613\n",
      "Train: Epoch [21], Batch [603/938], Loss: 0.41526904702186584\n",
      "Train: Epoch [21], Batch [604/938], Loss: 0.5478295087814331\n",
      "Train: Epoch [21], Batch [605/938], Loss: 0.3631293773651123\n",
      "Train: Epoch [21], Batch [606/938], Loss: 0.42175257205963135\n",
      "Train: Epoch [21], Batch [607/938], Loss: 0.48985710740089417\n",
      "Train: Epoch [21], Batch [608/938], Loss: 0.3981221914291382\n",
      "Train: Epoch [21], Batch [609/938], Loss: 0.4572908580303192\n",
      "Train: Epoch [21], Batch [610/938], Loss: 0.5134520530700684\n",
      "Train: Epoch [21], Batch [611/938], Loss: 0.446718692779541\n",
      "Train: Epoch [21], Batch [612/938], Loss: 0.4634379744529724\n",
      "Train: Epoch [21], Batch [613/938], Loss: 0.38727062940597534\n",
      "Train: Epoch [21], Batch [614/938], Loss: 0.4694446623325348\n",
      "Train: Epoch [21], Batch [615/938], Loss: 0.4539180397987366\n",
      "Train: Epoch [21], Batch [616/938], Loss: 0.2851753830909729\n",
      "Train: Epoch [21], Batch [617/938], Loss: 0.3402256369590759\n",
      "Train: Epoch [21], Batch [618/938], Loss: 0.3300507366657257\n",
      "Train: Epoch [21], Batch [619/938], Loss: 0.4444851875305176\n",
      "Train: Epoch [21], Batch [620/938], Loss: 0.5983218550682068\n",
      "Train: Epoch [21], Batch [621/938], Loss: 0.37307360768318176\n",
      "Train: Epoch [21], Batch [622/938], Loss: 0.3307061195373535\n",
      "Train: Epoch [21], Batch [623/938], Loss: 0.3930838704109192\n",
      "Train: Epoch [21], Batch [624/938], Loss: 0.4123952388763428\n",
      "Train: Epoch [21], Batch [625/938], Loss: 0.5046404004096985\n",
      "Train: Epoch [21], Batch [626/938], Loss: 0.33986905217170715\n",
      "Train: Epoch [21], Batch [627/938], Loss: 0.4661920964717865\n",
      "Train: Epoch [21], Batch [628/938], Loss: 0.278103232383728\n",
      "Train: Epoch [21], Batch [629/938], Loss: 0.5018870830535889\n",
      "Train: Epoch [21], Batch [630/938], Loss: 0.5335099697113037\n",
      "Train: Epoch [21], Batch [631/938], Loss: 0.3596709966659546\n",
      "Train: Epoch [21], Batch [632/938], Loss: 0.34731656312942505\n",
      "Train: Epoch [21], Batch [633/938], Loss: 0.445648193359375\n",
      "Train: Epoch [21], Batch [634/938], Loss: 0.36538735032081604\n",
      "Train: Epoch [21], Batch [635/938], Loss: 0.4831206500530243\n",
      "Train: Epoch [21], Batch [636/938], Loss: 0.6011682748794556\n",
      "Train: Epoch [21], Batch [637/938], Loss: 0.4541458487510681\n",
      "Train: Epoch [21], Batch [638/938], Loss: 0.5568923354148865\n",
      "Train: Epoch [21], Batch [639/938], Loss: 0.5051710605621338\n",
      "Train: Epoch [21], Batch [640/938], Loss: 0.3150519132614136\n",
      "Train: Epoch [21], Batch [641/938], Loss: 0.3642085790634155\n",
      "Train: Epoch [21], Batch [642/938], Loss: 0.4602731764316559\n",
      "Train: Epoch [21], Batch [643/938], Loss: 0.36001482605934143\n",
      "Train: Epoch [21], Batch [644/938], Loss: 0.6101104617118835\n",
      "Train: Epoch [21], Batch [645/938], Loss: 0.5336893796920776\n",
      "Train: Epoch [21], Batch [646/938], Loss: 0.28462040424346924\n",
      "Train: Epoch [21], Batch [647/938], Loss: 0.34292909502983093\n",
      "Train: Epoch [21], Batch [648/938], Loss: 0.31166714429855347\n",
      "Train: Epoch [21], Batch [649/938], Loss: 0.4778221845626831\n",
      "Train: Epoch [21], Batch [650/938], Loss: 0.4539986550807953\n",
      "Train: Epoch [21], Batch [651/938], Loss: 0.32869404554367065\n",
      "Train: Epoch [21], Batch [652/938], Loss: 0.35107696056365967\n",
      "Train: Epoch [21], Batch [653/938], Loss: 0.31768178939819336\n",
      "Train: Epoch [21], Batch [654/938], Loss: 0.3425043821334839\n",
      "Train: Epoch [21], Batch [655/938], Loss: 0.4138793349266052\n",
      "Train: Epoch [21], Batch [656/938], Loss: 0.31774136424064636\n",
      "Train: Epoch [21], Batch [657/938], Loss: 0.5149705410003662\n",
      "Train: Epoch [21], Batch [658/938], Loss: 0.43873268365859985\n",
      "Train: Epoch [21], Batch [659/938], Loss: 0.4630260467529297\n",
      "Train: Epoch [21], Batch [660/938], Loss: 0.29819947481155396\n",
      "Train: Epoch [21], Batch [661/938], Loss: 0.4591037631034851\n",
      "Train: Epoch [21], Batch [662/938], Loss: 0.3569984436035156\n",
      "Train: Epoch [21], Batch [663/938], Loss: 0.4016587436199188\n",
      "Train: Epoch [21], Batch [664/938], Loss: 0.43186479806900024\n",
      "Train: Epoch [21], Batch [665/938], Loss: 0.3702714145183563\n",
      "Train: Epoch [21], Batch [666/938], Loss: 0.41926512122154236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [21], Batch [667/938], Loss: 0.4265536069869995\n",
      "Train: Epoch [21], Batch [668/938], Loss: 0.4031810164451599\n",
      "Train: Epoch [21], Batch [669/938], Loss: 0.5207488536834717\n",
      "Train: Epoch [21], Batch [670/938], Loss: 0.4441695809364319\n",
      "Train: Epoch [21], Batch [671/938], Loss: 0.46342623233795166\n",
      "Train: Epoch [21], Batch [672/938], Loss: 0.3440244793891907\n",
      "Train: Epoch [21], Batch [673/938], Loss: 0.4572834372520447\n",
      "Train: Epoch [21], Batch [674/938], Loss: 0.37126272916793823\n",
      "Train: Epoch [21], Batch [675/938], Loss: 0.2766321301460266\n",
      "Train: Epoch [21], Batch [676/938], Loss: 0.45893800258636475\n",
      "Train: Epoch [21], Batch [677/938], Loss: 0.322339802980423\n",
      "Train: Epoch [21], Batch [678/938], Loss: 0.3957725167274475\n",
      "Train: Epoch [21], Batch [679/938], Loss: 0.44105201959609985\n",
      "Train: Epoch [21], Batch [680/938], Loss: 0.5094637274742126\n",
      "Train: Epoch [21], Batch [681/938], Loss: 0.42609038949012756\n",
      "Train: Epoch [21], Batch [682/938], Loss: 0.45329636335372925\n",
      "Train: Epoch [21], Batch [683/938], Loss: 0.4364674687385559\n",
      "Train: Epoch [21], Batch [684/938], Loss: 0.3836216926574707\n",
      "Train: Epoch [21], Batch [685/938], Loss: 0.31421956419944763\n",
      "Train: Epoch [21], Batch [686/938], Loss: 0.5660609006881714\n",
      "Train: Epoch [21], Batch [687/938], Loss: 0.28796958923339844\n",
      "Train: Epoch [21], Batch [688/938], Loss: 0.5311253070831299\n",
      "Train: Epoch [21], Batch [689/938], Loss: 0.38827377557754517\n",
      "Train: Epoch [21], Batch [690/938], Loss: 0.4183731973171234\n",
      "Train: Epoch [21], Batch [691/938], Loss: 0.49096935987472534\n",
      "Train: Epoch [21], Batch [692/938], Loss: 0.3949568271636963\n",
      "Train: Epoch [21], Batch [693/938], Loss: 0.3887603282928467\n",
      "Train: Epoch [21], Batch [694/938], Loss: 0.4558282792568207\n",
      "Train: Epoch [21], Batch [695/938], Loss: 0.35937637090682983\n",
      "Train: Epoch [21], Batch [696/938], Loss: 0.3576173186302185\n",
      "Train: Epoch [21], Batch [697/938], Loss: 0.4224885106086731\n",
      "Train: Epoch [21], Batch [698/938], Loss: 0.44366830587387085\n",
      "Train: Epoch [21], Batch [699/938], Loss: 0.41378819942474365\n",
      "Train: Epoch [21], Batch [700/938], Loss: 0.5043612718582153\n",
      "Train: Epoch [21], Batch [701/938], Loss: 0.5390701293945312\n",
      "Train: Epoch [21], Batch [702/938], Loss: 0.3359755575656891\n",
      "Train: Epoch [21], Batch [703/938], Loss: 0.33039331436157227\n",
      "Train: Epoch [21], Batch [704/938], Loss: 0.4708896279335022\n",
      "Train: Epoch [21], Batch [705/938], Loss: 0.438956618309021\n",
      "Train: Epoch [21], Batch [706/938], Loss: 0.566463828086853\n",
      "Train: Epoch [21], Batch [707/938], Loss: 0.38469621539115906\n",
      "Train: Epoch [21], Batch [708/938], Loss: 0.3144266903400421\n",
      "Train: Epoch [21], Batch [709/938], Loss: 0.5805857181549072\n",
      "Train: Epoch [21], Batch [710/938], Loss: 0.38613665103912354\n",
      "Train: Epoch [21], Batch [711/938], Loss: 0.28559592366218567\n",
      "Train: Epoch [21], Batch [712/938], Loss: 0.48203492164611816\n",
      "Train: Epoch [21], Batch [713/938], Loss: 0.36126989126205444\n",
      "Train: Epoch [21], Batch [714/938], Loss: 0.4185134768486023\n",
      "Train: Epoch [21], Batch [715/938], Loss: 0.2846243381500244\n",
      "Train: Epoch [21], Batch [716/938], Loss: 0.29172879457473755\n",
      "Train: Epoch [21], Batch [717/938], Loss: 0.3734073340892792\n",
      "Train: Epoch [21], Batch [718/938], Loss: 0.2611945867538452\n",
      "Train: Epoch [21], Batch [719/938], Loss: 0.37457776069641113\n",
      "Train: Epoch [21], Batch [720/938], Loss: 0.37720340490341187\n",
      "Train: Epoch [21], Batch [721/938], Loss: 0.4233081042766571\n",
      "Train: Epoch [21], Batch [722/938], Loss: 0.5786092281341553\n",
      "Train: Epoch [21], Batch [723/938], Loss: 0.39859360456466675\n",
      "Train: Epoch [21], Batch [724/938], Loss: 0.3554726243019104\n",
      "Train: Epoch [21], Batch [725/938], Loss: 0.3227546811103821\n",
      "Train: Epoch [21], Batch [726/938], Loss: 0.27233901619911194\n",
      "Train: Epoch [21], Batch [727/938], Loss: 0.3855612277984619\n",
      "Train: Epoch [21], Batch [728/938], Loss: 0.27053454518318176\n",
      "Train: Epoch [21], Batch [729/938], Loss: 0.28879398107528687\n",
      "Train: Epoch [21], Batch [730/938], Loss: 0.4215308725833893\n",
      "Train: Epoch [21], Batch [731/938], Loss: 0.4061240553855896\n",
      "Train: Epoch [21], Batch [732/938], Loss: 0.37268489599227905\n",
      "Train: Epoch [21], Batch [733/938], Loss: 0.5877930521965027\n",
      "Train: Epoch [21], Batch [734/938], Loss: 0.3099173307418823\n",
      "Train: Epoch [21], Batch [735/938], Loss: 0.5476873517036438\n",
      "Train: Epoch [21], Batch [736/938], Loss: 0.5299760103225708\n",
      "Train: Epoch [21], Batch [737/938], Loss: 0.5264867544174194\n",
      "Train: Epoch [21], Batch [738/938], Loss: 0.2828579843044281\n",
      "Train: Epoch [21], Batch [739/938], Loss: 0.3335750102996826\n",
      "Train: Epoch [21], Batch [740/938], Loss: 0.26342272758483887\n",
      "Train: Epoch [21], Batch [741/938], Loss: 0.42976588010787964\n",
      "Train: Epoch [21], Batch [742/938], Loss: 0.396660715341568\n",
      "Train: Epoch [21], Batch [743/938], Loss: 0.24154505133628845\n",
      "Train: Epoch [21], Batch [744/938], Loss: 0.24589362740516663\n",
      "Train: Epoch [21], Batch [745/938], Loss: 0.33212602138519287\n",
      "Train: Epoch [21], Batch [746/938], Loss: 0.34459128975868225\n",
      "Train: Epoch [21], Batch [747/938], Loss: 0.4862724840641022\n",
      "Train: Epoch [21], Batch [748/938], Loss: 0.43705105781555176\n",
      "Train: Epoch [21], Batch [749/938], Loss: 0.610191822052002\n",
      "Train: Epoch [21], Batch [750/938], Loss: 0.3445820212364197\n",
      "Train: Epoch [21], Batch [751/938], Loss: 0.5769193768501282\n",
      "Train: Epoch [21], Batch [752/938], Loss: 0.4048422873020172\n",
      "Train: Epoch [21], Batch [753/938], Loss: 0.4644959270954132\n",
      "Train: Epoch [21], Batch [754/938], Loss: 0.3299117684364319\n",
      "Train: Epoch [21], Batch [755/938], Loss: 0.44749006628990173\n",
      "Train: Epoch [21], Batch [756/938], Loss: 0.2506621479988098\n",
      "Train: Epoch [21], Batch [757/938], Loss: 0.5625278353691101\n",
      "Train: Epoch [21], Batch [758/938], Loss: 0.29996055364608765\n",
      "Train: Epoch [21], Batch [759/938], Loss: 0.4575226306915283\n",
      "Train: Epoch [21], Batch [760/938], Loss: 0.4160941243171692\n",
      "Train: Epoch [21], Batch [761/938], Loss: 0.37545761466026306\n",
      "Train: Epoch [21], Batch [762/938], Loss: 0.5592089295387268\n",
      "Train: Epoch [21], Batch [763/938], Loss: 0.2076863944530487\n",
      "Train: Epoch [21], Batch [764/938], Loss: 0.2707839012145996\n",
      "Train: Epoch [21], Batch [765/938], Loss: 0.3128606677055359\n",
      "Train: Epoch [21], Batch [766/938], Loss: 0.4278274178504944\n",
      "Train: Epoch [21], Batch [767/938], Loss: 0.2748100459575653\n",
      "Train: Epoch [21], Batch [768/938], Loss: 0.3316969871520996\n",
      "Train: Epoch [21], Batch [769/938], Loss: 0.2849079966545105\n",
      "Train: Epoch [21], Batch [770/938], Loss: 0.3113698661327362\n",
      "Train: Epoch [21], Batch [771/938], Loss: 0.2804062068462372\n",
      "Train: Epoch [21], Batch [772/938], Loss: 0.5648248195648193\n",
      "Train: Epoch [21], Batch [773/938], Loss: 0.3443068563938141\n",
      "Train: Epoch [21], Batch [774/938], Loss: 0.3081357479095459\n",
      "Train: Epoch [21], Batch [775/938], Loss: 0.4684334397315979\n",
      "Train: Epoch [21], Batch [776/938], Loss: 0.3132578432559967\n",
      "Train: Epoch [21], Batch [777/938], Loss: 0.1659603863954544\n",
      "Train: Epoch [21], Batch [778/938], Loss: 0.5124080777168274\n",
      "Train: Epoch [21], Batch [779/938], Loss: 0.5418261289596558\n",
      "Train: Epoch [21], Batch [780/938], Loss: 0.43632394075393677\n",
      "Train: Epoch [21], Batch [781/938], Loss: 0.47659173607826233\n",
      "Train: Epoch [21], Batch [782/938], Loss: 0.4141964018344879\n",
      "Train: Epoch [21], Batch [783/938], Loss: 0.5640137195587158\n",
      "Train: Epoch [21], Batch [784/938], Loss: 0.43207526206970215\n",
      "Train: Epoch [21], Batch [785/938], Loss: 0.47224506735801697\n",
      "Train: Epoch [21], Batch [786/938], Loss: 0.4955550730228424\n",
      "Train: Epoch [21], Batch [787/938], Loss: 0.5186938047409058\n",
      "Train: Epoch [21], Batch [788/938], Loss: 0.48447805643081665\n",
      "Train: Epoch [21], Batch [789/938], Loss: 0.49240538477897644\n",
      "Train: Epoch [21], Batch [790/938], Loss: 0.36126017570495605\n",
      "Train: Epoch [21], Batch [791/938], Loss: 0.3493608832359314\n",
      "Train: Epoch [21], Batch [792/938], Loss: 0.485804945230484\n",
      "Train: Epoch [21], Batch [793/938], Loss: 0.3810665011405945\n",
      "Train: Epoch [21], Batch [794/938], Loss: 0.5149844884872437\n",
      "Train: Epoch [21], Batch [795/938], Loss: 0.30482399463653564\n",
      "Train: Epoch [21], Batch [796/938], Loss: 0.5059819221496582\n",
      "Train: Epoch [21], Batch [797/938], Loss: 0.3527608811855316\n",
      "Train: Epoch [21], Batch [798/938], Loss: 0.2846090793609619\n",
      "Train: Epoch [21], Batch [799/938], Loss: 0.365965336561203\n",
      "Train: Epoch [21], Batch [800/938], Loss: 0.38086843490600586\n",
      "Train: Epoch [21], Batch [801/938], Loss: 0.4637637734413147\n",
      "Train: Epoch [21], Batch [802/938], Loss: 0.33183199167251587\n",
      "Train: Epoch [21], Batch [803/938], Loss: 0.47373342514038086\n",
      "Train: Epoch [21], Batch [804/938], Loss: 0.4041813015937805\n",
      "Train: Epoch [21], Batch [805/938], Loss: 0.22066178917884827\n",
      "Train: Epoch [21], Batch [806/938], Loss: 0.42327427864074707\n",
      "Train: Epoch [21], Batch [807/938], Loss: 0.4422743618488312\n",
      "Train: Epoch [21], Batch [808/938], Loss: 0.28301602602005005\n",
      "Train: Epoch [21], Batch [809/938], Loss: 0.36624377965927124\n",
      "Train: Epoch [21], Batch [810/938], Loss: 0.6260728240013123\n",
      "Train: Epoch [21], Batch [811/938], Loss: 0.43814802169799805\n",
      "Train: Epoch [21], Batch [812/938], Loss: 0.5238338708877563\n",
      "Train: Epoch [21], Batch [813/938], Loss: 0.48281359672546387\n",
      "Train: Epoch [21], Batch [814/938], Loss: 0.3059064745903015\n",
      "Train: Epoch [21], Batch [815/938], Loss: 0.4058389663696289\n",
      "Train: Epoch [21], Batch [816/938], Loss: 0.33437028527259827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [21], Batch [817/938], Loss: 0.2543279528617859\n",
      "Train: Epoch [21], Batch [818/938], Loss: 0.47913816571235657\n",
      "Train: Epoch [21], Batch [819/938], Loss: 0.3580150604248047\n",
      "Train: Epoch [21], Batch [820/938], Loss: 0.38212502002716064\n",
      "Train: Epoch [21], Batch [821/938], Loss: 0.4068499207496643\n",
      "Train: Epoch [21], Batch [822/938], Loss: 0.35802581906318665\n",
      "Train: Epoch [21], Batch [823/938], Loss: 0.4096878170967102\n",
      "Train: Epoch [21], Batch [824/938], Loss: 0.4745568037033081\n",
      "Train: Epoch [21], Batch [825/938], Loss: 0.2897326350212097\n",
      "Train: Epoch [21], Batch [826/938], Loss: 0.33322852849960327\n",
      "Train: Epoch [21], Batch [827/938], Loss: 0.38586756587028503\n",
      "Train: Epoch [21], Batch [828/938], Loss: 0.3016459345817566\n",
      "Train: Epoch [21], Batch [829/938], Loss: 0.4147379398345947\n",
      "Train: Epoch [21], Batch [830/938], Loss: 0.48403987288475037\n",
      "Train: Epoch [21], Batch [831/938], Loss: 0.5680508613586426\n",
      "Train: Epoch [21], Batch [832/938], Loss: 0.5293883085250854\n",
      "Train: Epoch [21], Batch [833/938], Loss: 0.47847339510917664\n",
      "Train: Epoch [21], Batch [834/938], Loss: 0.2599790692329407\n",
      "Train: Epoch [21], Batch [835/938], Loss: 0.420357346534729\n",
      "Train: Epoch [21], Batch [836/938], Loss: 0.47121164202690125\n",
      "Train: Epoch [21], Batch [837/938], Loss: 0.47063693404197693\n",
      "Train: Epoch [21], Batch [838/938], Loss: 0.39132773876190186\n",
      "Train: Epoch [21], Batch [839/938], Loss: 0.5836856961250305\n",
      "Train: Epoch [21], Batch [840/938], Loss: 0.35069891810417175\n",
      "Train: Epoch [21], Batch [841/938], Loss: 0.33856987953186035\n",
      "Train: Epoch [21], Batch [842/938], Loss: 0.3755000829696655\n",
      "Train: Epoch [21], Batch [843/938], Loss: 0.23387950658798218\n",
      "Train: Epoch [21], Batch [844/938], Loss: 0.24897557497024536\n",
      "Train: Epoch [21], Batch [845/938], Loss: 0.5662964582443237\n",
      "Train: Epoch [21], Batch [846/938], Loss: 0.5612525939941406\n",
      "Train: Epoch [21], Batch [847/938], Loss: 0.3768172264099121\n",
      "Train: Epoch [21], Batch [848/938], Loss: 0.3647952079772949\n",
      "Train: Epoch [21], Batch [849/938], Loss: 0.3598121404647827\n",
      "Train: Epoch [21], Batch [850/938], Loss: 0.2608048915863037\n",
      "Train: Epoch [21], Batch [851/938], Loss: 0.5865872502326965\n",
      "Train: Epoch [21], Batch [852/938], Loss: 0.5176684856414795\n",
      "Train: Epoch [21], Batch [853/938], Loss: 0.27556824684143066\n",
      "Train: Epoch [21], Batch [854/938], Loss: 0.327158659696579\n",
      "Train: Epoch [21], Batch [855/938], Loss: 0.22758036851882935\n",
      "Train: Epoch [21], Batch [856/938], Loss: 0.2023705542087555\n",
      "Train: Epoch [21], Batch [857/938], Loss: 0.30853456258773804\n",
      "Train: Epoch [21], Batch [858/938], Loss: 0.5482139587402344\n",
      "Train: Epoch [21], Batch [859/938], Loss: 0.30855804681777954\n",
      "Train: Epoch [21], Batch [860/938], Loss: 0.3844149708747864\n",
      "Train: Epoch [21], Batch [861/938], Loss: 0.4745118021965027\n",
      "Train: Epoch [21], Batch [862/938], Loss: 0.5184719562530518\n",
      "Train: Epoch [21], Batch [863/938], Loss: 0.5181224942207336\n",
      "Train: Epoch [21], Batch [864/938], Loss: 0.29603177309036255\n",
      "Train: Epoch [21], Batch [865/938], Loss: 0.5286381840705872\n",
      "Train: Epoch [21], Batch [866/938], Loss: 0.4491805136203766\n",
      "Train: Epoch [21], Batch [867/938], Loss: 0.3365926444530487\n",
      "Train: Epoch [21], Batch [868/938], Loss: 0.3015558421611786\n",
      "Train: Epoch [21], Batch [869/938], Loss: 0.4513549208641052\n",
      "Train: Epoch [21], Batch [870/938], Loss: 0.24230635166168213\n",
      "Train: Epoch [21], Batch [871/938], Loss: 0.5389919281005859\n",
      "Train: Epoch [21], Batch [872/938], Loss: 0.2976014316082001\n",
      "Train: Epoch [21], Batch [873/938], Loss: 0.31925761699676514\n",
      "Train: Epoch [21], Batch [874/938], Loss: 0.5851526260375977\n",
      "Train: Epoch [21], Batch [875/938], Loss: 0.40791481733322144\n",
      "Train: Epoch [21], Batch [876/938], Loss: 0.5967488884925842\n",
      "Train: Epoch [21], Batch [877/938], Loss: 0.5891240835189819\n",
      "Train: Epoch [21], Batch [878/938], Loss: 0.4252721667289734\n",
      "Train: Epoch [21], Batch [879/938], Loss: 0.38864225149154663\n",
      "Train: Epoch [21], Batch [880/938], Loss: 0.506911039352417\n",
      "Train: Epoch [21], Batch [881/938], Loss: 0.4440379738807678\n",
      "Train: Epoch [21], Batch [882/938], Loss: 0.48636767268180847\n",
      "Train: Epoch [21], Batch [883/938], Loss: 0.2989217936992645\n",
      "Train: Epoch [21], Batch [884/938], Loss: 0.28310272097587585\n",
      "Train: Epoch [21], Batch [885/938], Loss: 0.31017395853996277\n",
      "Train: Epoch [21], Batch [886/938], Loss: 0.4858427345752716\n",
      "Train: Epoch [21], Batch [887/938], Loss: 0.37054985761642456\n",
      "Train: Epoch [21], Batch [888/938], Loss: 0.2714192569255829\n",
      "Train: Epoch [21], Batch [889/938], Loss: 0.2827644348144531\n",
      "Train: Epoch [21], Batch [890/938], Loss: 0.3842560052871704\n",
      "Train: Epoch [21], Batch [891/938], Loss: 0.3522377908229828\n",
      "Train: Epoch [21], Batch [892/938], Loss: 0.1943604052066803\n",
      "Train: Epoch [21], Batch [893/938], Loss: 0.2998395562171936\n",
      "Train: Epoch [21], Batch [894/938], Loss: 0.21164244413375854\n",
      "Train: Epoch [21], Batch [895/938], Loss: 0.5348530411720276\n",
      "Train: Epoch [21], Batch [896/938], Loss: 0.36152184009552\n",
      "Train: Epoch [21], Batch [897/938], Loss: 0.4293805658817291\n",
      "Train: Epoch [21], Batch [898/938], Loss: 0.5846171379089355\n",
      "Train: Epoch [21], Batch [899/938], Loss: 0.5182921886444092\n",
      "Train: Epoch [21], Batch [900/938], Loss: 0.2792968153953552\n",
      "Train: Epoch [21], Batch [901/938], Loss: 0.3232586681842804\n",
      "Train: Epoch [21], Batch [902/938], Loss: 0.37841975688934326\n",
      "Train: Epoch [21], Batch [903/938], Loss: 0.36359918117523193\n",
      "Train: Epoch [21], Batch [904/938], Loss: 0.23999691009521484\n",
      "Train: Epoch [21], Batch [905/938], Loss: 0.5467246770858765\n",
      "Train: Epoch [21], Batch [906/938], Loss: 0.43194055557250977\n",
      "Train: Epoch [21], Batch [907/938], Loss: 0.27178454399108887\n",
      "Train: Epoch [21], Batch [908/938], Loss: 0.3652634024620056\n",
      "Train: Epoch [21], Batch [909/938], Loss: 0.328978955745697\n",
      "Train: Epoch [21], Batch [910/938], Loss: 0.42465072870254517\n",
      "Train: Epoch [21], Batch [911/938], Loss: 0.3996524512767792\n",
      "Train: Epoch [21], Batch [912/938], Loss: 0.3420849144458771\n",
      "Train: Epoch [21], Batch [913/938], Loss: 0.3243196904659271\n",
      "Train: Epoch [21], Batch [914/938], Loss: 0.3024402856826782\n",
      "Train: Epoch [21], Batch [915/938], Loss: 0.5937530398368835\n",
      "Train: Epoch [21], Batch [916/938], Loss: 0.4197917580604553\n",
      "Train: Epoch [21], Batch [917/938], Loss: 0.31822478771209717\n",
      "Train: Epoch [21], Batch [918/938], Loss: 0.37256184220314026\n",
      "Train: Epoch [21], Batch [919/938], Loss: 0.33160826563835144\n",
      "Train: Epoch [21], Batch [920/938], Loss: 0.33380064368247986\n",
      "Train: Epoch [21], Batch [921/938], Loss: 0.39513373374938965\n",
      "Train: Epoch [21], Batch [922/938], Loss: 0.5585554838180542\n",
      "Train: Epoch [21], Batch [923/938], Loss: 0.24058926105499268\n",
      "Train: Epoch [21], Batch [924/938], Loss: 0.462217777967453\n",
      "Train: Epoch [21], Batch [925/938], Loss: 0.5330237150192261\n",
      "Train: Epoch [21], Batch [926/938], Loss: 0.34406304359436035\n",
      "Train: Epoch [21], Batch [927/938], Loss: 0.5188827514648438\n",
      "Train: Epoch [21], Batch [928/938], Loss: 0.3780965209007263\n",
      "Train: Epoch [21], Batch [929/938], Loss: 0.45800280570983887\n",
      "Train: Epoch [21], Batch [930/938], Loss: 0.46866893768310547\n",
      "Train: Epoch [21], Batch [931/938], Loss: 0.5725927352905273\n",
      "Train: Epoch [21], Batch [932/938], Loss: 0.5437049865722656\n",
      "Train: Epoch [21], Batch [933/938], Loss: 0.25676774978637695\n",
      "Train: Epoch [21], Batch [934/938], Loss: 0.34743478894233704\n",
      "Train: Epoch [21], Batch [935/938], Loss: 0.745228111743927\n",
      "Train: Epoch [21], Batch [936/938], Loss: 0.3135790228843689\n",
      "Train: Epoch [21], Batch [937/938], Loss: 0.38167595863342285\n",
      "Train: Epoch [21], Batch [938/938], Loss: 0.5062519311904907\n",
      "Accuracy of train set: 0.8574333333333334\n",
      "Validation: Epoch [21], Batch [1/938], Loss: 0.33149582147598267\n",
      "Validation: Epoch [21], Batch [2/938], Loss: 0.4239625632762909\n",
      "Validation: Epoch [21], Batch [3/938], Loss: 0.32543909549713135\n",
      "Validation: Epoch [21], Batch [4/938], Loss: 0.4014474153518677\n",
      "Validation: Epoch [21], Batch [5/938], Loss: 0.5154041051864624\n",
      "Validation: Epoch [21], Batch [6/938], Loss: 0.41376441717147827\n",
      "Validation: Epoch [21], Batch [7/938], Loss: 0.3869243562221527\n",
      "Validation: Epoch [21], Batch [8/938], Loss: 0.20506295561790466\n",
      "Validation: Epoch [21], Batch [9/938], Loss: 0.3757065534591675\n",
      "Validation: Epoch [21], Batch [10/938], Loss: 0.34328898787498474\n",
      "Validation: Epoch [21], Batch [11/938], Loss: 0.44992274045944214\n",
      "Validation: Epoch [21], Batch [12/938], Loss: 0.5037541389465332\n",
      "Validation: Epoch [21], Batch [13/938], Loss: 0.5472054481506348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [14/938], Loss: 0.6275527477264404\n",
      "Validation: Epoch [21], Batch [15/938], Loss: 0.29951050877571106\n",
      "Validation: Epoch [21], Batch [16/938], Loss: 0.8552863597869873\n",
      "Validation: Epoch [21], Batch [17/938], Loss: 0.5070540904998779\n",
      "Validation: Epoch [21], Batch [18/938], Loss: 0.3285256028175354\n",
      "Validation: Epoch [21], Batch [19/938], Loss: 0.5252894759178162\n",
      "Validation: Epoch [21], Batch [20/938], Loss: 0.4698638916015625\n",
      "Validation: Epoch [21], Batch [21/938], Loss: 0.3766590356826782\n",
      "Validation: Epoch [21], Batch [22/938], Loss: 0.28449252247810364\n",
      "Validation: Epoch [21], Batch [23/938], Loss: 0.302967369556427\n",
      "Validation: Epoch [21], Batch [24/938], Loss: 0.5301769971847534\n",
      "Validation: Epoch [21], Batch [25/938], Loss: 0.47844719886779785\n",
      "Validation: Epoch [21], Batch [26/938], Loss: 0.4190577566623688\n",
      "Validation: Epoch [21], Batch [27/938], Loss: 0.38456469774246216\n",
      "Validation: Epoch [21], Batch [28/938], Loss: 0.3315908908843994\n",
      "Validation: Epoch [21], Batch [29/938], Loss: 0.3416725993156433\n",
      "Validation: Epoch [21], Batch [30/938], Loss: 0.38748228549957275\n",
      "Validation: Epoch [21], Batch [31/938], Loss: 0.47146373987197876\n",
      "Validation: Epoch [21], Batch [32/938], Loss: 0.29057976603507996\n",
      "Validation: Epoch [21], Batch [33/938], Loss: 0.5508844256401062\n",
      "Validation: Epoch [21], Batch [34/938], Loss: 0.5440247058868408\n",
      "Validation: Epoch [21], Batch [35/938], Loss: 0.6501638889312744\n",
      "Validation: Epoch [21], Batch [36/938], Loss: 0.29723504185676575\n",
      "Validation: Epoch [21], Batch [37/938], Loss: 0.5926265716552734\n",
      "Validation: Epoch [21], Batch [38/938], Loss: 0.4274442493915558\n",
      "Validation: Epoch [21], Batch [39/938], Loss: 0.3288172483444214\n",
      "Validation: Epoch [21], Batch [40/938], Loss: 0.28970232605934143\n",
      "Validation: Epoch [21], Batch [41/938], Loss: 0.4156377613544464\n",
      "Validation: Epoch [21], Batch [42/938], Loss: 0.4299744665622711\n",
      "Validation: Epoch [21], Batch [43/938], Loss: 0.5102242827415466\n",
      "Validation: Epoch [21], Batch [44/938], Loss: 0.42119887471199036\n",
      "Validation: Epoch [21], Batch [45/938], Loss: 0.258474200963974\n",
      "Validation: Epoch [21], Batch [46/938], Loss: 0.22027811408042908\n",
      "Validation: Epoch [21], Batch [47/938], Loss: 0.40635350346565247\n",
      "Validation: Epoch [21], Batch [48/938], Loss: 0.5287175178527832\n",
      "Validation: Epoch [21], Batch [49/938], Loss: 0.45180684328079224\n",
      "Validation: Epoch [21], Batch [50/938], Loss: 0.5299177765846252\n",
      "Validation: Epoch [21], Batch [51/938], Loss: 0.2765016555786133\n",
      "Validation: Epoch [21], Batch [52/938], Loss: 0.46306777000427246\n",
      "Validation: Epoch [21], Batch [53/938], Loss: 0.4129220247268677\n",
      "Validation: Epoch [21], Batch [54/938], Loss: 0.5676543116569519\n",
      "Validation: Epoch [21], Batch [55/938], Loss: 0.3236566185951233\n",
      "Validation: Epoch [21], Batch [56/938], Loss: 0.4514826536178589\n",
      "Validation: Epoch [21], Batch [57/938], Loss: 0.45691949129104614\n",
      "Validation: Epoch [21], Batch [58/938], Loss: 0.3509182333946228\n",
      "Validation: Epoch [21], Batch [59/938], Loss: 0.3892228603363037\n",
      "Validation: Epoch [21], Batch [60/938], Loss: 0.3302803635597229\n",
      "Validation: Epoch [21], Batch [61/938], Loss: 0.3679156005382538\n",
      "Validation: Epoch [21], Batch [62/938], Loss: 0.4180324673652649\n",
      "Validation: Epoch [21], Batch [63/938], Loss: 0.3655562400817871\n",
      "Validation: Epoch [21], Batch [64/938], Loss: 0.4152770936489105\n",
      "Validation: Epoch [21], Batch [65/938], Loss: 0.4205710291862488\n",
      "Validation: Epoch [21], Batch [66/938], Loss: 0.3959537148475647\n",
      "Validation: Epoch [21], Batch [67/938], Loss: 0.45391589403152466\n",
      "Validation: Epoch [21], Batch [68/938], Loss: 0.40987280011177063\n",
      "Validation: Epoch [21], Batch [69/938], Loss: 0.4549143612384796\n",
      "Validation: Epoch [21], Batch [70/938], Loss: 0.5260750651359558\n",
      "Validation: Epoch [21], Batch [71/938], Loss: 0.5096607804298401\n",
      "Validation: Epoch [21], Batch [72/938], Loss: 0.2947441339492798\n",
      "Validation: Epoch [21], Batch [73/938], Loss: 0.4826716482639313\n",
      "Validation: Epoch [21], Batch [74/938], Loss: 0.49333295226097107\n",
      "Validation: Epoch [21], Batch [75/938], Loss: 0.6322038769721985\n",
      "Validation: Epoch [21], Batch [76/938], Loss: 0.46656590700149536\n",
      "Validation: Epoch [21], Batch [77/938], Loss: 0.4866037666797638\n",
      "Validation: Epoch [21], Batch [78/938], Loss: 0.30146658420562744\n",
      "Validation: Epoch [21], Batch [79/938], Loss: 0.5013002157211304\n",
      "Validation: Epoch [21], Batch [80/938], Loss: 0.438308984041214\n",
      "Validation: Epoch [21], Batch [81/938], Loss: 0.45593979954719543\n",
      "Validation: Epoch [21], Batch [82/938], Loss: 0.44989901781082153\n",
      "Validation: Epoch [21], Batch [83/938], Loss: 0.45134130120277405\n",
      "Validation: Epoch [21], Batch [84/938], Loss: 0.3509382903575897\n",
      "Validation: Epoch [21], Batch [85/938], Loss: 0.6589516401290894\n",
      "Validation: Epoch [21], Batch [86/938], Loss: 0.7020547389984131\n",
      "Validation: Epoch [21], Batch [87/938], Loss: 0.4155363440513611\n",
      "Validation: Epoch [21], Batch [88/938], Loss: 0.4565090239048004\n",
      "Validation: Epoch [21], Batch [89/938], Loss: 0.4336886405944824\n",
      "Validation: Epoch [21], Batch [90/938], Loss: 0.5727095603942871\n",
      "Validation: Epoch [21], Batch [91/938], Loss: 0.39741837978363037\n",
      "Validation: Epoch [21], Batch [92/938], Loss: 0.2899520993232727\n",
      "Validation: Epoch [21], Batch [93/938], Loss: 0.5650609731674194\n",
      "Validation: Epoch [21], Batch [94/938], Loss: 0.4594680368900299\n",
      "Validation: Epoch [21], Batch [95/938], Loss: 0.5755926966667175\n",
      "Validation: Epoch [21], Batch [96/938], Loss: 0.37451010942459106\n",
      "Validation: Epoch [21], Batch [97/938], Loss: 0.5512526631355286\n",
      "Validation: Epoch [21], Batch [98/938], Loss: 0.3504103422164917\n",
      "Validation: Epoch [21], Batch [99/938], Loss: 0.3733360767364502\n",
      "Validation: Epoch [21], Batch [100/938], Loss: 0.4355819821357727\n",
      "Validation: Epoch [21], Batch [101/938], Loss: 0.32564854621887207\n",
      "Validation: Epoch [21], Batch [102/938], Loss: 0.30558210611343384\n",
      "Validation: Epoch [21], Batch [103/938], Loss: 0.47240644693374634\n",
      "Validation: Epoch [21], Batch [104/938], Loss: 0.32437604665756226\n",
      "Validation: Epoch [21], Batch [105/938], Loss: 0.4084869921207428\n",
      "Validation: Epoch [21], Batch [106/938], Loss: 0.6320074796676636\n",
      "Validation: Epoch [21], Batch [107/938], Loss: 0.23190191388130188\n",
      "Validation: Epoch [21], Batch [108/938], Loss: 0.33070245385169983\n",
      "Validation: Epoch [21], Batch [109/938], Loss: 0.3003349304199219\n",
      "Validation: Epoch [21], Batch [110/938], Loss: 0.39746880531311035\n",
      "Validation: Epoch [21], Batch [111/938], Loss: 0.4349835515022278\n",
      "Validation: Epoch [21], Batch [112/938], Loss: 0.6284827589988708\n",
      "Validation: Epoch [21], Batch [113/938], Loss: 0.4002232849597931\n",
      "Validation: Epoch [21], Batch [114/938], Loss: 0.3296930491924286\n",
      "Validation: Epoch [21], Batch [115/938], Loss: 0.2841317653656006\n",
      "Validation: Epoch [21], Batch [116/938], Loss: 0.3850116431713104\n",
      "Validation: Epoch [21], Batch [117/938], Loss: 0.38029205799102783\n",
      "Validation: Epoch [21], Batch [118/938], Loss: 0.42413437366485596\n",
      "Validation: Epoch [21], Batch [119/938], Loss: 0.6739059090614319\n",
      "Validation: Epoch [21], Batch [120/938], Loss: 0.31550925970077515\n",
      "Validation: Epoch [21], Batch [121/938], Loss: 0.6660136580467224\n",
      "Validation: Epoch [21], Batch [122/938], Loss: 0.4059578478336334\n",
      "Validation: Epoch [21], Batch [123/938], Loss: 0.390324205160141\n",
      "Validation: Epoch [21], Batch [124/938], Loss: 0.43227890133857727\n",
      "Validation: Epoch [21], Batch [125/938], Loss: 0.47156184911727905\n",
      "Validation: Epoch [21], Batch [126/938], Loss: 0.48187822103500366\n",
      "Validation: Epoch [21], Batch [127/938], Loss: 0.44532668590545654\n",
      "Validation: Epoch [21], Batch [128/938], Loss: 0.35581812262535095\n",
      "Validation: Epoch [21], Batch [129/938], Loss: 0.4114327132701874\n",
      "Validation: Epoch [21], Batch [130/938], Loss: 0.31586161255836487\n",
      "Validation: Epoch [21], Batch [131/938], Loss: 0.3761548399925232\n",
      "Validation: Epoch [21], Batch [132/938], Loss: 0.5160762071609497\n",
      "Validation: Epoch [21], Batch [133/938], Loss: 0.1692558377981186\n",
      "Validation: Epoch [21], Batch [134/938], Loss: 0.2529185116291046\n",
      "Validation: Epoch [21], Batch [135/938], Loss: 0.4947684407234192\n",
      "Validation: Epoch [21], Batch [136/938], Loss: 0.4276140332221985\n",
      "Validation: Epoch [21], Batch [137/938], Loss: 0.3658212125301361\n",
      "Validation: Epoch [21], Batch [138/938], Loss: 0.6868549585342407\n",
      "Validation: Epoch [21], Batch [139/938], Loss: 0.2552877962589264\n",
      "Validation: Epoch [21], Batch [140/938], Loss: 0.3398342728614807\n",
      "Validation: Epoch [21], Batch [141/938], Loss: 0.42111945152282715\n",
      "Validation: Epoch [21], Batch [142/938], Loss: 0.48030686378479004\n",
      "Validation: Epoch [21], Batch [143/938], Loss: 0.23356273770332336\n",
      "Validation: Epoch [21], Batch [144/938], Loss: 0.5126519799232483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [145/938], Loss: 0.38532882928848267\n",
      "Validation: Epoch [21], Batch [146/938], Loss: 0.48440951108932495\n",
      "Validation: Epoch [21], Batch [147/938], Loss: 0.403962105512619\n",
      "Validation: Epoch [21], Batch [148/938], Loss: 0.372685968875885\n",
      "Validation: Epoch [21], Batch [149/938], Loss: 0.5806455016136169\n",
      "Validation: Epoch [21], Batch [150/938], Loss: 0.36698228120803833\n",
      "Validation: Epoch [21], Batch [151/938], Loss: 0.4318004548549652\n",
      "Validation: Epoch [21], Batch [152/938], Loss: 0.34708648920059204\n",
      "Validation: Epoch [21], Batch [153/938], Loss: 0.38310620188713074\n",
      "Validation: Epoch [21], Batch [154/938], Loss: 0.48738908767700195\n",
      "Validation: Epoch [21], Batch [155/938], Loss: 0.4221382439136505\n",
      "Validation: Epoch [21], Batch [156/938], Loss: 0.2274133712053299\n",
      "Validation: Epoch [21], Batch [157/938], Loss: 0.5087652206420898\n",
      "Validation: Epoch [21], Batch [158/938], Loss: 0.4739318788051605\n",
      "Validation: Epoch [21], Batch [159/938], Loss: 0.4072915017604828\n",
      "Validation: Epoch [21], Batch [160/938], Loss: 0.34224024415016174\n",
      "Validation: Epoch [21], Batch [161/938], Loss: 0.6396929621696472\n",
      "Validation: Epoch [21], Batch [162/938], Loss: 0.6574876308441162\n",
      "Validation: Epoch [21], Batch [163/938], Loss: 0.4798714220523834\n",
      "Validation: Epoch [21], Batch [164/938], Loss: 0.5399335622787476\n",
      "Validation: Epoch [21], Batch [165/938], Loss: 0.5584219694137573\n",
      "Validation: Epoch [21], Batch [166/938], Loss: 0.3444962501525879\n",
      "Validation: Epoch [21], Batch [167/938], Loss: 0.44694727659225464\n",
      "Validation: Epoch [21], Batch [168/938], Loss: 0.2418917715549469\n",
      "Validation: Epoch [21], Batch [169/938], Loss: 0.5145633220672607\n",
      "Validation: Epoch [21], Batch [170/938], Loss: 0.36586230993270874\n",
      "Validation: Epoch [21], Batch [171/938], Loss: 0.543489396572113\n",
      "Validation: Epoch [21], Batch [172/938], Loss: 0.3454364538192749\n",
      "Validation: Epoch [21], Batch [173/938], Loss: 0.45832523703575134\n",
      "Validation: Epoch [21], Batch [174/938], Loss: 0.31362369656562805\n",
      "Validation: Epoch [21], Batch [175/938], Loss: 0.34028738737106323\n",
      "Validation: Epoch [21], Batch [176/938], Loss: 0.5726594924926758\n",
      "Validation: Epoch [21], Batch [177/938], Loss: 0.46564412117004395\n",
      "Validation: Epoch [21], Batch [178/938], Loss: 0.35343557596206665\n",
      "Validation: Epoch [21], Batch [179/938], Loss: 0.2780682146549225\n",
      "Validation: Epoch [21], Batch [180/938], Loss: 0.426268070936203\n",
      "Validation: Epoch [21], Batch [181/938], Loss: 0.22936531901359558\n",
      "Validation: Epoch [21], Batch [182/938], Loss: 0.31973111629486084\n",
      "Validation: Epoch [21], Batch [183/938], Loss: 0.359999418258667\n",
      "Validation: Epoch [21], Batch [184/938], Loss: 0.3865334093570709\n",
      "Validation: Epoch [21], Batch [185/938], Loss: 0.2671380937099457\n",
      "Validation: Epoch [21], Batch [186/938], Loss: 0.3851448595523834\n",
      "Validation: Epoch [21], Batch [187/938], Loss: 0.32434046268463135\n",
      "Validation: Epoch [21], Batch [188/938], Loss: 0.3322671353816986\n",
      "Validation: Epoch [21], Batch [189/938], Loss: 0.30691829323768616\n",
      "Validation: Epoch [21], Batch [190/938], Loss: 0.44306790828704834\n",
      "Validation: Epoch [21], Batch [191/938], Loss: 0.37036052346229553\n",
      "Validation: Epoch [21], Batch [192/938], Loss: 0.51389479637146\n",
      "Validation: Epoch [21], Batch [193/938], Loss: 0.3548610210418701\n",
      "Validation: Epoch [21], Batch [194/938], Loss: 0.43003976345062256\n",
      "Validation: Epoch [21], Batch [195/938], Loss: 0.31586456298828125\n",
      "Validation: Epoch [21], Batch [196/938], Loss: 0.4135149419307709\n",
      "Validation: Epoch [21], Batch [197/938], Loss: 0.46715593338012695\n",
      "Validation: Epoch [21], Batch [198/938], Loss: 0.23216398060321808\n",
      "Validation: Epoch [21], Batch [199/938], Loss: 0.45628464221954346\n",
      "Validation: Epoch [21], Batch [200/938], Loss: 0.46991923451423645\n",
      "Validation: Epoch [21], Batch [201/938], Loss: 0.36538904905319214\n",
      "Validation: Epoch [21], Batch [202/938], Loss: 0.5275622606277466\n",
      "Validation: Epoch [21], Batch [203/938], Loss: 0.5946378111839294\n",
      "Validation: Epoch [21], Batch [204/938], Loss: 0.4500562250614166\n",
      "Validation: Epoch [21], Batch [205/938], Loss: 0.22396531701087952\n",
      "Validation: Epoch [21], Batch [206/938], Loss: 0.5755678415298462\n",
      "Validation: Epoch [21], Batch [207/938], Loss: 0.456991583108902\n",
      "Validation: Epoch [21], Batch [208/938], Loss: 0.4218493700027466\n",
      "Validation: Epoch [21], Batch [209/938], Loss: 0.37732619047164917\n",
      "Validation: Epoch [21], Batch [210/938], Loss: 0.3018699884414673\n",
      "Validation: Epoch [21], Batch [211/938], Loss: 0.6513679623603821\n",
      "Validation: Epoch [21], Batch [212/938], Loss: 0.49573400616645813\n",
      "Validation: Epoch [21], Batch [213/938], Loss: 0.4655740261077881\n",
      "Validation: Epoch [21], Batch [214/938], Loss: 0.4351640045642853\n",
      "Validation: Epoch [21], Batch [215/938], Loss: 0.34631121158599854\n",
      "Validation: Epoch [21], Batch [216/938], Loss: 0.3394696116447449\n",
      "Validation: Epoch [21], Batch [217/938], Loss: 0.5347965955734253\n",
      "Validation: Epoch [21], Batch [218/938], Loss: 0.5360756516456604\n",
      "Validation: Epoch [21], Batch [219/938], Loss: 0.4200217127799988\n",
      "Validation: Epoch [21], Batch [220/938], Loss: 0.3238374590873718\n",
      "Validation: Epoch [21], Batch [221/938], Loss: 0.5216970443725586\n",
      "Validation: Epoch [21], Batch [222/938], Loss: 0.6261191368103027\n",
      "Validation: Epoch [21], Batch [223/938], Loss: 0.2348334938287735\n",
      "Validation: Epoch [21], Batch [224/938], Loss: 0.3386096954345703\n",
      "Validation: Epoch [21], Batch [225/938], Loss: 0.27861738204956055\n",
      "Validation: Epoch [21], Batch [226/938], Loss: 0.3580390214920044\n",
      "Validation: Epoch [21], Batch [227/938], Loss: 0.22863692045211792\n",
      "Validation: Epoch [21], Batch [228/938], Loss: 0.34557193517684937\n",
      "Validation: Epoch [21], Batch [229/938], Loss: 0.4692770838737488\n",
      "Validation: Epoch [21], Batch [230/938], Loss: 0.34744992852211\n",
      "Validation: Epoch [21], Batch [231/938], Loss: 0.5194110870361328\n",
      "Validation: Epoch [21], Batch [232/938], Loss: 0.2754186987876892\n",
      "Validation: Epoch [21], Batch [233/938], Loss: 0.4094553589820862\n",
      "Validation: Epoch [21], Batch [234/938], Loss: 0.29313063621520996\n",
      "Validation: Epoch [21], Batch [235/938], Loss: 0.4264848828315735\n",
      "Validation: Epoch [21], Batch [236/938], Loss: 0.2666785717010498\n",
      "Validation: Epoch [21], Batch [237/938], Loss: 0.5776270627975464\n",
      "Validation: Epoch [21], Batch [238/938], Loss: 0.40020161867141724\n",
      "Validation: Epoch [21], Batch [239/938], Loss: 0.6681114435195923\n",
      "Validation: Epoch [21], Batch [240/938], Loss: 0.4204973876476288\n",
      "Validation: Epoch [21], Batch [241/938], Loss: 0.4197646975517273\n",
      "Validation: Epoch [21], Batch [242/938], Loss: 0.5256873369216919\n",
      "Validation: Epoch [21], Batch [243/938], Loss: 0.3699404001235962\n",
      "Validation: Epoch [21], Batch [244/938], Loss: 0.48800891637802124\n",
      "Validation: Epoch [21], Batch [245/938], Loss: 0.36118781566619873\n",
      "Validation: Epoch [21], Batch [246/938], Loss: 0.47425639629364014\n",
      "Validation: Epoch [21], Batch [247/938], Loss: 0.4115651845932007\n",
      "Validation: Epoch [21], Batch [248/938], Loss: 0.5346463918685913\n",
      "Validation: Epoch [21], Batch [249/938], Loss: 0.6143680810928345\n",
      "Validation: Epoch [21], Batch [250/938], Loss: 0.29073721170425415\n",
      "Validation: Epoch [21], Batch [251/938], Loss: 0.5029466152191162\n",
      "Validation: Epoch [21], Batch [252/938], Loss: 0.43394482135772705\n",
      "Validation: Epoch [21], Batch [253/938], Loss: 0.6715149879455566\n",
      "Validation: Epoch [21], Batch [254/938], Loss: 0.3491799235343933\n",
      "Validation: Epoch [21], Batch [255/938], Loss: 0.4551469087600708\n",
      "Validation: Epoch [21], Batch [256/938], Loss: 0.40886422991752625\n",
      "Validation: Epoch [21], Batch [257/938], Loss: 0.561671257019043\n",
      "Validation: Epoch [21], Batch [258/938], Loss: 0.5442230701446533\n",
      "Validation: Epoch [21], Batch [259/938], Loss: 0.34946584701538086\n",
      "Validation: Epoch [21], Batch [260/938], Loss: 0.4802151918411255\n",
      "Validation: Epoch [21], Batch [261/938], Loss: 0.44583290815353394\n",
      "Validation: Epoch [21], Batch [262/938], Loss: 0.47720280289649963\n",
      "Validation: Epoch [21], Batch [263/938], Loss: 0.5361571311950684\n",
      "Validation: Epoch [21], Batch [264/938], Loss: 0.454928994178772\n",
      "Validation: Epoch [21], Batch [265/938], Loss: 0.3420678377151489\n",
      "Validation: Epoch [21], Batch [266/938], Loss: 0.4605017602443695\n",
      "Validation: Epoch [21], Batch [267/938], Loss: 0.35971465706825256\n",
      "Validation: Epoch [21], Batch [268/938], Loss: 0.5730808973312378\n",
      "Validation: Epoch [21], Batch [269/938], Loss: 0.44056224822998047\n",
      "Validation: Epoch [21], Batch [270/938], Loss: 0.34235498309135437\n",
      "Validation: Epoch [21], Batch [271/938], Loss: 0.3572835922241211\n",
      "Validation: Epoch [21], Batch [272/938], Loss: 0.24481245875358582\n",
      "Validation: Epoch [21], Batch [273/938], Loss: 0.4476225972175598\n",
      "Validation: Epoch [21], Batch [274/938], Loss: 0.39158639311790466\n",
      "Validation: Epoch [21], Batch [275/938], Loss: 0.5284317135810852\n",
      "Validation: Epoch [21], Batch [276/938], Loss: 0.7902361154556274\n",
      "Validation: Epoch [21], Batch [277/938], Loss: 0.6524183750152588\n",
      "Validation: Epoch [21], Batch [278/938], Loss: 0.45378637313842773\n",
      "Validation: Epoch [21], Batch [279/938], Loss: 0.38715121150016785\n",
      "Validation: Epoch [21], Batch [280/938], Loss: 0.39250123500823975\n",
      "Validation: Epoch [21], Batch [281/938], Loss: 0.584335446357727\n",
      "Validation: Epoch [21], Batch [282/938], Loss: 0.3615933656692505\n",
      "Validation: Epoch [21], Batch [283/938], Loss: 0.43460720777511597\n",
      "Validation: Epoch [21], Batch [284/938], Loss: 0.4589836299419403\n",
      "Validation: Epoch [21], Batch [285/938], Loss: 0.5514293909072876\n",
      "Validation: Epoch [21], Batch [286/938], Loss: 0.6299107074737549\n",
      "Validation: Epoch [21], Batch [287/938], Loss: 0.4348614811897278\n",
      "Validation: Epoch [21], Batch [288/938], Loss: 0.32102739810943604\n",
      "Validation: Epoch [21], Batch [289/938], Loss: 0.4057760238647461\n",
      "Validation: Epoch [21], Batch [290/938], Loss: 0.49776801466941833\n",
      "Validation: Epoch [21], Batch [291/938], Loss: 0.3887242078781128\n",
      "Validation: Epoch [21], Batch [292/938], Loss: 0.4004538655281067\n",
      "Validation: Epoch [21], Batch [293/938], Loss: 0.3739978075027466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [294/938], Loss: 0.4016979932785034\n",
      "Validation: Epoch [21], Batch [295/938], Loss: 0.4486241042613983\n",
      "Validation: Epoch [21], Batch [296/938], Loss: 0.4723655879497528\n",
      "Validation: Epoch [21], Batch [297/938], Loss: 0.4098045229911804\n",
      "Validation: Epoch [21], Batch [298/938], Loss: 0.5008177757263184\n",
      "Validation: Epoch [21], Batch [299/938], Loss: 0.36218318343162537\n",
      "Validation: Epoch [21], Batch [300/938], Loss: 0.49575382471084595\n",
      "Validation: Epoch [21], Batch [301/938], Loss: 0.46448132395744324\n",
      "Validation: Epoch [21], Batch [302/938], Loss: 0.468273401260376\n",
      "Validation: Epoch [21], Batch [303/938], Loss: 0.20967358350753784\n",
      "Validation: Epoch [21], Batch [304/938], Loss: 0.6404002904891968\n",
      "Validation: Epoch [21], Batch [305/938], Loss: 0.5055550336837769\n",
      "Validation: Epoch [21], Batch [306/938], Loss: 0.34328126907348633\n",
      "Validation: Epoch [21], Batch [307/938], Loss: 0.3132910430431366\n",
      "Validation: Epoch [21], Batch [308/938], Loss: 0.4592125117778778\n",
      "Validation: Epoch [21], Batch [309/938], Loss: 0.6093127131462097\n",
      "Validation: Epoch [21], Batch [310/938], Loss: 0.511781632900238\n",
      "Validation: Epoch [21], Batch [311/938], Loss: 0.28247880935668945\n",
      "Validation: Epoch [21], Batch [312/938], Loss: 0.30160313844680786\n",
      "Validation: Epoch [21], Batch [313/938], Loss: 0.34481948614120483\n",
      "Validation: Epoch [21], Batch [314/938], Loss: 0.34621649980545044\n",
      "Validation: Epoch [21], Batch [315/938], Loss: 0.4236591160297394\n",
      "Validation: Epoch [21], Batch [316/938], Loss: 0.42240938544273376\n",
      "Validation: Epoch [21], Batch [317/938], Loss: 0.5563821792602539\n",
      "Validation: Epoch [21], Batch [318/938], Loss: 0.3474081754684448\n",
      "Validation: Epoch [21], Batch [319/938], Loss: 0.48541131615638733\n",
      "Validation: Epoch [21], Batch [320/938], Loss: 0.6083428859710693\n",
      "Validation: Epoch [21], Batch [321/938], Loss: 0.4195414185523987\n",
      "Validation: Epoch [21], Batch [322/938], Loss: 0.49226757884025574\n",
      "Validation: Epoch [21], Batch [323/938], Loss: 0.7435305118560791\n",
      "Validation: Epoch [21], Batch [324/938], Loss: 0.34009888768196106\n",
      "Validation: Epoch [21], Batch [325/938], Loss: 0.27903902530670166\n",
      "Validation: Epoch [21], Batch [326/938], Loss: 0.30936676263809204\n",
      "Validation: Epoch [21], Batch [327/938], Loss: 0.2331075519323349\n",
      "Validation: Epoch [21], Batch [328/938], Loss: 0.40362781286239624\n",
      "Validation: Epoch [21], Batch [329/938], Loss: 0.3175112009048462\n",
      "Validation: Epoch [21], Batch [330/938], Loss: 0.35199061036109924\n",
      "Validation: Epoch [21], Batch [331/938], Loss: 0.6134030818939209\n",
      "Validation: Epoch [21], Batch [332/938], Loss: 0.348467081785202\n",
      "Validation: Epoch [21], Batch [333/938], Loss: 0.47572511434555054\n",
      "Validation: Epoch [21], Batch [334/938], Loss: 0.43331634998321533\n",
      "Validation: Epoch [21], Batch [335/938], Loss: 0.27137526869773865\n",
      "Validation: Epoch [21], Batch [336/938], Loss: 0.4077032208442688\n",
      "Validation: Epoch [21], Batch [337/938], Loss: 0.36916583776474\n",
      "Validation: Epoch [21], Batch [338/938], Loss: 0.5828229188919067\n",
      "Validation: Epoch [21], Batch [339/938], Loss: 0.5356556177139282\n",
      "Validation: Epoch [21], Batch [340/938], Loss: 0.45672664046287537\n",
      "Validation: Epoch [21], Batch [341/938], Loss: 0.38006851077079773\n",
      "Validation: Epoch [21], Batch [342/938], Loss: 0.23289214074611664\n",
      "Validation: Epoch [21], Batch [343/938], Loss: 0.5566040277481079\n",
      "Validation: Epoch [21], Batch [344/938], Loss: 0.680743932723999\n",
      "Validation: Epoch [21], Batch [345/938], Loss: 0.5839091539382935\n",
      "Validation: Epoch [21], Batch [346/938], Loss: 0.36648228764533997\n",
      "Validation: Epoch [21], Batch [347/938], Loss: 0.36627137660980225\n",
      "Validation: Epoch [21], Batch [348/938], Loss: 0.552929162979126\n",
      "Validation: Epoch [21], Batch [349/938], Loss: 0.4297119379043579\n",
      "Validation: Epoch [21], Batch [350/938], Loss: 0.40518414974212646\n",
      "Validation: Epoch [21], Batch [351/938], Loss: 0.35208559036254883\n",
      "Validation: Epoch [21], Batch [352/938], Loss: 0.42847102880477905\n",
      "Validation: Epoch [21], Batch [353/938], Loss: 0.5394550561904907\n",
      "Validation: Epoch [21], Batch [354/938], Loss: 0.5022581219673157\n",
      "Validation: Epoch [21], Batch [355/938], Loss: 0.45282477140426636\n",
      "Validation: Epoch [21], Batch [356/938], Loss: 0.3795612156391144\n",
      "Validation: Epoch [21], Batch [357/938], Loss: 0.30528101325035095\n",
      "Validation: Epoch [21], Batch [358/938], Loss: 0.3401753902435303\n",
      "Validation: Epoch [21], Batch [359/938], Loss: 0.42209377884864807\n",
      "Validation: Epoch [21], Batch [360/938], Loss: 0.3986831307411194\n",
      "Validation: Epoch [21], Batch [361/938], Loss: 0.356433629989624\n",
      "Validation: Epoch [21], Batch [362/938], Loss: 0.46325764060020447\n",
      "Validation: Epoch [21], Batch [363/938], Loss: 0.33602896332740784\n",
      "Validation: Epoch [21], Batch [364/938], Loss: 0.4583933353424072\n",
      "Validation: Epoch [21], Batch [365/938], Loss: 0.5748029947280884\n",
      "Validation: Epoch [21], Batch [366/938], Loss: 0.432214617729187\n",
      "Validation: Epoch [21], Batch [367/938], Loss: 0.27210795879364014\n",
      "Validation: Epoch [21], Batch [368/938], Loss: 0.3642957806587219\n",
      "Validation: Epoch [21], Batch [369/938], Loss: 0.4867995083332062\n",
      "Validation: Epoch [21], Batch [370/938], Loss: 0.35292088985443115\n",
      "Validation: Epoch [21], Batch [371/938], Loss: 0.518722414970398\n",
      "Validation: Epoch [21], Batch [372/938], Loss: 0.35304272174835205\n",
      "Validation: Epoch [21], Batch [373/938], Loss: 0.36903393268585205\n",
      "Validation: Epoch [21], Batch [374/938], Loss: 0.4992905855178833\n",
      "Validation: Epoch [21], Batch [375/938], Loss: 0.4715598225593567\n",
      "Validation: Epoch [21], Batch [376/938], Loss: 0.38161855936050415\n",
      "Validation: Epoch [21], Batch [377/938], Loss: 0.4438011348247528\n",
      "Validation: Epoch [21], Batch [378/938], Loss: 0.48610126972198486\n",
      "Validation: Epoch [21], Batch [379/938], Loss: 0.4322894215583801\n",
      "Validation: Epoch [21], Batch [380/938], Loss: 0.3864803910255432\n",
      "Validation: Epoch [21], Batch [381/938], Loss: 0.5754425525665283\n",
      "Validation: Epoch [21], Batch [382/938], Loss: 0.3654414713382721\n",
      "Validation: Epoch [21], Batch [383/938], Loss: 0.4214699864387512\n",
      "Validation: Epoch [21], Batch [384/938], Loss: 0.7627278566360474\n",
      "Validation: Epoch [21], Batch [385/938], Loss: 0.5334543585777283\n",
      "Validation: Epoch [21], Batch [386/938], Loss: 0.30844539403915405\n",
      "Validation: Epoch [21], Batch [387/938], Loss: 0.3396776020526886\n",
      "Validation: Epoch [21], Batch [388/938], Loss: 0.4037669003009796\n",
      "Validation: Epoch [21], Batch [389/938], Loss: 0.4450971484184265\n",
      "Validation: Epoch [21], Batch [390/938], Loss: 0.38036268949508667\n",
      "Validation: Epoch [21], Batch [391/938], Loss: 0.4190032184123993\n",
      "Validation: Epoch [21], Batch [392/938], Loss: 0.3059154450893402\n",
      "Validation: Epoch [21], Batch [393/938], Loss: 0.514755368232727\n",
      "Validation: Epoch [21], Batch [394/938], Loss: 0.40159744024276733\n",
      "Validation: Epoch [21], Batch [395/938], Loss: 0.404328316450119\n",
      "Validation: Epoch [21], Batch [396/938], Loss: 0.5971400141716003\n",
      "Validation: Epoch [21], Batch [397/938], Loss: 0.5876954793930054\n",
      "Validation: Epoch [21], Batch [398/938], Loss: 0.430854856967926\n",
      "Validation: Epoch [21], Batch [399/938], Loss: 0.25834447145462036\n",
      "Validation: Epoch [21], Batch [400/938], Loss: 0.28097003698349\n",
      "Validation: Epoch [21], Batch [401/938], Loss: 0.526369571685791\n",
      "Validation: Epoch [21], Batch [402/938], Loss: 0.3581693172454834\n",
      "Validation: Epoch [21], Batch [403/938], Loss: 0.624183714389801\n",
      "Validation: Epoch [21], Batch [404/938], Loss: 0.5406902432441711\n",
      "Validation: Epoch [21], Batch [405/938], Loss: 0.5342644453048706\n",
      "Validation: Epoch [21], Batch [406/938], Loss: 0.4461187720298767\n",
      "Validation: Epoch [21], Batch [407/938], Loss: 0.36863091588020325\n",
      "Validation: Epoch [21], Batch [408/938], Loss: 0.31290432810783386\n",
      "Validation: Epoch [21], Batch [409/938], Loss: 0.31039413809776306\n",
      "Validation: Epoch [21], Batch [410/938], Loss: 0.4904877543449402\n",
      "Validation: Epoch [21], Batch [411/938], Loss: 0.41506528854370117\n",
      "Validation: Epoch [21], Batch [412/938], Loss: 0.4971429407596588\n",
      "Validation: Epoch [21], Batch [413/938], Loss: 0.4590105712413788\n",
      "Validation: Epoch [21], Batch [414/938], Loss: 0.33824512362480164\n",
      "Validation: Epoch [21], Batch [415/938], Loss: 0.2802659273147583\n",
      "Validation: Epoch [21], Batch [416/938], Loss: 0.5277011394500732\n",
      "Validation: Epoch [21], Batch [417/938], Loss: 0.6004458665847778\n",
      "Validation: Epoch [21], Batch [418/938], Loss: 0.4632672667503357\n",
      "Validation: Epoch [21], Batch [419/938], Loss: 0.4257376194000244\n",
      "Validation: Epoch [21], Batch [420/938], Loss: 0.46894168853759766\n",
      "Validation: Epoch [21], Batch [421/938], Loss: 0.4018317461013794\n",
      "Validation: Epoch [21], Batch [422/938], Loss: 0.522107720375061\n",
      "Validation: Epoch [21], Batch [423/938], Loss: 0.4366658926010132\n",
      "Validation: Epoch [21], Batch [424/938], Loss: 0.6352406740188599\n",
      "Validation: Epoch [21], Batch [425/938], Loss: 0.331111341714859\n",
      "Validation: Epoch [21], Batch [426/938], Loss: 0.42226719856262207\n",
      "Validation: Epoch [21], Batch [427/938], Loss: 0.399253249168396\n",
      "Validation: Epoch [21], Batch [428/938], Loss: 0.49413013458251953\n",
      "Validation: Epoch [21], Batch [429/938], Loss: 0.4855192303657532\n",
      "Validation: Epoch [21], Batch [430/938], Loss: 0.4606749415397644\n",
      "Validation: Epoch [21], Batch [431/938], Loss: 0.44997936487197876\n",
      "Validation: Epoch [21], Batch [432/938], Loss: 0.32034409046173096\n",
      "Validation: Epoch [21], Batch [433/938], Loss: 0.5947771072387695\n",
      "Validation: Epoch [21], Batch [434/938], Loss: 0.3467005491256714\n",
      "Validation: Epoch [21], Batch [435/938], Loss: 0.18670299649238586\n",
      "Validation: Epoch [21], Batch [436/938], Loss: 0.5218600630760193\n",
      "Validation: Epoch [21], Batch [437/938], Loss: 0.37031620740890503\n",
      "Validation: Epoch [21], Batch [438/938], Loss: 0.4797370135784149\n",
      "Validation: Epoch [21], Batch [439/938], Loss: 0.6679147481918335\n",
      "Validation: Epoch [21], Batch [440/938], Loss: 0.307199627161026\n",
      "Validation: Epoch [21], Batch [441/938], Loss: 0.422658234834671\n",
      "Validation: Epoch [21], Batch [442/938], Loss: 0.6473715901374817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [443/938], Loss: 0.3967404365539551\n",
      "Validation: Epoch [21], Batch [444/938], Loss: 0.3993613123893738\n",
      "Validation: Epoch [21], Batch [445/938], Loss: 0.44311007857322693\n",
      "Validation: Epoch [21], Batch [446/938], Loss: 0.3836892247200012\n",
      "Validation: Epoch [21], Batch [447/938], Loss: 0.36208492517471313\n",
      "Validation: Epoch [21], Batch [448/938], Loss: 0.4441277086734772\n",
      "Validation: Epoch [21], Batch [449/938], Loss: 0.3670494258403778\n",
      "Validation: Epoch [21], Batch [450/938], Loss: 0.2890303134918213\n",
      "Validation: Epoch [21], Batch [451/938], Loss: 0.3573688268661499\n",
      "Validation: Epoch [21], Batch [452/938], Loss: 0.4565909504890442\n",
      "Validation: Epoch [21], Batch [453/938], Loss: 0.29793089628219604\n",
      "Validation: Epoch [21], Batch [454/938], Loss: 0.5327378511428833\n",
      "Validation: Epoch [21], Batch [455/938], Loss: 0.4624696373939514\n",
      "Validation: Epoch [21], Batch [456/938], Loss: 0.24863888323307037\n",
      "Validation: Epoch [21], Batch [457/938], Loss: 0.6408699750900269\n",
      "Validation: Epoch [21], Batch [458/938], Loss: 0.4676022529602051\n",
      "Validation: Epoch [21], Batch [459/938], Loss: 0.33806371688842773\n",
      "Validation: Epoch [21], Batch [460/938], Loss: 0.5737295746803284\n",
      "Validation: Epoch [21], Batch [461/938], Loss: 0.3424675464630127\n",
      "Validation: Epoch [21], Batch [462/938], Loss: 0.5541660785675049\n",
      "Validation: Epoch [21], Batch [463/938], Loss: 0.3336787223815918\n",
      "Validation: Epoch [21], Batch [464/938], Loss: 0.44351595640182495\n",
      "Validation: Epoch [21], Batch [465/938], Loss: 0.3177271783351898\n",
      "Validation: Epoch [21], Batch [466/938], Loss: 0.31568655371665955\n",
      "Validation: Epoch [21], Batch [467/938], Loss: 0.37721896171569824\n",
      "Validation: Epoch [21], Batch [468/938], Loss: 0.5140122175216675\n",
      "Validation: Epoch [21], Batch [469/938], Loss: 0.3685963749885559\n",
      "Validation: Epoch [21], Batch [470/938], Loss: 0.5010361075401306\n",
      "Validation: Epoch [21], Batch [471/938], Loss: 0.3995073437690735\n",
      "Validation: Epoch [21], Batch [472/938], Loss: 0.39806225895881653\n",
      "Validation: Epoch [21], Batch [473/938], Loss: 0.35032159090042114\n",
      "Validation: Epoch [21], Batch [474/938], Loss: 0.42214110493659973\n",
      "Validation: Epoch [21], Batch [475/938], Loss: 0.4448956251144409\n",
      "Validation: Epoch [21], Batch [476/938], Loss: 0.4194115996360779\n",
      "Validation: Epoch [21], Batch [477/938], Loss: 0.5800763964653015\n",
      "Validation: Epoch [21], Batch [478/938], Loss: 0.32176363468170166\n",
      "Validation: Epoch [21], Batch [479/938], Loss: 0.3306977450847626\n",
      "Validation: Epoch [21], Batch [480/938], Loss: 0.3353568911552429\n",
      "Validation: Epoch [21], Batch [481/938], Loss: 0.5923590660095215\n",
      "Validation: Epoch [21], Batch [482/938], Loss: 0.4916703402996063\n",
      "Validation: Epoch [21], Batch [483/938], Loss: 0.48308736085891724\n",
      "Validation: Epoch [21], Batch [484/938], Loss: 0.4472164213657379\n",
      "Validation: Epoch [21], Batch [485/938], Loss: 0.41579219698905945\n",
      "Validation: Epoch [21], Batch [486/938], Loss: 0.4326067268848419\n",
      "Validation: Epoch [21], Batch [487/938], Loss: 0.6124061346054077\n",
      "Validation: Epoch [21], Batch [488/938], Loss: 0.4737706482410431\n",
      "Validation: Epoch [21], Batch [489/938], Loss: 0.34826067090034485\n",
      "Validation: Epoch [21], Batch [490/938], Loss: 0.5958352088928223\n",
      "Validation: Epoch [21], Batch [491/938], Loss: 0.3656296133995056\n",
      "Validation: Epoch [21], Batch [492/938], Loss: 0.37491267919540405\n",
      "Validation: Epoch [21], Batch [493/938], Loss: 0.3264772891998291\n",
      "Validation: Epoch [21], Batch [494/938], Loss: 0.45524370670318604\n",
      "Validation: Epoch [21], Batch [495/938], Loss: 0.4115537405014038\n",
      "Validation: Epoch [21], Batch [496/938], Loss: 0.3116808533668518\n",
      "Validation: Epoch [21], Batch [497/938], Loss: 0.4410940706729889\n",
      "Validation: Epoch [21], Batch [498/938], Loss: 0.259851336479187\n",
      "Validation: Epoch [21], Batch [499/938], Loss: 0.3118695318698883\n",
      "Validation: Epoch [21], Batch [500/938], Loss: 0.3067519962787628\n",
      "Validation: Epoch [21], Batch [501/938], Loss: 0.41503340005874634\n",
      "Validation: Epoch [21], Batch [502/938], Loss: 0.42943525314331055\n",
      "Validation: Epoch [21], Batch [503/938], Loss: 0.2951595187187195\n",
      "Validation: Epoch [21], Batch [504/938], Loss: 0.34145569801330566\n",
      "Validation: Epoch [21], Batch [505/938], Loss: 0.5235068202018738\n",
      "Validation: Epoch [21], Batch [506/938], Loss: 0.46194735169410706\n",
      "Validation: Epoch [21], Batch [507/938], Loss: 0.30924028158187866\n",
      "Validation: Epoch [21], Batch [508/938], Loss: 0.30514824390411377\n",
      "Validation: Epoch [21], Batch [509/938], Loss: 0.44517290592193604\n",
      "Validation: Epoch [21], Batch [510/938], Loss: 0.44590702652931213\n",
      "Validation: Epoch [21], Batch [511/938], Loss: 0.44027629494667053\n",
      "Validation: Epoch [21], Batch [512/938], Loss: 0.3639167547225952\n",
      "Validation: Epoch [21], Batch [513/938], Loss: 0.4887022078037262\n",
      "Validation: Epoch [21], Batch [514/938], Loss: 0.7045801877975464\n",
      "Validation: Epoch [21], Batch [515/938], Loss: 0.4818081259727478\n",
      "Validation: Epoch [21], Batch [516/938], Loss: 0.3748568296432495\n",
      "Validation: Epoch [21], Batch [517/938], Loss: 0.433351069688797\n",
      "Validation: Epoch [21], Batch [518/938], Loss: 0.38025763630867004\n",
      "Validation: Epoch [21], Batch [519/938], Loss: 0.5077943205833435\n",
      "Validation: Epoch [21], Batch [520/938], Loss: 0.3529724180698395\n",
      "Validation: Epoch [21], Batch [521/938], Loss: 0.41111770272254944\n",
      "Validation: Epoch [21], Batch [522/938], Loss: 0.3989384174346924\n",
      "Validation: Epoch [21], Batch [523/938], Loss: 0.5835485458374023\n",
      "Validation: Epoch [21], Batch [524/938], Loss: 0.2112904191017151\n",
      "Validation: Epoch [21], Batch [525/938], Loss: 0.29758620262145996\n",
      "Validation: Epoch [21], Batch [526/938], Loss: 0.4927845895290375\n",
      "Validation: Epoch [21], Batch [527/938], Loss: 0.4527130424976349\n",
      "Validation: Epoch [21], Batch [528/938], Loss: 0.3649801015853882\n",
      "Validation: Epoch [21], Batch [529/938], Loss: 0.3815554678440094\n",
      "Validation: Epoch [21], Batch [530/938], Loss: 0.2912265658378601\n",
      "Validation: Epoch [21], Batch [531/938], Loss: 0.6275037527084351\n",
      "Validation: Epoch [21], Batch [532/938], Loss: 0.3999605178833008\n",
      "Validation: Epoch [21], Batch [533/938], Loss: 0.3488941788673401\n",
      "Validation: Epoch [21], Batch [534/938], Loss: 0.3398796617984772\n",
      "Validation: Epoch [21], Batch [535/938], Loss: 0.46844354271888733\n",
      "Validation: Epoch [21], Batch [536/938], Loss: 0.5364462733268738\n",
      "Validation: Epoch [21], Batch [537/938], Loss: 0.4903881847858429\n",
      "Validation: Epoch [21], Batch [538/938], Loss: 0.4097747802734375\n",
      "Validation: Epoch [21], Batch [539/938], Loss: 0.4288904070854187\n",
      "Validation: Epoch [21], Batch [540/938], Loss: 0.5269118547439575\n",
      "Validation: Epoch [21], Batch [541/938], Loss: 0.25884366035461426\n",
      "Validation: Epoch [21], Batch [542/938], Loss: 0.3574370741844177\n",
      "Validation: Epoch [21], Batch [543/938], Loss: 0.46647998690605164\n",
      "Validation: Epoch [21], Batch [544/938], Loss: 0.2692098319530487\n",
      "Validation: Epoch [21], Batch [545/938], Loss: 0.43390223383903503\n",
      "Validation: Epoch [21], Batch [546/938], Loss: 0.4589370787143707\n",
      "Validation: Epoch [21], Batch [547/938], Loss: 0.48508691787719727\n",
      "Validation: Epoch [21], Batch [548/938], Loss: 0.27247193455696106\n",
      "Validation: Epoch [21], Batch [549/938], Loss: 0.46576207876205444\n",
      "Validation: Epoch [21], Batch [550/938], Loss: 0.4758005738258362\n",
      "Validation: Epoch [21], Batch [551/938], Loss: 0.2821279466152191\n",
      "Validation: Epoch [21], Batch [552/938], Loss: 0.4724695384502411\n",
      "Validation: Epoch [21], Batch [553/938], Loss: 0.37629181146621704\n",
      "Validation: Epoch [21], Batch [554/938], Loss: 0.5037895441055298\n",
      "Validation: Epoch [21], Batch [555/938], Loss: 0.49432528018951416\n",
      "Validation: Epoch [21], Batch [556/938], Loss: 0.396939218044281\n",
      "Validation: Epoch [21], Batch [557/938], Loss: 0.37885212898254395\n",
      "Validation: Epoch [21], Batch [558/938], Loss: 0.47994181513786316\n",
      "Validation: Epoch [21], Batch [559/938], Loss: 0.5547599196434021\n",
      "Validation: Epoch [21], Batch [560/938], Loss: 0.5930277705192566\n",
      "Validation: Epoch [21], Batch [561/938], Loss: 0.2886761426925659\n",
      "Validation: Epoch [21], Batch [562/938], Loss: 0.436019629240036\n",
      "Validation: Epoch [21], Batch [563/938], Loss: 0.5862655639648438\n",
      "Validation: Epoch [21], Batch [564/938], Loss: 0.47373872995376587\n",
      "Validation: Epoch [21], Batch [565/938], Loss: 0.5737698078155518\n",
      "Validation: Epoch [21], Batch [566/938], Loss: 0.48330771923065186\n",
      "Validation: Epoch [21], Batch [567/938], Loss: 0.33990806341171265\n",
      "Validation: Epoch [21], Batch [568/938], Loss: 0.4669758677482605\n",
      "Validation: Epoch [21], Batch [569/938], Loss: 0.21385198831558228\n",
      "Validation: Epoch [21], Batch [570/938], Loss: 0.3787347972393036\n",
      "Validation: Epoch [21], Batch [571/938], Loss: 0.6148431301116943\n",
      "Validation: Epoch [21], Batch [572/938], Loss: 0.29946333169937134\n",
      "Validation: Epoch [21], Batch [573/938], Loss: 0.3866005837917328\n",
      "Validation: Epoch [21], Batch [574/938], Loss: 0.5120905637741089\n",
      "Validation: Epoch [21], Batch [575/938], Loss: 0.5473722815513611\n",
      "Validation: Epoch [21], Batch [576/938], Loss: 0.4483281970024109\n",
      "Validation: Epoch [21], Batch [577/938], Loss: 0.5313279628753662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [578/938], Loss: 0.4539637267589569\n",
      "Validation: Epoch [21], Batch [579/938], Loss: 0.31755661964416504\n",
      "Validation: Epoch [21], Batch [580/938], Loss: 0.457426518201828\n",
      "Validation: Epoch [21], Batch [581/938], Loss: 0.3254619836807251\n",
      "Validation: Epoch [21], Batch [582/938], Loss: 0.28765785694122314\n",
      "Validation: Epoch [21], Batch [583/938], Loss: 0.4443049728870392\n",
      "Validation: Epoch [21], Batch [584/938], Loss: 0.4516187012195587\n",
      "Validation: Epoch [21], Batch [585/938], Loss: 0.27265915274620056\n",
      "Validation: Epoch [21], Batch [586/938], Loss: 0.3110436201095581\n",
      "Validation: Epoch [21], Batch [587/938], Loss: 0.3711433410644531\n",
      "Validation: Epoch [21], Batch [588/938], Loss: 0.5870224833488464\n",
      "Validation: Epoch [21], Batch [589/938], Loss: 0.542242169380188\n",
      "Validation: Epoch [21], Batch [590/938], Loss: 0.5032589435577393\n",
      "Validation: Epoch [21], Batch [591/938], Loss: 0.5837331414222717\n",
      "Validation: Epoch [21], Batch [592/938], Loss: 0.40009379386901855\n",
      "Validation: Epoch [21], Batch [593/938], Loss: 0.48587703704833984\n",
      "Validation: Epoch [21], Batch [594/938], Loss: 0.4636916220188141\n",
      "Validation: Epoch [21], Batch [595/938], Loss: 0.6692367792129517\n",
      "Validation: Epoch [21], Batch [596/938], Loss: 0.467274010181427\n",
      "Validation: Epoch [21], Batch [597/938], Loss: 0.5390638709068298\n",
      "Validation: Epoch [21], Batch [598/938], Loss: 0.4318448603153229\n",
      "Validation: Epoch [21], Batch [599/938], Loss: 0.3651910424232483\n",
      "Validation: Epoch [21], Batch [600/938], Loss: 0.35941505432128906\n",
      "Validation: Epoch [21], Batch [601/938], Loss: 0.38861507177352905\n",
      "Validation: Epoch [21], Batch [602/938], Loss: 0.41393202543258667\n",
      "Validation: Epoch [21], Batch [603/938], Loss: 0.6427712440490723\n",
      "Validation: Epoch [21], Batch [604/938], Loss: 0.5796777606010437\n",
      "Validation: Epoch [21], Batch [605/938], Loss: 0.24867932498455048\n",
      "Validation: Epoch [21], Batch [606/938], Loss: 0.35268524289131165\n",
      "Validation: Epoch [21], Batch [607/938], Loss: 0.4367632269859314\n",
      "Validation: Epoch [21], Batch [608/938], Loss: 0.4202631711959839\n",
      "Validation: Epoch [21], Batch [609/938], Loss: 0.5549108982086182\n",
      "Validation: Epoch [21], Batch [610/938], Loss: 0.4926455020904541\n",
      "Validation: Epoch [21], Batch [611/938], Loss: 0.4749563932418823\n",
      "Validation: Epoch [21], Batch [612/938], Loss: 0.4770013689994812\n",
      "Validation: Epoch [21], Batch [613/938], Loss: 0.4565610885620117\n",
      "Validation: Epoch [21], Batch [614/938], Loss: 0.4746687412261963\n",
      "Validation: Epoch [21], Batch [615/938], Loss: 0.6434891223907471\n",
      "Validation: Epoch [21], Batch [616/938], Loss: 0.3296911418437958\n",
      "Validation: Epoch [21], Batch [617/938], Loss: 0.31625065207481384\n",
      "Validation: Epoch [21], Batch [618/938], Loss: 0.5951018333435059\n",
      "Validation: Epoch [21], Batch [619/938], Loss: 0.49796783924102783\n",
      "Validation: Epoch [21], Batch [620/938], Loss: 0.37467148900032043\n",
      "Validation: Epoch [21], Batch [621/938], Loss: 0.20904876291751862\n",
      "Validation: Epoch [21], Batch [622/938], Loss: 0.4442763030529022\n",
      "Validation: Epoch [21], Batch [623/938], Loss: 0.2392793595790863\n",
      "Validation: Epoch [21], Batch [624/938], Loss: 0.5245847702026367\n",
      "Validation: Epoch [21], Batch [625/938], Loss: 0.5166578888893127\n",
      "Validation: Epoch [21], Batch [626/938], Loss: 0.28991347551345825\n",
      "Validation: Epoch [21], Batch [627/938], Loss: 0.34635037183761597\n",
      "Validation: Epoch [21], Batch [628/938], Loss: 0.319170206785202\n",
      "Validation: Epoch [21], Batch [629/938], Loss: 0.4887508153915405\n",
      "Validation: Epoch [21], Batch [630/938], Loss: 0.5083917379379272\n",
      "Validation: Epoch [21], Batch [631/938], Loss: 0.3406316339969635\n",
      "Validation: Epoch [21], Batch [632/938], Loss: 0.36169111728668213\n",
      "Validation: Epoch [21], Batch [633/938], Loss: 0.33907458186149597\n",
      "Validation: Epoch [21], Batch [634/938], Loss: 0.3839084804058075\n",
      "Validation: Epoch [21], Batch [635/938], Loss: 0.4880868196487427\n",
      "Validation: Epoch [21], Batch [636/938], Loss: 0.23700809478759766\n",
      "Validation: Epoch [21], Batch [637/938], Loss: 0.4273945689201355\n",
      "Validation: Epoch [21], Batch [638/938], Loss: 0.46382850408554077\n",
      "Validation: Epoch [21], Batch [639/938], Loss: 0.40213078260421753\n",
      "Validation: Epoch [21], Batch [640/938], Loss: 0.40236330032348633\n",
      "Validation: Epoch [21], Batch [641/938], Loss: 0.6171532869338989\n",
      "Validation: Epoch [21], Batch [642/938], Loss: 0.34932100772857666\n",
      "Validation: Epoch [21], Batch [643/938], Loss: 0.48363378643989563\n",
      "Validation: Epoch [21], Batch [644/938], Loss: 0.3235887289047241\n",
      "Validation: Epoch [21], Batch [645/938], Loss: 0.545030951499939\n",
      "Validation: Epoch [21], Batch [646/938], Loss: 0.34101995825767517\n",
      "Validation: Epoch [21], Batch [647/938], Loss: 0.5501486659049988\n",
      "Validation: Epoch [21], Batch [648/938], Loss: 0.3582236170768738\n",
      "Validation: Epoch [21], Batch [649/938], Loss: 0.40776902437210083\n",
      "Validation: Epoch [21], Batch [650/938], Loss: 0.38302987813949585\n",
      "Validation: Epoch [21], Batch [651/938], Loss: 0.35883063077926636\n",
      "Validation: Epoch [21], Batch [652/938], Loss: 0.4551332890987396\n",
      "Validation: Epoch [21], Batch [653/938], Loss: 0.44434574246406555\n",
      "Validation: Epoch [21], Batch [654/938], Loss: 0.5088258385658264\n",
      "Validation: Epoch [21], Batch [655/938], Loss: 0.3002265691757202\n",
      "Validation: Epoch [21], Batch [656/938], Loss: 0.45897263288497925\n",
      "Validation: Epoch [21], Batch [657/938], Loss: 0.2777552008628845\n",
      "Validation: Epoch [21], Batch [658/938], Loss: 0.2878410816192627\n",
      "Validation: Epoch [21], Batch [659/938], Loss: 0.5023894309997559\n",
      "Validation: Epoch [21], Batch [660/938], Loss: 0.4306285083293915\n",
      "Validation: Epoch [21], Batch [661/938], Loss: 0.3478090763092041\n",
      "Validation: Epoch [21], Batch [662/938], Loss: 0.44025447964668274\n",
      "Validation: Epoch [21], Batch [663/938], Loss: 0.3436651825904846\n",
      "Validation: Epoch [21], Batch [664/938], Loss: 0.6717551946640015\n",
      "Validation: Epoch [21], Batch [665/938], Loss: 0.29940012097358704\n",
      "Validation: Epoch [21], Batch [666/938], Loss: 0.4323754906654358\n",
      "Validation: Epoch [21], Batch [667/938], Loss: 0.2581642270088196\n",
      "Validation: Epoch [21], Batch [668/938], Loss: 0.5468254089355469\n",
      "Validation: Epoch [21], Batch [669/938], Loss: 0.4714784026145935\n",
      "Validation: Epoch [21], Batch [670/938], Loss: 0.3835704028606415\n",
      "Validation: Epoch [21], Batch [671/938], Loss: 0.7289462089538574\n",
      "Validation: Epoch [21], Batch [672/938], Loss: 0.6576582193374634\n",
      "Validation: Epoch [21], Batch [673/938], Loss: 0.4521711468696594\n",
      "Validation: Epoch [21], Batch [674/938], Loss: 0.3616098463535309\n",
      "Validation: Epoch [21], Batch [675/938], Loss: 0.3855871558189392\n",
      "Validation: Epoch [21], Batch [676/938], Loss: 0.30344298481941223\n",
      "Validation: Epoch [21], Batch [677/938], Loss: 0.7597339153289795\n",
      "Validation: Epoch [21], Batch [678/938], Loss: 0.5767691135406494\n",
      "Validation: Epoch [21], Batch [679/938], Loss: 0.4684821367263794\n",
      "Validation: Epoch [21], Batch [680/938], Loss: 0.47684794664382935\n",
      "Validation: Epoch [21], Batch [681/938], Loss: 0.5476317405700684\n",
      "Validation: Epoch [21], Batch [682/938], Loss: 0.2852713465690613\n",
      "Validation: Epoch [21], Batch [683/938], Loss: 0.3921942412853241\n",
      "Validation: Epoch [21], Batch [684/938], Loss: 0.46723902225494385\n",
      "Validation: Epoch [21], Batch [685/938], Loss: 0.40332943201065063\n",
      "Validation: Epoch [21], Batch [686/938], Loss: 0.37573719024658203\n",
      "Validation: Epoch [21], Batch [687/938], Loss: 0.3812476396560669\n",
      "Validation: Epoch [21], Batch [688/938], Loss: 0.445547878742218\n",
      "Validation: Epoch [21], Batch [689/938], Loss: 0.4014129042625427\n",
      "Validation: Epoch [21], Batch [690/938], Loss: 0.4351189434528351\n",
      "Validation: Epoch [21], Batch [691/938], Loss: 0.2999125123023987\n",
      "Validation: Epoch [21], Batch [692/938], Loss: 0.35492175817489624\n",
      "Validation: Epoch [21], Batch [693/938], Loss: 0.2148500680923462\n",
      "Validation: Epoch [21], Batch [694/938], Loss: 0.3502258062362671\n",
      "Validation: Epoch [21], Batch [695/938], Loss: 0.4317352771759033\n",
      "Validation: Epoch [21], Batch [696/938], Loss: 0.3480887711048126\n",
      "Validation: Epoch [21], Batch [697/938], Loss: 0.3630155324935913\n",
      "Validation: Epoch [21], Batch [698/938], Loss: 0.5053137540817261\n",
      "Validation: Epoch [21], Batch [699/938], Loss: 0.3482789695262909\n",
      "Validation: Epoch [21], Batch [700/938], Loss: 0.3367476165294647\n",
      "Validation: Epoch [21], Batch [701/938], Loss: 0.24892616271972656\n",
      "Validation: Epoch [21], Batch [702/938], Loss: 0.7561674118041992\n",
      "Validation: Epoch [21], Batch [703/938], Loss: 0.4275680482387543\n",
      "Validation: Epoch [21], Batch [704/938], Loss: 0.39090269804000854\n",
      "Validation: Epoch [21], Batch [705/938], Loss: 0.30829334259033203\n",
      "Validation: Epoch [21], Batch [706/938], Loss: 0.2446865439414978\n",
      "Validation: Epoch [21], Batch [707/938], Loss: 0.49971285462379456\n",
      "Validation: Epoch [21], Batch [708/938], Loss: 0.4090721011161804\n",
      "Validation: Epoch [21], Batch [709/938], Loss: 0.36257174611091614\n",
      "Validation: Epoch [21], Batch [710/938], Loss: 0.37407833337783813\n",
      "Validation: Epoch [21], Batch [711/938], Loss: 0.5611251592636108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [712/938], Loss: 0.6186709403991699\n",
      "Validation: Epoch [21], Batch [713/938], Loss: 0.27584534883499146\n",
      "Validation: Epoch [21], Batch [714/938], Loss: 0.3662759065628052\n",
      "Validation: Epoch [21], Batch [715/938], Loss: 0.3316183090209961\n",
      "Validation: Epoch [21], Batch [716/938], Loss: 0.33770114183425903\n",
      "Validation: Epoch [21], Batch [717/938], Loss: 0.34494876861572266\n",
      "Validation: Epoch [21], Batch [718/938], Loss: 0.45932888984680176\n",
      "Validation: Epoch [21], Batch [719/938], Loss: 0.3172908425331116\n",
      "Validation: Epoch [21], Batch [720/938], Loss: 0.3433910608291626\n",
      "Validation: Epoch [21], Batch [721/938], Loss: 0.5088248252868652\n",
      "Validation: Epoch [21], Batch [722/938], Loss: 0.2847687602043152\n",
      "Validation: Epoch [21], Batch [723/938], Loss: 0.39491310715675354\n",
      "Validation: Epoch [21], Batch [724/938], Loss: 0.30190593004226685\n",
      "Validation: Epoch [21], Batch [725/938], Loss: 0.2032593935728073\n",
      "Validation: Epoch [21], Batch [726/938], Loss: 0.32270538806915283\n",
      "Validation: Epoch [21], Batch [727/938], Loss: 0.5600340962409973\n",
      "Validation: Epoch [21], Batch [728/938], Loss: 0.3194027245044708\n",
      "Validation: Epoch [21], Batch [729/938], Loss: 0.7062499523162842\n",
      "Validation: Epoch [21], Batch [730/938], Loss: 0.26024511456489563\n",
      "Validation: Epoch [21], Batch [731/938], Loss: 0.4881458580493927\n",
      "Validation: Epoch [21], Batch [732/938], Loss: 0.43909624218940735\n",
      "Validation: Epoch [21], Batch [733/938], Loss: 0.43399375677108765\n",
      "Validation: Epoch [21], Batch [734/938], Loss: 0.43867039680480957\n",
      "Validation: Epoch [21], Batch [735/938], Loss: 0.44335269927978516\n",
      "Validation: Epoch [21], Batch [736/938], Loss: 0.5863814949989319\n",
      "Validation: Epoch [21], Batch [737/938], Loss: 0.36631566286087036\n",
      "Validation: Epoch [21], Batch [738/938], Loss: 0.3168005347251892\n",
      "Validation: Epoch [21], Batch [739/938], Loss: 0.475369930267334\n",
      "Validation: Epoch [21], Batch [740/938], Loss: 0.7514585256576538\n",
      "Validation: Epoch [21], Batch [741/938], Loss: 0.43951064348220825\n",
      "Validation: Epoch [21], Batch [742/938], Loss: 0.4236990511417389\n",
      "Validation: Epoch [21], Batch [743/938], Loss: 0.5122592449188232\n",
      "Validation: Epoch [21], Batch [744/938], Loss: 0.5437912344932556\n",
      "Validation: Epoch [21], Batch [745/938], Loss: 0.5391981601715088\n",
      "Validation: Epoch [21], Batch [746/938], Loss: 0.28799861669540405\n",
      "Validation: Epoch [21], Batch [747/938], Loss: 0.4886784553527832\n",
      "Validation: Epoch [21], Batch [748/938], Loss: 0.4554249346256256\n",
      "Validation: Epoch [21], Batch [749/938], Loss: 0.4772112965583801\n",
      "Validation: Epoch [21], Batch [750/938], Loss: 0.42960551381111145\n",
      "Validation: Epoch [21], Batch [751/938], Loss: 0.6216521263122559\n",
      "Validation: Epoch [21], Batch [752/938], Loss: 0.36553096771240234\n",
      "Validation: Epoch [21], Batch [753/938], Loss: 0.40009552240371704\n",
      "Validation: Epoch [21], Batch [754/938], Loss: 0.41207337379455566\n",
      "Validation: Epoch [21], Batch [755/938], Loss: 0.4600641131401062\n",
      "Validation: Epoch [21], Batch [756/938], Loss: 0.3083849549293518\n",
      "Validation: Epoch [21], Batch [757/938], Loss: 0.3569011390209198\n",
      "Validation: Epoch [21], Batch [758/938], Loss: 0.3077468276023865\n",
      "Validation: Epoch [21], Batch [759/938], Loss: 0.4578402638435364\n",
      "Validation: Epoch [21], Batch [760/938], Loss: 0.22025880217552185\n",
      "Validation: Epoch [21], Batch [761/938], Loss: 0.36800235509872437\n",
      "Validation: Epoch [21], Batch [762/938], Loss: 0.49652373790740967\n",
      "Validation: Epoch [21], Batch [763/938], Loss: 0.5381762385368347\n",
      "Validation: Epoch [21], Batch [764/938], Loss: 0.4111682176589966\n",
      "Validation: Epoch [21], Batch [765/938], Loss: 0.6873623132705688\n",
      "Validation: Epoch [21], Batch [766/938], Loss: 0.5564560890197754\n",
      "Validation: Epoch [21], Batch [767/938], Loss: 0.5133780837059021\n",
      "Validation: Epoch [21], Batch [768/938], Loss: 0.3048730492591858\n",
      "Validation: Epoch [21], Batch [769/938], Loss: 0.34554335474967957\n",
      "Validation: Epoch [21], Batch [770/938], Loss: 0.4003940224647522\n",
      "Validation: Epoch [21], Batch [771/938], Loss: 0.299251914024353\n",
      "Validation: Epoch [21], Batch [772/938], Loss: 0.5365292429924011\n",
      "Validation: Epoch [21], Batch [773/938], Loss: 0.34995001554489136\n",
      "Validation: Epoch [21], Batch [774/938], Loss: 0.3740098476409912\n",
      "Validation: Epoch [21], Batch [775/938], Loss: 0.659216046333313\n",
      "Validation: Epoch [21], Batch [776/938], Loss: 0.37646207213401794\n",
      "Validation: Epoch [21], Batch [777/938], Loss: 0.3259422779083252\n",
      "Validation: Epoch [21], Batch [778/938], Loss: 0.47899308800697327\n",
      "Validation: Epoch [21], Batch [779/938], Loss: 0.2940828204154968\n",
      "Validation: Epoch [21], Batch [780/938], Loss: 0.450412392616272\n",
      "Validation: Epoch [21], Batch [781/938], Loss: 0.40934333205223083\n",
      "Validation: Epoch [21], Batch [782/938], Loss: 0.5155148506164551\n",
      "Validation: Epoch [21], Batch [783/938], Loss: 0.458141565322876\n",
      "Validation: Epoch [21], Batch [784/938], Loss: 0.633265495300293\n",
      "Validation: Epoch [21], Batch [785/938], Loss: 0.5428749322891235\n",
      "Validation: Epoch [21], Batch [786/938], Loss: 0.7050298452377319\n",
      "Validation: Epoch [21], Batch [787/938], Loss: 0.534404456615448\n",
      "Validation: Epoch [21], Batch [788/938], Loss: 0.48219889402389526\n",
      "Validation: Epoch [21], Batch [789/938], Loss: 0.4382741451263428\n",
      "Validation: Epoch [21], Batch [790/938], Loss: 0.3541377782821655\n",
      "Validation: Epoch [21], Batch [791/938], Loss: 0.6171286106109619\n",
      "Validation: Epoch [21], Batch [792/938], Loss: 0.6070432662963867\n",
      "Validation: Epoch [21], Batch [793/938], Loss: 0.43536466360092163\n",
      "Validation: Epoch [21], Batch [794/938], Loss: 0.3582697808742523\n",
      "Validation: Epoch [21], Batch [795/938], Loss: 0.36390507221221924\n",
      "Validation: Epoch [21], Batch [796/938], Loss: 0.5233962535858154\n",
      "Validation: Epoch [21], Batch [797/938], Loss: 0.3030293583869934\n",
      "Validation: Epoch [21], Batch [798/938], Loss: 0.36087945103645325\n",
      "Validation: Epoch [21], Batch [799/938], Loss: 0.39930254220962524\n",
      "Validation: Epoch [21], Batch [800/938], Loss: 0.4146934449672699\n",
      "Validation: Epoch [21], Batch [801/938], Loss: 0.3092302680015564\n",
      "Validation: Epoch [21], Batch [802/938], Loss: 0.3135356903076172\n",
      "Validation: Epoch [21], Batch [803/938], Loss: 0.3889366686344147\n",
      "Validation: Epoch [21], Batch [804/938], Loss: 0.2963528633117676\n",
      "Validation: Epoch [21], Batch [805/938], Loss: 0.4370839595794678\n",
      "Validation: Epoch [21], Batch [806/938], Loss: 0.5708457231521606\n",
      "Validation: Epoch [21], Batch [807/938], Loss: 0.5110585689544678\n",
      "Validation: Epoch [21], Batch [808/938], Loss: 0.4652765095233917\n",
      "Validation: Epoch [21], Batch [809/938], Loss: 0.6896611452102661\n",
      "Validation: Epoch [21], Batch [810/938], Loss: 0.4049229323863983\n",
      "Validation: Epoch [21], Batch [811/938], Loss: 0.5077857375144958\n",
      "Validation: Epoch [21], Batch [812/938], Loss: 0.4857819080352783\n",
      "Validation: Epoch [21], Batch [813/938], Loss: 0.42660966515541077\n",
      "Validation: Epoch [21], Batch [814/938], Loss: 0.49688249826431274\n",
      "Validation: Epoch [21], Batch [815/938], Loss: 0.3069523572921753\n",
      "Validation: Epoch [21], Batch [816/938], Loss: 0.4869115948677063\n",
      "Validation: Epoch [21], Batch [817/938], Loss: 0.36268576979637146\n",
      "Validation: Epoch [21], Batch [818/938], Loss: 0.38696402311325073\n",
      "Validation: Epoch [21], Batch [819/938], Loss: 0.3523746728897095\n",
      "Validation: Epoch [21], Batch [820/938], Loss: 0.45129308104515076\n",
      "Validation: Epoch [21], Batch [821/938], Loss: 0.39529311656951904\n",
      "Validation: Epoch [21], Batch [822/938], Loss: 0.36924394965171814\n",
      "Validation: Epoch [21], Batch [823/938], Loss: 0.4113016724586487\n",
      "Validation: Epoch [21], Batch [824/938], Loss: 0.4694957733154297\n",
      "Validation: Epoch [21], Batch [825/938], Loss: 0.41026896238327026\n",
      "Validation: Epoch [21], Batch [826/938], Loss: 0.3418203890323639\n",
      "Validation: Epoch [21], Batch [827/938], Loss: 0.44617530703544617\n",
      "Validation: Epoch [21], Batch [828/938], Loss: 0.4636581540107727\n",
      "Validation: Epoch [21], Batch [829/938], Loss: 0.4677063226699829\n",
      "Validation: Epoch [21], Batch [830/938], Loss: 0.48845183849334717\n",
      "Validation: Epoch [21], Batch [831/938], Loss: 0.4965052306652069\n",
      "Validation: Epoch [21], Batch [832/938], Loss: 0.35160893201828003\n",
      "Validation: Epoch [21], Batch [833/938], Loss: 0.22905142605304718\n",
      "Validation: Epoch [21], Batch [834/938], Loss: 0.3687360882759094\n",
      "Validation: Epoch [21], Batch [835/938], Loss: 0.4414123296737671\n",
      "Validation: Epoch [21], Batch [836/938], Loss: 0.4774358868598938\n",
      "Validation: Epoch [21], Batch [837/938], Loss: 0.2713175117969513\n",
      "Validation: Epoch [21], Batch [838/938], Loss: 0.648617684841156\n",
      "Validation: Epoch [21], Batch [839/938], Loss: 0.33278605341911316\n",
      "Validation: Epoch [21], Batch [840/938], Loss: 0.57492995262146\n",
      "Validation: Epoch [21], Batch [841/938], Loss: 0.35872572660446167\n",
      "Validation: Epoch [21], Batch [842/938], Loss: 0.19906610250473022\n",
      "Validation: Epoch [21], Batch [843/938], Loss: 0.3074426054954529\n",
      "Validation: Epoch [21], Batch [844/938], Loss: 0.3684818744659424\n",
      "Validation: Epoch [21], Batch [845/938], Loss: 0.4858817756175995\n",
      "Validation: Epoch [21], Batch [846/938], Loss: 0.57236647605896\n",
      "Validation: Epoch [21], Batch [847/938], Loss: 0.5376574397087097\n",
      "Validation: Epoch [21], Batch [848/938], Loss: 0.493294894695282\n",
      "Validation: Epoch [21], Batch [849/938], Loss: 0.3375788629055023\n",
      "Validation: Epoch [21], Batch [850/938], Loss: 0.2633029520511627\n",
      "Validation: Epoch [21], Batch [851/938], Loss: 0.473579466342926\n",
      "Validation: Epoch [21], Batch [852/938], Loss: 0.6711245775222778\n",
      "Validation: Epoch [21], Batch [853/938], Loss: 0.2509366273880005\n",
      "Validation: Epoch [21], Batch [854/938], Loss: 0.4552381634712219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [21], Batch [855/938], Loss: 0.41382110118865967\n",
      "Validation: Epoch [21], Batch [856/938], Loss: 0.5218427181243896\n",
      "Validation: Epoch [21], Batch [857/938], Loss: 0.29967159032821655\n",
      "Validation: Epoch [21], Batch [858/938], Loss: 0.26074808835983276\n",
      "Validation: Epoch [21], Batch [859/938], Loss: 0.45200809836387634\n",
      "Validation: Epoch [21], Batch [860/938], Loss: 0.44609344005584717\n",
      "Validation: Epoch [21], Batch [861/938], Loss: 0.5171452760696411\n",
      "Validation: Epoch [21], Batch [862/938], Loss: 0.4288490414619446\n",
      "Validation: Epoch [21], Batch [863/938], Loss: 0.37317705154418945\n",
      "Validation: Epoch [21], Batch [864/938], Loss: 0.40493395924568176\n",
      "Validation: Epoch [21], Batch [865/938], Loss: 0.17792463302612305\n",
      "Validation: Epoch [21], Batch [866/938], Loss: 0.2897129952907562\n",
      "Validation: Epoch [21], Batch [867/938], Loss: 0.41805776953697205\n",
      "Validation: Epoch [21], Batch [868/938], Loss: 0.3077727258205414\n",
      "Validation: Epoch [21], Batch [869/938], Loss: 0.5176499485969543\n",
      "Validation: Epoch [21], Batch [870/938], Loss: 0.5915361642837524\n",
      "Validation: Epoch [21], Batch [871/938], Loss: 0.2310684770345688\n",
      "Validation: Epoch [21], Batch [872/938], Loss: 0.5056552886962891\n",
      "Validation: Epoch [21], Batch [873/938], Loss: 0.28299933671951294\n",
      "Validation: Epoch [21], Batch [874/938], Loss: 0.5656561255455017\n",
      "Validation: Epoch [21], Batch [875/938], Loss: 0.5046507120132446\n",
      "Validation: Epoch [21], Batch [876/938], Loss: 0.2954680323600769\n",
      "Validation: Epoch [21], Batch [877/938], Loss: 0.6797387003898621\n",
      "Validation: Epoch [21], Batch [878/938], Loss: 0.2654443383216858\n",
      "Validation: Epoch [21], Batch [879/938], Loss: 0.42039376497268677\n",
      "Validation: Epoch [21], Batch [880/938], Loss: 0.6220723390579224\n",
      "Validation: Epoch [21], Batch [881/938], Loss: 0.48024851083755493\n",
      "Validation: Epoch [21], Batch [882/938], Loss: 0.49991822242736816\n",
      "Validation: Epoch [21], Batch [883/938], Loss: 0.5649679899215698\n",
      "Validation: Epoch [21], Batch [884/938], Loss: 0.5899913907051086\n",
      "Validation: Epoch [21], Batch [885/938], Loss: 0.5113465785980225\n",
      "Validation: Epoch [21], Batch [886/938], Loss: 0.32164421677589417\n",
      "Validation: Epoch [21], Batch [887/938], Loss: 0.43689101934432983\n",
      "Validation: Epoch [21], Batch [888/938], Loss: 0.3470199406147003\n",
      "Validation: Epoch [21], Batch [889/938], Loss: 0.2803809344768524\n",
      "Validation: Epoch [21], Batch [890/938], Loss: 0.18194924294948578\n",
      "Validation: Epoch [21], Batch [891/938], Loss: 0.26877719163894653\n",
      "Validation: Epoch [21], Batch [892/938], Loss: 0.3946484923362732\n",
      "Validation: Epoch [21], Batch [893/938], Loss: 0.3413948714733124\n",
      "Validation: Epoch [21], Batch [894/938], Loss: 0.43513309955596924\n",
      "Validation: Epoch [21], Batch [895/938], Loss: 0.24463018774986267\n",
      "Validation: Epoch [21], Batch [896/938], Loss: 0.3887525498867035\n",
      "Validation: Epoch [21], Batch [897/938], Loss: 0.5360656976699829\n",
      "Validation: Epoch [21], Batch [898/938], Loss: 0.25999224185943604\n",
      "Validation: Epoch [21], Batch [899/938], Loss: 0.4856095016002655\n",
      "Validation: Epoch [21], Batch [900/938], Loss: 0.43191656470298767\n",
      "Validation: Epoch [21], Batch [901/938], Loss: 0.6876001358032227\n",
      "Validation: Epoch [21], Batch [902/938], Loss: 0.47835683822631836\n",
      "Validation: Epoch [21], Batch [903/938], Loss: 0.4406852126121521\n",
      "Validation: Epoch [21], Batch [904/938], Loss: 0.42718303203582764\n",
      "Validation: Epoch [21], Batch [905/938], Loss: 0.23162350058555603\n",
      "Validation: Epoch [21], Batch [906/938], Loss: 0.5311379432678223\n",
      "Validation: Epoch [21], Batch [907/938], Loss: 0.4728362262248993\n",
      "Validation: Epoch [21], Batch [908/938], Loss: 0.6838904023170471\n",
      "Validation: Epoch [21], Batch [909/938], Loss: 0.38364866375923157\n",
      "Validation: Epoch [21], Batch [910/938], Loss: 0.19601669907569885\n",
      "Validation: Epoch [21], Batch [911/938], Loss: 0.3885851800441742\n",
      "Validation: Epoch [21], Batch [912/938], Loss: 0.3609989285469055\n",
      "Validation: Epoch [21], Batch [913/938], Loss: 0.5206098556518555\n",
      "Validation: Epoch [21], Batch [914/938], Loss: 0.4019467830657959\n",
      "Validation: Epoch [21], Batch [915/938], Loss: 0.2863667607307434\n",
      "Validation: Epoch [21], Batch [916/938], Loss: 0.5562703609466553\n",
      "Validation: Epoch [21], Batch [917/938], Loss: 0.5163149833679199\n",
      "Validation: Epoch [21], Batch [918/938], Loss: 0.36403346061706543\n",
      "Validation: Epoch [21], Batch [919/938], Loss: 0.40232348442077637\n",
      "Validation: Epoch [21], Batch [920/938], Loss: 0.3821653127670288\n",
      "Validation: Epoch [21], Batch [921/938], Loss: 0.3633992075920105\n",
      "Validation: Epoch [21], Batch [922/938], Loss: 0.5273631811141968\n",
      "Validation: Epoch [21], Batch [923/938], Loss: 0.4789005219936371\n",
      "Validation: Epoch [21], Batch [924/938], Loss: 0.4424123764038086\n",
      "Validation: Epoch [21], Batch [925/938], Loss: 0.44922882318496704\n",
      "Validation: Epoch [21], Batch [926/938], Loss: 0.612895131111145\n",
      "Validation: Epoch [21], Batch [927/938], Loss: 0.5729573965072632\n",
      "Validation: Epoch [21], Batch [928/938], Loss: 0.3430330455303192\n",
      "Validation: Epoch [21], Batch [929/938], Loss: 0.356967031955719\n",
      "Validation: Epoch [21], Batch [930/938], Loss: 0.3451381325721741\n",
      "Validation: Epoch [21], Batch [931/938], Loss: 0.34233972430229187\n",
      "Validation: Epoch [21], Batch [932/938], Loss: 0.47303086519241333\n",
      "Validation: Epoch [21], Batch [933/938], Loss: 0.37278711795806885\n",
      "Validation: Epoch [21], Batch [934/938], Loss: 0.4832151532173157\n",
      "Validation: Epoch [21], Batch [935/938], Loss: 0.5611727237701416\n",
      "Validation: Epoch [21], Batch [936/938], Loss: 0.31820088624954224\n",
      "Validation: Epoch [21], Batch [937/938], Loss: 0.4268656373023987\n",
      "Validation: Epoch [21], Batch [938/938], Loss: 0.42341530323028564\n",
      "Accuracy of test set: 0.8489\n",
      "Train: Epoch [22], Batch [1/938], Loss: 0.27953097224235535\n",
      "Train: Epoch [22], Batch [2/938], Loss: 0.31294965744018555\n",
      "Train: Epoch [22], Batch [3/938], Loss: 0.41872820258140564\n",
      "Train: Epoch [22], Batch [4/938], Loss: 0.2595387101173401\n",
      "Train: Epoch [22], Batch [5/938], Loss: 0.45870906114578247\n",
      "Train: Epoch [22], Batch [6/938], Loss: 0.4633423686027527\n",
      "Train: Epoch [22], Batch [7/938], Loss: 0.41829556226730347\n",
      "Train: Epoch [22], Batch [8/938], Loss: 0.41203391551971436\n",
      "Train: Epoch [22], Batch [9/938], Loss: 0.37572771310806274\n",
      "Train: Epoch [22], Batch [10/938], Loss: 0.5460088849067688\n",
      "Train: Epoch [22], Batch [11/938], Loss: 0.3096453547477722\n",
      "Train: Epoch [22], Batch [12/938], Loss: 0.3632955849170685\n",
      "Train: Epoch [22], Batch [13/938], Loss: 0.4754483103752136\n",
      "Train: Epoch [22], Batch [14/938], Loss: 0.37379562854766846\n",
      "Train: Epoch [22], Batch [15/938], Loss: 0.572277307510376\n",
      "Train: Epoch [22], Batch [16/938], Loss: 0.465670645236969\n",
      "Train: Epoch [22], Batch [17/938], Loss: 0.2877765893936157\n",
      "Train: Epoch [22], Batch [18/938], Loss: 0.37166517972946167\n",
      "Train: Epoch [22], Batch [19/938], Loss: 0.36699771881103516\n",
      "Train: Epoch [22], Batch [20/938], Loss: 0.48953476548194885\n",
      "Train: Epoch [22], Batch [21/938], Loss: 0.43782925605773926\n",
      "Train: Epoch [22], Batch [22/938], Loss: 0.4916382431983948\n",
      "Train: Epoch [22], Batch [23/938], Loss: 0.33790498971939087\n",
      "Train: Epoch [22], Batch [24/938], Loss: 0.23379571735858917\n",
      "Train: Epoch [22], Batch [25/938], Loss: 0.28173428773880005\n",
      "Train: Epoch [22], Batch [26/938], Loss: 0.38337844610214233\n",
      "Train: Epoch [22], Batch [27/938], Loss: 0.3553348779678345\n",
      "Train: Epoch [22], Batch [28/938], Loss: 0.5545544624328613\n",
      "Train: Epoch [22], Batch [29/938], Loss: 0.4170050323009491\n",
      "Train: Epoch [22], Batch [30/938], Loss: 0.44194236397743225\n",
      "Train: Epoch [22], Batch [31/938], Loss: 0.46390557289123535\n",
      "Train: Epoch [22], Batch [32/938], Loss: 0.5105857849121094\n",
      "Train: Epoch [22], Batch [33/938], Loss: 0.3711126446723938\n",
      "Train: Epoch [22], Batch [34/938], Loss: 0.5346512794494629\n",
      "Train: Epoch [22], Batch [35/938], Loss: 0.49184566736221313\n",
      "Train: Epoch [22], Batch [36/938], Loss: 0.4996388852596283\n",
      "Train: Epoch [22], Batch [37/938], Loss: 0.4622892737388611\n",
      "Train: Epoch [22], Batch [38/938], Loss: 0.3703538179397583\n",
      "Train: Epoch [22], Batch [39/938], Loss: 0.5384160280227661\n",
      "Train: Epoch [22], Batch [40/938], Loss: 0.4166920781135559\n",
      "Train: Epoch [22], Batch [41/938], Loss: 0.2819026708602905\n",
      "Train: Epoch [22], Batch [42/938], Loss: 0.2800673544406891\n",
      "Train: Epoch [22], Batch [43/938], Loss: 0.24066945910453796\n",
      "Train: Epoch [22], Batch [44/938], Loss: 0.4509001672267914\n",
      "Train: Epoch [22], Batch [45/938], Loss: 0.5610480308532715\n",
      "Train: Epoch [22], Batch [46/938], Loss: 0.3733401894569397\n",
      "Train: Epoch [22], Batch [47/938], Loss: 0.4281464219093323\n",
      "Train: Epoch [22], Batch [48/938], Loss: 0.6210436820983887\n",
      "Train: Epoch [22], Batch [49/938], Loss: 0.350807785987854\n",
      "Train: Epoch [22], Batch [50/938], Loss: 0.35357314348220825\n",
      "Train: Epoch [22], Batch [51/938], Loss: 0.3008618652820587\n",
      "Train: Epoch [22], Batch [52/938], Loss: 0.43843895196914673\n",
      "Train: Epoch [22], Batch [53/938], Loss: 0.6475651264190674\n",
      "Train: Epoch [22], Batch [54/938], Loss: 0.3457856774330139\n",
      "Train: Epoch [22], Batch [55/938], Loss: 0.4366177022457123\n",
      "Train: Epoch [22], Batch [56/938], Loss: 0.31605133414268494\n",
      "Train: Epoch [22], Batch [57/938], Loss: 0.29090848565101624\n",
      "Train: Epoch [22], Batch [58/938], Loss: 0.4390231668949127\n",
      "Train: Epoch [22], Batch [59/938], Loss: 0.35157784819602966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [60/938], Loss: 0.3853389024734497\n",
      "Train: Epoch [22], Batch [61/938], Loss: 0.3988295793533325\n",
      "Train: Epoch [22], Batch [62/938], Loss: 0.3567502498626709\n",
      "Train: Epoch [22], Batch [63/938], Loss: 0.43208199739456177\n",
      "Train: Epoch [22], Batch [64/938], Loss: 0.35155779123306274\n",
      "Train: Epoch [22], Batch [65/938], Loss: 0.48298513889312744\n",
      "Train: Epoch [22], Batch [66/938], Loss: 0.27451425790786743\n",
      "Train: Epoch [22], Batch [67/938], Loss: 0.48836207389831543\n",
      "Train: Epoch [22], Batch [68/938], Loss: 0.29222264885902405\n",
      "Train: Epoch [22], Batch [69/938], Loss: 0.3912898898124695\n",
      "Train: Epoch [22], Batch [70/938], Loss: 0.29879942536354065\n",
      "Train: Epoch [22], Batch [71/938], Loss: 0.36242416501045227\n",
      "Train: Epoch [22], Batch [72/938], Loss: 0.38381874561309814\n",
      "Train: Epoch [22], Batch [73/938], Loss: 0.4682900011539459\n",
      "Train: Epoch [22], Batch [74/938], Loss: 0.3633682131767273\n",
      "Train: Epoch [22], Batch [75/938], Loss: 0.3173925280570984\n",
      "Train: Epoch [22], Batch [76/938], Loss: 0.4600296914577484\n",
      "Train: Epoch [22], Batch [77/938], Loss: 0.5190933346748352\n",
      "Train: Epoch [22], Batch [78/938], Loss: 0.6474753022193909\n",
      "Train: Epoch [22], Batch [79/938], Loss: 0.3624013066291809\n",
      "Train: Epoch [22], Batch [80/938], Loss: 0.5055744647979736\n",
      "Train: Epoch [22], Batch [81/938], Loss: 0.4463273882865906\n",
      "Train: Epoch [22], Batch [82/938], Loss: 0.33887600898742676\n",
      "Train: Epoch [22], Batch [83/938], Loss: 0.5419899225234985\n",
      "Train: Epoch [22], Batch [84/938], Loss: 0.2529076039791107\n",
      "Train: Epoch [22], Batch [85/938], Loss: 0.45885273814201355\n",
      "Train: Epoch [22], Batch [86/938], Loss: 0.40990033745765686\n",
      "Train: Epoch [22], Batch [87/938], Loss: 0.5245670080184937\n",
      "Train: Epoch [22], Batch [88/938], Loss: 0.5763250589370728\n",
      "Train: Epoch [22], Batch [89/938], Loss: 0.5730143189430237\n",
      "Train: Epoch [22], Batch [90/938], Loss: 0.39085689187049866\n",
      "Train: Epoch [22], Batch [91/938], Loss: 0.3680339455604553\n",
      "Train: Epoch [22], Batch [92/938], Loss: 0.5614177584648132\n",
      "Train: Epoch [22], Batch [93/938], Loss: 0.40326642990112305\n",
      "Train: Epoch [22], Batch [94/938], Loss: 0.35840919613838196\n",
      "Train: Epoch [22], Batch [95/938], Loss: 0.5758599042892456\n",
      "Train: Epoch [22], Batch [96/938], Loss: 0.28479450941085815\n",
      "Train: Epoch [22], Batch [97/938], Loss: 0.3256695866584778\n",
      "Train: Epoch [22], Batch [98/938], Loss: 0.38049596548080444\n",
      "Train: Epoch [22], Batch [99/938], Loss: 0.387858510017395\n",
      "Train: Epoch [22], Batch [100/938], Loss: 0.42219850420951843\n",
      "Train: Epoch [22], Batch [101/938], Loss: 0.601685643196106\n",
      "Train: Epoch [22], Batch [102/938], Loss: 0.3157096207141876\n",
      "Train: Epoch [22], Batch [103/938], Loss: 0.4161341190338135\n",
      "Train: Epoch [22], Batch [104/938], Loss: 0.3382761478424072\n",
      "Train: Epoch [22], Batch [105/938], Loss: 0.45152080059051514\n",
      "Train: Epoch [22], Batch [106/938], Loss: 0.4363667070865631\n",
      "Train: Epoch [22], Batch [107/938], Loss: 0.364301860332489\n",
      "Train: Epoch [22], Batch [108/938], Loss: 0.2889888882637024\n",
      "Train: Epoch [22], Batch [109/938], Loss: 0.2786317467689514\n",
      "Train: Epoch [22], Batch [110/938], Loss: 0.26172366738319397\n",
      "Train: Epoch [22], Batch [111/938], Loss: 0.3562975227832794\n",
      "Train: Epoch [22], Batch [112/938], Loss: 0.37759286165237427\n",
      "Train: Epoch [22], Batch [113/938], Loss: 0.4289199411869049\n",
      "Train: Epoch [22], Batch [114/938], Loss: 0.5039620399475098\n",
      "Train: Epoch [22], Batch [115/938], Loss: 0.20865784585475922\n",
      "Train: Epoch [22], Batch [116/938], Loss: 0.20107941329479218\n",
      "Train: Epoch [22], Batch [117/938], Loss: 0.40539786219596863\n",
      "Train: Epoch [22], Batch [118/938], Loss: 0.34439751505851746\n",
      "Train: Epoch [22], Batch [119/938], Loss: 0.4298182725906372\n",
      "Train: Epoch [22], Batch [120/938], Loss: 0.3307950496673584\n",
      "Train: Epoch [22], Batch [121/938], Loss: 0.2882517874240875\n",
      "Train: Epoch [22], Batch [122/938], Loss: 0.39275020360946655\n",
      "Train: Epoch [22], Batch [123/938], Loss: 0.24932029843330383\n",
      "Train: Epoch [22], Batch [124/938], Loss: 0.2334451973438263\n",
      "Train: Epoch [22], Batch [125/938], Loss: 0.27705496549606323\n",
      "Train: Epoch [22], Batch [126/938], Loss: 0.5011836290359497\n",
      "Train: Epoch [22], Batch [127/938], Loss: 0.4963119626045227\n",
      "Train: Epoch [22], Batch [128/938], Loss: 0.426369309425354\n",
      "Train: Epoch [22], Batch [129/938], Loss: 0.2738937735557556\n",
      "Train: Epoch [22], Batch [130/938], Loss: 0.22118356823921204\n",
      "Train: Epoch [22], Batch [131/938], Loss: 0.35226011276245117\n",
      "Train: Epoch [22], Batch [132/938], Loss: 0.3680667579174042\n",
      "Train: Epoch [22], Batch [133/938], Loss: 0.46792346239089966\n",
      "Train: Epoch [22], Batch [134/938], Loss: 0.38189512491226196\n",
      "Train: Epoch [22], Batch [135/938], Loss: 0.3008774518966675\n",
      "Train: Epoch [22], Batch [136/938], Loss: 0.3185773193836212\n",
      "Train: Epoch [22], Batch [137/938], Loss: 0.32906484603881836\n",
      "Train: Epoch [22], Batch [138/938], Loss: 0.48884522914886475\n",
      "Train: Epoch [22], Batch [139/938], Loss: 0.43636777997016907\n",
      "Train: Epoch [22], Batch [140/938], Loss: 0.21209046244621277\n",
      "Train: Epoch [22], Batch [141/938], Loss: 0.5608702301979065\n",
      "Train: Epoch [22], Batch [142/938], Loss: 0.3500482738018036\n",
      "Train: Epoch [22], Batch [143/938], Loss: 0.43784791231155396\n",
      "Train: Epoch [22], Batch [144/938], Loss: 0.27177995443344116\n",
      "Train: Epoch [22], Batch [145/938], Loss: 0.4285423159599304\n",
      "Train: Epoch [22], Batch [146/938], Loss: 0.3659515380859375\n",
      "Train: Epoch [22], Batch [147/938], Loss: 0.4443693161010742\n",
      "Train: Epoch [22], Batch [148/938], Loss: 0.3619775176048279\n",
      "Train: Epoch [22], Batch [149/938], Loss: 0.5853415131568909\n",
      "Train: Epoch [22], Batch [150/938], Loss: 0.2043149471282959\n",
      "Train: Epoch [22], Batch [151/938], Loss: 0.5806121230125427\n",
      "Train: Epoch [22], Batch [152/938], Loss: 0.3986842930316925\n",
      "Train: Epoch [22], Batch [153/938], Loss: 0.4043447971343994\n",
      "Train: Epoch [22], Batch [154/938], Loss: 0.2277059257030487\n",
      "Train: Epoch [22], Batch [155/938], Loss: 0.27070456743240356\n",
      "Train: Epoch [22], Batch [156/938], Loss: 0.3747127652168274\n",
      "Train: Epoch [22], Batch [157/938], Loss: 0.3920762240886688\n",
      "Train: Epoch [22], Batch [158/938], Loss: 0.3710365295410156\n",
      "Train: Epoch [22], Batch [159/938], Loss: 0.33680716156959534\n",
      "Train: Epoch [22], Batch [160/938], Loss: 0.4327705502510071\n",
      "Train: Epoch [22], Batch [161/938], Loss: 0.42867350578308105\n",
      "Train: Epoch [22], Batch [162/938], Loss: 0.5343120098114014\n",
      "Train: Epoch [22], Batch [163/938], Loss: 0.5320671796798706\n",
      "Train: Epoch [22], Batch [164/938], Loss: 0.2643716037273407\n",
      "Train: Epoch [22], Batch [165/938], Loss: 0.368937611579895\n",
      "Train: Epoch [22], Batch [166/938], Loss: 0.27415230870246887\n",
      "Train: Epoch [22], Batch [167/938], Loss: 0.5334243774414062\n",
      "Train: Epoch [22], Batch [168/938], Loss: 0.2562098503112793\n",
      "Train: Epoch [22], Batch [169/938], Loss: 0.31756675243377686\n",
      "Train: Epoch [22], Batch [170/938], Loss: 0.4078640639781952\n",
      "Train: Epoch [22], Batch [171/938], Loss: 0.24024587869644165\n",
      "Train: Epoch [22], Batch [172/938], Loss: 0.24294303357601166\n",
      "Train: Epoch [22], Batch [173/938], Loss: 0.6223090291023254\n",
      "Train: Epoch [22], Batch [174/938], Loss: 0.30948907136917114\n",
      "Train: Epoch [22], Batch [175/938], Loss: 0.4763217270374298\n",
      "Train: Epoch [22], Batch [176/938], Loss: 0.5571276545524597\n",
      "Train: Epoch [22], Batch [177/938], Loss: 0.3614550232887268\n",
      "Train: Epoch [22], Batch [178/938], Loss: 0.24698038399219513\n",
      "Train: Epoch [22], Batch [179/938], Loss: 0.5055500864982605\n",
      "Train: Epoch [22], Batch [180/938], Loss: 0.39566630125045776\n",
      "Train: Epoch [22], Batch [181/938], Loss: 0.44359883666038513\n",
      "Train: Epoch [22], Batch [182/938], Loss: 0.506708025932312\n",
      "Train: Epoch [22], Batch [183/938], Loss: 0.5000985860824585\n",
      "Train: Epoch [22], Batch [184/938], Loss: 0.3507707118988037\n",
      "Train: Epoch [22], Batch [185/938], Loss: 0.5182313919067383\n",
      "Train: Epoch [22], Batch [186/938], Loss: 0.4400438368320465\n",
      "Train: Epoch [22], Batch [187/938], Loss: 0.3281834125518799\n",
      "Train: Epoch [22], Batch [188/938], Loss: 0.41073521971702576\n",
      "Train: Epoch [22], Batch [189/938], Loss: 0.44550004601478577\n",
      "Train: Epoch [22], Batch [190/938], Loss: 0.3480641841888428\n",
      "Train: Epoch [22], Batch [191/938], Loss: 0.22970180213451385\n",
      "Train: Epoch [22], Batch [192/938], Loss: 0.2967709004878998\n",
      "Train: Epoch [22], Batch [193/938], Loss: 0.39400631189346313\n",
      "Train: Epoch [22], Batch [194/938], Loss: 0.4823412597179413\n",
      "Train: Epoch [22], Batch [195/938], Loss: 0.20539972186088562\n",
      "Train: Epoch [22], Batch [196/938], Loss: 0.3366101384162903\n",
      "Train: Epoch [22], Batch [197/938], Loss: 0.6361334323883057\n",
      "Train: Epoch [22], Batch [198/938], Loss: 0.4730151891708374\n",
      "Train: Epoch [22], Batch [199/938], Loss: 0.432762086391449\n",
      "Train: Epoch [22], Batch [200/938], Loss: 0.3049028217792511\n",
      "Train: Epoch [22], Batch [201/938], Loss: 0.431476354598999\n",
      "Train: Epoch [22], Batch [202/938], Loss: 0.8104410171508789\n",
      "Train: Epoch [22], Batch [203/938], Loss: 0.5764760971069336\n",
      "Train: Epoch [22], Batch [204/938], Loss: 0.5061511397361755\n",
      "Train: Epoch [22], Batch [205/938], Loss: 0.4307782053947449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [206/938], Loss: 0.46219372749328613\n",
      "Train: Epoch [22], Batch [207/938], Loss: 0.42219164967536926\n",
      "Train: Epoch [22], Batch [208/938], Loss: 0.2926751971244812\n",
      "Train: Epoch [22], Batch [209/938], Loss: 0.30604737997055054\n",
      "Train: Epoch [22], Batch [210/938], Loss: 0.3566685914993286\n",
      "Train: Epoch [22], Batch [211/938], Loss: 0.33283093571662903\n",
      "Train: Epoch [22], Batch [212/938], Loss: 0.421538770198822\n",
      "Train: Epoch [22], Batch [213/938], Loss: 0.27793049812316895\n",
      "Train: Epoch [22], Batch [214/938], Loss: 0.5469196438789368\n",
      "Train: Epoch [22], Batch [215/938], Loss: 0.29946666955947876\n",
      "Train: Epoch [22], Batch [216/938], Loss: 0.7805196046829224\n",
      "Train: Epoch [22], Batch [217/938], Loss: 0.34136611223220825\n",
      "Train: Epoch [22], Batch [218/938], Loss: 0.19008883833885193\n",
      "Train: Epoch [22], Batch [219/938], Loss: 0.2532002329826355\n",
      "Train: Epoch [22], Batch [220/938], Loss: 0.4506492018699646\n",
      "Train: Epoch [22], Batch [221/938], Loss: 0.5996394157409668\n",
      "Train: Epoch [22], Batch [222/938], Loss: 0.29637736082077026\n",
      "Train: Epoch [22], Batch [223/938], Loss: 0.4747687578201294\n",
      "Train: Epoch [22], Batch [224/938], Loss: 0.46974050998687744\n",
      "Train: Epoch [22], Batch [225/938], Loss: 0.42788127064704895\n",
      "Train: Epoch [22], Batch [226/938], Loss: 0.3398870527744293\n",
      "Train: Epoch [22], Batch [227/938], Loss: 0.21138304471969604\n",
      "Train: Epoch [22], Batch [228/938], Loss: 0.49122709035873413\n",
      "Train: Epoch [22], Batch [229/938], Loss: 0.3077046871185303\n",
      "Train: Epoch [22], Batch [230/938], Loss: 0.46614599227905273\n",
      "Train: Epoch [22], Batch [231/938], Loss: 0.35287773609161377\n",
      "Train: Epoch [22], Batch [232/938], Loss: 0.39019066095352173\n",
      "Train: Epoch [22], Batch [233/938], Loss: 0.3401501774787903\n",
      "Train: Epoch [22], Batch [234/938], Loss: 0.3638201057910919\n",
      "Train: Epoch [22], Batch [235/938], Loss: 0.46036848425865173\n",
      "Train: Epoch [22], Batch [236/938], Loss: 0.5042905807495117\n",
      "Train: Epoch [22], Batch [237/938], Loss: 0.34864556789398193\n",
      "Train: Epoch [22], Batch [238/938], Loss: 0.42876917123794556\n",
      "Train: Epoch [22], Batch [239/938], Loss: 0.36388352513313293\n",
      "Train: Epoch [22], Batch [240/938], Loss: 0.3388713598251343\n",
      "Train: Epoch [22], Batch [241/938], Loss: 0.3182311952114105\n",
      "Train: Epoch [22], Batch [242/938], Loss: 0.6173497438430786\n",
      "Train: Epoch [22], Batch [243/938], Loss: 0.2811111509799957\n",
      "Train: Epoch [22], Batch [244/938], Loss: 0.5582694411277771\n",
      "Train: Epoch [22], Batch [245/938], Loss: 0.44392454624176025\n",
      "Train: Epoch [22], Batch [246/938], Loss: 0.34840214252471924\n",
      "Train: Epoch [22], Batch [247/938], Loss: 0.41234785318374634\n",
      "Train: Epoch [22], Batch [248/938], Loss: 0.4642961025238037\n",
      "Train: Epoch [22], Batch [249/938], Loss: 0.49437063932418823\n",
      "Train: Epoch [22], Batch [250/938], Loss: 0.47030022740364075\n",
      "Train: Epoch [22], Batch [251/938], Loss: 0.5678294897079468\n",
      "Train: Epoch [22], Batch [252/938], Loss: 0.28784647583961487\n",
      "Train: Epoch [22], Batch [253/938], Loss: 0.41110366582870483\n",
      "Train: Epoch [22], Batch [254/938], Loss: 0.19361017644405365\n",
      "Train: Epoch [22], Batch [255/938], Loss: 0.4016849398612976\n",
      "Train: Epoch [22], Batch [256/938], Loss: 0.2880775034427643\n",
      "Train: Epoch [22], Batch [257/938], Loss: 0.3907240629196167\n",
      "Train: Epoch [22], Batch [258/938], Loss: 0.4428935647010803\n",
      "Train: Epoch [22], Batch [259/938], Loss: 0.32813116908073425\n",
      "Train: Epoch [22], Batch [260/938], Loss: 0.38864582777023315\n",
      "Train: Epoch [22], Batch [261/938], Loss: 0.47274115681648254\n",
      "Train: Epoch [22], Batch [262/938], Loss: 0.4199168086051941\n",
      "Train: Epoch [22], Batch [263/938], Loss: 0.6095489859580994\n",
      "Train: Epoch [22], Batch [264/938], Loss: 0.4409034252166748\n",
      "Train: Epoch [22], Batch [265/938], Loss: 0.396870493888855\n",
      "Train: Epoch [22], Batch [266/938], Loss: 0.40089496970176697\n",
      "Train: Epoch [22], Batch [267/938], Loss: 0.34653133153915405\n",
      "Train: Epoch [22], Batch [268/938], Loss: 0.3611240088939667\n",
      "Train: Epoch [22], Batch [269/938], Loss: 0.46465978026390076\n",
      "Train: Epoch [22], Batch [270/938], Loss: 0.3981727063655853\n",
      "Train: Epoch [22], Batch [271/938], Loss: 0.4384614825248718\n",
      "Train: Epoch [22], Batch [272/938], Loss: 0.4432726204395294\n",
      "Train: Epoch [22], Batch [273/938], Loss: 0.22820189595222473\n",
      "Train: Epoch [22], Batch [274/938], Loss: 0.3291065990924835\n",
      "Train: Epoch [22], Batch [275/938], Loss: 0.3478345274925232\n",
      "Train: Epoch [22], Batch [276/938], Loss: 0.6515730619430542\n",
      "Train: Epoch [22], Batch [277/938], Loss: 0.3514097034931183\n",
      "Train: Epoch [22], Batch [278/938], Loss: 0.3013622760772705\n",
      "Train: Epoch [22], Batch [279/938], Loss: 0.3252354860305786\n",
      "Train: Epoch [22], Batch [280/938], Loss: 0.2869184613227844\n",
      "Train: Epoch [22], Batch [281/938], Loss: 0.4165075719356537\n",
      "Train: Epoch [22], Batch [282/938], Loss: 0.44697144627571106\n",
      "Train: Epoch [22], Batch [283/938], Loss: 0.5028144121170044\n",
      "Train: Epoch [22], Batch [284/938], Loss: 0.44201868772506714\n",
      "Train: Epoch [22], Batch [285/938], Loss: 0.3416571617126465\n",
      "Train: Epoch [22], Batch [286/938], Loss: 0.3309822082519531\n",
      "Train: Epoch [22], Batch [287/938], Loss: 0.4313866198062897\n",
      "Train: Epoch [22], Batch [288/938], Loss: 0.3737151324748993\n",
      "Train: Epoch [22], Batch [289/938], Loss: 0.4477081596851349\n",
      "Train: Epoch [22], Batch [290/938], Loss: 0.3059176206588745\n",
      "Train: Epoch [22], Batch [291/938], Loss: 0.44114407896995544\n",
      "Train: Epoch [22], Batch [292/938], Loss: 0.420820415019989\n",
      "Train: Epoch [22], Batch [293/938], Loss: 0.21151500940322876\n",
      "Train: Epoch [22], Batch [294/938], Loss: 0.40654969215393066\n",
      "Train: Epoch [22], Batch [295/938], Loss: 0.4123815894126892\n",
      "Train: Epoch [22], Batch [296/938], Loss: 0.5325484275817871\n",
      "Train: Epoch [22], Batch [297/938], Loss: 0.5022262334823608\n",
      "Train: Epoch [22], Batch [298/938], Loss: 0.19061775505542755\n",
      "Train: Epoch [22], Batch [299/938], Loss: 0.48986026644706726\n",
      "Train: Epoch [22], Batch [300/938], Loss: 0.4376618266105652\n",
      "Train: Epoch [22], Batch [301/938], Loss: 0.27628985047340393\n",
      "Train: Epoch [22], Batch [302/938], Loss: 0.2710703909397125\n",
      "Train: Epoch [22], Batch [303/938], Loss: 0.3257923126220703\n",
      "Train: Epoch [22], Batch [304/938], Loss: 0.31811511516571045\n",
      "Train: Epoch [22], Batch [305/938], Loss: 0.3701263666152954\n",
      "Train: Epoch [22], Batch [306/938], Loss: 0.39901483058929443\n",
      "Train: Epoch [22], Batch [307/938], Loss: 0.6185343265533447\n",
      "Train: Epoch [22], Batch [308/938], Loss: 0.4356973171234131\n",
      "Train: Epoch [22], Batch [309/938], Loss: 0.6250959634780884\n",
      "Train: Epoch [22], Batch [310/938], Loss: 0.2712247371673584\n",
      "Train: Epoch [22], Batch [311/938], Loss: 0.3144606947898865\n",
      "Train: Epoch [22], Batch [312/938], Loss: 0.4494660794734955\n",
      "Train: Epoch [22], Batch [313/938], Loss: 0.5208382606506348\n",
      "Train: Epoch [22], Batch [314/938], Loss: 0.4490662217140198\n",
      "Train: Epoch [22], Batch [315/938], Loss: 0.5243338346481323\n",
      "Train: Epoch [22], Batch [316/938], Loss: 0.47173139452934265\n",
      "Train: Epoch [22], Batch [317/938], Loss: 0.3400827646255493\n",
      "Train: Epoch [22], Batch [318/938], Loss: 0.5562471151351929\n",
      "Train: Epoch [22], Batch [319/938], Loss: 0.41615474224090576\n",
      "Train: Epoch [22], Batch [320/938], Loss: 0.31659409403800964\n",
      "Train: Epoch [22], Batch [321/938], Loss: 0.23043835163116455\n",
      "Train: Epoch [22], Batch [322/938], Loss: 0.4270588457584381\n",
      "Train: Epoch [22], Batch [323/938], Loss: 0.4967731237411499\n",
      "Train: Epoch [22], Batch [324/938], Loss: 0.5217949151992798\n",
      "Train: Epoch [22], Batch [325/938], Loss: 0.3978341221809387\n",
      "Train: Epoch [22], Batch [326/938], Loss: 0.50018310546875\n",
      "Train: Epoch [22], Batch [327/938], Loss: 0.4610145688056946\n",
      "Train: Epoch [22], Batch [328/938], Loss: 0.1781478226184845\n",
      "Train: Epoch [22], Batch [329/938], Loss: 0.5675112009048462\n",
      "Train: Epoch [22], Batch [330/938], Loss: 0.3400293290615082\n",
      "Train: Epoch [22], Batch [331/938], Loss: 0.30312708020210266\n",
      "Train: Epoch [22], Batch [332/938], Loss: 0.23822270333766937\n",
      "Train: Epoch [22], Batch [333/938], Loss: 0.2549028694629669\n",
      "Train: Epoch [22], Batch [334/938], Loss: 0.3490910828113556\n",
      "Train: Epoch [22], Batch [335/938], Loss: 0.5522911548614502\n",
      "Train: Epoch [22], Batch [336/938], Loss: 0.3472299873828888\n",
      "Train: Epoch [22], Batch [337/938], Loss: 0.6445846557617188\n",
      "Train: Epoch [22], Batch [338/938], Loss: 0.3182947635650635\n",
      "Train: Epoch [22], Batch [339/938], Loss: 0.2964632511138916\n",
      "Train: Epoch [22], Batch [340/938], Loss: 0.3880663812160492\n",
      "Train: Epoch [22], Batch [341/938], Loss: 0.433917760848999\n",
      "Train: Epoch [22], Batch [342/938], Loss: 0.3812302350997925\n",
      "Train: Epoch [22], Batch [343/938], Loss: 0.4627009928226471\n",
      "Train: Epoch [22], Batch [344/938], Loss: 0.3918624520301819\n",
      "Train: Epoch [22], Batch [345/938], Loss: 0.4148058295249939\n",
      "Train: Epoch [22], Batch [346/938], Loss: 0.439369261264801\n",
      "Train: Epoch [22], Batch [347/938], Loss: 0.4115111827850342\n",
      "Train: Epoch [22], Batch [348/938], Loss: 0.46531349420547485\n",
      "Train: Epoch [22], Batch [349/938], Loss: 0.3996385931968689\n",
      "Train: Epoch [22], Batch [350/938], Loss: 0.4985380470752716\n",
      "Train: Epoch [22], Batch [351/938], Loss: 0.4114779829978943\n",
      "Train: Epoch [22], Batch [352/938], Loss: 0.3417651355266571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [353/938], Loss: 0.4116378724575043\n",
      "Train: Epoch [22], Batch [354/938], Loss: 0.4028638005256653\n",
      "Train: Epoch [22], Batch [355/938], Loss: 0.43261194229125977\n",
      "Train: Epoch [22], Batch [356/938], Loss: 0.43038010597229004\n",
      "Train: Epoch [22], Batch [357/938], Loss: 0.29130494594573975\n",
      "Train: Epoch [22], Batch [358/938], Loss: 0.24915073812007904\n",
      "Train: Epoch [22], Batch [359/938], Loss: 0.31728050112724304\n",
      "Train: Epoch [22], Batch [360/938], Loss: 0.3778783679008484\n",
      "Train: Epoch [22], Batch [361/938], Loss: 0.4335072934627533\n",
      "Train: Epoch [22], Batch [362/938], Loss: 0.47595328092575073\n",
      "Train: Epoch [22], Batch [363/938], Loss: 0.4489656686782837\n",
      "Train: Epoch [22], Batch [364/938], Loss: 0.35571038722991943\n",
      "Train: Epoch [22], Batch [365/938], Loss: 0.29825299978256226\n",
      "Train: Epoch [22], Batch [366/938], Loss: 0.3515993356704712\n",
      "Train: Epoch [22], Batch [367/938], Loss: 0.3724423348903656\n",
      "Train: Epoch [22], Batch [368/938], Loss: 0.497017502784729\n",
      "Train: Epoch [22], Batch [369/938], Loss: 0.35028010606765747\n",
      "Train: Epoch [22], Batch [370/938], Loss: 0.367561399936676\n",
      "Train: Epoch [22], Batch [371/938], Loss: 0.37619420886039734\n",
      "Train: Epoch [22], Batch [372/938], Loss: 0.4821630120277405\n",
      "Train: Epoch [22], Batch [373/938], Loss: 0.32901620864868164\n",
      "Train: Epoch [22], Batch [374/938], Loss: 0.3712393641471863\n",
      "Train: Epoch [22], Batch [375/938], Loss: 0.5472640991210938\n",
      "Train: Epoch [22], Batch [376/938], Loss: 0.3836458921432495\n",
      "Train: Epoch [22], Batch [377/938], Loss: 0.3491387963294983\n",
      "Train: Epoch [22], Batch [378/938], Loss: 0.4426552951335907\n",
      "Train: Epoch [22], Batch [379/938], Loss: 0.2913406491279602\n",
      "Train: Epoch [22], Batch [380/938], Loss: 0.6060856580734253\n",
      "Train: Epoch [22], Batch [381/938], Loss: 0.3405066430568695\n",
      "Train: Epoch [22], Batch [382/938], Loss: 0.4666629433631897\n",
      "Train: Epoch [22], Batch [383/938], Loss: 0.30109888315200806\n",
      "Train: Epoch [22], Batch [384/938], Loss: 0.46782124042510986\n",
      "Train: Epoch [22], Batch [385/938], Loss: 0.5846475958824158\n",
      "Train: Epoch [22], Batch [386/938], Loss: 0.3713860809803009\n",
      "Train: Epoch [22], Batch [387/938], Loss: 0.4148145914077759\n",
      "Train: Epoch [22], Batch [388/938], Loss: 0.5297344326972961\n",
      "Train: Epoch [22], Batch [389/938], Loss: 0.2692929804325104\n",
      "Train: Epoch [22], Batch [390/938], Loss: 0.34422966837882996\n",
      "Train: Epoch [22], Batch [391/938], Loss: 0.3338996171951294\n",
      "Train: Epoch [22], Batch [392/938], Loss: 0.736384928226471\n",
      "Train: Epoch [22], Batch [393/938], Loss: 0.3506659269332886\n",
      "Train: Epoch [22], Batch [394/938], Loss: 0.398211807012558\n",
      "Train: Epoch [22], Batch [395/938], Loss: 0.3446371555328369\n",
      "Train: Epoch [22], Batch [396/938], Loss: 0.390356183052063\n",
      "Train: Epoch [22], Batch [397/938], Loss: 0.4601176381111145\n",
      "Train: Epoch [22], Batch [398/938], Loss: 0.4912814199924469\n",
      "Train: Epoch [22], Batch [399/938], Loss: 0.4406159520149231\n",
      "Train: Epoch [22], Batch [400/938], Loss: 0.5642584562301636\n",
      "Train: Epoch [22], Batch [401/938], Loss: 0.28454673290252686\n",
      "Train: Epoch [22], Batch [402/938], Loss: 0.38865089416503906\n",
      "Train: Epoch [22], Batch [403/938], Loss: 0.45445388555526733\n",
      "Train: Epoch [22], Batch [404/938], Loss: 0.34317803382873535\n",
      "Train: Epoch [22], Batch [405/938], Loss: 0.5852155685424805\n",
      "Train: Epoch [22], Batch [406/938], Loss: 0.5093437433242798\n",
      "Train: Epoch [22], Batch [407/938], Loss: 0.399899423122406\n",
      "Train: Epoch [22], Batch [408/938], Loss: 0.31196343898773193\n",
      "Train: Epoch [22], Batch [409/938], Loss: 0.2630842328071594\n",
      "Train: Epoch [22], Batch [410/938], Loss: 0.4733287990093231\n",
      "Train: Epoch [22], Batch [411/938], Loss: 0.38830408453941345\n",
      "Train: Epoch [22], Batch [412/938], Loss: 0.5705004930496216\n",
      "Train: Epoch [22], Batch [413/938], Loss: 0.43006467819213867\n",
      "Train: Epoch [22], Batch [414/938], Loss: 0.3033430576324463\n",
      "Train: Epoch [22], Batch [415/938], Loss: 0.6053799986839294\n",
      "Train: Epoch [22], Batch [416/938], Loss: 0.2186749279499054\n",
      "Train: Epoch [22], Batch [417/938], Loss: 0.5447662472724915\n",
      "Train: Epoch [22], Batch [418/938], Loss: 0.7050622701644897\n",
      "Train: Epoch [22], Batch [419/938], Loss: 0.27824920415878296\n",
      "Train: Epoch [22], Batch [420/938], Loss: 0.3294883966445923\n",
      "Train: Epoch [22], Batch [421/938], Loss: 0.2535257637500763\n",
      "Train: Epoch [22], Batch [422/938], Loss: 0.30092960596084595\n",
      "Train: Epoch [22], Batch [423/938], Loss: 0.43725836277008057\n",
      "Train: Epoch [22], Batch [424/938], Loss: 0.16447120904922485\n",
      "Train: Epoch [22], Batch [425/938], Loss: 0.38593822717666626\n",
      "Train: Epoch [22], Batch [426/938], Loss: 0.46314120292663574\n",
      "Train: Epoch [22], Batch [427/938], Loss: 0.344971239566803\n",
      "Train: Epoch [22], Batch [428/938], Loss: 0.2929295301437378\n",
      "Train: Epoch [22], Batch [429/938], Loss: 0.6427733898162842\n",
      "Train: Epoch [22], Batch [430/938], Loss: 0.34753522276878357\n",
      "Train: Epoch [22], Batch [431/938], Loss: 0.49517980217933655\n",
      "Train: Epoch [22], Batch [432/938], Loss: 0.5138899087905884\n",
      "Train: Epoch [22], Batch [433/938], Loss: 0.2965475022792816\n",
      "Train: Epoch [22], Batch [434/938], Loss: 0.33110347390174866\n",
      "Train: Epoch [22], Batch [435/938], Loss: 0.4234776496887207\n",
      "Train: Epoch [22], Batch [436/938], Loss: 0.4049883782863617\n",
      "Train: Epoch [22], Batch [437/938], Loss: 0.4087083637714386\n",
      "Train: Epoch [22], Batch [438/938], Loss: 0.3522297441959381\n",
      "Train: Epoch [22], Batch [439/938], Loss: 0.2584703266620636\n",
      "Train: Epoch [22], Batch [440/938], Loss: 0.26016589999198914\n",
      "Train: Epoch [22], Batch [441/938], Loss: 0.4977095127105713\n",
      "Train: Epoch [22], Batch [442/938], Loss: 0.35003912448883057\n",
      "Train: Epoch [22], Batch [443/938], Loss: 0.320889413356781\n",
      "Train: Epoch [22], Batch [444/938], Loss: 0.5325385928153992\n",
      "Train: Epoch [22], Batch [445/938], Loss: 0.33277690410614014\n",
      "Train: Epoch [22], Batch [446/938], Loss: 0.38974863290786743\n",
      "Train: Epoch [22], Batch [447/938], Loss: 0.35894179344177246\n",
      "Train: Epoch [22], Batch [448/938], Loss: 0.37588366866111755\n",
      "Train: Epoch [22], Batch [449/938], Loss: 0.5227565765380859\n",
      "Train: Epoch [22], Batch [450/938], Loss: 0.5366353988647461\n",
      "Train: Epoch [22], Batch [451/938], Loss: 0.526922881603241\n",
      "Train: Epoch [22], Batch [452/938], Loss: 0.4312472641468048\n",
      "Train: Epoch [22], Batch [453/938], Loss: 0.31655412912368774\n",
      "Train: Epoch [22], Batch [454/938], Loss: 0.4676833152770996\n",
      "Train: Epoch [22], Batch [455/938], Loss: 0.2663276791572571\n",
      "Train: Epoch [22], Batch [456/938], Loss: 0.356897234916687\n",
      "Train: Epoch [22], Batch [457/938], Loss: 0.5865434408187866\n",
      "Train: Epoch [22], Batch [458/938], Loss: 0.3245425820350647\n",
      "Train: Epoch [22], Batch [459/938], Loss: 0.7979905605316162\n",
      "Train: Epoch [22], Batch [460/938], Loss: 0.33151793479919434\n",
      "Train: Epoch [22], Batch [461/938], Loss: 0.27422401309013367\n",
      "Train: Epoch [22], Batch [462/938], Loss: 0.3475514054298401\n",
      "Train: Epoch [22], Batch [463/938], Loss: 0.38192611932754517\n",
      "Train: Epoch [22], Batch [464/938], Loss: 0.36702460050582886\n",
      "Train: Epoch [22], Batch [465/938], Loss: 0.4537195563316345\n",
      "Train: Epoch [22], Batch [466/938], Loss: 0.41596466302871704\n",
      "Train: Epoch [22], Batch [467/938], Loss: 0.5265231728553772\n",
      "Train: Epoch [22], Batch [468/938], Loss: 0.34962961077690125\n",
      "Train: Epoch [22], Batch [469/938], Loss: 0.28799110651016235\n",
      "Train: Epoch [22], Batch [470/938], Loss: 0.30291518568992615\n",
      "Train: Epoch [22], Batch [471/938], Loss: 0.3517782688140869\n",
      "Train: Epoch [22], Batch [472/938], Loss: 0.38275232911109924\n",
      "Train: Epoch [22], Batch [473/938], Loss: 0.363048791885376\n",
      "Train: Epoch [22], Batch [474/938], Loss: 0.4162144064903259\n",
      "Train: Epoch [22], Batch [475/938], Loss: 0.2772260904312134\n",
      "Train: Epoch [22], Batch [476/938], Loss: 0.38061901926994324\n",
      "Train: Epoch [22], Batch [477/938], Loss: 0.4404789209365845\n",
      "Train: Epoch [22], Batch [478/938], Loss: 0.4777568578720093\n",
      "Train: Epoch [22], Batch [479/938], Loss: 0.22582051157951355\n",
      "Train: Epoch [22], Batch [480/938], Loss: 0.7586642503738403\n",
      "Train: Epoch [22], Batch [481/938], Loss: 0.6914220452308655\n",
      "Train: Epoch [22], Batch [482/938], Loss: 0.40444329380989075\n",
      "Train: Epoch [22], Batch [483/938], Loss: 0.29337936639785767\n",
      "Train: Epoch [22], Batch [484/938], Loss: 0.3347321152687073\n",
      "Train: Epoch [22], Batch [485/938], Loss: 0.3086721897125244\n",
      "Train: Epoch [22], Batch [486/938], Loss: 0.29745110869407654\n",
      "Train: Epoch [22], Batch [487/938], Loss: 0.42295601963996887\n",
      "Train: Epoch [22], Batch [488/938], Loss: 0.5369551777839661\n",
      "Train: Epoch [22], Batch [489/938], Loss: 0.45308351516723633\n",
      "Train: Epoch [22], Batch [490/938], Loss: 0.27140212059020996\n",
      "Train: Epoch [22], Batch [491/938], Loss: 0.31547507643699646\n",
      "Train: Epoch [22], Batch [492/938], Loss: 0.2863624095916748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [493/938], Loss: 0.34276819229125977\n",
      "Train: Epoch [22], Batch [494/938], Loss: 0.491496741771698\n",
      "Train: Epoch [22], Batch [495/938], Loss: 0.40025463700294495\n",
      "Train: Epoch [22], Batch [496/938], Loss: 0.34559449553489685\n",
      "Train: Epoch [22], Batch [497/938], Loss: 0.3182787597179413\n",
      "Train: Epoch [22], Batch [498/938], Loss: 0.3704400062561035\n",
      "Train: Epoch [22], Batch [499/938], Loss: 0.3376423120498657\n",
      "Train: Epoch [22], Batch [500/938], Loss: 0.28007909655570984\n",
      "Train: Epoch [22], Batch [501/938], Loss: 0.4101361036300659\n",
      "Train: Epoch [22], Batch [502/938], Loss: 0.5476315021514893\n",
      "Train: Epoch [22], Batch [503/938], Loss: 0.33817923069000244\n",
      "Train: Epoch [22], Batch [504/938], Loss: 0.30682119727134705\n",
      "Train: Epoch [22], Batch [505/938], Loss: 0.3690187931060791\n",
      "Train: Epoch [22], Batch [506/938], Loss: 0.3464643359184265\n",
      "Train: Epoch [22], Batch [507/938], Loss: 0.385738343000412\n",
      "Train: Epoch [22], Batch [508/938], Loss: 0.37084412574768066\n",
      "Train: Epoch [22], Batch [509/938], Loss: 0.32598191499710083\n",
      "Train: Epoch [22], Batch [510/938], Loss: 0.22957254946231842\n",
      "Train: Epoch [22], Batch [511/938], Loss: 0.5704782009124756\n",
      "Train: Epoch [22], Batch [512/938], Loss: 0.43837717175483704\n",
      "Train: Epoch [22], Batch [513/938], Loss: 0.3147541284561157\n",
      "Train: Epoch [22], Batch [514/938], Loss: 0.39831429719924927\n",
      "Train: Epoch [22], Batch [515/938], Loss: 0.3396312892436981\n",
      "Train: Epoch [22], Batch [516/938], Loss: 0.42018336057662964\n",
      "Train: Epoch [22], Batch [517/938], Loss: 0.38325801491737366\n",
      "Train: Epoch [22], Batch [518/938], Loss: 0.23752549290657043\n",
      "Train: Epoch [22], Batch [519/938], Loss: 0.37508782744407654\n",
      "Train: Epoch [22], Batch [520/938], Loss: 0.44397059082984924\n",
      "Train: Epoch [22], Batch [521/938], Loss: 0.32989174127578735\n",
      "Train: Epoch [22], Batch [522/938], Loss: 0.4143245816230774\n",
      "Train: Epoch [22], Batch [523/938], Loss: 0.37788093090057373\n",
      "Train: Epoch [22], Batch [524/938], Loss: 0.5893781185150146\n",
      "Train: Epoch [22], Batch [525/938], Loss: 0.34177201986312866\n",
      "Train: Epoch [22], Batch [526/938], Loss: 0.36057591438293457\n",
      "Train: Epoch [22], Batch [527/938], Loss: 0.528689980506897\n",
      "Train: Epoch [22], Batch [528/938], Loss: 0.5800561308860779\n",
      "Train: Epoch [22], Batch [529/938], Loss: 0.5839210152626038\n",
      "Train: Epoch [22], Batch [530/938], Loss: 0.41792410612106323\n",
      "Train: Epoch [22], Batch [531/938], Loss: 0.3881264925003052\n",
      "Train: Epoch [22], Batch [532/938], Loss: 0.6433159708976746\n",
      "Train: Epoch [22], Batch [533/938], Loss: 0.34459972381591797\n",
      "Train: Epoch [22], Batch [534/938], Loss: 0.21640416979789734\n",
      "Train: Epoch [22], Batch [535/938], Loss: 0.2694007158279419\n",
      "Train: Epoch [22], Batch [536/938], Loss: 0.3598761558532715\n",
      "Train: Epoch [22], Batch [537/938], Loss: 0.4636988341808319\n",
      "Train: Epoch [22], Batch [538/938], Loss: 0.422435998916626\n",
      "Train: Epoch [22], Batch [539/938], Loss: 0.4167240858078003\n",
      "Train: Epoch [22], Batch [540/938], Loss: 0.5330301523208618\n",
      "Train: Epoch [22], Batch [541/938], Loss: 0.29228442907333374\n",
      "Train: Epoch [22], Batch [542/938], Loss: 0.37603241205215454\n",
      "Train: Epoch [22], Batch [543/938], Loss: 0.4501204192638397\n",
      "Train: Epoch [22], Batch [544/938], Loss: 0.3348264694213867\n",
      "Train: Epoch [22], Batch [545/938], Loss: 0.39087265729904175\n",
      "Train: Epoch [22], Batch [546/938], Loss: 0.36034032702445984\n",
      "Train: Epoch [22], Batch [547/938], Loss: 0.4007109999656677\n",
      "Train: Epoch [22], Batch [548/938], Loss: 0.3220585584640503\n",
      "Train: Epoch [22], Batch [549/938], Loss: 0.3143482506275177\n",
      "Train: Epoch [22], Batch [550/938], Loss: 0.32533106207847595\n",
      "Train: Epoch [22], Batch [551/938], Loss: 0.3404764235019684\n",
      "Train: Epoch [22], Batch [552/938], Loss: 0.38101157546043396\n",
      "Train: Epoch [22], Batch [553/938], Loss: 0.36539649963378906\n",
      "Train: Epoch [22], Batch [554/938], Loss: 0.49018001556396484\n",
      "Train: Epoch [22], Batch [555/938], Loss: 0.31956934928894043\n",
      "Train: Epoch [22], Batch [556/938], Loss: 0.24009506404399872\n",
      "Train: Epoch [22], Batch [557/938], Loss: 0.4197896420955658\n",
      "Train: Epoch [22], Batch [558/938], Loss: 0.44775328040122986\n",
      "Train: Epoch [22], Batch [559/938], Loss: 0.5291045904159546\n",
      "Train: Epoch [22], Batch [560/938], Loss: 0.3416452407836914\n",
      "Train: Epoch [22], Batch [561/938], Loss: 0.30278271436691284\n",
      "Train: Epoch [22], Batch [562/938], Loss: 0.47321760654449463\n",
      "Train: Epoch [22], Batch [563/938], Loss: 0.3891412615776062\n",
      "Train: Epoch [22], Batch [564/938], Loss: 0.39355340600013733\n",
      "Train: Epoch [22], Batch [565/938], Loss: 0.3389219641685486\n",
      "Train: Epoch [22], Batch [566/938], Loss: 0.24359577894210815\n",
      "Train: Epoch [22], Batch [567/938], Loss: 0.2710173428058624\n",
      "Train: Epoch [22], Batch [568/938], Loss: 0.2983110547065735\n",
      "Train: Epoch [22], Batch [569/938], Loss: 0.6325289011001587\n",
      "Train: Epoch [22], Batch [570/938], Loss: 0.4412928819656372\n",
      "Train: Epoch [22], Batch [571/938], Loss: 0.7333899736404419\n",
      "Train: Epoch [22], Batch [572/938], Loss: 0.2916199862957001\n",
      "Train: Epoch [22], Batch [573/938], Loss: 0.3814789056777954\n",
      "Train: Epoch [22], Batch [574/938], Loss: 0.4175927937030792\n",
      "Train: Epoch [22], Batch [575/938], Loss: 0.43947213888168335\n",
      "Train: Epoch [22], Batch [576/938], Loss: 0.48513105511665344\n",
      "Train: Epoch [22], Batch [577/938], Loss: 0.38460034132003784\n",
      "Train: Epoch [22], Batch [578/938], Loss: 0.48077118396759033\n",
      "Train: Epoch [22], Batch [579/938], Loss: 0.23146432638168335\n",
      "Train: Epoch [22], Batch [580/938], Loss: 0.2679451107978821\n",
      "Train: Epoch [22], Batch [581/938], Loss: 0.42525988817214966\n",
      "Train: Epoch [22], Batch [582/938], Loss: 0.29298505187034607\n",
      "Train: Epoch [22], Batch [583/938], Loss: 0.37104088068008423\n",
      "Train: Epoch [22], Batch [584/938], Loss: 0.4283037781715393\n",
      "Train: Epoch [22], Batch [585/938], Loss: 0.337630033493042\n",
      "Train: Epoch [22], Batch [586/938], Loss: 0.4560152292251587\n",
      "Train: Epoch [22], Batch [587/938], Loss: 0.5807446837425232\n",
      "Train: Epoch [22], Batch [588/938], Loss: 0.3055441975593567\n",
      "Train: Epoch [22], Batch [589/938], Loss: 0.3180222809314728\n",
      "Train: Epoch [22], Batch [590/938], Loss: 0.3162938952445984\n",
      "Train: Epoch [22], Batch [591/938], Loss: 0.34598246216773987\n",
      "Train: Epoch [22], Batch [592/938], Loss: 0.3704240322113037\n",
      "Train: Epoch [22], Batch [593/938], Loss: 0.41110759973526\n",
      "Train: Epoch [22], Batch [594/938], Loss: 0.296785831451416\n",
      "Train: Epoch [22], Batch [595/938], Loss: 0.37280726432800293\n",
      "Train: Epoch [22], Batch [596/938], Loss: 0.33662259578704834\n",
      "Train: Epoch [22], Batch [597/938], Loss: 0.26130712032318115\n",
      "Train: Epoch [22], Batch [598/938], Loss: 0.3249383568763733\n",
      "Train: Epoch [22], Batch [599/938], Loss: 0.39840102195739746\n",
      "Train: Epoch [22], Batch [600/938], Loss: 0.6303250789642334\n",
      "Train: Epoch [22], Batch [601/938], Loss: 0.3456750512123108\n",
      "Train: Epoch [22], Batch [602/938], Loss: 0.4412485361099243\n",
      "Train: Epoch [22], Batch [603/938], Loss: 0.5198307037353516\n",
      "Train: Epoch [22], Batch [604/938], Loss: 0.5418429970741272\n",
      "Train: Epoch [22], Batch [605/938], Loss: 0.4454105496406555\n",
      "Train: Epoch [22], Batch [606/938], Loss: 0.3500328063964844\n",
      "Train: Epoch [22], Batch [607/938], Loss: 0.23903998732566833\n",
      "Train: Epoch [22], Batch [608/938], Loss: 0.3205450177192688\n",
      "Train: Epoch [22], Batch [609/938], Loss: 0.22880122065544128\n",
      "Train: Epoch [22], Batch [610/938], Loss: 0.346822589635849\n",
      "Train: Epoch [22], Batch [611/938], Loss: 0.4770714044570923\n",
      "Train: Epoch [22], Batch [612/938], Loss: 0.6182723045349121\n",
      "Train: Epoch [22], Batch [613/938], Loss: 0.29127493500709534\n",
      "Train: Epoch [22], Batch [614/938], Loss: 0.4299166798591614\n",
      "Train: Epoch [22], Batch [615/938], Loss: 0.4855181872844696\n",
      "Train: Epoch [22], Batch [616/938], Loss: 0.6436254978179932\n",
      "Train: Epoch [22], Batch [617/938], Loss: 0.4378458857536316\n",
      "Train: Epoch [22], Batch [618/938], Loss: 0.27497923374176025\n",
      "Train: Epoch [22], Batch [619/938], Loss: 0.283181756734848\n",
      "Train: Epoch [22], Batch [620/938], Loss: 0.2415994107723236\n",
      "Train: Epoch [22], Batch [621/938], Loss: 0.5359041690826416\n",
      "Train: Epoch [22], Batch [622/938], Loss: 0.4605315029621124\n",
      "Train: Epoch [22], Batch [623/938], Loss: 0.3197547197341919\n",
      "Train: Epoch [22], Batch [624/938], Loss: 0.399740606546402\n",
      "Train: Epoch [22], Batch [625/938], Loss: 0.23601102828979492\n",
      "Train: Epoch [22], Batch [626/938], Loss: 0.34353649616241455\n",
      "Train: Epoch [22], Batch [627/938], Loss: 0.39141231775283813\n",
      "Train: Epoch [22], Batch [628/938], Loss: 0.23516371846199036\n",
      "Train: Epoch [22], Batch [629/938], Loss: 0.25512659549713135\n",
      "Train: Epoch [22], Batch [630/938], Loss: 0.2882077693939209\n",
      "Train: Epoch [22], Batch [631/938], Loss: 0.34604930877685547\n",
      "Train: Epoch [22], Batch [632/938], Loss: 0.4353191554546356\n",
      "Train: Epoch [22], Batch [633/938], Loss: 0.3246694505214691\n",
      "Train: Epoch [22], Batch [634/938], Loss: 0.3831835985183716\n",
      "Train: Epoch [22], Batch [635/938], Loss: 0.3892691731452942\n",
      "Train: Epoch [22], Batch [636/938], Loss: 0.546798825263977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [637/938], Loss: 0.3722079396247864\n",
      "Train: Epoch [22], Batch [638/938], Loss: 0.5378886461257935\n",
      "Train: Epoch [22], Batch [639/938], Loss: 0.22786404192447662\n",
      "Train: Epoch [22], Batch [640/938], Loss: 0.37224289774894714\n",
      "Train: Epoch [22], Batch [641/938], Loss: 0.35041362047195435\n",
      "Train: Epoch [22], Batch [642/938], Loss: 0.6147755980491638\n",
      "Train: Epoch [22], Batch [643/938], Loss: 0.3417934477329254\n",
      "Train: Epoch [22], Batch [644/938], Loss: 0.4374135136604309\n",
      "Train: Epoch [22], Batch [645/938], Loss: 0.41996631026268005\n",
      "Train: Epoch [22], Batch [646/938], Loss: 0.3002357482910156\n",
      "Train: Epoch [22], Batch [647/938], Loss: 0.27779173851013184\n",
      "Train: Epoch [22], Batch [648/938], Loss: 0.28114962577819824\n",
      "Train: Epoch [22], Batch [649/938], Loss: 0.32615184783935547\n",
      "Train: Epoch [22], Batch [650/938], Loss: 0.3438153862953186\n",
      "Train: Epoch [22], Batch [651/938], Loss: 0.5384359359741211\n",
      "Train: Epoch [22], Batch [652/938], Loss: 0.5027709603309631\n",
      "Train: Epoch [22], Batch [653/938], Loss: 0.41809403896331787\n",
      "Train: Epoch [22], Batch [654/938], Loss: 0.5251702070236206\n",
      "Train: Epoch [22], Batch [655/938], Loss: 0.31376221776008606\n",
      "Train: Epoch [22], Batch [656/938], Loss: 0.30633309483528137\n",
      "Train: Epoch [22], Batch [657/938], Loss: 0.4062214195728302\n",
      "Train: Epoch [22], Batch [658/938], Loss: 0.5177220106124878\n",
      "Train: Epoch [22], Batch [659/938], Loss: 0.46500104665756226\n",
      "Train: Epoch [22], Batch [660/938], Loss: 0.2887491285800934\n",
      "Train: Epoch [22], Batch [661/938], Loss: 0.5381752848625183\n",
      "Train: Epoch [22], Batch [662/938], Loss: 0.46730276942253113\n",
      "Train: Epoch [22], Batch [663/938], Loss: 0.3737737834453583\n",
      "Train: Epoch [22], Batch [664/938], Loss: 0.35377010703086853\n",
      "Train: Epoch [22], Batch [665/938], Loss: 0.5181801319122314\n",
      "Train: Epoch [22], Batch [666/938], Loss: 0.4644325375556946\n",
      "Train: Epoch [22], Batch [667/938], Loss: 0.32354384660720825\n",
      "Train: Epoch [22], Batch [668/938], Loss: 0.5831002593040466\n",
      "Train: Epoch [22], Batch [669/938], Loss: 0.34796783328056335\n",
      "Train: Epoch [22], Batch [670/938], Loss: 0.3932495713233948\n",
      "Train: Epoch [22], Batch [671/938], Loss: 0.6183475255966187\n",
      "Train: Epoch [22], Batch [672/938], Loss: 0.30125388503074646\n",
      "Train: Epoch [22], Batch [673/938], Loss: 0.32916346192359924\n",
      "Train: Epoch [22], Batch [674/938], Loss: 0.2665368914604187\n",
      "Train: Epoch [22], Batch [675/938], Loss: 0.31197306513786316\n",
      "Train: Epoch [22], Batch [676/938], Loss: 0.30006593465805054\n",
      "Train: Epoch [22], Batch [677/938], Loss: 0.214667409658432\n",
      "Train: Epoch [22], Batch [678/938], Loss: 0.42585039138793945\n",
      "Train: Epoch [22], Batch [679/938], Loss: 0.41351377964019775\n",
      "Train: Epoch [22], Batch [680/938], Loss: 0.3608669638633728\n",
      "Train: Epoch [22], Batch [681/938], Loss: 0.4048919975757599\n",
      "Train: Epoch [22], Batch [682/938], Loss: 0.5291662812232971\n",
      "Train: Epoch [22], Batch [683/938], Loss: 0.5189802646636963\n",
      "Train: Epoch [22], Batch [684/938], Loss: 0.2815987467765808\n",
      "Train: Epoch [22], Batch [685/938], Loss: 0.372408390045166\n",
      "Train: Epoch [22], Batch [686/938], Loss: 0.2634343206882477\n",
      "Train: Epoch [22], Batch [687/938], Loss: 0.24839015305042267\n",
      "Train: Epoch [22], Batch [688/938], Loss: 0.4334990382194519\n",
      "Train: Epoch [22], Batch [689/938], Loss: 0.33082476258277893\n",
      "Train: Epoch [22], Batch [690/938], Loss: 0.2843872308731079\n",
      "Train: Epoch [22], Batch [691/938], Loss: 0.6489635705947876\n",
      "Train: Epoch [22], Batch [692/938], Loss: 0.650461733341217\n",
      "Train: Epoch [22], Batch [693/938], Loss: 0.3779950737953186\n",
      "Train: Epoch [22], Batch [694/938], Loss: 0.4548638164997101\n",
      "Train: Epoch [22], Batch [695/938], Loss: 0.3133179843425751\n",
      "Train: Epoch [22], Batch [696/938], Loss: 0.29211223125457764\n",
      "Train: Epoch [22], Batch [697/938], Loss: 0.8751897811889648\n",
      "Train: Epoch [22], Batch [698/938], Loss: 0.6086581945419312\n",
      "Train: Epoch [22], Batch [699/938], Loss: 0.3870657682418823\n",
      "Train: Epoch [22], Batch [700/938], Loss: 0.2037675976753235\n",
      "Train: Epoch [22], Batch [701/938], Loss: 0.2806064486503601\n",
      "Train: Epoch [22], Batch [702/938], Loss: 0.38437265157699585\n",
      "Train: Epoch [22], Batch [703/938], Loss: 0.2842031419277191\n",
      "Train: Epoch [22], Batch [704/938], Loss: 0.47576290369033813\n",
      "Train: Epoch [22], Batch [705/938], Loss: 0.4144077003002167\n",
      "Train: Epoch [22], Batch [706/938], Loss: 0.32169216871261597\n",
      "Train: Epoch [22], Batch [707/938], Loss: 0.40830063819885254\n",
      "Train: Epoch [22], Batch [708/938], Loss: 0.44387030601501465\n",
      "Train: Epoch [22], Batch [709/938], Loss: 0.2536572217941284\n",
      "Train: Epoch [22], Batch [710/938], Loss: 0.638038158416748\n",
      "Train: Epoch [22], Batch [711/938], Loss: 0.41978517174720764\n",
      "Train: Epoch [22], Batch [712/938], Loss: 0.3687966763973236\n",
      "Train: Epoch [22], Batch [713/938], Loss: 0.40402811765670776\n",
      "Train: Epoch [22], Batch [714/938], Loss: 0.4737115800380707\n",
      "Train: Epoch [22], Batch [715/938], Loss: 0.5147926807403564\n",
      "Train: Epoch [22], Batch [716/938], Loss: 0.45401495695114136\n",
      "Train: Epoch [22], Batch [717/938], Loss: 0.42057257890701294\n",
      "Train: Epoch [22], Batch [718/938], Loss: 0.38297900557518005\n",
      "Train: Epoch [22], Batch [719/938], Loss: 0.45043057203292847\n",
      "Train: Epoch [22], Batch [720/938], Loss: 0.3445219397544861\n",
      "Train: Epoch [22], Batch [721/938], Loss: 0.5847980976104736\n",
      "Train: Epoch [22], Batch [722/938], Loss: 0.3109528124332428\n",
      "Train: Epoch [22], Batch [723/938], Loss: 0.5076384544372559\n",
      "Train: Epoch [22], Batch [724/938], Loss: 0.48702192306518555\n",
      "Train: Epoch [22], Batch [725/938], Loss: 0.3327961266040802\n",
      "Train: Epoch [22], Batch [726/938], Loss: 0.3160462975502014\n",
      "Train: Epoch [22], Batch [727/938], Loss: 0.3228391408920288\n",
      "Train: Epoch [22], Batch [728/938], Loss: 0.47972244024276733\n",
      "Train: Epoch [22], Batch [729/938], Loss: 0.28696227073669434\n",
      "Train: Epoch [22], Batch [730/938], Loss: 0.33544105291366577\n",
      "Train: Epoch [22], Batch [731/938], Loss: 0.3596540093421936\n",
      "Train: Epoch [22], Batch [732/938], Loss: 0.2900676429271698\n",
      "Train: Epoch [22], Batch [733/938], Loss: 0.4767161011695862\n",
      "Train: Epoch [22], Batch [734/938], Loss: 0.5723133683204651\n",
      "Train: Epoch [22], Batch [735/938], Loss: 0.38916298747062683\n",
      "Train: Epoch [22], Batch [736/938], Loss: 0.4895649254322052\n",
      "Train: Epoch [22], Batch [737/938], Loss: 0.3914668560028076\n",
      "Train: Epoch [22], Batch [738/938], Loss: 0.36624568700790405\n",
      "Train: Epoch [22], Batch [739/938], Loss: 0.3451840877532959\n",
      "Train: Epoch [22], Batch [740/938], Loss: 0.3975580632686615\n",
      "Train: Epoch [22], Batch [741/938], Loss: 0.4346897006034851\n",
      "Train: Epoch [22], Batch [742/938], Loss: 0.547158420085907\n",
      "Train: Epoch [22], Batch [743/938], Loss: 0.2835465669631958\n",
      "Train: Epoch [22], Batch [744/938], Loss: 0.4538906216621399\n",
      "Train: Epoch [22], Batch [745/938], Loss: 0.38647857308387756\n",
      "Train: Epoch [22], Batch [746/938], Loss: 0.49041736125946045\n",
      "Train: Epoch [22], Batch [747/938], Loss: 0.49428853392601013\n",
      "Train: Epoch [22], Batch [748/938], Loss: 0.4046868681907654\n",
      "Train: Epoch [22], Batch [749/938], Loss: 0.34937864542007446\n",
      "Train: Epoch [22], Batch [750/938], Loss: 0.4614691436290741\n",
      "Train: Epoch [22], Batch [751/938], Loss: 0.4335508644580841\n",
      "Train: Epoch [22], Batch [752/938], Loss: 0.4814111888408661\n",
      "Train: Epoch [22], Batch [753/938], Loss: 0.30307090282440186\n",
      "Train: Epoch [22], Batch [754/938], Loss: 0.2242683321237564\n",
      "Train: Epoch [22], Batch [755/938], Loss: 0.34799930453300476\n",
      "Train: Epoch [22], Batch [756/938], Loss: 0.4629303812980652\n",
      "Train: Epoch [22], Batch [757/938], Loss: 0.4714913070201874\n",
      "Train: Epoch [22], Batch [758/938], Loss: 0.33802539110183716\n",
      "Train: Epoch [22], Batch [759/938], Loss: 0.4300655722618103\n",
      "Train: Epoch [22], Batch [760/938], Loss: 0.32260435819625854\n",
      "Train: Epoch [22], Batch [761/938], Loss: 0.24248550832271576\n",
      "Train: Epoch [22], Batch [762/938], Loss: 0.40839847922325134\n",
      "Train: Epoch [22], Batch [763/938], Loss: 0.4001300036907196\n",
      "Train: Epoch [22], Batch [764/938], Loss: 0.3614901602268219\n",
      "Train: Epoch [22], Batch [765/938], Loss: 0.4374523162841797\n",
      "Train: Epoch [22], Batch [766/938], Loss: 0.47218409180641174\n",
      "Train: Epoch [22], Batch [767/938], Loss: 0.5736809968948364\n",
      "Train: Epoch [22], Batch [768/938], Loss: 0.4500446617603302\n",
      "Train: Epoch [22], Batch [769/938], Loss: 0.48277056217193604\n",
      "Train: Epoch [22], Batch [770/938], Loss: 0.4636647403240204\n",
      "Train: Epoch [22], Batch [771/938], Loss: 0.3127933740615845\n",
      "Train: Epoch [22], Batch [772/938], Loss: 0.4199850857257843\n",
      "Train: Epoch [22], Batch [773/938], Loss: 0.4277232587337494\n",
      "Train: Epoch [22], Batch [774/938], Loss: 0.29768067598342896\n",
      "Train: Epoch [22], Batch [775/938], Loss: 0.37753331661224365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [776/938], Loss: 0.2887027859687805\n",
      "Train: Epoch [22], Batch [777/938], Loss: 0.6336416006088257\n",
      "Train: Epoch [22], Batch [778/938], Loss: 0.45084521174430847\n",
      "Train: Epoch [22], Batch [779/938], Loss: 0.4431678056716919\n",
      "Train: Epoch [22], Batch [780/938], Loss: 0.4826365113258362\n",
      "Train: Epoch [22], Batch [781/938], Loss: 0.3324905037879944\n",
      "Train: Epoch [22], Batch [782/938], Loss: 0.387816846370697\n",
      "Train: Epoch [22], Batch [783/938], Loss: 0.17941831052303314\n",
      "Train: Epoch [22], Batch [784/938], Loss: 0.4199987053871155\n",
      "Train: Epoch [22], Batch [785/938], Loss: 0.3715277314186096\n",
      "Train: Epoch [22], Batch [786/938], Loss: 0.3766455054283142\n",
      "Train: Epoch [22], Batch [787/938], Loss: 0.35188621282577515\n",
      "Train: Epoch [22], Batch [788/938], Loss: 0.3727644085884094\n",
      "Train: Epoch [22], Batch [789/938], Loss: 0.21333007514476776\n",
      "Train: Epoch [22], Batch [790/938], Loss: 0.5612563490867615\n",
      "Train: Epoch [22], Batch [791/938], Loss: 0.42665866017341614\n",
      "Train: Epoch [22], Batch [792/938], Loss: 0.344133198261261\n",
      "Train: Epoch [22], Batch [793/938], Loss: 0.21446119248867035\n",
      "Train: Epoch [22], Batch [794/938], Loss: 0.6351574659347534\n",
      "Train: Epoch [22], Batch [795/938], Loss: 0.3812122642993927\n",
      "Train: Epoch [22], Batch [796/938], Loss: 0.2512569725513458\n",
      "Train: Epoch [22], Batch [797/938], Loss: 0.36948126554489136\n",
      "Train: Epoch [22], Batch [798/938], Loss: 0.5037406086921692\n",
      "Train: Epoch [22], Batch [799/938], Loss: 0.3241138458251953\n",
      "Train: Epoch [22], Batch [800/938], Loss: 0.33886218070983887\n",
      "Train: Epoch [22], Batch [801/938], Loss: 0.323924720287323\n",
      "Train: Epoch [22], Batch [802/938], Loss: 0.32716429233551025\n",
      "Train: Epoch [22], Batch [803/938], Loss: 0.5749137997627258\n",
      "Train: Epoch [22], Batch [804/938], Loss: 0.4658604860305786\n",
      "Train: Epoch [22], Batch [805/938], Loss: 0.5860323905944824\n",
      "Train: Epoch [22], Batch [806/938], Loss: 0.48532038927078247\n",
      "Train: Epoch [22], Batch [807/938], Loss: 0.340617835521698\n",
      "Train: Epoch [22], Batch [808/938], Loss: 0.28569748997688293\n",
      "Train: Epoch [22], Batch [809/938], Loss: 0.8131364583969116\n",
      "Train: Epoch [22], Batch [810/938], Loss: 0.45344430208206177\n",
      "Train: Epoch [22], Batch [811/938], Loss: 0.36953049898147583\n",
      "Train: Epoch [22], Batch [812/938], Loss: 0.2454824149608612\n",
      "Train: Epoch [22], Batch [813/938], Loss: 0.43300336599349976\n",
      "Train: Epoch [22], Batch [814/938], Loss: 0.575874388217926\n",
      "Train: Epoch [22], Batch [815/938], Loss: 0.36762040853500366\n",
      "Train: Epoch [22], Batch [816/938], Loss: 0.3504837453365326\n",
      "Train: Epoch [22], Batch [817/938], Loss: 0.41331785917282104\n",
      "Train: Epoch [22], Batch [818/938], Loss: 0.37170109152793884\n",
      "Train: Epoch [22], Batch [819/938], Loss: 0.18035228550434113\n",
      "Train: Epoch [22], Batch [820/938], Loss: 0.38220059871673584\n",
      "Train: Epoch [22], Batch [821/938], Loss: 0.40217745304107666\n",
      "Train: Epoch [22], Batch [822/938], Loss: 0.26305562257766724\n",
      "Train: Epoch [22], Batch [823/938], Loss: 0.687969446182251\n",
      "Train: Epoch [22], Batch [824/938], Loss: 0.4651649296283722\n",
      "Train: Epoch [22], Batch [825/938], Loss: 0.3790602385997772\n",
      "Train: Epoch [22], Batch [826/938], Loss: 0.4715127944946289\n",
      "Train: Epoch [22], Batch [827/938], Loss: 0.32167041301727295\n",
      "Train: Epoch [22], Batch [828/938], Loss: 0.4251483678817749\n",
      "Train: Epoch [22], Batch [829/938], Loss: 0.5309329032897949\n",
      "Train: Epoch [22], Batch [830/938], Loss: 0.43879878520965576\n",
      "Train: Epoch [22], Batch [831/938], Loss: 0.4002469778060913\n",
      "Train: Epoch [22], Batch [832/938], Loss: 0.3066987693309784\n",
      "Train: Epoch [22], Batch [833/938], Loss: 0.5170196294784546\n",
      "Train: Epoch [22], Batch [834/938], Loss: 0.3329785168170929\n",
      "Train: Epoch [22], Batch [835/938], Loss: 0.5101908445358276\n",
      "Train: Epoch [22], Batch [836/938], Loss: 0.444866418838501\n",
      "Train: Epoch [22], Batch [837/938], Loss: 0.21925397217273712\n",
      "Train: Epoch [22], Batch [838/938], Loss: 0.3901154398918152\n",
      "Train: Epoch [22], Batch [839/938], Loss: 0.6263324022293091\n",
      "Train: Epoch [22], Batch [840/938], Loss: 0.479143887758255\n",
      "Train: Epoch [22], Batch [841/938], Loss: 0.7668368816375732\n",
      "Train: Epoch [22], Batch [842/938], Loss: 0.24438336491584778\n",
      "Train: Epoch [22], Batch [843/938], Loss: 0.30004915595054626\n",
      "Train: Epoch [22], Batch [844/938], Loss: 0.42875081300735474\n",
      "Train: Epoch [22], Batch [845/938], Loss: 0.2806556224822998\n",
      "Train: Epoch [22], Batch [846/938], Loss: 0.4058913588523865\n",
      "Train: Epoch [22], Batch [847/938], Loss: 0.40612128376960754\n",
      "Train: Epoch [22], Batch [848/938], Loss: 0.26758795976638794\n",
      "Train: Epoch [22], Batch [849/938], Loss: 0.3197605013847351\n",
      "Train: Epoch [22], Batch [850/938], Loss: 0.4367120862007141\n",
      "Train: Epoch [22], Batch [851/938], Loss: 0.31427448987960815\n",
      "Train: Epoch [22], Batch [852/938], Loss: 0.3405015170574188\n",
      "Train: Epoch [22], Batch [853/938], Loss: 0.4295028746128082\n",
      "Train: Epoch [22], Batch [854/938], Loss: 0.4018005132675171\n",
      "Train: Epoch [22], Batch [855/938], Loss: 0.337982714176178\n",
      "Train: Epoch [22], Batch [856/938], Loss: 0.3605745732784271\n",
      "Train: Epoch [22], Batch [857/938], Loss: 0.2549186050891876\n",
      "Train: Epoch [22], Batch [858/938], Loss: 0.4415682852268219\n",
      "Train: Epoch [22], Batch [859/938], Loss: 0.263825386762619\n",
      "Train: Epoch [22], Batch [860/938], Loss: 0.5842346549034119\n",
      "Train: Epoch [22], Batch [861/938], Loss: 0.5280561447143555\n",
      "Train: Epoch [22], Batch [862/938], Loss: 0.5326563119888306\n",
      "Train: Epoch [22], Batch [863/938], Loss: 0.47109323740005493\n",
      "Train: Epoch [22], Batch [864/938], Loss: 0.5185095071792603\n",
      "Train: Epoch [22], Batch [865/938], Loss: 0.33403900265693665\n",
      "Train: Epoch [22], Batch [866/938], Loss: 0.3543059229850769\n",
      "Train: Epoch [22], Batch [867/938], Loss: 0.4422529935836792\n",
      "Train: Epoch [22], Batch [868/938], Loss: 0.43305113911628723\n",
      "Train: Epoch [22], Batch [869/938], Loss: 0.49209845066070557\n",
      "Train: Epoch [22], Batch [870/938], Loss: 0.33706650137901306\n",
      "Train: Epoch [22], Batch [871/938], Loss: 0.35962003469467163\n",
      "Train: Epoch [22], Batch [872/938], Loss: 0.49137091636657715\n",
      "Train: Epoch [22], Batch [873/938], Loss: 0.36888808012008667\n",
      "Train: Epoch [22], Batch [874/938], Loss: 0.44397175312042236\n",
      "Train: Epoch [22], Batch [875/938], Loss: 0.24133777618408203\n",
      "Train: Epoch [22], Batch [876/938], Loss: 0.48639383912086487\n",
      "Train: Epoch [22], Batch [877/938], Loss: 0.35151731967926025\n",
      "Train: Epoch [22], Batch [878/938], Loss: 0.37185150384902954\n",
      "Train: Epoch [22], Batch [879/938], Loss: 0.34006786346435547\n",
      "Train: Epoch [22], Batch [880/938], Loss: 0.4382598400115967\n",
      "Train: Epoch [22], Batch [881/938], Loss: 0.27316173911094666\n",
      "Train: Epoch [22], Batch [882/938], Loss: 0.368329793214798\n",
      "Train: Epoch [22], Batch [883/938], Loss: 0.3213252127170563\n",
      "Train: Epoch [22], Batch [884/938], Loss: 0.46269088983535767\n",
      "Train: Epoch [22], Batch [885/938], Loss: 0.2617686986923218\n",
      "Train: Epoch [22], Batch [886/938], Loss: 0.38523873686790466\n",
      "Train: Epoch [22], Batch [887/938], Loss: 0.3289565443992615\n",
      "Train: Epoch [22], Batch [888/938], Loss: 0.3604819178581238\n",
      "Train: Epoch [22], Batch [889/938], Loss: 0.6001070141792297\n",
      "Train: Epoch [22], Batch [890/938], Loss: 0.4872375726699829\n",
      "Train: Epoch [22], Batch [891/938], Loss: 0.46024036407470703\n",
      "Train: Epoch [22], Batch [892/938], Loss: 0.45451003313064575\n",
      "Train: Epoch [22], Batch [893/938], Loss: 0.43008488416671753\n",
      "Train: Epoch [22], Batch [894/938], Loss: 0.42299407720565796\n",
      "Train: Epoch [22], Batch [895/938], Loss: 0.2994539737701416\n",
      "Train: Epoch [22], Batch [896/938], Loss: 0.2631659507751465\n",
      "Train: Epoch [22], Batch [897/938], Loss: 0.255859375\n",
      "Train: Epoch [22], Batch [898/938], Loss: 0.3677515387535095\n",
      "Train: Epoch [22], Batch [899/938], Loss: 0.3752847909927368\n",
      "Train: Epoch [22], Batch [900/938], Loss: 0.3974301218986511\n",
      "Train: Epoch [22], Batch [901/938], Loss: 0.3951947093009949\n",
      "Train: Epoch [22], Batch [902/938], Loss: 0.37857308983802795\n",
      "Train: Epoch [22], Batch [903/938], Loss: 0.22104769945144653\n",
      "Train: Epoch [22], Batch [904/938], Loss: 0.41141870617866516\n",
      "Train: Epoch [22], Batch [905/938], Loss: 0.37272506952285767\n",
      "Train: Epoch [22], Batch [906/938], Loss: 0.3724096119403839\n",
      "Train: Epoch [22], Batch [907/938], Loss: 0.3636090159416199\n",
      "Train: Epoch [22], Batch [908/938], Loss: 0.37802010774612427\n",
      "Train: Epoch [22], Batch [909/938], Loss: 0.5210439562797546\n",
      "Train: Epoch [22], Batch [910/938], Loss: 0.4202533960342407\n",
      "Train: Epoch [22], Batch [911/938], Loss: 0.4697999358177185\n",
      "Train: Epoch [22], Batch [912/938], Loss: 0.35051512718200684\n",
      "Train: Epoch [22], Batch [913/938], Loss: 0.43894168734550476\n",
      "Train: Epoch [22], Batch [914/938], Loss: 0.49960917234420776\n",
      "Train: Epoch [22], Batch [915/938], Loss: 0.3157162070274353\n",
      "Train: Epoch [22], Batch [916/938], Loss: 0.49255070090293884\n",
      "Train: Epoch [22], Batch [917/938], Loss: 0.2556217610836029\n",
      "Train: Epoch [22], Batch [918/938], Loss: 0.2761959731578827\n",
      "Train: Epoch [22], Batch [919/938], Loss: 0.34305649995803833\n",
      "Train: Epoch [22], Batch [920/938], Loss: 0.3389919698238373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [22], Batch [921/938], Loss: 0.4600147604942322\n",
      "Train: Epoch [22], Batch [922/938], Loss: 0.4371449947357178\n",
      "Train: Epoch [22], Batch [923/938], Loss: 0.44400542974472046\n",
      "Train: Epoch [22], Batch [924/938], Loss: 0.4665969908237457\n",
      "Train: Epoch [22], Batch [925/938], Loss: 0.34128981828689575\n",
      "Train: Epoch [22], Batch [926/938], Loss: 0.3690125644207001\n",
      "Train: Epoch [22], Batch [927/938], Loss: 0.5868570804595947\n",
      "Train: Epoch [22], Batch [928/938], Loss: 0.4373471438884735\n",
      "Train: Epoch [22], Batch [929/938], Loss: 0.4186439514160156\n",
      "Train: Epoch [22], Batch [930/938], Loss: 0.4146445095539093\n",
      "Train: Epoch [22], Batch [931/938], Loss: 0.5062838792800903\n",
      "Train: Epoch [22], Batch [932/938], Loss: 0.29449594020843506\n",
      "Train: Epoch [22], Batch [933/938], Loss: 0.3226102590560913\n",
      "Train: Epoch [22], Batch [934/938], Loss: 0.32625052332878113\n",
      "Train: Epoch [22], Batch [935/938], Loss: 0.4857396185398102\n",
      "Train: Epoch [22], Batch [936/938], Loss: 0.3062135875225067\n",
      "Train: Epoch [22], Batch [937/938], Loss: 0.3673868775367737\n",
      "Train: Epoch [22], Batch [938/938], Loss: 0.26505088806152344\n",
      "Accuracy of train set: 0.8591333333333333\n",
      "Validation: Epoch [22], Batch [1/938], Loss: 0.31592726707458496\n",
      "Validation: Epoch [22], Batch [2/938], Loss: 0.4572376012802124\n",
      "Validation: Epoch [22], Batch [3/938], Loss: 0.564164936542511\n",
      "Validation: Epoch [22], Batch [4/938], Loss: 0.2596037685871124\n",
      "Validation: Epoch [22], Batch [5/938], Loss: 0.22851242125034332\n",
      "Validation: Epoch [22], Batch [6/938], Loss: 0.37290048599243164\n",
      "Validation: Epoch [22], Batch [7/938], Loss: 0.38610512018203735\n",
      "Validation: Epoch [22], Batch [8/938], Loss: 0.3101031184196472\n",
      "Validation: Epoch [22], Batch [9/938], Loss: 0.4496370553970337\n",
      "Validation: Epoch [22], Batch [10/938], Loss: 0.6205587387084961\n",
      "Validation: Epoch [22], Batch [11/938], Loss: 0.221048504114151\n",
      "Validation: Epoch [22], Batch [12/938], Loss: 0.3192225694656372\n",
      "Validation: Epoch [22], Batch [13/938], Loss: 0.32474517822265625\n",
      "Validation: Epoch [22], Batch [14/938], Loss: 0.3754032552242279\n",
      "Validation: Epoch [22], Batch [15/938], Loss: 0.6099581122398376\n",
      "Validation: Epoch [22], Batch [16/938], Loss: 0.38298889994621277\n",
      "Validation: Epoch [22], Batch [17/938], Loss: 0.40872347354888916\n",
      "Validation: Epoch [22], Batch [18/938], Loss: 0.3496902585029602\n",
      "Validation: Epoch [22], Batch [19/938], Loss: 0.2806413173675537\n",
      "Validation: Epoch [22], Batch [20/938], Loss: 0.2518576383590698\n",
      "Validation: Epoch [22], Batch [21/938], Loss: 0.426520437002182\n",
      "Validation: Epoch [22], Batch [22/938], Loss: 0.3635905683040619\n",
      "Validation: Epoch [22], Batch [23/938], Loss: 0.42681726813316345\n",
      "Validation: Epoch [22], Batch [24/938], Loss: 0.33334118127822876\n",
      "Validation: Epoch [22], Batch [25/938], Loss: 0.37479251623153687\n",
      "Validation: Epoch [22], Batch [26/938], Loss: 0.42543113231658936\n",
      "Validation: Epoch [22], Batch [27/938], Loss: 0.3623998165130615\n",
      "Validation: Epoch [22], Batch [28/938], Loss: 0.30646562576293945\n",
      "Validation: Epoch [22], Batch [29/938], Loss: 0.36619389057159424\n",
      "Validation: Epoch [22], Batch [30/938], Loss: 0.4554778039455414\n",
      "Validation: Epoch [22], Batch [31/938], Loss: 0.28583425283432007\n",
      "Validation: Epoch [22], Batch [32/938], Loss: 0.4237571954727173\n",
      "Validation: Epoch [22], Batch [33/938], Loss: 0.5142505168914795\n",
      "Validation: Epoch [22], Batch [34/938], Loss: 0.39693987369537354\n",
      "Validation: Epoch [22], Batch [35/938], Loss: 0.33815133571624756\n",
      "Validation: Epoch [22], Batch [36/938], Loss: 0.5329439640045166\n",
      "Validation: Epoch [22], Batch [37/938], Loss: 0.41494062542915344\n",
      "Validation: Epoch [22], Batch [38/938], Loss: 0.3408410847187042\n",
      "Validation: Epoch [22], Batch [39/938], Loss: 0.49610090255737305\n",
      "Validation: Epoch [22], Batch [40/938], Loss: 0.34280335903167725\n",
      "Validation: Epoch [22], Batch [41/938], Loss: 0.4362621605396271\n",
      "Validation: Epoch [22], Batch [42/938], Loss: 0.35662949085235596\n",
      "Validation: Epoch [22], Batch [43/938], Loss: 0.3147340416908264\n",
      "Validation: Epoch [22], Batch [44/938], Loss: 0.44098684191703796\n",
      "Validation: Epoch [22], Batch [45/938], Loss: 0.2304663062095642\n",
      "Validation: Epoch [22], Batch [46/938], Loss: 0.4023871421813965\n",
      "Validation: Epoch [22], Batch [47/938], Loss: 0.4110741913318634\n",
      "Validation: Epoch [22], Batch [48/938], Loss: 0.4404817223548889\n",
      "Validation: Epoch [22], Batch [49/938], Loss: 0.3203403353691101\n",
      "Validation: Epoch [22], Batch [50/938], Loss: 0.4119899570941925\n",
      "Validation: Epoch [22], Batch [51/938], Loss: 0.33532750606536865\n",
      "Validation: Epoch [22], Batch [52/938], Loss: 0.30060845613479614\n",
      "Validation: Epoch [22], Batch [53/938], Loss: 0.22538617253303528\n",
      "Validation: Epoch [22], Batch [54/938], Loss: 0.3978382349014282\n",
      "Validation: Epoch [22], Batch [55/938], Loss: 0.4009416401386261\n",
      "Validation: Epoch [22], Batch [56/938], Loss: 0.49868303537368774\n",
      "Validation: Epoch [22], Batch [57/938], Loss: 0.4574686884880066\n",
      "Validation: Epoch [22], Batch [58/938], Loss: 0.2759868800640106\n",
      "Validation: Epoch [22], Batch [59/938], Loss: 0.5277798175811768\n",
      "Validation: Epoch [22], Batch [60/938], Loss: 0.29312098026275635\n",
      "Validation: Epoch [22], Batch [61/938], Loss: 0.28968751430511475\n",
      "Validation: Epoch [22], Batch [62/938], Loss: 0.3395926356315613\n",
      "Validation: Epoch [22], Batch [63/938], Loss: 0.3741755187511444\n",
      "Validation: Epoch [22], Batch [64/938], Loss: 0.5773265361785889\n",
      "Validation: Epoch [22], Batch [65/938], Loss: 0.36436203122138977\n",
      "Validation: Epoch [22], Batch [66/938], Loss: 0.30444610118865967\n",
      "Validation: Epoch [22], Batch [67/938], Loss: 0.6489080190658569\n",
      "Validation: Epoch [22], Batch [68/938], Loss: 0.3556824028491974\n",
      "Validation: Epoch [22], Batch [69/938], Loss: 0.5155652165412903\n",
      "Validation: Epoch [22], Batch [70/938], Loss: 0.3614325225353241\n",
      "Validation: Epoch [22], Batch [71/938], Loss: 0.5806821584701538\n",
      "Validation: Epoch [22], Batch [72/938], Loss: 0.28194376826286316\n",
      "Validation: Epoch [22], Batch [73/938], Loss: 0.49570393562316895\n",
      "Validation: Epoch [22], Batch [74/938], Loss: 0.32516226172447205\n",
      "Validation: Epoch [22], Batch [75/938], Loss: 0.49818307161331177\n",
      "Validation: Epoch [22], Batch [76/938], Loss: 0.32932454347610474\n",
      "Validation: Epoch [22], Batch [77/938], Loss: 0.48080629110336304\n",
      "Validation: Epoch [22], Batch [78/938], Loss: 0.3871322274208069\n",
      "Validation: Epoch [22], Batch [79/938], Loss: 0.269748330116272\n",
      "Validation: Epoch [22], Batch [80/938], Loss: 0.5289256572723389\n",
      "Validation: Epoch [22], Batch [81/938], Loss: 0.33579516410827637\n",
      "Validation: Epoch [22], Batch [82/938], Loss: 0.46081048250198364\n",
      "Validation: Epoch [22], Batch [83/938], Loss: 0.49677643179893494\n",
      "Validation: Epoch [22], Batch [84/938], Loss: 0.3454940915107727\n",
      "Validation: Epoch [22], Batch [85/938], Loss: 0.44515931606292725\n",
      "Validation: Epoch [22], Batch [86/938], Loss: 0.40997084975242615\n",
      "Validation: Epoch [22], Batch [87/938], Loss: 0.35506725311279297\n",
      "Validation: Epoch [22], Batch [88/938], Loss: 0.2844831943511963\n",
      "Validation: Epoch [22], Batch [89/938], Loss: 0.3393317461013794\n",
      "Validation: Epoch [22], Batch [90/938], Loss: 0.5443810224533081\n",
      "Validation: Epoch [22], Batch [91/938], Loss: 0.3688817024230957\n",
      "Validation: Epoch [22], Batch [92/938], Loss: 0.3946501910686493\n",
      "Validation: Epoch [22], Batch [93/938], Loss: 0.522702693939209\n",
      "Validation: Epoch [22], Batch [94/938], Loss: 0.4368002414703369\n",
      "Validation: Epoch [22], Batch [95/938], Loss: 0.270568311214447\n",
      "Validation: Epoch [22], Batch [96/938], Loss: 0.49693763256073\n",
      "Validation: Epoch [22], Batch [97/938], Loss: 0.32075679302215576\n",
      "Validation: Epoch [22], Batch [98/938], Loss: 0.3107074797153473\n",
      "Validation: Epoch [22], Batch [99/938], Loss: 0.41626453399658203\n",
      "Validation: Epoch [22], Batch [100/938], Loss: 0.3002486228942871\n",
      "Validation: Epoch [22], Batch [101/938], Loss: 0.46922025084495544\n",
      "Validation: Epoch [22], Batch [102/938], Loss: 0.18294444680213928\n",
      "Validation: Epoch [22], Batch [103/938], Loss: 0.3315325379371643\n",
      "Validation: Epoch [22], Batch [104/938], Loss: 0.5159844756126404\n",
      "Validation: Epoch [22], Batch [105/938], Loss: 0.39065808057785034\n",
      "Validation: Epoch [22], Batch [106/938], Loss: 0.46405357122421265\n",
      "Validation: Epoch [22], Batch [107/938], Loss: 0.484596848487854\n",
      "Validation: Epoch [22], Batch [108/938], Loss: 0.20116016268730164\n",
      "Validation: Epoch [22], Batch [109/938], Loss: 0.38845062255859375\n",
      "Validation: Epoch [22], Batch [110/938], Loss: 0.6539337038993835\n",
      "Validation: Epoch [22], Batch [111/938], Loss: 0.5046984553337097\n",
      "Validation: Epoch [22], Batch [112/938], Loss: 0.22938862442970276\n",
      "Validation: Epoch [22], Batch [113/938], Loss: 0.2887052893638611\n",
      "Validation: Epoch [22], Batch [114/938], Loss: 0.27286985516548157\n",
      "Validation: Epoch [22], Batch [115/938], Loss: 0.46705693006515503\n",
      "Validation: Epoch [22], Batch [116/938], Loss: 0.37450289726257324\n",
      "Validation: Epoch [22], Batch [117/938], Loss: 0.33202412724494934\n",
      "Validation: Epoch [22], Batch [118/938], Loss: 0.31548380851745605\n",
      "Validation: Epoch [22], Batch [119/938], Loss: 0.6253335475921631\n",
      "Validation: Epoch [22], Batch [120/938], Loss: 0.43871691823005676\n",
      "Validation: Epoch [22], Batch [121/938], Loss: 0.4169614911079407\n",
      "Validation: Epoch [22], Batch [122/938], Loss: 0.4065299332141876\n",
      "Validation: Epoch [22], Batch [123/938], Loss: 0.25042665004730225\n",
      "Validation: Epoch [22], Batch [124/938], Loss: 0.4015214741230011\n",
      "Validation: Epoch [22], Batch [125/938], Loss: 0.3476409316062927\n",
      "Validation: Epoch [22], Batch [126/938], Loss: 0.3757571280002594\n",
      "Validation: Epoch [22], Batch [127/938], Loss: 0.3020961284637451\n",
      "Validation: Epoch [22], Batch [128/938], Loss: 0.3116028308868408\n",
      "Validation: Epoch [22], Batch [129/938], Loss: 0.4157426357269287\n",
      "Validation: Epoch [22], Batch [130/938], Loss: 0.4476169943809509\n",
      "Validation: Epoch [22], Batch [131/938], Loss: 0.6202296614646912\n",
      "Validation: Epoch [22], Batch [132/938], Loss: 0.32069897651672363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [133/938], Loss: 0.4484136700630188\n",
      "Validation: Epoch [22], Batch [134/938], Loss: 0.454065203666687\n",
      "Validation: Epoch [22], Batch [135/938], Loss: 0.5508044958114624\n",
      "Validation: Epoch [22], Batch [136/938], Loss: 0.4324414134025574\n",
      "Validation: Epoch [22], Batch [137/938], Loss: 0.3954143524169922\n",
      "Validation: Epoch [22], Batch [138/938], Loss: 0.4224802255630493\n",
      "Validation: Epoch [22], Batch [139/938], Loss: 0.2474391907453537\n",
      "Validation: Epoch [22], Batch [140/938], Loss: 0.33230286836624146\n",
      "Validation: Epoch [22], Batch [141/938], Loss: 0.44332316517829895\n",
      "Validation: Epoch [22], Batch [142/938], Loss: 0.3028986155986786\n",
      "Validation: Epoch [22], Batch [143/938], Loss: 0.37504300475120544\n",
      "Validation: Epoch [22], Batch [144/938], Loss: 0.42503252625465393\n",
      "Validation: Epoch [22], Batch [145/938], Loss: 0.2508609890937805\n",
      "Validation: Epoch [22], Batch [146/938], Loss: 0.5030896067619324\n",
      "Validation: Epoch [22], Batch [147/938], Loss: 0.3271801769733429\n",
      "Validation: Epoch [22], Batch [148/938], Loss: 0.40821996331214905\n",
      "Validation: Epoch [22], Batch [149/938], Loss: 0.4026716947555542\n",
      "Validation: Epoch [22], Batch [150/938], Loss: 0.4547103941440582\n",
      "Validation: Epoch [22], Batch [151/938], Loss: 0.39304405450820923\n",
      "Validation: Epoch [22], Batch [152/938], Loss: 0.3973749876022339\n",
      "Validation: Epoch [22], Batch [153/938], Loss: 0.34121277928352356\n",
      "Validation: Epoch [22], Batch [154/938], Loss: 0.3189721405506134\n",
      "Validation: Epoch [22], Batch [155/938], Loss: 0.3580743670463562\n",
      "Validation: Epoch [22], Batch [156/938], Loss: 0.5625025629997253\n",
      "Validation: Epoch [22], Batch [157/938], Loss: 0.2877628803253174\n",
      "Validation: Epoch [22], Batch [158/938], Loss: 0.43242883682250977\n",
      "Validation: Epoch [22], Batch [159/938], Loss: 0.4561936855316162\n",
      "Validation: Epoch [22], Batch [160/938], Loss: 0.38898271322250366\n",
      "Validation: Epoch [22], Batch [161/938], Loss: 0.3680422008037567\n",
      "Validation: Epoch [22], Batch [162/938], Loss: 0.3673579692840576\n",
      "Validation: Epoch [22], Batch [163/938], Loss: 0.4960475265979767\n",
      "Validation: Epoch [22], Batch [164/938], Loss: 0.21436387300491333\n",
      "Validation: Epoch [22], Batch [165/938], Loss: 0.4739503860473633\n",
      "Validation: Epoch [22], Batch [166/938], Loss: 0.3348725438117981\n",
      "Validation: Epoch [22], Batch [167/938], Loss: 0.45666229724884033\n",
      "Validation: Epoch [22], Batch [168/938], Loss: 0.3106234073638916\n",
      "Validation: Epoch [22], Batch [169/938], Loss: 0.3657606542110443\n",
      "Validation: Epoch [22], Batch [170/938], Loss: 0.31105273962020874\n",
      "Validation: Epoch [22], Batch [171/938], Loss: 0.29341965913772583\n",
      "Validation: Epoch [22], Batch [172/938], Loss: 0.34112775325775146\n",
      "Validation: Epoch [22], Batch [173/938], Loss: 0.3645845055580139\n",
      "Validation: Epoch [22], Batch [174/938], Loss: 0.48224717378616333\n",
      "Validation: Epoch [22], Batch [175/938], Loss: 0.2768823802471161\n",
      "Validation: Epoch [22], Batch [176/938], Loss: 0.35720717906951904\n",
      "Validation: Epoch [22], Batch [177/938], Loss: 0.4478161633014679\n",
      "Validation: Epoch [22], Batch [178/938], Loss: 0.25146329402923584\n",
      "Validation: Epoch [22], Batch [179/938], Loss: 0.4435867667198181\n",
      "Validation: Epoch [22], Batch [180/938], Loss: 0.5348740816116333\n",
      "Validation: Epoch [22], Batch [181/938], Loss: 0.45988723635673523\n",
      "Validation: Epoch [22], Batch [182/938], Loss: 0.3983732759952545\n",
      "Validation: Epoch [22], Batch [183/938], Loss: 0.2486514151096344\n",
      "Validation: Epoch [22], Batch [184/938], Loss: 0.27552148699760437\n",
      "Validation: Epoch [22], Batch [185/938], Loss: 0.32750821113586426\n",
      "Validation: Epoch [22], Batch [186/938], Loss: 0.2494511604309082\n",
      "Validation: Epoch [22], Batch [187/938], Loss: 0.37306490540504456\n",
      "Validation: Epoch [22], Batch [188/938], Loss: 0.285203218460083\n",
      "Validation: Epoch [22], Batch [189/938], Loss: 0.39129552245140076\n",
      "Validation: Epoch [22], Batch [190/938], Loss: 0.27999764680862427\n",
      "Validation: Epoch [22], Batch [191/938], Loss: 0.3728044331073761\n",
      "Validation: Epoch [22], Batch [192/938], Loss: 0.25676366686820984\n",
      "Validation: Epoch [22], Batch [193/938], Loss: 0.44977933168411255\n",
      "Validation: Epoch [22], Batch [194/938], Loss: 0.2745363414287567\n",
      "Validation: Epoch [22], Batch [195/938], Loss: 0.4041154384613037\n",
      "Validation: Epoch [22], Batch [196/938], Loss: 0.43986424803733826\n",
      "Validation: Epoch [22], Batch [197/938], Loss: 0.3838651478290558\n",
      "Validation: Epoch [22], Batch [198/938], Loss: 0.24046340584754944\n",
      "Validation: Epoch [22], Batch [199/938], Loss: 0.47422415018081665\n",
      "Validation: Epoch [22], Batch [200/938], Loss: 0.3585315942764282\n",
      "Validation: Epoch [22], Batch [201/938], Loss: 0.45551133155822754\n",
      "Validation: Epoch [22], Batch [202/938], Loss: 0.2996048927307129\n",
      "Validation: Epoch [22], Batch [203/938], Loss: 0.37420982122421265\n",
      "Validation: Epoch [22], Batch [204/938], Loss: 0.4283703863620758\n",
      "Validation: Epoch [22], Batch [205/938], Loss: 0.29679548740386963\n",
      "Validation: Epoch [22], Batch [206/938], Loss: 0.5813974738121033\n",
      "Validation: Epoch [22], Batch [207/938], Loss: 0.3951081335544586\n",
      "Validation: Epoch [22], Batch [208/938], Loss: 0.587766706943512\n",
      "Validation: Epoch [22], Batch [209/938], Loss: 0.3803413510322571\n",
      "Validation: Epoch [22], Batch [210/938], Loss: 0.43165963888168335\n",
      "Validation: Epoch [22], Batch [211/938], Loss: 0.2998102307319641\n",
      "Validation: Epoch [22], Batch [212/938], Loss: 0.340284138917923\n",
      "Validation: Epoch [22], Batch [213/938], Loss: 0.3237432539463043\n",
      "Validation: Epoch [22], Batch [214/938], Loss: 0.48944342136383057\n",
      "Validation: Epoch [22], Batch [215/938], Loss: 0.22285892069339752\n",
      "Validation: Epoch [22], Batch [216/938], Loss: 0.6071407198905945\n",
      "Validation: Epoch [22], Batch [217/938], Loss: 0.34656989574432373\n",
      "Validation: Epoch [22], Batch [218/938], Loss: 0.5113049149513245\n",
      "Validation: Epoch [22], Batch [219/938], Loss: 0.7001707553863525\n",
      "Validation: Epoch [22], Batch [220/938], Loss: 0.42212170362472534\n",
      "Validation: Epoch [22], Batch [221/938], Loss: 0.21231114864349365\n",
      "Validation: Epoch [22], Batch [222/938], Loss: 0.3394845724105835\n",
      "Validation: Epoch [22], Batch [223/938], Loss: 0.31181105971336365\n",
      "Validation: Epoch [22], Batch [224/938], Loss: 0.2476317584514618\n",
      "Validation: Epoch [22], Batch [225/938], Loss: 0.3056330680847168\n",
      "Validation: Epoch [22], Batch [226/938], Loss: 0.36720818281173706\n",
      "Validation: Epoch [22], Batch [227/938], Loss: 0.5662898421287537\n",
      "Validation: Epoch [22], Batch [228/938], Loss: 0.24815811216831207\n",
      "Validation: Epoch [22], Batch [229/938], Loss: 0.6082634925842285\n",
      "Validation: Epoch [22], Batch [230/938], Loss: 0.3493881821632385\n",
      "Validation: Epoch [22], Batch [231/938], Loss: 0.4142969846725464\n",
      "Validation: Epoch [22], Batch [232/938], Loss: 0.4728770852088928\n",
      "Validation: Epoch [22], Batch [233/938], Loss: 0.3628324270248413\n",
      "Validation: Epoch [22], Batch [234/938], Loss: 0.3301047086715698\n",
      "Validation: Epoch [22], Batch [235/938], Loss: 0.3406541347503662\n",
      "Validation: Epoch [22], Batch [236/938], Loss: 0.46685343980789185\n",
      "Validation: Epoch [22], Batch [237/938], Loss: 0.4025585651397705\n",
      "Validation: Epoch [22], Batch [238/938], Loss: 0.2975594401359558\n",
      "Validation: Epoch [22], Batch [239/938], Loss: 0.3988317549228668\n",
      "Validation: Epoch [22], Batch [240/938], Loss: 0.2301023304462433\n",
      "Validation: Epoch [22], Batch [241/938], Loss: 0.5737123489379883\n",
      "Validation: Epoch [22], Batch [242/938], Loss: 0.45869508385658264\n",
      "Validation: Epoch [22], Batch [243/938], Loss: 0.36114972829818726\n",
      "Validation: Epoch [22], Batch [244/938], Loss: 0.3270801901817322\n",
      "Validation: Epoch [22], Batch [245/938], Loss: 0.3983158469200134\n",
      "Validation: Epoch [22], Batch [246/938], Loss: 0.45874732732772827\n",
      "Validation: Epoch [22], Batch [247/938], Loss: 0.2579636871814728\n",
      "Validation: Epoch [22], Batch [248/938], Loss: 0.6363258957862854\n",
      "Validation: Epoch [22], Batch [249/938], Loss: 0.40732839703559875\n",
      "Validation: Epoch [22], Batch [250/938], Loss: 0.42515239119529724\n",
      "Validation: Epoch [22], Batch [251/938], Loss: 0.40156233310699463\n",
      "Validation: Epoch [22], Batch [252/938], Loss: 0.4145391583442688\n",
      "Validation: Epoch [22], Batch [253/938], Loss: 0.39707058668136597\n",
      "Validation: Epoch [22], Batch [254/938], Loss: 0.3248768746852875\n",
      "Validation: Epoch [22], Batch [255/938], Loss: 0.4506258964538574\n",
      "Validation: Epoch [22], Batch [256/938], Loss: 0.4105268716812134\n",
      "Validation: Epoch [22], Batch [257/938], Loss: 0.3150829076766968\n",
      "Validation: Epoch [22], Batch [258/938], Loss: 0.5801364183425903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [259/938], Loss: 0.330727756023407\n",
      "Validation: Epoch [22], Batch [260/938], Loss: 0.19554933905601501\n",
      "Validation: Epoch [22], Batch [261/938], Loss: 0.26735299825668335\n",
      "Validation: Epoch [22], Batch [262/938], Loss: 0.502166211605072\n",
      "Validation: Epoch [22], Batch [263/938], Loss: 0.27501562237739563\n",
      "Validation: Epoch [22], Batch [264/938], Loss: 0.3095989227294922\n",
      "Validation: Epoch [22], Batch [265/938], Loss: 0.3477776348590851\n",
      "Validation: Epoch [22], Batch [266/938], Loss: 0.2766243815422058\n",
      "Validation: Epoch [22], Batch [267/938], Loss: 0.432816743850708\n",
      "Validation: Epoch [22], Batch [268/938], Loss: 0.45908015966415405\n",
      "Validation: Epoch [22], Batch [269/938], Loss: 0.4026159644126892\n",
      "Validation: Epoch [22], Batch [270/938], Loss: 0.19805973768234253\n",
      "Validation: Epoch [22], Batch [271/938], Loss: 0.4130200445652008\n",
      "Validation: Epoch [22], Batch [272/938], Loss: 0.31676748394966125\n",
      "Validation: Epoch [22], Batch [273/938], Loss: 0.41675859689712524\n",
      "Validation: Epoch [22], Batch [274/938], Loss: 0.3763255178928375\n",
      "Validation: Epoch [22], Batch [275/938], Loss: 0.3230383098125458\n",
      "Validation: Epoch [22], Batch [276/938], Loss: 0.313696950674057\n",
      "Validation: Epoch [22], Batch [277/938], Loss: 0.5965872406959534\n",
      "Validation: Epoch [22], Batch [278/938], Loss: 0.524978756904602\n",
      "Validation: Epoch [22], Batch [279/938], Loss: 0.4139772355556488\n",
      "Validation: Epoch [22], Batch [280/938], Loss: 0.24862892925739288\n",
      "Validation: Epoch [22], Batch [281/938], Loss: 0.3166126310825348\n",
      "Validation: Epoch [22], Batch [282/938], Loss: 0.34518787264823914\n",
      "Validation: Epoch [22], Batch [283/938], Loss: 0.4400181770324707\n",
      "Validation: Epoch [22], Batch [284/938], Loss: 0.5715628266334534\n",
      "Validation: Epoch [22], Batch [285/938], Loss: 0.4380573034286499\n",
      "Validation: Epoch [22], Batch [286/938], Loss: 0.3363322615623474\n",
      "Validation: Epoch [22], Batch [287/938], Loss: 0.32135093212127686\n",
      "Validation: Epoch [22], Batch [288/938], Loss: 0.4413895010948181\n",
      "Validation: Epoch [22], Batch [289/938], Loss: 0.38174304366111755\n",
      "Validation: Epoch [22], Batch [290/938], Loss: 0.28117915987968445\n",
      "Validation: Epoch [22], Batch [291/938], Loss: 0.2971135973930359\n",
      "Validation: Epoch [22], Batch [292/938], Loss: 0.47837793827056885\n",
      "Validation: Epoch [22], Batch [293/938], Loss: 0.6404756903648376\n",
      "Validation: Epoch [22], Batch [294/938], Loss: 0.45670077204704285\n",
      "Validation: Epoch [22], Batch [295/938], Loss: 0.500446081161499\n",
      "Validation: Epoch [22], Batch [296/938], Loss: 0.39078789949417114\n",
      "Validation: Epoch [22], Batch [297/938], Loss: 0.3940414786338806\n",
      "Validation: Epoch [22], Batch [298/938], Loss: 0.4928874373435974\n",
      "Validation: Epoch [22], Batch [299/938], Loss: 0.34448421001434326\n",
      "Validation: Epoch [22], Batch [300/938], Loss: 0.4480541944503784\n",
      "Validation: Epoch [22], Batch [301/938], Loss: 0.30908191204071045\n",
      "Validation: Epoch [22], Batch [302/938], Loss: 0.39191433787345886\n",
      "Validation: Epoch [22], Batch [303/938], Loss: 0.5477117300033569\n",
      "Validation: Epoch [22], Batch [304/938], Loss: 0.43956631422042847\n",
      "Validation: Epoch [22], Batch [305/938], Loss: 0.4559231400489807\n",
      "Validation: Epoch [22], Batch [306/938], Loss: 0.4202057719230652\n",
      "Validation: Epoch [22], Batch [307/938], Loss: 0.2890641391277313\n",
      "Validation: Epoch [22], Batch [308/938], Loss: 0.30349913239479065\n",
      "Validation: Epoch [22], Batch [309/938], Loss: 0.4442121982574463\n",
      "Validation: Epoch [22], Batch [310/938], Loss: 0.3455314338207245\n",
      "Validation: Epoch [22], Batch [311/938], Loss: 0.3720530569553375\n",
      "Validation: Epoch [22], Batch [312/938], Loss: 0.35404402017593384\n",
      "Validation: Epoch [22], Batch [313/938], Loss: 0.5592215061187744\n",
      "Validation: Epoch [22], Batch [314/938], Loss: 0.3382599353790283\n",
      "Validation: Epoch [22], Batch [315/938], Loss: 0.3355061411857605\n",
      "Validation: Epoch [22], Batch [316/938], Loss: 0.4183960258960724\n",
      "Validation: Epoch [22], Batch [317/938], Loss: 0.22596964240074158\n",
      "Validation: Epoch [22], Batch [318/938], Loss: 0.5238291025161743\n",
      "Validation: Epoch [22], Batch [319/938], Loss: 0.3330695629119873\n",
      "Validation: Epoch [22], Batch [320/938], Loss: 0.39480504393577576\n",
      "Validation: Epoch [22], Batch [321/938], Loss: 0.33343958854675293\n",
      "Validation: Epoch [22], Batch [322/938], Loss: 0.32789814472198486\n",
      "Validation: Epoch [22], Batch [323/938], Loss: 0.468894898891449\n",
      "Validation: Epoch [22], Batch [324/938], Loss: 0.4760567247867584\n",
      "Validation: Epoch [22], Batch [325/938], Loss: 0.18810993432998657\n",
      "Validation: Epoch [22], Batch [326/938], Loss: 0.43854862451553345\n",
      "Validation: Epoch [22], Batch [327/938], Loss: 0.4435635507106781\n",
      "Validation: Epoch [22], Batch [328/938], Loss: 0.4001231789588928\n",
      "Validation: Epoch [22], Batch [329/938], Loss: 0.36503541469573975\n",
      "Validation: Epoch [22], Batch [330/938], Loss: 0.387864351272583\n",
      "Validation: Epoch [22], Batch [331/938], Loss: 0.3659452199935913\n",
      "Validation: Epoch [22], Batch [332/938], Loss: 0.5434930324554443\n",
      "Validation: Epoch [22], Batch [333/938], Loss: 0.4072200655937195\n",
      "Validation: Epoch [22], Batch [334/938], Loss: 0.3432976305484772\n",
      "Validation: Epoch [22], Batch [335/938], Loss: 0.3223581314086914\n",
      "Validation: Epoch [22], Batch [336/938], Loss: 0.3133850395679474\n",
      "Validation: Epoch [22], Batch [337/938], Loss: 0.31116601824760437\n",
      "Validation: Epoch [22], Batch [338/938], Loss: 0.4933907389640808\n",
      "Validation: Epoch [22], Batch [339/938], Loss: 0.3820708692073822\n",
      "Validation: Epoch [22], Batch [340/938], Loss: 0.3002847731113434\n",
      "Validation: Epoch [22], Batch [341/938], Loss: 0.4874122142791748\n",
      "Validation: Epoch [22], Batch [342/938], Loss: 0.43211793899536133\n",
      "Validation: Epoch [22], Batch [343/938], Loss: 0.44695568084716797\n",
      "Validation: Epoch [22], Batch [344/938], Loss: 0.3297708034515381\n",
      "Validation: Epoch [22], Batch [345/938], Loss: 0.34635549783706665\n",
      "Validation: Epoch [22], Batch [346/938], Loss: 0.4117264151573181\n",
      "Validation: Epoch [22], Batch [347/938], Loss: 0.43048518896102905\n",
      "Validation: Epoch [22], Batch [348/938], Loss: 0.5756254196166992\n",
      "Validation: Epoch [22], Batch [349/938], Loss: 0.37139517068862915\n",
      "Validation: Epoch [22], Batch [350/938], Loss: 0.45156729221343994\n",
      "Validation: Epoch [22], Batch [351/938], Loss: 0.42816296219825745\n",
      "Validation: Epoch [22], Batch [352/938], Loss: 0.2558322846889496\n",
      "Validation: Epoch [22], Batch [353/938], Loss: 0.2178489714860916\n",
      "Validation: Epoch [22], Batch [354/938], Loss: 0.42841440439224243\n",
      "Validation: Epoch [22], Batch [355/938], Loss: 0.35148173570632935\n",
      "Validation: Epoch [22], Batch [356/938], Loss: 0.5291221141815186\n",
      "Validation: Epoch [22], Batch [357/938], Loss: 0.29468458890914917\n",
      "Validation: Epoch [22], Batch [358/938], Loss: 0.3157455325126648\n",
      "Validation: Epoch [22], Batch [359/938], Loss: 0.44695818424224854\n",
      "Validation: Epoch [22], Batch [360/938], Loss: 0.508747398853302\n",
      "Validation: Epoch [22], Batch [361/938], Loss: 0.3824244737625122\n",
      "Validation: Epoch [22], Batch [362/938], Loss: 0.38298317790031433\n",
      "Validation: Epoch [22], Batch [363/938], Loss: 0.26714837551116943\n",
      "Validation: Epoch [22], Batch [364/938], Loss: 0.396833211183548\n",
      "Validation: Epoch [22], Batch [365/938], Loss: 0.32215380668640137\n",
      "Validation: Epoch [22], Batch [366/938], Loss: 0.2904515266418457\n",
      "Validation: Epoch [22], Batch [367/938], Loss: 0.3342708349227905\n",
      "Validation: Epoch [22], Batch [368/938], Loss: 0.5116114616394043\n",
      "Validation: Epoch [22], Batch [369/938], Loss: 0.3368220329284668\n",
      "Validation: Epoch [22], Batch [370/938], Loss: 0.3932170867919922\n",
      "Validation: Epoch [22], Batch [371/938], Loss: 0.35943204164505005\n",
      "Validation: Epoch [22], Batch [372/938], Loss: 0.3371538817882538\n",
      "Validation: Epoch [22], Batch [373/938], Loss: 0.3906860947608948\n",
      "Validation: Epoch [22], Batch [374/938], Loss: 0.222234308719635\n",
      "Validation: Epoch [22], Batch [375/938], Loss: 0.3370874226093292\n",
      "Validation: Epoch [22], Batch [376/938], Loss: 0.4143106937408447\n",
      "Validation: Epoch [22], Batch [377/938], Loss: 0.3742969036102295\n",
      "Validation: Epoch [22], Batch [378/938], Loss: 0.3420217037200928\n",
      "Validation: Epoch [22], Batch [379/938], Loss: 0.3555673658847809\n",
      "Validation: Epoch [22], Batch [380/938], Loss: 0.38777416944503784\n",
      "Validation: Epoch [22], Batch [381/938], Loss: 0.4484332799911499\n",
      "Validation: Epoch [22], Batch [382/938], Loss: 0.30605068802833557\n",
      "Validation: Epoch [22], Batch [383/938], Loss: 0.2856438159942627\n",
      "Validation: Epoch [22], Batch [384/938], Loss: 0.4161149263381958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [385/938], Loss: 0.32512250542640686\n",
      "Validation: Epoch [22], Batch [386/938], Loss: 0.44942396879196167\n",
      "Validation: Epoch [22], Batch [387/938], Loss: 0.2454107105731964\n",
      "Validation: Epoch [22], Batch [388/938], Loss: 0.3442464768886566\n",
      "Validation: Epoch [22], Batch [389/938], Loss: 0.31297850608825684\n",
      "Validation: Epoch [22], Batch [390/938], Loss: 0.36164939403533936\n",
      "Validation: Epoch [22], Batch [391/938], Loss: 0.29168036580085754\n",
      "Validation: Epoch [22], Batch [392/938], Loss: 0.3402210474014282\n",
      "Validation: Epoch [22], Batch [393/938], Loss: 0.40788933634757996\n",
      "Validation: Epoch [22], Batch [394/938], Loss: 0.36920979619026184\n",
      "Validation: Epoch [22], Batch [395/938], Loss: 0.3109586834907532\n",
      "Validation: Epoch [22], Batch [396/938], Loss: 0.34233951568603516\n",
      "Validation: Epoch [22], Batch [397/938], Loss: 0.31916314363479614\n",
      "Validation: Epoch [22], Batch [398/938], Loss: 0.28634437918663025\n",
      "Validation: Epoch [22], Batch [399/938], Loss: 0.5198441743850708\n",
      "Validation: Epoch [22], Batch [400/938], Loss: 0.41877037286758423\n",
      "Validation: Epoch [22], Batch [401/938], Loss: 0.39663809537887573\n",
      "Validation: Epoch [22], Batch [402/938], Loss: 0.3239481747150421\n",
      "Validation: Epoch [22], Batch [403/938], Loss: 0.2529943287372589\n",
      "Validation: Epoch [22], Batch [404/938], Loss: 0.30699649453163147\n",
      "Validation: Epoch [22], Batch [405/938], Loss: 0.45370781421661377\n",
      "Validation: Epoch [22], Batch [406/938], Loss: 0.25609827041625977\n",
      "Validation: Epoch [22], Batch [407/938], Loss: 0.46854573488235474\n",
      "Validation: Epoch [22], Batch [408/938], Loss: 0.3299296498298645\n",
      "Validation: Epoch [22], Batch [409/938], Loss: 0.5210632085800171\n",
      "Validation: Epoch [22], Batch [410/938], Loss: 0.359927237033844\n",
      "Validation: Epoch [22], Batch [411/938], Loss: 0.4087265133857727\n",
      "Validation: Epoch [22], Batch [412/938], Loss: 0.4796515703201294\n",
      "Validation: Epoch [22], Batch [413/938], Loss: 0.42894506454467773\n",
      "Validation: Epoch [22], Batch [414/938], Loss: 0.21688368916511536\n",
      "Validation: Epoch [22], Batch [415/938], Loss: 0.2938539981842041\n",
      "Validation: Epoch [22], Batch [416/938], Loss: 0.4091351628303528\n",
      "Validation: Epoch [22], Batch [417/938], Loss: 0.5401204824447632\n",
      "Validation: Epoch [22], Batch [418/938], Loss: 0.42513224482536316\n",
      "Validation: Epoch [22], Batch [419/938], Loss: 0.4331788122653961\n",
      "Validation: Epoch [22], Batch [420/938], Loss: 0.3989471197128296\n",
      "Validation: Epoch [22], Batch [421/938], Loss: 0.5034202337265015\n",
      "Validation: Epoch [22], Batch [422/938], Loss: 0.43192580342292786\n",
      "Validation: Epoch [22], Batch [423/938], Loss: 0.2515087127685547\n",
      "Validation: Epoch [22], Batch [424/938], Loss: 0.5191878080368042\n",
      "Validation: Epoch [22], Batch [425/938], Loss: 0.25923553109169006\n",
      "Validation: Epoch [22], Batch [426/938], Loss: 0.3968692719936371\n",
      "Validation: Epoch [22], Batch [427/938], Loss: 0.3962056040763855\n",
      "Validation: Epoch [22], Batch [428/938], Loss: 0.3404833674430847\n",
      "Validation: Epoch [22], Batch [429/938], Loss: 0.4453663229942322\n",
      "Validation: Epoch [22], Batch [430/938], Loss: 0.33420294523239136\n",
      "Validation: Epoch [22], Batch [431/938], Loss: 0.29498180747032166\n",
      "Validation: Epoch [22], Batch [432/938], Loss: 0.38916391134262085\n",
      "Validation: Epoch [22], Batch [433/938], Loss: 0.2148718386888504\n",
      "Validation: Epoch [22], Batch [434/938], Loss: 0.24182796478271484\n",
      "Validation: Epoch [22], Batch [435/938], Loss: 0.3160284161567688\n",
      "Validation: Epoch [22], Batch [436/938], Loss: 0.3224916160106659\n",
      "Validation: Epoch [22], Batch [437/938], Loss: 0.41214874386787415\n",
      "Validation: Epoch [22], Batch [438/938], Loss: 0.3305886685848236\n",
      "Validation: Epoch [22], Batch [439/938], Loss: 0.38780224323272705\n",
      "Validation: Epoch [22], Batch [440/938], Loss: 0.360114723443985\n",
      "Validation: Epoch [22], Batch [441/938], Loss: 0.3041399121284485\n",
      "Validation: Epoch [22], Batch [442/938], Loss: 0.365139901638031\n",
      "Validation: Epoch [22], Batch [443/938], Loss: 0.3211774230003357\n",
      "Validation: Epoch [22], Batch [444/938], Loss: 0.5261285305023193\n",
      "Validation: Epoch [22], Batch [445/938], Loss: 0.25579357147216797\n",
      "Validation: Epoch [22], Batch [446/938], Loss: 0.321899950504303\n",
      "Validation: Epoch [22], Batch [447/938], Loss: 0.25312602519989014\n",
      "Validation: Epoch [22], Batch [448/938], Loss: 0.30895906686782837\n",
      "Validation: Epoch [22], Batch [449/938], Loss: 0.2708844542503357\n",
      "Validation: Epoch [22], Batch [450/938], Loss: 0.35794442892074585\n",
      "Validation: Epoch [22], Batch [451/938], Loss: 0.2781050205230713\n",
      "Validation: Epoch [22], Batch [452/938], Loss: 0.33143556118011475\n",
      "Validation: Epoch [22], Batch [453/938], Loss: 0.40050745010375977\n",
      "Validation: Epoch [22], Batch [454/938], Loss: 0.3709378242492676\n",
      "Validation: Epoch [22], Batch [455/938], Loss: 0.3637131154537201\n",
      "Validation: Epoch [22], Batch [456/938], Loss: 0.2908061444759369\n",
      "Validation: Epoch [22], Batch [457/938], Loss: 0.40424397587776184\n",
      "Validation: Epoch [22], Batch [458/938], Loss: 0.3137509524822235\n",
      "Validation: Epoch [22], Batch [459/938], Loss: 0.4903949201107025\n",
      "Validation: Epoch [22], Batch [460/938], Loss: 0.4401526153087616\n",
      "Validation: Epoch [22], Batch [461/938], Loss: 0.32669878005981445\n",
      "Validation: Epoch [22], Batch [462/938], Loss: 0.5476357936859131\n",
      "Validation: Epoch [22], Batch [463/938], Loss: 0.3986409306526184\n",
      "Validation: Epoch [22], Batch [464/938], Loss: 0.44668328762054443\n",
      "Validation: Epoch [22], Batch [465/938], Loss: 0.5090618133544922\n",
      "Validation: Epoch [22], Batch [466/938], Loss: 0.37470799684524536\n",
      "Validation: Epoch [22], Batch [467/938], Loss: 0.5581749081611633\n",
      "Validation: Epoch [22], Batch [468/938], Loss: 0.46818864345550537\n",
      "Validation: Epoch [22], Batch [469/938], Loss: 0.44688883423805237\n",
      "Validation: Epoch [22], Batch [470/938], Loss: 0.3654680550098419\n",
      "Validation: Epoch [22], Batch [471/938], Loss: 0.341433048248291\n",
      "Validation: Epoch [22], Batch [472/938], Loss: 0.417184054851532\n",
      "Validation: Epoch [22], Batch [473/938], Loss: 0.3916184902191162\n",
      "Validation: Epoch [22], Batch [474/938], Loss: 0.3983249366283417\n",
      "Validation: Epoch [22], Batch [475/938], Loss: 0.3875965476036072\n",
      "Validation: Epoch [22], Batch [476/938], Loss: 0.5388996601104736\n",
      "Validation: Epoch [22], Batch [477/938], Loss: 0.2447989284992218\n",
      "Validation: Epoch [22], Batch [478/938], Loss: 0.6125092506408691\n",
      "Validation: Epoch [22], Batch [479/938], Loss: 0.22335952520370483\n",
      "Validation: Epoch [22], Batch [480/938], Loss: 0.552217423915863\n",
      "Validation: Epoch [22], Batch [481/938], Loss: 0.42689400911331177\n",
      "Validation: Epoch [22], Batch [482/938], Loss: 0.4621899127960205\n",
      "Validation: Epoch [22], Batch [483/938], Loss: 0.29215532541275024\n",
      "Validation: Epoch [22], Batch [484/938], Loss: 0.5288196206092834\n",
      "Validation: Epoch [22], Batch [485/938], Loss: 0.3836742341518402\n",
      "Validation: Epoch [22], Batch [486/938], Loss: 0.6328827142715454\n",
      "Validation: Epoch [22], Batch [487/938], Loss: 0.28181660175323486\n",
      "Validation: Epoch [22], Batch [488/938], Loss: 0.5247307419776917\n",
      "Validation: Epoch [22], Batch [489/938], Loss: 0.47758418321609497\n",
      "Validation: Epoch [22], Batch [490/938], Loss: 0.40278926491737366\n",
      "Validation: Epoch [22], Batch [491/938], Loss: 0.25686144828796387\n",
      "Validation: Epoch [22], Batch [492/938], Loss: 0.4174988865852356\n",
      "Validation: Epoch [22], Batch [493/938], Loss: 0.36972784996032715\n",
      "Validation: Epoch [22], Batch [494/938], Loss: 0.3325064778327942\n",
      "Validation: Epoch [22], Batch [495/938], Loss: 0.3300172686576843\n",
      "Validation: Epoch [22], Batch [496/938], Loss: 0.5699217319488525\n",
      "Validation: Epoch [22], Batch [497/938], Loss: 0.47313255071640015\n",
      "Validation: Epoch [22], Batch [498/938], Loss: 0.46544015407562256\n",
      "Validation: Epoch [22], Batch [499/938], Loss: 0.25622913241386414\n",
      "Validation: Epoch [22], Batch [500/938], Loss: 0.43093135952949524\n",
      "Validation: Epoch [22], Batch [501/938], Loss: 0.6028027534484863\n",
      "Validation: Epoch [22], Batch [502/938], Loss: 0.41317546367645264\n",
      "Validation: Epoch [22], Batch [503/938], Loss: 0.29255324602127075\n",
      "Validation: Epoch [22], Batch [504/938], Loss: 0.3521299958229065\n",
      "Validation: Epoch [22], Batch [505/938], Loss: 0.3697759509086609\n",
      "Validation: Epoch [22], Batch [506/938], Loss: 0.29284822940826416\n",
      "Validation: Epoch [22], Batch [507/938], Loss: 0.2483350932598114\n",
      "Validation: Epoch [22], Batch [508/938], Loss: 0.2824864685535431\n",
      "Validation: Epoch [22], Batch [509/938], Loss: 0.32764920592308044\n",
      "Validation: Epoch [22], Batch [510/938], Loss: 0.34022435545921326\n",
      "Validation: Epoch [22], Batch [511/938], Loss: 0.24864071607589722\n",
      "Validation: Epoch [22], Batch [512/938], Loss: 0.39190664887428284\n",
      "Validation: Epoch [22], Batch [513/938], Loss: 0.47351083159446716\n",
      "Validation: Epoch [22], Batch [514/938], Loss: 0.2712210416793823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [515/938], Loss: 0.48573681712150574\n",
      "Validation: Epoch [22], Batch [516/938], Loss: 0.24935582280158997\n",
      "Validation: Epoch [22], Batch [517/938], Loss: 0.4269651770591736\n",
      "Validation: Epoch [22], Batch [518/938], Loss: 0.28446581959724426\n",
      "Validation: Epoch [22], Batch [519/938], Loss: 0.34218570590019226\n",
      "Validation: Epoch [22], Batch [520/938], Loss: 0.2825368344783783\n",
      "Validation: Epoch [22], Batch [521/938], Loss: 0.4181208610534668\n",
      "Validation: Epoch [22], Batch [522/938], Loss: 0.414286345243454\n",
      "Validation: Epoch [22], Batch [523/938], Loss: 0.3632385730743408\n",
      "Validation: Epoch [22], Batch [524/938], Loss: 0.5379080772399902\n",
      "Validation: Epoch [22], Batch [525/938], Loss: 0.4791857600212097\n",
      "Validation: Epoch [22], Batch [526/938], Loss: 0.44106560945510864\n",
      "Validation: Epoch [22], Batch [527/938], Loss: 0.36205780506134033\n",
      "Validation: Epoch [22], Batch [528/938], Loss: 0.26345279812812805\n",
      "Validation: Epoch [22], Batch [529/938], Loss: 0.3357316255569458\n",
      "Validation: Epoch [22], Batch [530/938], Loss: 0.3909622132778168\n",
      "Validation: Epoch [22], Batch [531/938], Loss: 0.3770614564418793\n",
      "Validation: Epoch [22], Batch [532/938], Loss: 0.31726542115211487\n",
      "Validation: Epoch [22], Batch [533/938], Loss: 0.29168805480003357\n",
      "Validation: Epoch [22], Batch [534/938], Loss: 0.2976534366607666\n",
      "Validation: Epoch [22], Batch [535/938], Loss: 0.2119528353214264\n",
      "Validation: Epoch [22], Batch [536/938], Loss: 0.3789839744567871\n",
      "Validation: Epoch [22], Batch [537/938], Loss: 0.49371829628944397\n",
      "Validation: Epoch [22], Batch [538/938], Loss: 0.32592734694480896\n",
      "Validation: Epoch [22], Batch [539/938], Loss: 0.37154367566108704\n",
      "Validation: Epoch [22], Batch [540/938], Loss: 0.28867948055267334\n",
      "Validation: Epoch [22], Batch [541/938], Loss: 0.3902134895324707\n",
      "Validation: Epoch [22], Batch [542/938], Loss: 0.35681310296058655\n",
      "Validation: Epoch [22], Batch [543/938], Loss: 0.329971045255661\n",
      "Validation: Epoch [22], Batch [544/938], Loss: 0.35302573442459106\n",
      "Validation: Epoch [22], Batch [545/938], Loss: 0.2909756898880005\n",
      "Validation: Epoch [22], Batch [546/938], Loss: 0.5049393177032471\n",
      "Validation: Epoch [22], Batch [547/938], Loss: 0.4492295980453491\n",
      "Validation: Epoch [22], Batch [548/938], Loss: 0.4335278868675232\n",
      "Validation: Epoch [22], Batch [549/938], Loss: 0.334214985370636\n",
      "Validation: Epoch [22], Batch [550/938], Loss: 0.5086094737052917\n",
      "Validation: Epoch [22], Batch [551/938], Loss: 0.4241352081298828\n",
      "Validation: Epoch [22], Batch [552/938], Loss: 0.2553156018257141\n",
      "Validation: Epoch [22], Batch [553/938], Loss: 0.2920739948749542\n",
      "Validation: Epoch [22], Batch [554/938], Loss: 0.44851160049438477\n",
      "Validation: Epoch [22], Batch [555/938], Loss: 0.5060529708862305\n",
      "Validation: Epoch [22], Batch [556/938], Loss: 0.42542192339897156\n",
      "Validation: Epoch [22], Batch [557/938], Loss: 0.4370693564414978\n",
      "Validation: Epoch [22], Batch [558/938], Loss: 0.27909913659095764\n",
      "Validation: Epoch [22], Batch [559/938], Loss: 0.3870413303375244\n",
      "Validation: Epoch [22], Batch [560/938], Loss: 0.3401910662651062\n",
      "Validation: Epoch [22], Batch [561/938], Loss: 0.3778432607650757\n",
      "Validation: Epoch [22], Batch [562/938], Loss: 0.5958540439605713\n",
      "Validation: Epoch [22], Batch [563/938], Loss: 0.2730166018009186\n",
      "Validation: Epoch [22], Batch [564/938], Loss: 0.3965415954589844\n",
      "Validation: Epoch [22], Batch [565/938], Loss: 0.29189905524253845\n",
      "Validation: Epoch [22], Batch [566/938], Loss: 0.2711901366710663\n",
      "Validation: Epoch [22], Batch [567/938], Loss: 0.38919946551322937\n",
      "Validation: Epoch [22], Batch [568/938], Loss: 0.4868854880332947\n",
      "Validation: Epoch [22], Batch [569/938], Loss: 0.22257152199745178\n",
      "Validation: Epoch [22], Batch [570/938], Loss: 0.315093457698822\n",
      "Validation: Epoch [22], Batch [571/938], Loss: 0.32168322801589966\n",
      "Validation: Epoch [22], Batch [572/938], Loss: 0.4850127696990967\n",
      "Validation: Epoch [22], Batch [573/938], Loss: 0.3326647877693176\n",
      "Validation: Epoch [22], Batch [574/938], Loss: 0.4159225821495056\n",
      "Validation: Epoch [22], Batch [575/938], Loss: 0.5582154989242554\n",
      "Validation: Epoch [22], Batch [576/938], Loss: 0.448407918214798\n",
      "Validation: Epoch [22], Batch [577/938], Loss: 0.36059239506721497\n",
      "Validation: Epoch [22], Batch [578/938], Loss: 0.35548239946365356\n",
      "Validation: Epoch [22], Batch [579/938], Loss: 0.35935086011886597\n",
      "Validation: Epoch [22], Batch [580/938], Loss: 0.27547794580459595\n",
      "Validation: Epoch [22], Batch [581/938], Loss: 0.4341181516647339\n",
      "Validation: Epoch [22], Batch [582/938], Loss: 0.32680249214172363\n",
      "Validation: Epoch [22], Batch [583/938], Loss: 0.24611690640449524\n",
      "Validation: Epoch [22], Batch [584/938], Loss: 0.7748061418533325\n",
      "Validation: Epoch [22], Batch [585/938], Loss: 0.38010114431381226\n",
      "Validation: Epoch [22], Batch [586/938], Loss: 0.4652736186981201\n",
      "Validation: Epoch [22], Batch [587/938], Loss: 0.20297333598136902\n",
      "Validation: Epoch [22], Batch [588/938], Loss: 0.47029757499694824\n",
      "Validation: Epoch [22], Batch [589/938], Loss: 0.34091445803642273\n",
      "Validation: Epoch [22], Batch [590/938], Loss: 0.45469391345977783\n",
      "Validation: Epoch [22], Batch [591/938], Loss: 0.6096230149269104\n",
      "Validation: Epoch [22], Batch [592/938], Loss: 0.33925822377204895\n",
      "Validation: Epoch [22], Batch [593/938], Loss: 0.545721173286438\n",
      "Validation: Epoch [22], Batch [594/938], Loss: 0.3324839174747467\n",
      "Validation: Epoch [22], Batch [595/938], Loss: 0.466160386800766\n",
      "Validation: Epoch [22], Batch [596/938], Loss: 0.35569286346435547\n",
      "Validation: Epoch [22], Batch [597/938], Loss: 0.30023759603500366\n",
      "Validation: Epoch [22], Batch [598/938], Loss: 0.394195020198822\n",
      "Validation: Epoch [22], Batch [599/938], Loss: 0.5355600714683533\n",
      "Validation: Epoch [22], Batch [600/938], Loss: 0.5831979513168335\n",
      "Validation: Epoch [22], Batch [601/938], Loss: 0.4347239136695862\n",
      "Validation: Epoch [22], Batch [602/938], Loss: 0.5670577883720398\n",
      "Validation: Epoch [22], Batch [603/938], Loss: 0.27077341079711914\n",
      "Validation: Epoch [22], Batch [604/938], Loss: 0.3173569142818451\n",
      "Validation: Epoch [22], Batch [605/938], Loss: 0.418412446975708\n",
      "Validation: Epoch [22], Batch [606/938], Loss: 0.47801685333251953\n",
      "Validation: Epoch [22], Batch [607/938], Loss: 0.3070746064186096\n",
      "Validation: Epoch [22], Batch [608/938], Loss: 0.4508587121963501\n",
      "Validation: Epoch [22], Batch [609/938], Loss: 0.28263068199157715\n",
      "Validation: Epoch [22], Batch [610/938], Loss: 0.36548668146133423\n",
      "Validation: Epoch [22], Batch [611/938], Loss: 0.23306138813495636\n",
      "Validation: Epoch [22], Batch [612/938], Loss: 0.35283225774765015\n",
      "Validation: Epoch [22], Batch [613/938], Loss: 0.33234697580337524\n",
      "Validation: Epoch [22], Batch [614/938], Loss: 0.3786373734474182\n",
      "Validation: Epoch [22], Batch [615/938], Loss: 0.4869123697280884\n",
      "Validation: Epoch [22], Batch [616/938], Loss: 0.2859421670436859\n",
      "Validation: Epoch [22], Batch [617/938], Loss: 0.4149966239929199\n",
      "Validation: Epoch [22], Batch [618/938], Loss: 0.28501302003860474\n",
      "Validation: Epoch [22], Batch [619/938], Loss: 0.31492894887924194\n",
      "Validation: Epoch [22], Batch [620/938], Loss: 0.48440980911254883\n",
      "Validation: Epoch [22], Batch [621/938], Loss: 0.41957300901412964\n",
      "Validation: Epoch [22], Batch [622/938], Loss: 0.43420469760894775\n",
      "Validation: Epoch [22], Batch [623/938], Loss: 0.34359508752822876\n",
      "Validation: Epoch [22], Batch [624/938], Loss: 0.3838624954223633\n",
      "Validation: Epoch [22], Batch [625/938], Loss: 0.22642715275287628\n",
      "Validation: Epoch [22], Batch [626/938], Loss: 0.2943492531776428\n",
      "Validation: Epoch [22], Batch [627/938], Loss: 0.3575645685195923\n",
      "Validation: Epoch [22], Batch [628/938], Loss: 0.7381108999252319\n",
      "Validation: Epoch [22], Batch [629/938], Loss: 0.6744417548179626\n",
      "Validation: Epoch [22], Batch [630/938], Loss: 0.4288043975830078\n",
      "Validation: Epoch [22], Batch [631/938], Loss: 0.40913429856300354\n",
      "Validation: Epoch [22], Batch [632/938], Loss: 0.44496822357177734\n",
      "Validation: Epoch [22], Batch [633/938], Loss: 0.20434147119522095\n",
      "Validation: Epoch [22], Batch [634/938], Loss: 0.47531285881996155\n",
      "Validation: Epoch [22], Batch [635/938], Loss: 0.310516893863678\n",
      "Validation: Epoch [22], Batch [636/938], Loss: 0.35642892122268677\n",
      "Validation: Epoch [22], Batch [637/938], Loss: 0.3271991014480591\n",
      "Validation: Epoch [22], Batch [638/938], Loss: 0.3937079906463623\n",
      "Validation: Epoch [22], Batch [639/938], Loss: 0.5734498500823975\n",
      "Validation: Epoch [22], Batch [640/938], Loss: 0.5401461124420166\n",
      "Validation: Epoch [22], Batch [641/938], Loss: 0.2843558192253113\n",
      "Validation: Epoch [22], Batch [642/938], Loss: 0.3433608114719391\n",
      "Validation: Epoch [22], Batch [643/938], Loss: 0.5512422323226929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [644/938], Loss: 0.3741859197616577\n",
      "Validation: Epoch [22], Batch [645/938], Loss: 0.3255641460418701\n",
      "Validation: Epoch [22], Batch [646/938], Loss: 0.48831889033317566\n",
      "Validation: Epoch [22], Batch [647/938], Loss: 0.37984198331832886\n",
      "Validation: Epoch [22], Batch [648/938], Loss: 0.19962887465953827\n",
      "Validation: Epoch [22], Batch [649/938], Loss: 0.27529627084732056\n",
      "Validation: Epoch [22], Batch [650/938], Loss: 0.3359662890434265\n",
      "Validation: Epoch [22], Batch [651/938], Loss: 0.2824077010154724\n",
      "Validation: Epoch [22], Batch [652/938], Loss: 0.294038325548172\n",
      "Validation: Epoch [22], Batch [653/938], Loss: 0.19377490878105164\n",
      "Validation: Epoch [22], Batch [654/938], Loss: 0.39768022298812866\n",
      "Validation: Epoch [22], Batch [655/938], Loss: 0.3406849801540375\n",
      "Validation: Epoch [22], Batch [656/938], Loss: 0.38251635432243347\n",
      "Validation: Epoch [22], Batch [657/938], Loss: 0.5079004168510437\n",
      "Validation: Epoch [22], Batch [658/938], Loss: 0.38801082968711853\n",
      "Validation: Epoch [22], Batch [659/938], Loss: 0.4088870882987976\n",
      "Validation: Epoch [22], Batch [660/938], Loss: 0.33697691559791565\n",
      "Validation: Epoch [22], Batch [661/938], Loss: 0.3972237706184387\n",
      "Validation: Epoch [22], Batch [662/938], Loss: 0.45473629236221313\n",
      "Validation: Epoch [22], Batch [663/938], Loss: 0.4104461073875427\n",
      "Validation: Epoch [22], Batch [664/938], Loss: 0.7880517244338989\n",
      "Validation: Epoch [22], Batch [665/938], Loss: 0.2091761827468872\n",
      "Validation: Epoch [22], Batch [666/938], Loss: 0.2694571912288666\n",
      "Validation: Epoch [22], Batch [667/938], Loss: 0.25741007924079895\n",
      "Validation: Epoch [22], Batch [668/938], Loss: 0.36037808656692505\n",
      "Validation: Epoch [22], Batch [669/938], Loss: 0.4599512219429016\n",
      "Validation: Epoch [22], Batch [670/938], Loss: 0.3766007721424103\n",
      "Validation: Epoch [22], Batch [671/938], Loss: 0.25305306911468506\n",
      "Validation: Epoch [22], Batch [672/938], Loss: 0.3127502501010895\n",
      "Validation: Epoch [22], Batch [673/938], Loss: 0.36042356491088867\n",
      "Validation: Epoch [22], Batch [674/938], Loss: 0.5013601779937744\n",
      "Validation: Epoch [22], Batch [675/938], Loss: 0.338469922542572\n",
      "Validation: Epoch [22], Batch [676/938], Loss: 0.46836209297180176\n",
      "Validation: Epoch [22], Batch [677/938], Loss: 0.6275644302368164\n",
      "Validation: Epoch [22], Batch [678/938], Loss: 0.28897765278816223\n",
      "Validation: Epoch [22], Batch [679/938], Loss: 0.3411257863044739\n",
      "Validation: Epoch [22], Batch [680/938], Loss: 0.3849541246891022\n",
      "Validation: Epoch [22], Batch [681/938], Loss: 0.45665740966796875\n",
      "Validation: Epoch [22], Batch [682/938], Loss: 0.40928828716278076\n",
      "Validation: Epoch [22], Batch [683/938], Loss: 0.2999556064605713\n",
      "Validation: Epoch [22], Batch [684/938], Loss: 0.26236581802368164\n",
      "Validation: Epoch [22], Batch [685/938], Loss: 0.4211321473121643\n",
      "Validation: Epoch [22], Batch [686/938], Loss: 0.25353726744651794\n",
      "Validation: Epoch [22], Batch [687/938], Loss: 0.32554692029953003\n",
      "Validation: Epoch [22], Batch [688/938], Loss: 0.4529455304145813\n",
      "Validation: Epoch [22], Batch [689/938], Loss: 0.23264843225479126\n",
      "Validation: Epoch [22], Batch [690/938], Loss: 0.5640155076980591\n",
      "Validation: Epoch [22], Batch [691/938], Loss: 0.3189537525177002\n",
      "Validation: Epoch [22], Batch [692/938], Loss: 0.22093698382377625\n",
      "Validation: Epoch [22], Batch [693/938], Loss: 0.33837461471557617\n",
      "Validation: Epoch [22], Batch [694/938], Loss: 0.3722776174545288\n",
      "Validation: Epoch [22], Batch [695/938], Loss: 0.27084270119667053\n",
      "Validation: Epoch [22], Batch [696/938], Loss: 0.35679981112480164\n",
      "Validation: Epoch [22], Batch [697/938], Loss: 0.3255203366279602\n",
      "Validation: Epoch [22], Batch [698/938], Loss: 0.3755314350128174\n",
      "Validation: Epoch [22], Batch [699/938], Loss: 0.4185097813606262\n",
      "Validation: Epoch [22], Batch [700/938], Loss: 0.3165937066078186\n",
      "Validation: Epoch [22], Batch [701/938], Loss: 0.3666672706604004\n",
      "Validation: Epoch [22], Batch [702/938], Loss: 0.3828316032886505\n",
      "Validation: Epoch [22], Batch [703/938], Loss: 0.3506808280944824\n",
      "Validation: Epoch [22], Batch [704/938], Loss: 0.7770242094993591\n",
      "Validation: Epoch [22], Batch [705/938], Loss: 0.5724079608917236\n",
      "Validation: Epoch [22], Batch [706/938], Loss: 0.27944546937942505\n",
      "Validation: Epoch [22], Batch [707/938], Loss: 0.19882601499557495\n",
      "Validation: Epoch [22], Batch [708/938], Loss: 0.28568899631500244\n",
      "Validation: Epoch [22], Batch [709/938], Loss: 0.3477948009967804\n",
      "Validation: Epoch [22], Batch [710/938], Loss: 0.24791134893894196\n",
      "Validation: Epoch [22], Batch [711/938], Loss: 0.3786967694759369\n",
      "Validation: Epoch [22], Batch [712/938], Loss: 0.31485575437545776\n",
      "Validation: Epoch [22], Batch [713/938], Loss: 0.2591913938522339\n",
      "Validation: Epoch [22], Batch [714/938], Loss: 0.34920352697372437\n",
      "Validation: Epoch [22], Batch [715/938], Loss: 0.5485345721244812\n",
      "Validation: Epoch [22], Batch [716/938], Loss: 0.52017742395401\n",
      "Validation: Epoch [22], Batch [717/938], Loss: 0.4323671758174896\n",
      "Validation: Epoch [22], Batch [718/938], Loss: 0.26907268166542053\n",
      "Validation: Epoch [22], Batch [719/938], Loss: 0.39670613408088684\n",
      "Validation: Epoch [22], Batch [720/938], Loss: 0.3105931282043457\n",
      "Validation: Epoch [22], Batch [721/938], Loss: 0.4364452660083771\n",
      "Validation: Epoch [22], Batch [722/938], Loss: 0.37372639775276184\n",
      "Validation: Epoch [22], Batch [723/938], Loss: 0.39932191371917725\n",
      "Validation: Epoch [22], Batch [724/938], Loss: 0.6533746719360352\n",
      "Validation: Epoch [22], Batch [725/938], Loss: 0.35920751094818115\n",
      "Validation: Epoch [22], Batch [726/938], Loss: 0.3309117555618286\n",
      "Validation: Epoch [22], Batch [727/938], Loss: 0.3897213935852051\n",
      "Validation: Epoch [22], Batch [728/938], Loss: 0.33618679642677307\n",
      "Validation: Epoch [22], Batch [729/938], Loss: 0.528508722782135\n",
      "Validation: Epoch [22], Batch [730/938], Loss: 0.4099965989589691\n",
      "Validation: Epoch [22], Batch [731/938], Loss: 0.33949506282806396\n",
      "Validation: Epoch [22], Batch [732/938], Loss: 0.35495632886886597\n",
      "Validation: Epoch [22], Batch [733/938], Loss: 0.30067533254623413\n",
      "Validation: Epoch [22], Batch [734/938], Loss: 0.4440852403640747\n",
      "Validation: Epoch [22], Batch [735/938], Loss: 0.3556663691997528\n",
      "Validation: Epoch [22], Batch [736/938], Loss: 0.3637000024318695\n",
      "Validation: Epoch [22], Batch [737/938], Loss: 0.46786707639694214\n",
      "Validation: Epoch [22], Batch [738/938], Loss: 0.36384978890419006\n",
      "Validation: Epoch [22], Batch [739/938], Loss: 0.4100542664527893\n",
      "Validation: Epoch [22], Batch [740/938], Loss: 0.48020899295806885\n",
      "Validation: Epoch [22], Batch [741/938], Loss: 0.25597643852233887\n",
      "Validation: Epoch [22], Batch [742/938], Loss: 0.4492456912994385\n",
      "Validation: Epoch [22], Batch [743/938], Loss: 0.40632325410842896\n",
      "Validation: Epoch [22], Batch [744/938], Loss: 0.3683580756187439\n",
      "Validation: Epoch [22], Batch [745/938], Loss: 0.36369797587394714\n",
      "Validation: Epoch [22], Batch [746/938], Loss: 0.3506366014480591\n",
      "Validation: Epoch [22], Batch [747/938], Loss: 0.7608621120452881\n",
      "Validation: Epoch [22], Batch [748/938], Loss: 0.5445016622543335\n",
      "Validation: Epoch [22], Batch [749/938], Loss: 0.31209442019462585\n",
      "Validation: Epoch [22], Batch [750/938], Loss: 0.25704509019851685\n",
      "Validation: Epoch [22], Batch [751/938], Loss: 0.47693413496017456\n",
      "Validation: Epoch [22], Batch [752/938], Loss: 0.5269941091537476\n",
      "Validation: Epoch [22], Batch [753/938], Loss: 0.4200547933578491\n",
      "Validation: Epoch [22], Batch [754/938], Loss: 0.3798840045928955\n",
      "Validation: Epoch [22], Batch [755/938], Loss: 0.6310141086578369\n",
      "Validation: Epoch [22], Batch [756/938], Loss: 0.2946345806121826\n",
      "Validation: Epoch [22], Batch [757/938], Loss: 0.32097482681274414\n",
      "Validation: Epoch [22], Batch [758/938], Loss: 0.45147114992141724\n",
      "Validation: Epoch [22], Batch [759/938], Loss: 0.27300092577934265\n",
      "Validation: Epoch [22], Batch [760/938], Loss: 0.655860424041748\n",
      "Validation: Epoch [22], Batch [761/938], Loss: 0.3050456941127777\n",
      "Validation: Epoch [22], Batch [762/938], Loss: 0.29802432656288147\n",
      "Validation: Epoch [22], Batch [763/938], Loss: 0.4015571177005768\n",
      "Validation: Epoch [22], Batch [764/938], Loss: 0.3537988066673279\n",
      "Validation: Epoch [22], Batch [765/938], Loss: 0.4299990236759186\n",
      "Validation: Epoch [22], Batch [766/938], Loss: 0.548406720161438\n",
      "Validation: Epoch [22], Batch [767/938], Loss: 0.4806748330593109\n",
      "Validation: Epoch [22], Batch [768/938], Loss: 0.3295603096485138\n",
      "Validation: Epoch [22], Batch [769/938], Loss: 0.3615351915359497\n",
      "Validation: Epoch [22], Batch [770/938], Loss: 0.3582077622413635\n",
      "Validation: Epoch [22], Batch [771/938], Loss: 0.29614195227622986\n",
      "Validation: Epoch [22], Batch [772/938], Loss: 0.6649800539016724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [773/938], Loss: 0.36766937375068665\n",
      "Validation: Epoch [22], Batch [774/938], Loss: 0.4608050286769867\n",
      "Validation: Epoch [22], Batch [775/938], Loss: 0.4116196036338806\n",
      "Validation: Epoch [22], Batch [776/938], Loss: 0.29026561975479126\n",
      "Validation: Epoch [22], Batch [777/938], Loss: 0.521127462387085\n",
      "Validation: Epoch [22], Batch [778/938], Loss: 0.4477880597114563\n",
      "Validation: Epoch [22], Batch [779/938], Loss: 0.4296169877052307\n",
      "Validation: Epoch [22], Batch [780/938], Loss: 0.27788394689559937\n",
      "Validation: Epoch [22], Batch [781/938], Loss: 0.4980292320251465\n",
      "Validation: Epoch [22], Batch [782/938], Loss: 0.30847617983818054\n",
      "Validation: Epoch [22], Batch [783/938], Loss: 0.332034707069397\n",
      "Validation: Epoch [22], Batch [784/938], Loss: 0.367613822221756\n",
      "Validation: Epoch [22], Batch [785/938], Loss: 0.2862510681152344\n",
      "Validation: Epoch [22], Batch [786/938], Loss: 0.4007699191570282\n",
      "Validation: Epoch [22], Batch [787/938], Loss: 0.36222583055496216\n",
      "Validation: Epoch [22], Batch [788/938], Loss: 0.4063728451728821\n",
      "Validation: Epoch [22], Batch [789/938], Loss: 0.41634276509284973\n",
      "Validation: Epoch [22], Batch [790/938], Loss: 0.4285133481025696\n",
      "Validation: Epoch [22], Batch [791/938], Loss: 0.3766234517097473\n",
      "Validation: Epoch [22], Batch [792/938], Loss: 0.353391170501709\n",
      "Validation: Epoch [22], Batch [793/938], Loss: 0.26963451504707336\n",
      "Validation: Epoch [22], Batch [794/938], Loss: 0.3409522771835327\n",
      "Validation: Epoch [22], Batch [795/938], Loss: 0.26153698563575745\n",
      "Validation: Epoch [22], Batch [796/938], Loss: 0.28190845251083374\n",
      "Validation: Epoch [22], Batch [797/938], Loss: 0.3872491121292114\n",
      "Validation: Epoch [22], Batch [798/938], Loss: 0.33905965089797974\n",
      "Validation: Epoch [22], Batch [799/938], Loss: 0.2004896104335785\n",
      "Validation: Epoch [22], Batch [800/938], Loss: 0.3008071184158325\n",
      "Validation: Epoch [22], Batch [801/938], Loss: 0.473032683134079\n",
      "Validation: Epoch [22], Batch [802/938], Loss: 0.43247026205062866\n",
      "Validation: Epoch [22], Batch [803/938], Loss: 0.42511528730392456\n",
      "Validation: Epoch [22], Batch [804/938], Loss: 0.22696679830551147\n",
      "Validation: Epoch [22], Batch [805/938], Loss: 0.4219922423362732\n",
      "Validation: Epoch [22], Batch [806/938], Loss: 0.44191956520080566\n",
      "Validation: Epoch [22], Batch [807/938], Loss: 0.4432312846183777\n",
      "Validation: Epoch [22], Batch [808/938], Loss: 0.27124422788619995\n",
      "Validation: Epoch [22], Batch [809/938], Loss: 0.4100944399833679\n",
      "Validation: Epoch [22], Batch [810/938], Loss: 0.42335236072540283\n",
      "Validation: Epoch [22], Batch [811/938], Loss: 0.2802738845348358\n",
      "Validation: Epoch [22], Batch [812/938], Loss: 0.5736109018325806\n",
      "Validation: Epoch [22], Batch [813/938], Loss: 0.38027846813201904\n",
      "Validation: Epoch [22], Batch [814/938], Loss: 0.4607488214969635\n",
      "Validation: Epoch [22], Batch [815/938], Loss: 0.32014375925064087\n",
      "Validation: Epoch [22], Batch [816/938], Loss: 0.4906436800956726\n",
      "Validation: Epoch [22], Batch [817/938], Loss: 0.32966530323028564\n",
      "Validation: Epoch [22], Batch [818/938], Loss: 0.41222918033599854\n",
      "Validation: Epoch [22], Batch [819/938], Loss: 0.4787454605102539\n",
      "Validation: Epoch [22], Batch [820/938], Loss: 0.3893391788005829\n",
      "Validation: Epoch [22], Batch [821/938], Loss: 0.3069341778755188\n",
      "Validation: Epoch [22], Batch [822/938], Loss: 0.33130526542663574\n",
      "Validation: Epoch [22], Batch [823/938], Loss: 0.3360331654548645\n",
      "Validation: Epoch [22], Batch [824/938], Loss: 0.49207380414009094\n",
      "Validation: Epoch [22], Batch [825/938], Loss: 0.37426719069480896\n",
      "Validation: Epoch [22], Batch [826/938], Loss: 0.37219762802124023\n",
      "Validation: Epoch [22], Batch [827/938], Loss: 0.43673276901245117\n",
      "Validation: Epoch [22], Batch [828/938], Loss: 0.40531784296035767\n",
      "Validation: Epoch [22], Batch [829/938], Loss: 0.3693086802959442\n",
      "Validation: Epoch [22], Batch [830/938], Loss: 0.305268794298172\n",
      "Validation: Epoch [22], Batch [831/938], Loss: 0.3881755471229553\n",
      "Validation: Epoch [22], Batch [832/938], Loss: 0.3201183080673218\n",
      "Validation: Epoch [22], Batch [833/938], Loss: 0.5347845554351807\n",
      "Validation: Epoch [22], Batch [834/938], Loss: 0.4967048764228821\n",
      "Validation: Epoch [22], Batch [835/938], Loss: 0.2854902446269989\n",
      "Validation: Epoch [22], Batch [836/938], Loss: 0.32431867718696594\n",
      "Validation: Epoch [22], Batch [837/938], Loss: 0.6423711180686951\n",
      "Validation: Epoch [22], Batch [838/938], Loss: 0.2372485101222992\n",
      "Validation: Epoch [22], Batch [839/938], Loss: 0.29275625944137573\n",
      "Validation: Epoch [22], Batch [840/938], Loss: 0.3151453733444214\n",
      "Validation: Epoch [22], Batch [841/938], Loss: 0.27544155716896057\n",
      "Validation: Epoch [22], Batch [842/938], Loss: 0.34966155886650085\n",
      "Validation: Epoch [22], Batch [843/938], Loss: 0.45703476667404175\n",
      "Validation: Epoch [22], Batch [844/938], Loss: 0.4890815317630768\n",
      "Validation: Epoch [22], Batch [845/938], Loss: 0.2529700696468353\n",
      "Validation: Epoch [22], Batch [846/938], Loss: 0.4423173666000366\n",
      "Validation: Epoch [22], Batch [847/938], Loss: 0.19884678721427917\n",
      "Validation: Epoch [22], Batch [848/938], Loss: 0.4197534918785095\n",
      "Validation: Epoch [22], Batch [849/938], Loss: 0.3691557049751282\n",
      "Validation: Epoch [22], Batch [850/938], Loss: 0.246246337890625\n",
      "Validation: Epoch [22], Batch [851/938], Loss: 0.36432796716690063\n",
      "Validation: Epoch [22], Batch [852/938], Loss: 0.3974483609199524\n",
      "Validation: Epoch [22], Batch [853/938], Loss: 0.27428680658340454\n",
      "Validation: Epoch [22], Batch [854/938], Loss: 0.5344386100769043\n",
      "Validation: Epoch [22], Batch [855/938], Loss: 0.41097015142440796\n",
      "Validation: Epoch [22], Batch [856/938], Loss: 0.5013488531112671\n",
      "Validation: Epoch [22], Batch [857/938], Loss: 0.40402060747146606\n",
      "Validation: Epoch [22], Batch [858/938], Loss: 0.21957941353321075\n",
      "Validation: Epoch [22], Batch [859/938], Loss: 0.29756709933280945\n",
      "Validation: Epoch [22], Batch [860/938], Loss: 0.39652299880981445\n",
      "Validation: Epoch [22], Batch [861/938], Loss: 0.33111459016799927\n",
      "Validation: Epoch [22], Batch [862/938], Loss: 0.46607717871665955\n",
      "Validation: Epoch [22], Batch [863/938], Loss: 0.3838367164134979\n",
      "Validation: Epoch [22], Batch [864/938], Loss: 0.29730623960494995\n",
      "Validation: Epoch [22], Batch [865/938], Loss: 0.2593516707420349\n",
      "Validation: Epoch [22], Batch [866/938], Loss: 0.4432198703289032\n",
      "Validation: Epoch [22], Batch [867/938], Loss: 0.2858210504055023\n",
      "Validation: Epoch [22], Batch [868/938], Loss: 0.4373214840888977\n",
      "Validation: Epoch [22], Batch [869/938], Loss: 0.19974446296691895\n",
      "Validation: Epoch [22], Batch [870/938], Loss: 0.4560433030128479\n",
      "Validation: Epoch [22], Batch [871/938], Loss: 0.472263365983963\n",
      "Validation: Epoch [22], Batch [872/938], Loss: 0.325695663690567\n",
      "Validation: Epoch [22], Batch [873/938], Loss: 0.4258062243461609\n",
      "Validation: Epoch [22], Batch [874/938], Loss: 0.3846879005432129\n",
      "Validation: Epoch [22], Batch [875/938], Loss: 0.360977441072464\n",
      "Validation: Epoch [22], Batch [876/938], Loss: 0.429835706949234\n",
      "Validation: Epoch [22], Batch [877/938], Loss: 0.4019008278846741\n",
      "Validation: Epoch [22], Batch [878/938], Loss: 0.44326841831207275\n",
      "Validation: Epoch [22], Batch [879/938], Loss: 0.4101710319519043\n",
      "Validation: Epoch [22], Batch [880/938], Loss: 0.478343665599823\n",
      "Validation: Epoch [22], Batch [881/938], Loss: 0.4718671441078186\n",
      "Validation: Epoch [22], Batch [882/938], Loss: 0.581297755241394\n",
      "Validation: Epoch [22], Batch [883/938], Loss: 0.47193145751953125\n",
      "Validation: Epoch [22], Batch [884/938], Loss: 0.32623517513275146\n",
      "Validation: Epoch [22], Batch [885/938], Loss: 0.4525490701198578\n",
      "Validation: Epoch [22], Batch [886/938], Loss: 0.2981802225112915\n",
      "Validation: Epoch [22], Batch [887/938], Loss: 0.3443600535392761\n",
      "Validation: Epoch [22], Batch [888/938], Loss: 0.47916173934936523\n",
      "Validation: Epoch [22], Batch [889/938], Loss: 0.3902876675128937\n",
      "Validation: Epoch [22], Batch [890/938], Loss: 0.42580729722976685\n",
      "Validation: Epoch [22], Batch [891/938], Loss: 0.5631595253944397\n",
      "Validation: Epoch [22], Batch [892/938], Loss: 0.43174421787261963\n",
      "Validation: Epoch [22], Batch [893/938], Loss: 0.33038634061813354\n",
      "Validation: Epoch [22], Batch [894/938], Loss: 0.35155153274536133\n",
      "Validation: Epoch [22], Batch [895/938], Loss: 0.3089025616645813\n",
      "Validation: Epoch [22], Batch [896/938], Loss: 0.45383521914482117\n",
      "Validation: Epoch [22], Batch [897/938], Loss: 0.5247128009796143\n",
      "Validation: Epoch [22], Batch [898/938], Loss: 0.37226223945617676\n",
      "Validation: Epoch [22], Batch [899/938], Loss: 0.4947277903556824\n",
      "Validation: Epoch [22], Batch [900/938], Loss: 0.44760429859161377\n",
      "Validation: Epoch [22], Batch [901/938], Loss: 0.32966747879981995\n",
      "Validation: Epoch [22], Batch [902/938], Loss: 0.24382463097572327\n",
      "Validation: Epoch [22], Batch [903/938], Loss: 0.2253364473581314\n",
      "Validation: Epoch [22], Batch [904/938], Loss: 0.439147025346756\n",
      "Validation: Epoch [22], Batch [905/938], Loss: 0.4442847669124603\n",
      "Validation: Epoch [22], Batch [906/938], Loss: 0.3128083348274231\n",
      "Validation: Epoch [22], Batch [907/938], Loss: 0.35198476910591125\n",
      "Validation: Epoch [22], Batch [908/938], Loss: 0.44562989473342896\n",
      "Validation: Epoch [22], Batch [909/938], Loss: 0.40707382559776306\n",
      "Validation: Epoch [22], Batch [910/938], Loss: 0.4410340487957001\n",
      "Validation: Epoch [22], Batch [911/938], Loss: 0.3918142020702362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [22], Batch [912/938], Loss: 0.5605743527412415\n",
      "Validation: Epoch [22], Batch [913/938], Loss: 0.23727118968963623\n",
      "Validation: Epoch [22], Batch [914/938], Loss: 0.4348490536212921\n",
      "Validation: Epoch [22], Batch [915/938], Loss: 0.4573592245578766\n",
      "Validation: Epoch [22], Batch [916/938], Loss: 0.39176446199417114\n",
      "Validation: Epoch [22], Batch [917/938], Loss: 0.4863360524177551\n",
      "Validation: Epoch [22], Batch [918/938], Loss: 0.3219667971134186\n",
      "Validation: Epoch [22], Batch [919/938], Loss: 0.3445248007774353\n",
      "Validation: Epoch [22], Batch [920/938], Loss: 0.24981112778186798\n",
      "Validation: Epoch [22], Batch [921/938], Loss: 0.5254393219947815\n",
      "Validation: Epoch [22], Batch [922/938], Loss: 0.374899685382843\n",
      "Validation: Epoch [22], Batch [923/938], Loss: 0.24745994806289673\n",
      "Validation: Epoch [22], Batch [924/938], Loss: 0.3695109188556671\n",
      "Validation: Epoch [22], Batch [925/938], Loss: 0.42049121856689453\n",
      "Validation: Epoch [22], Batch [926/938], Loss: 0.572696328163147\n",
      "Validation: Epoch [22], Batch [927/938], Loss: 0.3065182566642761\n",
      "Validation: Epoch [22], Batch [928/938], Loss: 0.4763861894607544\n",
      "Validation: Epoch [22], Batch [929/938], Loss: 0.32252955436706543\n",
      "Validation: Epoch [22], Batch [930/938], Loss: 0.30167704820632935\n",
      "Validation: Epoch [22], Batch [931/938], Loss: 0.30393052101135254\n",
      "Validation: Epoch [22], Batch [932/938], Loss: 0.45582443475723267\n",
      "Validation: Epoch [22], Batch [933/938], Loss: 0.33637920022010803\n",
      "Validation: Epoch [22], Batch [934/938], Loss: 0.40812820196151733\n",
      "Validation: Epoch [22], Batch [935/938], Loss: 0.441276490688324\n",
      "Validation: Epoch [22], Batch [936/938], Loss: 0.4121694564819336\n",
      "Validation: Epoch [22], Batch [937/938], Loss: 0.5547340512275696\n",
      "Validation: Epoch [22], Batch [938/938], Loss: 0.29688966274261475\n",
      "Accuracy of test set: 0.8649333333333333\n",
      "Train: Epoch [23], Batch [1/938], Loss: 0.26679837703704834\n",
      "Train: Epoch [23], Batch [2/938], Loss: 0.37935203313827515\n",
      "Train: Epoch [23], Batch [3/938], Loss: 0.24032001197338104\n",
      "Train: Epoch [23], Batch [4/938], Loss: 0.28042393922805786\n",
      "Train: Epoch [23], Batch [5/938], Loss: 0.33359724283218384\n",
      "Train: Epoch [23], Batch [6/938], Loss: 0.3490995764732361\n",
      "Train: Epoch [23], Batch [7/938], Loss: 0.3906605541706085\n",
      "Train: Epoch [23], Batch [8/938], Loss: 0.4891495704650879\n",
      "Train: Epoch [23], Batch [9/938], Loss: 0.5012557506561279\n",
      "Train: Epoch [23], Batch [10/938], Loss: 0.35941413044929504\n",
      "Train: Epoch [23], Batch [11/938], Loss: 0.45435044169425964\n",
      "Train: Epoch [23], Batch [12/938], Loss: 0.4010184407234192\n",
      "Train: Epoch [23], Batch [13/938], Loss: 0.3855743408203125\n",
      "Train: Epoch [23], Batch [14/938], Loss: 0.2656092047691345\n",
      "Train: Epoch [23], Batch [15/938], Loss: 0.6258302927017212\n",
      "Train: Epoch [23], Batch [16/938], Loss: 0.32463446259498596\n",
      "Train: Epoch [23], Batch [17/938], Loss: 0.35574156045913696\n",
      "Train: Epoch [23], Batch [18/938], Loss: 0.3107423782348633\n",
      "Train: Epoch [23], Batch [19/938], Loss: 0.2932456135749817\n",
      "Train: Epoch [23], Batch [20/938], Loss: 0.43490591645240784\n",
      "Train: Epoch [23], Batch [21/938], Loss: 0.45996636152267456\n",
      "Train: Epoch [23], Batch [22/938], Loss: 0.35543152689933777\n",
      "Train: Epoch [23], Batch [23/938], Loss: 0.4258292317390442\n",
      "Train: Epoch [23], Batch [24/938], Loss: 0.2921842634677887\n",
      "Train: Epoch [23], Batch [25/938], Loss: 0.4318450689315796\n",
      "Train: Epoch [23], Batch [26/938], Loss: 0.5398949980735779\n",
      "Train: Epoch [23], Batch [27/938], Loss: 0.5021741390228271\n",
      "Train: Epoch [23], Batch [28/938], Loss: 0.3661416172981262\n",
      "Train: Epoch [23], Batch [29/938], Loss: 0.3731537163257599\n",
      "Train: Epoch [23], Batch [30/938], Loss: 0.44345712661743164\n",
      "Train: Epoch [23], Batch [31/938], Loss: 0.4106229543685913\n",
      "Train: Epoch [23], Batch [32/938], Loss: 0.35419729351997375\n",
      "Train: Epoch [23], Batch [33/938], Loss: 0.4891347587108612\n",
      "Train: Epoch [23], Batch [34/938], Loss: 0.27832913398742676\n",
      "Train: Epoch [23], Batch [35/938], Loss: 0.37759509682655334\n",
      "Train: Epoch [23], Batch [36/938], Loss: 0.2752349376678467\n",
      "Train: Epoch [23], Batch [37/938], Loss: 0.27867481112480164\n",
      "Train: Epoch [23], Batch [38/938], Loss: 0.41515201330184937\n",
      "Train: Epoch [23], Batch [39/938], Loss: 0.31324538588523865\n",
      "Train: Epoch [23], Batch [40/938], Loss: 0.2595471143722534\n",
      "Train: Epoch [23], Batch [41/938], Loss: 0.41325098276138306\n",
      "Train: Epoch [23], Batch [42/938], Loss: 0.4317895472049713\n",
      "Train: Epoch [23], Batch [43/938], Loss: 0.2942241430282593\n",
      "Train: Epoch [23], Batch [44/938], Loss: 0.34048086404800415\n",
      "Train: Epoch [23], Batch [45/938], Loss: 0.5227223634719849\n",
      "Train: Epoch [23], Batch [46/938], Loss: 0.32890743017196655\n",
      "Train: Epoch [23], Batch [47/938], Loss: 0.45304539799690247\n",
      "Train: Epoch [23], Batch [48/938], Loss: 0.35998421907424927\n",
      "Train: Epoch [23], Batch [49/938], Loss: 0.3065703511238098\n",
      "Train: Epoch [23], Batch [50/938], Loss: 0.3033141791820526\n",
      "Train: Epoch [23], Batch [51/938], Loss: 0.3456546664237976\n",
      "Train: Epoch [23], Batch [52/938], Loss: 0.40796488523483276\n",
      "Train: Epoch [23], Batch [53/938], Loss: 0.17318448424339294\n",
      "Train: Epoch [23], Batch [54/938], Loss: 0.4054102897644043\n",
      "Train: Epoch [23], Batch [55/938], Loss: 0.3873865306377411\n",
      "Train: Epoch [23], Batch [56/938], Loss: 0.3392203450202942\n",
      "Train: Epoch [23], Batch [57/938], Loss: 0.2560054063796997\n",
      "Train: Epoch [23], Batch [58/938], Loss: 0.30850738286972046\n",
      "Train: Epoch [23], Batch [59/938], Loss: 0.2890462875366211\n",
      "Train: Epoch [23], Batch [60/938], Loss: 0.4158889055252075\n",
      "Train: Epoch [23], Batch [61/938], Loss: 0.4769795835018158\n",
      "Train: Epoch [23], Batch [62/938], Loss: 0.3059500753879547\n",
      "Train: Epoch [23], Batch [63/938], Loss: 0.31447166204452515\n",
      "Train: Epoch [23], Batch [64/938], Loss: 0.3788796067237854\n",
      "Train: Epoch [23], Batch [65/938], Loss: 0.319740355014801\n",
      "Train: Epoch [23], Batch [66/938], Loss: 0.5986728668212891\n",
      "Train: Epoch [23], Batch [67/938], Loss: 0.4506515860557556\n",
      "Train: Epoch [23], Batch [68/938], Loss: 0.3585980534553528\n",
      "Train: Epoch [23], Batch [69/938], Loss: 0.40548473596572876\n",
      "Train: Epoch [23], Batch [70/938], Loss: 0.45906102657318115\n",
      "Train: Epoch [23], Batch [71/938], Loss: 0.31472429633140564\n",
      "Train: Epoch [23], Batch [72/938], Loss: 0.3842625021934509\n",
      "Train: Epoch [23], Batch [73/938], Loss: 0.31547319889068604\n",
      "Train: Epoch [23], Batch [74/938], Loss: 0.4010816812515259\n",
      "Train: Epoch [23], Batch [75/938], Loss: 0.32060176134109497\n",
      "Train: Epoch [23], Batch [76/938], Loss: 0.3819464445114136\n",
      "Train: Epoch [23], Batch [77/938], Loss: 0.449690580368042\n",
      "Train: Epoch [23], Batch [78/938], Loss: 0.25292032957077026\n",
      "Train: Epoch [23], Batch [79/938], Loss: 0.3042846620082855\n",
      "Train: Epoch [23], Batch [80/938], Loss: 0.6016124486923218\n",
      "Train: Epoch [23], Batch [81/938], Loss: 0.252951979637146\n",
      "Train: Epoch [23], Batch [82/938], Loss: 0.31461265683174133\n",
      "Train: Epoch [23], Batch [83/938], Loss: 0.27918559312820435\n",
      "Train: Epoch [23], Batch [84/938], Loss: 0.29795441031455994\n",
      "Train: Epoch [23], Batch [85/938], Loss: 0.4257858991622925\n",
      "Train: Epoch [23], Batch [86/938], Loss: 0.3441222608089447\n",
      "Train: Epoch [23], Batch [87/938], Loss: 0.5077952146530151\n",
      "Train: Epoch [23], Batch [88/938], Loss: 0.3953810930252075\n",
      "Train: Epoch [23], Batch [89/938], Loss: 0.3355816900730133\n",
      "Train: Epoch [23], Batch [90/938], Loss: 0.4232330024242401\n",
      "Train: Epoch [23], Batch [91/938], Loss: 0.37977901101112366\n",
      "Train: Epoch [23], Batch [92/938], Loss: 0.4270137548446655\n",
      "Train: Epoch [23], Batch [93/938], Loss: 0.4812588095664978\n",
      "Train: Epoch [23], Batch [94/938], Loss: 0.27191734313964844\n",
      "Train: Epoch [23], Batch [95/938], Loss: 0.2900890111923218\n",
      "Train: Epoch [23], Batch [96/938], Loss: 0.40511730313301086\n",
      "Train: Epoch [23], Batch [97/938], Loss: 0.45259663462638855\n",
      "Train: Epoch [23], Batch [98/938], Loss: 0.18410733342170715\n",
      "Train: Epoch [23], Batch [99/938], Loss: 0.4927064776420593\n",
      "Train: Epoch [23], Batch [100/938], Loss: 0.24864602088928223\n",
      "Train: Epoch [23], Batch [101/938], Loss: 0.42028099298477173\n",
      "Train: Epoch [23], Batch [102/938], Loss: 0.46058231592178345\n",
      "Train: Epoch [23], Batch [103/938], Loss: 0.4001845717430115\n",
      "Train: Epoch [23], Batch [104/938], Loss: 0.41197505593299866\n",
      "Train: Epoch [23], Batch [105/938], Loss: 0.608526885509491\n",
      "Train: Epoch [23], Batch [106/938], Loss: 0.6138104796409607\n",
      "Train: Epoch [23], Batch [107/938], Loss: 0.36395373940467834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [23], Batch [108/938], Loss: 0.4854434132575989\n",
      "Train: Epoch [23], Batch [109/938], Loss: 0.343416690826416\n",
      "Train: Epoch [23], Batch [110/938], Loss: 0.46135473251342773\n",
      "Train: Epoch [23], Batch [111/938], Loss: 0.36254531145095825\n",
      "Train: Epoch [23], Batch [112/938], Loss: 0.4101274907588959\n",
      "Train: Epoch [23], Batch [113/938], Loss: 0.36820581555366516\n",
      "Train: Epoch [23], Batch [114/938], Loss: 0.2716939449310303\n",
      "Train: Epoch [23], Batch [115/938], Loss: 0.28221431374549866\n",
      "Train: Epoch [23], Batch [116/938], Loss: 0.3945736289024353\n",
      "Train: Epoch [23], Batch [117/938], Loss: 0.532233715057373\n",
      "Train: Epoch [23], Batch [118/938], Loss: 0.5063623785972595\n",
      "Train: Epoch [23], Batch [119/938], Loss: 0.32080215215682983\n",
      "Train: Epoch [23], Batch [120/938], Loss: 0.48844414949417114\n",
      "Train: Epoch [23], Batch [121/938], Loss: 0.41258782148361206\n",
      "Train: Epoch [23], Batch [122/938], Loss: 0.352395623922348\n",
      "Train: Epoch [23], Batch [123/938], Loss: 0.44838201999664307\n",
      "Train: Epoch [23], Batch [124/938], Loss: 0.4331511855125427\n",
      "Train: Epoch [23], Batch [125/938], Loss: 0.3999345898628235\n",
      "Train: Epoch [23], Batch [126/938], Loss: 0.3156054615974426\n",
      "Train: Epoch [23], Batch [127/938], Loss: 0.3463425636291504\n",
      "Train: Epoch [23], Batch [128/938], Loss: 0.2587464451789856\n",
      "Train: Epoch [23], Batch [129/938], Loss: 0.5691084265708923\n",
      "Train: Epoch [23], Batch [130/938], Loss: 0.33265089988708496\n",
      "Train: Epoch [23], Batch [131/938], Loss: 0.43479204177856445\n",
      "Train: Epoch [23], Batch [132/938], Loss: 0.394281804561615\n",
      "Train: Epoch [23], Batch [133/938], Loss: 0.44889816641807556\n",
      "Train: Epoch [23], Batch [134/938], Loss: 0.328029990196228\n",
      "Train: Epoch [23], Batch [135/938], Loss: 0.2744820713996887\n",
      "Train: Epoch [23], Batch [136/938], Loss: 0.3736594617366791\n",
      "Train: Epoch [23], Batch [137/938], Loss: 0.2588074207305908\n",
      "Train: Epoch [23], Batch [138/938], Loss: 0.497107595205307\n",
      "Train: Epoch [23], Batch [139/938], Loss: 0.2501043379306793\n",
      "Train: Epoch [23], Batch [140/938], Loss: 0.3962952494621277\n",
      "Train: Epoch [23], Batch [141/938], Loss: 0.7453992962837219\n",
      "Train: Epoch [23], Batch [142/938], Loss: 0.5570708513259888\n",
      "Train: Epoch [23], Batch [143/938], Loss: 0.3185768723487854\n",
      "Train: Epoch [23], Batch [144/938], Loss: 0.23471978306770325\n",
      "Train: Epoch [23], Batch [145/938], Loss: 0.2642226219177246\n",
      "Train: Epoch [23], Batch [146/938], Loss: 0.5305726528167725\n",
      "Train: Epoch [23], Batch [147/938], Loss: 0.4207591116428375\n",
      "Train: Epoch [23], Batch [148/938], Loss: 0.4747917652130127\n",
      "Train: Epoch [23], Batch [149/938], Loss: 0.3749484717845917\n",
      "Train: Epoch [23], Batch [150/938], Loss: 0.3471013307571411\n",
      "Train: Epoch [23], Batch [151/938], Loss: 0.29053041338920593\n",
      "Train: Epoch [23], Batch [152/938], Loss: 0.4305404722690582\n",
      "Train: Epoch [23], Batch [153/938], Loss: 0.3362082540988922\n",
      "Train: Epoch [23], Batch [154/938], Loss: 0.379332035779953\n",
      "Train: Epoch [23], Batch [155/938], Loss: 0.45365798473358154\n",
      "Train: Epoch [23], Batch [156/938], Loss: 0.4454580843448639\n",
      "Train: Epoch [23], Batch [157/938], Loss: 0.32723942399024963\n",
      "Train: Epoch [23], Batch [158/938], Loss: 0.5719438791275024\n",
      "Train: Epoch [23], Batch [159/938], Loss: 0.36739861965179443\n",
      "Train: Epoch [23], Batch [160/938], Loss: 0.39890241622924805\n",
      "Train: Epoch [23], Batch [161/938], Loss: 0.31318503618240356\n",
      "Train: Epoch [23], Batch [162/938], Loss: 0.47804298996925354\n",
      "Train: Epoch [23], Batch [163/938], Loss: 0.41093918681144714\n",
      "Train: Epoch [23], Batch [164/938], Loss: 0.3479016125202179\n",
      "Train: Epoch [23], Batch [165/938], Loss: 0.46276533603668213\n",
      "Train: Epoch [23], Batch [166/938], Loss: 0.3466142416000366\n",
      "Train: Epoch [23], Batch [167/938], Loss: 0.5855540037155151\n",
      "Train: Epoch [23], Batch [168/938], Loss: 0.2967011332511902\n",
      "Train: Epoch [23], Batch [169/938], Loss: 0.4397384822368622\n",
      "Train: Epoch [23], Batch [170/938], Loss: 0.39911240339279175\n",
      "Train: Epoch [23], Batch [171/938], Loss: 0.31433847546577454\n",
      "Train: Epoch [23], Batch [172/938], Loss: 0.3418430685997009\n",
      "Train: Epoch [23], Batch [173/938], Loss: 0.5103477239608765\n",
      "Train: Epoch [23], Batch [174/938], Loss: 0.3535647690296173\n",
      "Train: Epoch [23], Batch [175/938], Loss: 0.40685904026031494\n",
      "Train: Epoch [23], Batch [176/938], Loss: 0.35737335681915283\n",
      "Train: Epoch [23], Batch [177/938], Loss: 0.3783821165561676\n",
      "Train: Epoch [23], Batch [178/938], Loss: 0.3165268301963806\n",
      "Train: Epoch [23], Batch [179/938], Loss: 0.33293241262435913\n",
      "Train: Epoch [23], Batch [180/938], Loss: 0.5300055146217346\n",
      "Train: Epoch [23], Batch [181/938], Loss: 0.3848390281200409\n",
      "Train: Epoch [23], Batch [182/938], Loss: 0.3859720826148987\n",
      "Train: Epoch [23], Batch [183/938], Loss: 0.3666015863418579\n",
      "Train: Epoch [23], Batch [184/938], Loss: 0.2955259680747986\n",
      "Train: Epoch [23], Batch [185/938], Loss: 0.2840706706047058\n",
      "Train: Epoch [23], Batch [186/938], Loss: 0.26654577255249023\n",
      "Train: Epoch [23], Batch [187/938], Loss: 0.4530882239341736\n",
      "Train: Epoch [23], Batch [188/938], Loss: 0.34166085720062256\n",
      "Train: Epoch [23], Batch [189/938], Loss: 0.27043992280960083\n",
      "Train: Epoch [23], Batch [190/938], Loss: 0.36024877429008484\n",
      "Train: Epoch [23], Batch [191/938], Loss: 0.3149832487106323\n",
      "Train: Epoch [23], Batch [192/938], Loss: 0.33023759722709656\n",
      "Train: Epoch [23], Batch [193/938], Loss: 0.4189988076686859\n",
      "Train: Epoch [23], Batch [194/938], Loss: 0.2580100893974304\n",
      "Train: Epoch [23], Batch [195/938], Loss: 0.32104212045669556\n",
      "Train: Epoch [23], Batch [196/938], Loss: 0.40671396255493164\n",
      "Train: Epoch [23], Batch [197/938], Loss: 0.17250031232833862\n",
      "Train: Epoch [23], Batch [198/938], Loss: 0.3707517683506012\n",
      "Train: Epoch [23], Batch [199/938], Loss: 0.3542966842651367\n",
      "Train: Epoch [23], Batch [200/938], Loss: 0.5365321040153503\n",
      "Train: Epoch [23], Batch [201/938], Loss: 0.4165806472301483\n",
      "Train: Epoch [23], Batch [202/938], Loss: 0.33922338485717773\n",
      "Train: Epoch [23], Batch [203/938], Loss: 0.40155166387557983\n",
      "Train: Epoch [23], Batch [204/938], Loss: 0.365335613489151\n",
      "Train: Epoch [23], Batch [205/938], Loss: 0.3710929751396179\n",
      "Train: Epoch [23], Batch [206/938], Loss: 0.4405432939529419\n",
      "Train: Epoch [23], Batch [207/938], Loss: 0.23953980207443237\n",
      "Train: Epoch [23], Batch [208/938], Loss: 0.4146590232849121\n",
      "Train: Epoch [23], Batch [209/938], Loss: 0.2849224805831909\n",
      "Train: Epoch [23], Batch [210/938], Loss: 0.5194198489189148\n",
      "Train: Epoch [23], Batch [211/938], Loss: 0.3096538186073303\n",
      "Train: Epoch [23], Batch [212/938], Loss: 0.512794017791748\n",
      "Train: Epoch [23], Batch [213/938], Loss: 0.288760781288147\n",
      "Train: Epoch [23], Batch [214/938], Loss: 0.4715918004512787\n",
      "Train: Epoch [23], Batch [215/938], Loss: 0.5235192775726318\n",
      "Train: Epoch [23], Batch [216/938], Loss: 0.5004449486732483\n",
      "Train: Epoch [23], Batch [217/938], Loss: 0.33997488021850586\n",
      "Train: Epoch [23], Batch [218/938], Loss: 0.469560831785202\n",
      "Train: Epoch [23], Batch [219/938], Loss: 0.5212770700454712\n",
      "Train: Epoch [23], Batch [220/938], Loss: 0.36354342103004456\n",
      "Train: Epoch [23], Batch [221/938], Loss: 0.4728718101978302\n",
      "Train: Epoch [23], Batch [222/938], Loss: 0.4215538203716278\n",
      "Train: Epoch [23], Batch [223/938], Loss: 0.3027355670928955\n",
      "Train: Epoch [23], Batch [224/938], Loss: 0.324118435382843\n",
      "Train: Epoch [23], Batch [225/938], Loss: 0.37791600823402405\n",
      "Train: Epoch [23], Batch [226/938], Loss: 0.571091890335083\n",
      "Train: Epoch [23], Batch [227/938], Loss: 0.26819559931755066\n",
      "Train: Epoch [23], Batch [228/938], Loss: 0.516682505607605\n",
      "Train: Epoch [23], Batch [229/938], Loss: 0.3855549693107605\n",
      "Train: Epoch [23], Batch [230/938], Loss: 0.31861448287963867\n",
      "Train: Epoch [23], Batch [231/938], Loss: 0.48778483271598816\n",
      "Train: Epoch [23], Batch [232/938], Loss: 0.5940316915512085\n",
      "Train: Epoch [23], Batch [233/938], Loss: 0.4991907477378845\n",
      "Train: Epoch [23], Batch [234/938], Loss: 0.2878885269165039\n",
      "Train: Epoch [23], Batch [235/938], Loss: 0.4416898787021637\n",
      "Train: Epoch [23], Batch [236/938], Loss: 0.564690887928009\n",
      "Train: Epoch [23], Batch [237/938], Loss: 0.598430335521698\n",
      "Train: Epoch [23], Batch [238/938], Loss: 0.3470327854156494\n",
      "Train: Epoch [23], Batch [239/938], Loss: 0.41725248098373413\n",
      "Train: Epoch [23], Batch [240/938], Loss: 0.49666106700897217\n",
      "Train: Epoch [23], Batch [241/938], Loss: 0.47788363695144653\n",
      "Train: Epoch [23], Batch [242/938], Loss: 0.3712594509124756\n",
      "Train: Epoch [23], Batch [243/938], Loss: 0.4168224334716797\n",
      "Train: Epoch [23], Batch [244/938], Loss: 0.24557696282863617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [23], Batch [245/938], Loss: 0.27631139755249023\n",
      "Train: Epoch [23], Batch [246/938], Loss: 0.39364922046661377\n",
      "Train: Epoch [23], Batch [247/938], Loss: 0.2781601846218109\n",
      "Train: Epoch [23], Batch [248/938], Loss: 0.3750459551811218\n",
      "Train: Epoch [23], Batch [249/938], Loss: 0.5786490440368652\n",
      "Train: Epoch [23], Batch [250/938], Loss: 0.44073328375816345\n",
      "Train: Epoch [23], Batch [251/938], Loss: 0.43776950240135193\n",
      "Train: Epoch [23], Batch [252/938], Loss: 0.3115946054458618\n",
      "Train: Epoch [23], Batch [253/938], Loss: 0.24803368747234344\n",
      "Train: Epoch [23], Batch [254/938], Loss: 0.46781039237976074\n",
      "Train: Epoch [23], Batch [255/938], Loss: 0.33500513434410095\n",
      "Train: Epoch [23], Batch [256/938], Loss: 0.39138972759246826\n",
      "Train: Epoch [23], Batch [257/938], Loss: 0.47762200236320496\n",
      "Train: Epoch [23], Batch [258/938], Loss: 0.27493318915367126\n",
      "Train: Epoch [23], Batch [259/938], Loss: 0.33780762553215027\n",
      "Train: Epoch [23], Batch [260/938], Loss: 0.4234907627105713\n",
      "Train: Epoch [23], Batch [261/938], Loss: 0.43031781911849976\n",
      "Train: Epoch [23], Batch [262/938], Loss: 0.4348512887954712\n",
      "Train: Epoch [23], Batch [263/938], Loss: 0.49827051162719727\n",
      "Train: Epoch [23], Batch [264/938], Loss: 0.36785078048706055\n",
      "Train: Epoch [23], Batch [265/938], Loss: 0.4417557120323181\n",
      "Train: Epoch [23], Batch [266/938], Loss: 0.23867589235305786\n",
      "Train: Epoch [23], Batch [267/938], Loss: 0.343140184879303\n",
      "Train: Epoch [23], Batch [268/938], Loss: 0.5111698508262634\n",
      "Train: Epoch [23], Batch [269/938], Loss: 0.29843634366989136\n",
      "Train: Epoch [23], Batch [270/938], Loss: 0.28336912393569946\n",
      "Train: Epoch [23], Batch [271/938], Loss: 0.2996549606323242\n",
      "Train: Epoch [23], Batch [272/938], Loss: 0.45724883675575256\n",
      "Train: Epoch [23], Batch [273/938], Loss: 0.4006405770778656\n",
      "Train: Epoch [23], Batch [274/938], Loss: 0.4463624954223633\n",
      "Train: Epoch [23], Batch [275/938], Loss: 0.31008586287498474\n",
      "Train: Epoch [23], Batch [276/938], Loss: 0.44318699836730957\n",
      "Train: Epoch [23], Batch [277/938], Loss: 0.3472345471382141\n",
      "Train: Epoch [23], Batch [278/938], Loss: 0.44991305470466614\n",
      "Train: Epoch [23], Batch [279/938], Loss: 0.418398916721344\n",
      "Train: Epoch [23], Batch [280/938], Loss: 0.43126004934310913\n",
      "Train: Epoch [23], Batch [281/938], Loss: 0.3314635455608368\n",
      "Train: Epoch [23], Batch [282/938], Loss: 0.5960190892219543\n",
      "Train: Epoch [23], Batch [283/938], Loss: 0.3515389859676361\n",
      "Train: Epoch [23], Batch [284/938], Loss: 0.21365681290626526\n",
      "Train: Epoch [23], Batch [285/938], Loss: 0.42751482129096985\n",
      "Train: Epoch [23], Batch [286/938], Loss: 0.3776848018169403\n",
      "Train: Epoch [23], Batch [287/938], Loss: 0.4102177321910858\n",
      "Train: Epoch [23], Batch [288/938], Loss: 0.46770673990249634\n",
      "Train: Epoch [23], Batch [289/938], Loss: 0.7532210946083069\n",
      "Train: Epoch [23], Batch [290/938], Loss: 0.3334818482398987\n",
      "Train: Epoch [23], Batch [291/938], Loss: 0.31990933418273926\n",
      "Train: Epoch [23], Batch [292/938], Loss: 0.2245248258113861\n",
      "Train: Epoch [23], Batch [293/938], Loss: 0.4234277009963989\n",
      "Train: Epoch [23], Batch [294/938], Loss: 0.4977286458015442\n",
      "Train: Epoch [23], Batch [295/938], Loss: 0.43357935547828674\n",
      "Train: Epoch [23], Batch [296/938], Loss: 0.3155147433280945\n",
      "Train: Epoch [23], Batch [297/938], Loss: 0.47538435459136963\n",
      "Train: Epoch [23], Batch [298/938], Loss: 0.42796921730041504\n",
      "Train: Epoch [23], Batch [299/938], Loss: 0.41828781366348267\n",
      "Train: Epoch [23], Batch [300/938], Loss: 0.26405811309814453\n",
      "Train: Epoch [23], Batch [301/938], Loss: 0.3184562623500824\n",
      "Train: Epoch [23], Batch [302/938], Loss: 0.46185731887817383\n",
      "Train: Epoch [23], Batch [303/938], Loss: 0.41925787925720215\n",
      "Train: Epoch [23], Batch [304/938], Loss: 0.4500836431980133\n",
      "Train: Epoch [23], Batch [305/938], Loss: 0.3225872218608856\n",
      "Train: Epoch [23], Batch [306/938], Loss: 0.4797806739807129\n",
      "Train: Epoch [23], Batch [307/938], Loss: 0.3727385997772217\n",
      "Train: Epoch [23], Batch [308/938], Loss: 0.3812074363231659\n",
      "Train: Epoch [23], Batch [309/938], Loss: 0.3811199963092804\n",
      "Train: Epoch [23], Batch [310/938], Loss: 0.21192139387130737\n",
      "Train: Epoch [23], Batch [311/938], Loss: 0.3508354723453522\n",
      "Train: Epoch [23], Batch [312/938], Loss: 0.4368990659713745\n",
      "Train: Epoch [23], Batch [313/938], Loss: 0.25043970346450806\n",
      "Train: Epoch [23], Batch [314/938], Loss: 0.40434491634368896\n",
      "Train: Epoch [23], Batch [315/938], Loss: 0.40323197841644287\n",
      "Train: Epoch [23], Batch [316/938], Loss: 0.3395959138870239\n",
      "Train: Epoch [23], Batch [317/938], Loss: 0.41209959983825684\n",
      "Train: Epoch [23], Batch [318/938], Loss: 0.39213821291923523\n",
      "Train: Epoch [23], Batch [319/938], Loss: 0.3228324055671692\n",
      "Train: Epoch [23], Batch [320/938], Loss: 0.18811872601509094\n",
      "Train: Epoch [23], Batch [321/938], Loss: 0.681027889251709\n",
      "Train: Epoch [23], Batch [322/938], Loss: 0.47708046436309814\n",
      "Train: Epoch [23], Batch [323/938], Loss: 0.38558289408683777\n",
      "Train: Epoch [23], Batch [324/938], Loss: 0.27909478545188904\n",
      "Train: Epoch [23], Batch [325/938], Loss: 0.24577286839485168\n",
      "Train: Epoch [23], Batch [326/938], Loss: 0.4727626442909241\n",
      "Train: Epoch [23], Batch [327/938], Loss: 0.499958872795105\n",
      "Train: Epoch [23], Batch [328/938], Loss: 0.3429569602012634\n",
      "Train: Epoch [23], Batch [329/938], Loss: 0.3859177231788635\n",
      "Train: Epoch [23], Batch [330/938], Loss: 0.25997495651245117\n",
      "Train: Epoch [23], Batch [331/938], Loss: 0.5014133453369141\n",
      "Train: Epoch [23], Batch [332/938], Loss: 0.2673872709274292\n",
      "Train: Epoch [23], Batch [333/938], Loss: 0.4036673307418823\n",
      "Train: Epoch [23], Batch [334/938], Loss: 0.4113747477531433\n",
      "Train: Epoch [23], Batch [335/938], Loss: 0.5205833911895752\n",
      "Train: Epoch [23], Batch [336/938], Loss: 0.3469805121421814\n",
      "Train: Epoch [23], Batch [337/938], Loss: 0.2846212685108185\n",
      "Train: Epoch [23], Batch [338/938], Loss: 0.3810030221939087\n",
      "Train: Epoch [23], Batch [339/938], Loss: 0.417671263217926\n",
      "Train: Epoch [23], Batch [340/938], Loss: 0.3757212162017822\n",
      "Train: Epoch [23], Batch [341/938], Loss: 0.4739610552787781\n",
      "Train: Epoch [23], Batch [342/938], Loss: 0.47116217017173767\n",
      "Train: Epoch [23], Batch [343/938], Loss: 0.4573623538017273\n",
      "Train: Epoch [23], Batch [344/938], Loss: 0.25561708211898804\n",
      "Train: Epoch [23], Batch [345/938], Loss: 0.4553234875202179\n",
      "Train: Epoch [23], Batch [346/938], Loss: 0.27432045340538025\n",
      "Train: Epoch [23], Batch [347/938], Loss: 0.26382195949554443\n",
      "Train: Epoch [23], Batch [348/938], Loss: 0.2814486026763916\n",
      "Train: Epoch [23], Batch [349/938], Loss: 0.5378254652023315\n",
      "Train: Epoch [23], Batch [350/938], Loss: 0.4647996127605438\n",
      "Train: Epoch [23], Batch [351/938], Loss: 0.327646940946579\n",
      "Train: Epoch [23], Batch [352/938], Loss: 0.2664717435836792\n",
      "Train: Epoch [23], Batch [353/938], Loss: 0.4980989396572113\n",
      "Train: Epoch [23], Batch [354/938], Loss: 0.4005947411060333\n",
      "Train: Epoch [23], Batch [355/938], Loss: 0.3652762770652771\n",
      "Train: Epoch [23], Batch [356/938], Loss: 0.555152177810669\n",
      "Train: Epoch [23], Batch [357/938], Loss: 0.2989594638347626\n",
      "Train: Epoch [23], Batch [358/938], Loss: 0.37655583024024963\n",
      "Train: Epoch [23], Batch [359/938], Loss: 0.377610981464386\n",
      "Train: Epoch [23], Batch [360/938], Loss: 0.4307047128677368\n",
      "Train: Epoch [23], Batch [361/938], Loss: 0.35382550954818726\n",
      "Train: Epoch [23], Batch [362/938], Loss: 0.43733489513397217\n",
      "Train: Epoch [23], Batch [363/938], Loss: 0.2721012234687805\n",
      "Train: Epoch [23], Batch [364/938], Loss: 0.5533803701400757\n",
      "Train: Epoch [23], Batch [365/938], Loss: 0.44762903451919556\n",
      "Train: Epoch [23], Batch [366/938], Loss: 0.35781195759773254\n",
      "Train: Epoch [23], Batch [367/938], Loss: 0.3816612958908081\n",
      "Train: Epoch [23], Batch [368/938], Loss: 0.7137316465377808\n",
      "Train: Epoch [23], Batch [369/938], Loss: 0.45734626054763794\n",
      "Train: Epoch [23], Batch [370/938], Loss: 0.4271981716156006\n",
      "Train: Epoch [23], Batch [371/938], Loss: 0.33669260144233704\n",
      "Train: Epoch [23], Batch [372/938], Loss: 0.308572381734848\n",
      "Train: Epoch [23], Batch [373/938], Loss: 0.2860415279865265\n",
      "Train: Epoch [23], Batch [374/938], Loss: 0.3874748647212982\n",
      "Train: Epoch [23], Batch [375/938], Loss: 0.27670034766197205\n",
      "Train: Epoch [23], Batch [376/938], Loss: 0.6252813339233398\n",
      "Train: Epoch [23], Batch [377/938], Loss: 0.5199604034423828\n",
      "Train: Epoch [23], Batch [378/938], Loss: 0.3161631226539612\n",
      "Train: Epoch [23], Batch [379/938], Loss: 0.33093389868736267\n",
      "Train: Epoch [23], Batch [380/938], Loss: 0.22878360748291016\n",
      "Train: Epoch [23], Batch [381/938], Loss: 0.4688663184642792\n",
      "Train: Epoch [23], Batch [382/938], Loss: 0.32930970191955566\n",
      "Train: Epoch [23], Batch [383/938], Loss: 0.3063826858997345\n",
      "Train: Epoch [23], Batch [384/938], Loss: 0.3896409273147583\n",
      "Train: Epoch [23], Batch [385/938], Loss: 0.28043612837791443\n",
      "Train: Epoch [23], Batch [386/938], Loss: 0.5183990001678467\n",
      "Train: Epoch [23], Batch [387/938], Loss: 0.3366530239582062\n",
      "Train: Epoch [23], Batch [388/938], Loss: 0.6154276132583618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [23], Batch [389/938], Loss: 0.2610783278942108\n",
      "Train: Epoch [23], Batch [390/938], Loss: 0.3538828492164612\n",
      "Train: Epoch [23], Batch [391/938], Loss: 0.35538554191589355\n",
      "Train: Epoch [23], Batch [392/938], Loss: 0.35195326805114746\n",
      "Train: Epoch [23], Batch [393/938], Loss: 0.46123406291007996\n",
      "Train: Epoch [23], Batch [394/938], Loss: 0.4070853888988495\n",
      "Train: Epoch [23], Batch [395/938], Loss: 0.34239262342453003\n",
      "Train: Epoch [23], Batch [396/938], Loss: 0.37082767486572266\n",
      "Train: Epoch [23], Batch [397/938], Loss: 0.4630509614944458\n",
      "Train: Epoch [23], Batch [398/938], Loss: 0.5360065698623657\n",
      "Train: Epoch [23], Batch [399/938], Loss: 0.4565046727657318\n",
      "Train: Epoch [23], Batch [400/938], Loss: 0.2982913851737976\n",
      "Train: Epoch [23], Batch [401/938], Loss: 0.5431997179985046\n",
      "Train: Epoch [23], Batch [402/938], Loss: 0.41985028982162476\n",
      "Train: Epoch [23], Batch [403/938], Loss: 0.279663622379303\n",
      "Train: Epoch [23], Batch [404/938], Loss: 0.37867313623428345\n",
      "Train: Epoch [23], Batch [405/938], Loss: 0.4003782868385315\n",
      "Train: Epoch [23], Batch [406/938], Loss: 0.5164774060249329\n",
      "Train: Epoch [23], Batch [407/938], Loss: 0.2855364680290222\n",
      "Train: Epoch [23], Batch [408/938], Loss: 0.3232976198196411\n",
      "Train: Epoch [23], Batch [409/938], Loss: 0.1999739706516266\n",
      "Train: Epoch [23], Batch [410/938], Loss: 0.49885252118110657\n",
      "Train: Epoch [23], Batch [411/938], Loss: 0.41445276141166687\n",
      "Train: Epoch [23], Batch [412/938], Loss: 0.3497302532196045\n",
      "Train: Epoch [23], Batch [413/938], Loss: 0.43695515394210815\n",
      "Train: Epoch [23], Batch [414/938], Loss: 0.44406741857528687\n",
      "Train: Epoch [23], Batch [415/938], Loss: 0.24738721549510956\n",
      "Train: Epoch [23], Batch [416/938], Loss: 0.5282944440841675\n",
      "Train: Epoch [23], Batch [417/938], Loss: 0.2732611298561096\n",
      "Train: Epoch [23], Batch [418/938], Loss: 0.5624183416366577\n",
      "Train: Epoch [23], Batch [419/938], Loss: 0.34560108184814453\n",
      "Train: Epoch [23], Batch [420/938], Loss: 0.27260738611221313\n",
      "Train: Epoch [23], Batch [421/938], Loss: 0.5547258257865906\n",
      "Train: Epoch [23], Batch [422/938], Loss: 0.36293020844459534\n",
      "Train: Epoch [23], Batch [423/938], Loss: 0.3609178960323334\n",
      "Train: Epoch [23], Batch [424/938], Loss: 0.4805196523666382\n",
      "Train: Epoch [23], Batch [425/938], Loss: 0.3304992914199829\n",
      "Train: Epoch [23], Batch [426/938], Loss: 0.423525333404541\n",
      "Train: Epoch [23], Batch [427/938], Loss: 0.39864474534988403\n",
      "Train: Epoch [23], Batch [428/938], Loss: 0.3191889524459839\n",
      "Train: Epoch [23], Batch [429/938], Loss: 0.5823927521705627\n",
      "Train: Epoch [23], Batch [430/938], Loss: 0.2703896760940552\n",
      "Train: Epoch [23], Batch [431/938], Loss: 0.49081626534461975\n",
      "Train: Epoch [23], Batch [432/938], Loss: 0.2966287136077881\n",
      "Train: Epoch [23], Batch [433/938], Loss: 0.2891385555267334\n",
      "Train: Epoch [23], Batch [434/938], Loss: 0.5320675373077393\n",
      "Train: Epoch [23], Batch [435/938], Loss: 0.21689563989639282\n",
      "Train: Epoch [23], Batch [436/938], Loss: 0.4946605861186981\n",
      "Train: Epoch [23], Batch [437/938], Loss: 0.25643253326416016\n",
      "Train: Epoch [23], Batch [438/938], Loss: 0.4337586760520935\n",
      "Train: Epoch [23], Batch [439/938], Loss: 0.3974994421005249\n",
      "Train: Epoch [23], Batch [440/938], Loss: 0.33208855986595154\n",
      "Train: Epoch [23], Batch [441/938], Loss: 0.5171948671340942\n",
      "Train: Epoch [23], Batch [442/938], Loss: 0.4616565406322479\n",
      "Train: Epoch [23], Batch [443/938], Loss: 0.3951380252838135\n",
      "Train: Epoch [23], Batch [444/938], Loss: 0.47293245792388916\n",
      "Train: Epoch [23], Batch [445/938], Loss: 0.39679497480392456\n",
      "Train: Epoch [23], Batch [446/938], Loss: 0.4319663643836975\n",
      "Train: Epoch [23], Batch [447/938], Loss: 0.5935792922973633\n",
      "Train: Epoch [23], Batch [448/938], Loss: 0.30519193410873413\n",
      "Train: Epoch [23], Batch [449/938], Loss: 0.43644800782203674\n",
      "Train: Epoch [23], Batch [450/938], Loss: 0.3643837869167328\n",
      "Train: Epoch [23], Batch [451/938], Loss: 0.3970479369163513\n",
      "Train: Epoch [23], Batch [452/938], Loss: 0.3852454423904419\n",
      "Train: Epoch [23], Batch [453/938], Loss: 0.34094005823135376\n",
      "Train: Epoch [23], Batch [454/938], Loss: 0.4699469804763794\n",
      "Train: Epoch [23], Batch [455/938], Loss: 0.2679661810398102\n",
      "Train: Epoch [23], Batch [456/938], Loss: 0.2879118323326111\n",
      "Train: Epoch [23], Batch [457/938], Loss: 0.5796256065368652\n",
      "Train: Epoch [23], Batch [458/938], Loss: 0.6450704336166382\n",
      "Train: Epoch [23], Batch [459/938], Loss: 0.3399670720100403\n",
      "Train: Epoch [23], Batch [460/938], Loss: 0.38013410568237305\n",
      "Train: Epoch [23], Batch [461/938], Loss: 0.49010834097862244\n",
      "Train: Epoch [23], Batch [462/938], Loss: 0.49405595660209656\n",
      "Train: Epoch [23], Batch [463/938], Loss: 0.37795841693878174\n",
      "Train: Epoch [23], Batch [464/938], Loss: 0.3078564703464508\n",
      "Train: Epoch [23], Batch [465/938], Loss: 0.24156558513641357\n",
      "Train: Epoch [23], Batch [466/938], Loss: 0.4230906665325165\n",
      "Train: Epoch [23], Batch [467/938], Loss: 0.668698787689209\n",
      "Train: Epoch [23], Batch [468/938], Loss: 0.49422740936279297\n",
      "Train: Epoch [23], Batch [469/938], Loss: 0.34487512707710266\n",
      "Train: Epoch [23], Batch [470/938], Loss: 0.42764174938201904\n",
      "Train: Epoch [23], Batch [471/938], Loss: 0.3319258689880371\n",
      "Train: Epoch [23], Batch [472/938], Loss: 0.4278814196586609\n",
      "Train: Epoch [23], Batch [473/938], Loss: 0.3836338222026825\n",
      "Train: Epoch [23], Batch [474/938], Loss: 0.4873325228691101\n",
      "Train: Epoch [23], Batch [475/938], Loss: 0.46361708641052246\n",
      "Train: Epoch [23], Batch [476/938], Loss: 0.25870949029922485\n",
      "Train: Epoch [23], Batch [477/938], Loss: 0.3899520933628082\n",
      "Train: Epoch [23], Batch [478/938], Loss: 0.4521424472332001\n",
      "Train: Epoch [23], Batch [479/938], Loss: 0.3938257694244385\n",
      "Train: Epoch [23], Batch [480/938], Loss: 0.5789456963539124\n",
      "Train: Epoch [23], Batch [481/938], Loss: 0.3897303342819214\n",
      "Train: Epoch [23], Batch [482/938], Loss: 0.4343729615211487\n",
      "Train: Epoch [23], Batch [483/938], Loss: 0.44338253140449524\n",
      "Train: Epoch [23], Batch [484/938], Loss: 0.5617215633392334\n",
      "Train: Epoch [23], Batch [485/938], Loss: 0.4568154215812683\n",
      "Train: Epoch [23], Batch [486/938], Loss: 0.5282638072967529\n",
      "Train: Epoch [23], Batch [487/938], Loss: 0.29259243607521057\n",
      "Train: Epoch [23], Batch [488/938], Loss: 0.2690277099609375\n",
      "Train: Epoch [23], Batch [489/938], Loss: 0.2881734371185303\n",
      "Train: Epoch [23], Batch [490/938], Loss: 0.2802514433860779\n",
      "Train: Epoch [23], Batch [491/938], Loss: 0.5122435092926025\n",
      "Train: Epoch [23], Batch [492/938], Loss: 0.4310855269432068\n",
      "Train: Epoch [23], Batch [493/938], Loss: 0.4279564321041107\n",
      "Train: Epoch [23], Batch [494/938], Loss: 0.2946951389312744\n",
      "Train: Epoch [23], Batch [495/938], Loss: 0.46039605140686035\n",
      "Train: Epoch [23], Batch [496/938], Loss: 0.39775586128234863\n",
      "Train: Epoch [23], Batch [497/938], Loss: 0.23363901674747467\n",
      "Train: Epoch [23], Batch [498/938], Loss: 0.33099624514579773\n",
      "Train: Epoch [23], Batch [499/938], Loss: 0.34072554111480713\n",
      "Train: Epoch [23], Batch [500/938], Loss: 0.35030943155288696\n",
      "Train: Epoch [23], Batch [501/938], Loss: 0.599152684211731\n",
      "Train: Epoch [23], Batch [502/938], Loss: 0.42091888189315796\n",
      "Train: Epoch [23], Batch [503/938], Loss: 0.4963837265968323\n",
      "Train: Epoch [23], Batch [504/938], Loss: 0.4570368528366089\n",
      "Train: Epoch [23], Batch [505/938], Loss: 0.4971824586391449\n",
      "Train: Epoch [23], Batch [506/938], Loss: 0.44641849398612976\n",
      "Train: Epoch [23], Batch [507/938], Loss: 0.3899499773979187\n",
      "Train: Epoch [23], Batch [508/938], Loss: 0.4732442796230316\n",
      "Train: Epoch [23], Batch [509/938], Loss: 0.7133241891860962\n",
      "Train: Epoch [23], Batch [510/938], Loss: 0.37143051624298096\n",
      "Train: Epoch [23], Batch [511/938], Loss: 0.21158048510551453\n",
      "Train: Epoch [23], Batch [512/938], Loss: 0.529437243938446\n",
      "Train: Epoch [23], Batch [513/938], Loss: 0.4375361204147339\n",
      "Train: Epoch [23], Batch [514/938], Loss: 0.3440714478492737\n",
      "Train: Epoch [23], Batch [515/938], Loss: 0.2762894034385681\n",
      "Train: Epoch [23], Batch [516/938], Loss: 0.5097600817680359\n",
      "Train: Epoch [23], Batch [517/938], Loss: 0.42558789253234863\n",
      "Train: Epoch [23], Batch [518/938], Loss: 0.2813307046890259\n",
      "Train: Epoch [23], Batch [519/938], Loss: 0.46704185009002686\n",
      "Train: Epoch [23], Batch [520/938], Loss: 0.34579846262931824\n",
      "Train: Epoch [23], Batch [521/938], Loss: 0.49481886625289917\n",
      "Train: Epoch [23], Batch [522/938], Loss: 0.3399909734725952\n",
      "Train: Epoch [23], Batch [523/938], Loss: 0.32615697383880615\n",
      "Train: Epoch [23], Batch [524/938], Loss: 0.26416537165641785\n",
      "Train: Epoch [23], Batch [525/938], Loss: 0.3975152373313904\n",
      "Train: Epoch [23], Batch [526/938], Loss: 0.3352017104625702\n",
      "Train: Epoch [23], Batch [527/938], Loss: 0.4090074896812439\n",
      "Train: Epoch [23], Batch [528/938], Loss: 0.513493537902832\n",
      "Train: Epoch [23], Batch [529/938], Loss: 0.277259886264801\n",
      "Train: Epoch [23], Batch [530/938], Loss: 0.35462459921836853\n",
      "Train: Epoch [23], Batch [531/938], Loss: 0.5137581825256348\n",
      "Train: Epoch [23], Batch [532/938], Loss: 0.44692477583885193\n",
      "Train: Epoch [23], Batch [533/938], Loss: 0.2671927511692047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [23], Batch [534/938], Loss: 0.3506247401237488\n",
      "Train: Epoch [23], Batch [535/938], Loss: 0.4102495312690735\n",
      "Train: Epoch [23], Batch [536/938], Loss: 0.3153539001941681\n",
      "Train: Epoch [23], Batch [537/938], Loss: 0.5157934427261353\n",
      "Train: Epoch [23], Batch [538/938], Loss: 0.32582148909568787\n",
      "Train: Epoch [23], Batch [539/938], Loss: 0.4209112524986267\n",
      "Train: Epoch [23], Batch [540/938], Loss: 0.34124183654785156\n",
      "Train: Epoch [23], Batch [541/938], Loss: 0.37014657258987427\n",
      "Train: Epoch [23], Batch [542/938], Loss: 0.2293911725282669\n",
      "Train: Epoch [23], Batch [543/938], Loss: 0.38702821731567383\n",
      "Train: Epoch [23], Batch [544/938], Loss: 0.3472574055194855\n",
      "Train: Epoch [23], Batch [545/938], Loss: 0.47227680683135986\n",
      "Train: Epoch [23], Batch [546/938], Loss: 0.4391931891441345\n",
      "Train: Epoch [23], Batch [547/938], Loss: 0.3669767379760742\n",
      "Train: Epoch [23], Batch [548/938], Loss: 0.4175736904144287\n",
      "Train: Epoch [23], Batch [549/938], Loss: 0.21202579140663147\n",
      "Train: Epoch [23], Batch [550/938], Loss: 0.43230369687080383\n",
      "Train: Epoch [23], Batch [551/938], Loss: 0.2894322872161865\n",
      "Train: Epoch [23], Batch [552/938], Loss: 0.46541333198547363\n",
      "Train: Epoch [23], Batch [553/938], Loss: 0.306152880191803\n",
      "Train: Epoch [23], Batch [554/938], Loss: 0.22146272659301758\n",
      "Train: Epoch [23], Batch [555/938], Loss: 0.4262758493423462\n",
      "Train: Epoch [23], Batch [556/938], Loss: 0.37945201992988586\n",
      "Train: Epoch [23], Batch [557/938], Loss: 0.35211917757987976\n",
      "Train: Epoch [23], Batch [558/938], Loss: 0.2802239656448364\n",
      "Train: Epoch [23], Batch [559/938], Loss: 0.4950389564037323\n",
      "Train: Epoch [23], Batch [560/938], Loss: 0.5193220376968384\n",
      "Train: Epoch [23], Batch [561/938], Loss: 0.4085143804550171\n",
      "Train: Epoch [23], Batch [562/938], Loss: 0.40096315741539\n",
      "Train: Epoch [23], Batch [563/938], Loss: 0.3797799348831177\n",
      "Train: Epoch [23], Batch [564/938], Loss: 0.37495267391204834\n",
      "Train: Epoch [23], Batch [565/938], Loss: 0.24253013730049133\n",
      "Train: Epoch [23], Batch [566/938], Loss: 0.3929446339607239\n",
      "Train: Epoch [23], Batch [567/938], Loss: 0.29243728518486023\n",
      "Train: Epoch [23], Batch [568/938], Loss: 0.4300350546836853\n",
      "Train: Epoch [23], Batch [569/938], Loss: 0.6246058940887451\n",
      "Train: Epoch [23], Batch [570/938], Loss: 0.34254515171051025\n",
      "Train: Epoch [23], Batch [571/938], Loss: 0.4561702311038971\n",
      "Train: Epoch [23], Batch [572/938], Loss: 0.2590422034263611\n",
      "Train: Epoch [23], Batch [573/938], Loss: 0.4280320405960083\n",
      "Train: Epoch [23], Batch [574/938], Loss: 0.569077730178833\n",
      "Train: Epoch [23], Batch [575/938], Loss: 0.32106825709342957\n",
      "Train: Epoch [23], Batch [576/938], Loss: 0.495143324136734\n",
      "Train: Epoch [23], Batch [577/938], Loss: 0.28017905354499817\n",
      "Train: Epoch [23], Batch [578/938], Loss: 0.3909919261932373\n",
      "Train: Epoch [23], Batch [579/938], Loss: 0.4317505657672882\n",
      "Train: Epoch [23], Batch [580/938], Loss: 0.35184338688850403\n",
      "Train: Epoch [23], Batch [581/938], Loss: 0.34971973299980164\n",
      "Train: Epoch [23], Batch [582/938], Loss: 0.5216866135597229\n",
      "Train: Epoch [23], Batch [583/938], Loss: 0.3034672737121582\n",
      "Train: Epoch [23], Batch [584/938], Loss: 0.2948664128780365\n",
      "Train: Epoch [23], Batch [585/938], Loss: 0.3292228877544403\n",
      "Train: Epoch [23], Batch [586/938], Loss: 0.3412033021450043\n",
      "Train: Epoch [23], Batch [587/938], Loss: 0.4031364619731903\n",
      "Train: Epoch [23], Batch [588/938], Loss: 0.4645051658153534\n",
      "Train: Epoch [23], Batch [589/938], Loss: 0.3274443745613098\n",
      "Train: Epoch [23], Batch [590/938], Loss: 0.2618410587310791\n",
      "Train: Epoch [23], Batch [591/938], Loss: 0.3064345419406891\n",
      "Train: Epoch [23], Batch [592/938], Loss: 0.3109492063522339\n",
      "Train: Epoch [23], Batch [593/938], Loss: 0.3035559058189392\n",
      "Train: Epoch [23], Batch [594/938], Loss: 0.34852612018585205\n",
      "Train: Epoch [23], Batch [595/938], Loss: 0.2684020698070526\n",
      "Train: Epoch [23], Batch [596/938], Loss: 0.5977395176887512\n",
      "Train: Epoch [23], Batch [597/938], Loss: 0.4419707655906677\n",
      "Train: Epoch [23], Batch [598/938], Loss: 0.28402334451675415\n",
      "Train: Epoch [23], Batch [599/938], Loss: 0.5009689331054688\n",
      "Train: Epoch [23], Batch [600/938], Loss: 0.42831137776374817\n",
      "Train: Epoch [23], Batch [601/938], Loss: 0.3018968999385834\n",
      "Train: Epoch [23], Batch [602/938], Loss: 0.4242027997970581\n",
      "Train: Epoch [23], Batch [603/938], Loss: 0.3206707835197449\n",
      "Train: Epoch [23], Batch [604/938], Loss: 0.44704771041870117\n",
      "Train: Epoch [23], Batch [605/938], Loss: 0.24151161313056946\n",
      "Train: Epoch [23], Batch [606/938], Loss: 0.3157449960708618\n",
      "Train: Epoch [23], Batch [607/938], Loss: 0.40358078479766846\n",
      "Train: Epoch [23], Batch [608/938], Loss: 0.36125367879867554\n",
      "Train: Epoch [23], Batch [609/938], Loss: 0.3355884850025177\n",
      "Train: Epoch [23], Batch [610/938], Loss: 0.3591287434101105\n",
      "Train: Epoch [23], Batch [611/938], Loss: 0.41071003675460815\n",
      "Train: Epoch [23], Batch [612/938], Loss: 0.28290191292762756\n",
      "Train: Epoch [23], Batch [613/938], Loss: 0.1981145441532135\n",
      "Train: Epoch [23], Batch [614/938], Loss: 0.28783831000328064\n",
      "Train: Epoch [23], Batch [615/938], Loss: 0.35437479615211487\n",
      "Train: Epoch [23], Batch [616/938], Loss: 0.36327290534973145\n",
      "Train: Epoch [23], Batch [617/938], Loss: 0.4786466062068939\n",
      "Train: Epoch [23], Batch [618/938], Loss: 0.3888223171234131\n",
      "Train: Epoch [23], Batch [619/938], Loss: 0.2416492998600006\n",
      "Train: Epoch [23], Batch [620/938], Loss: 0.5735390186309814\n",
      "Train: Epoch [23], Batch [621/938], Loss: 0.3320932388305664\n",
      "Train: Epoch [23], Batch [622/938], Loss: 0.5543808937072754\n",
      "Train: Epoch [23], Batch [623/938], Loss: 0.3990522623062134\n",
      "Train: Epoch [23], Batch [624/938], Loss: 0.3004496097564697\n",
      "Train: Epoch [23], Batch [625/938], Loss: 0.4961596429347992\n",
      "Train: Epoch [23], Batch [626/938], Loss: 0.3432164788246155\n",
      "Train: Epoch [23], Batch [627/938], Loss: 0.541716992855072\n",
      "Train: Epoch [23], Batch [628/938], Loss: 0.24423199892044067\n",
      "Train: Epoch [23], Batch [629/938], Loss: 0.3594481945037842\n",
      "Train: Epoch [23], Batch [630/938], Loss: 0.39852777123451233\n",
      "Train: Epoch [23], Batch [631/938], Loss: 0.42220693826675415\n",
      "Train: Epoch [23], Batch [632/938], Loss: 0.5457898378372192\n",
      "Train: Epoch [23], Batch [633/938], Loss: 0.4624773859977722\n",
      "Train: Epoch [23], Batch [634/938], Loss: 0.259185254573822\n",
      "Train: Epoch [23], Batch [635/938], Loss: 0.479187548160553\n",
      "Train: Epoch [23], Batch [636/938], Loss: 0.32421767711639404\n",
      "Train: Epoch [23], Batch [637/938], Loss: 0.2871795892715454\n",
      "Train: Epoch [23], Batch [638/938], Loss: 0.317486047744751\n",
      "Train: Epoch [23], Batch [639/938], Loss: 0.3252568244934082\n",
      "Train: Epoch [23], Batch [640/938], Loss: 0.5095451474189758\n",
      "Train: Epoch [23], Batch [641/938], Loss: 0.34353235363960266\n",
      "Train: Epoch [23], Batch [642/938], Loss: 0.3150816559791565\n",
      "Train: Epoch [23], Batch [643/938], Loss: 0.42563948035240173\n",
      "Train: Epoch [23], Batch [644/938], Loss: 0.3924572765827179\n",
      "Train: Epoch [23], Batch [645/938], Loss: 0.3971019983291626\n",
      "Train: Epoch [23], Batch [646/938], Loss: 0.4561437666416168\n",
      "Train: Epoch [23], Batch [647/938], Loss: 0.463074266910553\n",
      "Train: Epoch [23], Batch [648/938], Loss: 0.38390660285949707\n",
      "Train: Epoch [23], Batch [649/938], Loss: 0.4239517152309418\n",
      "Train: Epoch [23], Batch [650/938], Loss: 0.416512131690979\n",
      "Train: Epoch [23], Batch [651/938], Loss: 0.3295047879219055\n",
      "Train: Epoch [23], Batch [652/938], Loss: 0.39421164989471436\n",
      "Train: Epoch [23], Batch [653/938], Loss: 0.4560113251209259\n",
      "Train: Epoch [23], Batch [654/938], Loss: 0.4188654124736786\n",
      "Train: Epoch [23], Batch [655/938], Loss: 0.31143057346343994\n",
      "Train: Epoch [23], Batch [656/938], Loss: 0.3571551442146301\n",
      "Train: Epoch [23], Batch [657/938], Loss: 0.38712453842163086\n",
      "Train: Epoch [23], Batch [658/938], Loss: 0.4554120898246765\n",
      "Train: Epoch [23], Batch [659/938], Loss: 0.32435083389282227\n",
      "Train: Epoch [23], Batch [660/938], Loss: 0.4784565269947052\n",
      "Train: Epoch [23], Batch [661/938], Loss: 0.4325591027736664\n",
      "Train: Epoch [23], Batch [662/938], Loss: 0.3134610652923584\n",
      "Train: Epoch [23], Batch [663/938], Loss: 0.25900277495384216\n",
      "Train: Epoch [23], Batch [664/938], Loss: 0.3638356626033783\n",
      "Train: Epoch [23], Batch [665/938], Loss: 0.43678778409957886\n",
      "Train: Epoch [23], Batch [666/938], Loss: 0.5005168318748474\n",
      "Train: Epoch [23], Batch [667/938], Loss: 0.38785621523857117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [23], Batch [668/938], Loss: 0.28185248374938965\n",
      "Train: Epoch [23], Batch [669/938], Loss: 0.29137447476387024\n",
      "Train: Epoch [23], Batch [670/938], Loss: 0.358268141746521\n",
      "Train: Epoch [23], Batch [671/938], Loss: 0.3722614049911499\n",
      "Train: Epoch [23], Batch [672/938], Loss: 0.3533724546432495\n",
      "Train: Epoch [23], Batch [673/938], Loss: 0.48358914256095886\n",
      "Train: Epoch [23], Batch [674/938], Loss: 0.4299241006374359\n",
      "Train: Epoch [23], Batch [675/938], Loss: 0.4655514061450958\n",
      "Train: Epoch [23], Batch [676/938], Loss: 0.1983393430709839\n",
      "Train: Epoch [23], Batch [677/938], Loss: 0.4182434380054474\n",
      "Train: Epoch [23], Batch [678/938], Loss: 0.32836124300956726\n",
      "Train: Epoch [23], Batch [679/938], Loss: 0.3537757396697998\n",
      "Train: Epoch [23], Batch [680/938], Loss: 0.31079941987991333\n",
      "Train: Epoch [23], Batch [681/938], Loss: 0.37693101167678833\n",
      "Train: Epoch [23], Batch [682/938], Loss: 0.4462783634662628\n",
      "Train: Epoch [23], Batch [683/938], Loss: 0.4181269705295563\n",
      "Train: Epoch [23], Batch [684/938], Loss: 0.49565693736076355\n",
      "Train: Epoch [23], Batch [685/938], Loss: 0.36569666862487793\n",
      "Train: Epoch [23], Batch [686/938], Loss: 0.32340922951698303\n",
      "Train: Epoch [23], Batch [687/938], Loss: 0.4183950424194336\n",
      "Train: Epoch [23], Batch [688/938], Loss: 0.4719800353050232\n",
      "Train: Epoch [23], Batch [689/938], Loss: 0.2875581979751587\n",
      "Train: Epoch [23], Batch [690/938], Loss: 0.3966180086135864\n",
      "Train: Epoch [23], Batch [691/938], Loss: 0.4504663646221161\n",
      "Train: Epoch [23], Batch [692/938], Loss: 0.5821364521980286\n",
      "Train: Epoch [23], Batch [693/938], Loss: 0.31645667552948\n",
      "Train: Epoch [23], Batch [694/938], Loss: 0.4283939003944397\n",
      "Train: Epoch [23], Batch [695/938], Loss: 0.6044360995292664\n",
      "Train: Epoch [23], Batch [696/938], Loss: 0.5566728115081787\n",
      "Train: Epoch [23], Batch [697/938], Loss: 0.4138125479221344\n",
      "Train: Epoch [23], Batch [698/938], Loss: 0.42415958642959595\n",
      "Train: Epoch [23], Batch [699/938], Loss: 0.34730905294418335\n",
      "Train: Epoch [23], Batch [700/938], Loss: 0.34947240352630615\n",
      "Train: Epoch [23], Batch [701/938], Loss: 0.47102969884872437\n",
      "Train: Epoch [23], Batch [702/938], Loss: 0.3781649172306061\n",
      "Train: Epoch [23], Batch [703/938], Loss: 0.5311578512191772\n",
      "Train: Epoch [23], Batch [704/938], Loss: 0.3364448547363281\n",
      "Train: Epoch [23], Batch [705/938], Loss: 0.2056645154953003\n",
      "Train: Epoch [23], Batch [706/938], Loss: 0.3632296621799469\n",
      "Train: Epoch [23], Batch [707/938], Loss: 0.5069170594215393\n",
      "Train: Epoch [23], Batch [708/938], Loss: 0.4386133551597595\n",
      "Train: Epoch [23], Batch [709/938], Loss: 0.5407636761665344\n",
      "Train: Epoch [23], Batch [710/938], Loss: 0.5837596654891968\n",
      "Train: Epoch [23], Batch [711/938], Loss: 0.784587025642395\n",
      "Train: Epoch [23], Batch [712/938], Loss: 0.31879162788391113\n",
      "Train: Epoch [23], Batch [713/938], Loss: 0.29736727476119995\n",
      "Train: Epoch [23], Batch [714/938], Loss: 0.6899726390838623\n",
      "Train: Epoch [23], Batch [715/938], Loss: 0.45182955265045166\n",
      "Train: Epoch [23], Batch [716/938], Loss: 0.2838920056819916\n",
      "Train: Epoch [23], Batch [717/938], Loss: 0.23980852961540222\n",
      "Train: Epoch [23], Batch [718/938], Loss: 0.26050370931625366\n",
      "Train: Epoch [23], Batch [719/938], Loss: 0.34771913290023804\n",
      "Train: Epoch [23], Batch [720/938], Loss: 0.5543736219406128\n",
      "Train: Epoch [23], Batch [721/938], Loss: 0.35013899207115173\n",
      "Train: Epoch [23], Batch [722/938], Loss: 0.23823919892311096\n",
      "Train: Epoch [23], Batch [723/938], Loss: 0.3222754895687103\n",
      "Train: Epoch [23], Batch [724/938], Loss: 0.34031859040260315\n",
      "Train: Epoch [23], Batch [725/938], Loss: 0.3179742395877838\n",
      "Train: Epoch [23], Batch [726/938], Loss: 0.3095892667770386\n",
      "Train: Epoch [23], Batch [727/938], Loss: 0.38610902428627014\n",
      "Train: Epoch [23], Batch [728/938], Loss: 0.2988724112510681\n",
      "Train: Epoch [23], Batch [729/938], Loss: 0.35050663352012634\n",
      "Train: Epoch [23], Batch [730/938], Loss: 0.3084027171134949\n",
      "Train: Epoch [23], Batch [731/938], Loss: 0.349856436252594\n",
      "Train: Epoch [23], Batch [732/938], Loss: 0.24411113560199738\n",
      "Train: Epoch [23], Batch [733/938], Loss: 0.5061641931533813\n",
      "Train: Epoch [23], Batch [734/938], Loss: 0.23088473081588745\n",
      "Train: Epoch [23], Batch [735/938], Loss: 0.5400375127792358\n",
      "Train: Epoch [23], Batch [736/938], Loss: 0.4727661609649658\n",
      "Train: Epoch [23], Batch [737/938], Loss: 0.4530197083950043\n",
      "Train: Epoch [23], Batch [738/938], Loss: 0.301774799823761\n",
      "Train: Epoch [23], Batch [739/938], Loss: 0.36311525106430054\n",
      "Train: Epoch [23], Batch [740/938], Loss: 0.6068694591522217\n",
      "Train: Epoch [23], Batch [741/938], Loss: 0.2896246910095215\n",
      "Train: Epoch [23], Batch [742/938], Loss: 0.3998880684375763\n",
      "Train: Epoch [23], Batch [743/938], Loss: 0.2950074076652527\n",
      "Train: Epoch [23], Batch [744/938], Loss: 0.3556138277053833\n",
      "Train: Epoch [23], Batch [745/938], Loss: 0.577212393283844\n",
      "Train: Epoch [23], Batch [746/938], Loss: 0.36306121945381165\n",
      "Train: Epoch [23], Batch [747/938], Loss: 0.4415952265262604\n",
      "Train: Epoch [23], Batch [748/938], Loss: 0.7390274405479431\n",
      "Train: Epoch [23], Batch [749/938], Loss: 0.4160199761390686\n",
      "Train: Epoch [23], Batch [750/938], Loss: 0.494778037071228\n",
      "Train: Epoch [23], Batch [751/938], Loss: 0.5927729606628418\n",
      "Train: Epoch [23], Batch [752/938], Loss: 0.4482623338699341\n",
      "Train: Epoch [23], Batch [753/938], Loss: 0.37727344036102295\n",
      "Train: Epoch [23], Batch [754/938], Loss: 0.24887673556804657\n",
      "Train: Epoch [23], Batch [755/938], Loss: 0.4056214392185211\n",
      "Train: Epoch [23], Batch [756/938], Loss: 0.4104439616203308\n",
      "Train: Epoch [23], Batch [757/938], Loss: 0.30244553089141846\n",
      "Train: Epoch [23], Batch [758/938], Loss: 0.39294305443763733\n",
      "Train: Epoch [23], Batch [759/938], Loss: 0.45996683835983276\n",
      "Train: Epoch [23], Batch [760/938], Loss: 0.2641342580318451\n",
      "Train: Epoch [23], Batch [761/938], Loss: 0.3246545195579529\n",
      "Train: Epoch [23], Batch [762/938], Loss: 0.4476211369037628\n",
      "Train: Epoch [23], Batch [763/938], Loss: 0.32849329710006714\n",
      "Train: Epoch [23], Batch [764/938], Loss: 0.5402106642723083\n",
      "Train: Epoch [23], Batch [765/938], Loss: 0.4177302122116089\n",
      "Train: Epoch [23], Batch [766/938], Loss: 0.48468565940856934\n",
      "Train: Epoch [23], Batch [767/938], Loss: 0.25373101234436035\n",
      "Train: Epoch [23], Batch [768/938], Loss: 0.49881380796432495\n",
      "Train: Epoch [23], Batch [769/938], Loss: 0.22984403371810913\n",
      "Train: Epoch [23], Batch [770/938], Loss: 0.2902839183807373\n",
      "Train: Epoch [23], Batch [771/938], Loss: 0.31988194584846497\n",
      "Train: Epoch [23], Batch [772/938], Loss: 0.49470049142837524\n",
      "Train: Epoch [23], Batch [773/938], Loss: 0.559249758720398\n",
      "Train: Epoch [23], Batch [774/938], Loss: 0.39825236797332764\n",
      "Train: Epoch [23], Batch [775/938], Loss: 0.4601503908634186\n",
      "Train: Epoch [23], Batch [776/938], Loss: 0.5162478685379028\n",
      "Train: Epoch [23], Batch [777/938], Loss: 0.3274582624435425\n",
      "Train: Epoch [23], Batch [778/938], Loss: 0.2551255226135254\n",
      "Train: Epoch [23], Batch [779/938], Loss: 0.3010323941707611\n",
      "Train: Epoch [23], Batch [780/938], Loss: 0.5090669393539429\n",
      "Train: Epoch [23], Batch [781/938], Loss: 0.5370061993598938\n",
      "Train: Epoch [23], Batch [782/938], Loss: 0.36564671993255615\n",
      "Train: Epoch [23], Batch [783/938], Loss: 0.41763898730278015\n",
      "Train: Epoch [23], Batch [784/938], Loss: 0.40651872754096985\n",
      "Train: Epoch [23], Batch [785/938], Loss: 0.2964921295642853\n",
      "Train: Epoch [23], Batch [786/938], Loss: 0.3004118800163269\n",
      "Train: Epoch [23], Batch [787/938], Loss: 0.3295924961566925\n",
      "Train: Epoch [23], Batch [788/938], Loss: 0.4901202917098999\n",
      "Train: Epoch [23], Batch [789/938], Loss: 0.5390112996101379\n",
      "Train: Epoch [23], Batch [790/938], Loss: 0.32790476083755493\n",
      "Train: Epoch [23], Batch [791/938], Loss: 0.3407021760940552\n",
      "Train: Epoch [23], Batch [792/938], Loss: 0.15793868899345398\n",
      "Train: Epoch [23], Batch [793/938], Loss: 0.35869574546813965\n",
      "Train: Epoch [23], Batch [794/938], Loss: 0.24677270650863647\n",
      "Train: Epoch [23], Batch [795/938], Loss: 0.35546165704727173\n",
      "Train: Epoch [23], Batch [796/938], Loss: 0.3598855137825012\n",
      "Train: Epoch [23], Batch [797/938], Loss: 0.2975519299507141\n",
      "Train: Epoch [23], Batch [798/938], Loss: 0.5985023975372314\n",
      "Train: Epoch [23], Batch [799/938], Loss: 0.3742032051086426\n",
      "Train: Epoch [23], Batch [800/938], Loss: 0.3592909574508667\n",
      "Train: Epoch [23], Batch [801/938], Loss: 0.41370439529418945\n",
      "Train: Epoch [23], Batch [802/938], Loss: 0.43739888072013855\n",
      "Train: Epoch [23], Batch [803/938], Loss: 0.3886912763118744\n",
      "Train: Epoch [23], Batch [804/938], Loss: 0.24522197246551514\n",
      "Train: Epoch [23], Batch [805/938], Loss: 0.4270458519458771\n",
      "Train: Epoch [23], Batch [806/938], Loss: 0.40721896290779114\n",
      "Train: Epoch [23], Batch [807/938], Loss: 0.4301784634590149\n",
      "Train: Epoch [23], Batch [808/938], Loss: 0.28242766857147217\n",
      "Train: Epoch [23], Batch [809/938], Loss: 0.4811446964740753\n",
      "Train: Epoch [23], Batch [810/938], Loss: 0.4452689290046692\n",
      "Train: Epoch [23], Batch [811/938], Loss: 0.39840537309646606\n",
      "Train: Epoch [23], Batch [812/938], Loss: 0.3893444836139679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [23], Batch [813/938], Loss: 0.36051246523857117\n",
      "Train: Epoch [23], Batch [814/938], Loss: 0.4125063717365265\n",
      "Train: Epoch [23], Batch [815/938], Loss: 0.3541208505630493\n",
      "Train: Epoch [23], Batch [816/938], Loss: 0.4063793420791626\n",
      "Train: Epoch [23], Batch [817/938], Loss: 0.4460200369358063\n",
      "Train: Epoch [23], Batch [818/938], Loss: 0.38988688588142395\n",
      "Train: Epoch [23], Batch [819/938], Loss: 0.5643348097801208\n",
      "Train: Epoch [23], Batch [820/938], Loss: 0.40146371722221375\n",
      "Train: Epoch [23], Batch [821/938], Loss: 0.27926257252693176\n",
      "Train: Epoch [23], Batch [822/938], Loss: 0.43287983536720276\n",
      "Train: Epoch [23], Batch [823/938], Loss: 0.41007164120674133\n",
      "Train: Epoch [23], Batch [824/938], Loss: 0.3595274090766907\n",
      "Train: Epoch [23], Batch [825/938], Loss: 0.43165236711502075\n",
      "Train: Epoch [23], Batch [826/938], Loss: 0.29970404505729675\n",
      "Train: Epoch [23], Batch [827/938], Loss: 0.40805745124816895\n",
      "Train: Epoch [23], Batch [828/938], Loss: 0.5484666228294373\n",
      "Train: Epoch [23], Batch [829/938], Loss: 0.33777517080307007\n",
      "Train: Epoch [23], Batch [830/938], Loss: 0.3311886191368103\n",
      "Train: Epoch [23], Batch [831/938], Loss: 0.5595791339874268\n",
      "Train: Epoch [23], Batch [832/938], Loss: 0.30397284030914307\n",
      "Train: Epoch [23], Batch [833/938], Loss: 0.3790629804134369\n",
      "Train: Epoch [23], Batch [834/938], Loss: 0.34930285811424255\n",
      "Train: Epoch [23], Batch [835/938], Loss: 0.4569239318370819\n",
      "Train: Epoch [23], Batch [836/938], Loss: 0.4298761487007141\n",
      "Train: Epoch [23], Batch [837/938], Loss: 0.440387099981308\n",
      "Train: Epoch [23], Batch [838/938], Loss: 0.3709893226623535\n",
      "Train: Epoch [23], Batch [839/938], Loss: 0.34648680686950684\n",
      "Train: Epoch [23], Batch [840/938], Loss: 0.2420288622379303\n",
      "Train: Epoch [23], Batch [841/938], Loss: 0.3628886938095093\n",
      "Train: Epoch [23], Batch [842/938], Loss: 0.38641566038131714\n",
      "Train: Epoch [23], Batch [843/938], Loss: 0.419950008392334\n",
      "Train: Epoch [23], Batch [844/938], Loss: 0.30437806248664856\n",
      "Train: Epoch [23], Batch [845/938], Loss: 0.30792519450187683\n",
      "Train: Epoch [23], Batch [846/938], Loss: 0.37943094968795776\n",
      "Train: Epoch [23], Batch [847/938], Loss: 0.47322195768356323\n",
      "Train: Epoch [23], Batch [848/938], Loss: 0.3026665151119232\n",
      "Train: Epoch [23], Batch [849/938], Loss: 0.27139419317245483\n",
      "Train: Epoch [23], Batch [850/938], Loss: 0.416812002658844\n",
      "Train: Epoch [23], Batch [851/938], Loss: 0.19307631254196167\n",
      "Train: Epoch [23], Batch [852/938], Loss: 0.4466603696346283\n",
      "Train: Epoch [23], Batch [853/938], Loss: 0.2706787586212158\n",
      "Train: Epoch [23], Batch [854/938], Loss: 0.5126510858535767\n",
      "Train: Epoch [23], Batch [855/938], Loss: 0.5217244625091553\n",
      "Train: Epoch [23], Batch [856/938], Loss: 0.3000852167606354\n",
      "Train: Epoch [23], Batch [857/938], Loss: 0.2462097406387329\n",
      "Train: Epoch [23], Batch [858/938], Loss: 0.5965996980667114\n",
      "Train: Epoch [23], Batch [859/938], Loss: 0.3668457865715027\n",
      "Train: Epoch [23], Batch [860/938], Loss: 0.5126323699951172\n",
      "Train: Epoch [23], Batch [861/938], Loss: 0.2057810127735138\n",
      "Train: Epoch [23], Batch [862/938], Loss: 0.3017045259475708\n",
      "Train: Epoch [23], Batch [863/938], Loss: 0.45709753036499023\n",
      "Train: Epoch [23], Batch [864/938], Loss: 0.46141767501831055\n",
      "Train: Epoch [23], Batch [865/938], Loss: 0.30912134051322937\n",
      "Train: Epoch [23], Batch [866/938], Loss: 0.3300625681877136\n",
      "Train: Epoch [23], Batch [867/938], Loss: 0.4354398846626282\n",
      "Train: Epoch [23], Batch [868/938], Loss: 0.2158716320991516\n",
      "Train: Epoch [23], Batch [869/938], Loss: 0.3368605375289917\n",
      "Train: Epoch [23], Batch [870/938], Loss: 0.29932504892349243\n",
      "Train: Epoch [23], Batch [871/938], Loss: 0.4200167655944824\n",
      "Train: Epoch [23], Batch [872/938], Loss: 0.4025251269340515\n",
      "Train: Epoch [23], Batch [873/938], Loss: 0.2007315456867218\n",
      "Train: Epoch [23], Batch [874/938], Loss: 0.3992210924625397\n",
      "Train: Epoch [23], Batch [875/938], Loss: 0.37911540269851685\n",
      "Train: Epoch [23], Batch [876/938], Loss: 0.4055353105068207\n",
      "Train: Epoch [23], Batch [877/938], Loss: 0.38814663887023926\n",
      "Train: Epoch [23], Batch [878/938], Loss: 0.36166059970855713\n",
      "Train: Epoch [23], Batch [879/938], Loss: 0.42756137251853943\n",
      "Train: Epoch [23], Batch [880/938], Loss: 0.40705937147140503\n",
      "Train: Epoch [23], Batch [881/938], Loss: 0.2928711175918579\n",
      "Train: Epoch [23], Batch [882/938], Loss: 0.4873536229133606\n",
      "Train: Epoch [23], Batch [883/938], Loss: 0.3932994306087494\n",
      "Train: Epoch [23], Batch [884/938], Loss: 0.42568719387054443\n",
      "Train: Epoch [23], Batch [885/938], Loss: 0.2755311131477356\n",
      "Train: Epoch [23], Batch [886/938], Loss: 0.5008512735366821\n",
      "Train: Epoch [23], Batch [887/938], Loss: 0.4168083965778351\n",
      "Train: Epoch [23], Batch [888/938], Loss: 0.4419773519039154\n",
      "Train: Epoch [23], Batch [889/938], Loss: 0.23686793446540833\n",
      "Train: Epoch [23], Batch [890/938], Loss: 0.19521388411521912\n",
      "Train: Epoch [23], Batch [891/938], Loss: 0.3766361474990845\n",
      "Train: Epoch [23], Batch [892/938], Loss: 0.18635709583759308\n",
      "Train: Epoch [23], Batch [893/938], Loss: 0.4826107919216156\n",
      "Train: Epoch [23], Batch [894/938], Loss: 0.27486079931259155\n",
      "Train: Epoch [23], Batch [895/938], Loss: 0.40148159861564636\n",
      "Train: Epoch [23], Batch [896/938], Loss: 0.48865634202957153\n",
      "Train: Epoch [23], Batch [897/938], Loss: 0.28171098232269287\n",
      "Train: Epoch [23], Batch [898/938], Loss: 0.38763654232025146\n",
      "Train: Epoch [23], Batch [899/938], Loss: 0.4893121123313904\n",
      "Train: Epoch [23], Batch [900/938], Loss: 0.3902152180671692\n",
      "Train: Epoch [23], Batch [901/938], Loss: 0.6348790526390076\n",
      "Train: Epoch [23], Batch [902/938], Loss: 0.4037199020385742\n",
      "Train: Epoch [23], Batch [903/938], Loss: 0.5527567863464355\n",
      "Train: Epoch [23], Batch [904/938], Loss: 0.4663904309272766\n",
      "Train: Epoch [23], Batch [905/938], Loss: 0.36479270458221436\n",
      "Train: Epoch [23], Batch [906/938], Loss: 0.27192485332489014\n",
      "Train: Epoch [23], Batch [907/938], Loss: 0.3396558463573456\n",
      "Train: Epoch [23], Batch [908/938], Loss: 0.49281013011932373\n",
      "Train: Epoch [23], Batch [909/938], Loss: 0.30966347455978394\n",
      "Train: Epoch [23], Batch [910/938], Loss: 0.32060694694519043\n",
      "Train: Epoch [23], Batch [911/938], Loss: 0.3786941170692444\n",
      "Train: Epoch [23], Batch [912/938], Loss: 0.36791616678237915\n",
      "Train: Epoch [23], Batch [913/938], Loss: 0.4985605478286743\n",
      "Train: Epoch [23], Batch [914/938], Loss: 0.4455704092979431\n",
      "Train: Epoch [23], Batch [915/938], Loss: 0.28222256898880005\n",
      "Train: Epoch [23], Batch [916/938], Loss: 0.32220277190208435\n",
      "Train: Epoch [23], Batch [917/938], Loss: 0.27095097303390503\n",
      "Train: Epoch [23], Batch [918/938], Loss: 0.7127351760864258\n",
      "Train: Epoch [23], Batch [919/938], Loss: 0.25081706047058105\n",
      "Train: Epoch [23], Batch [920/938], Loss: 0.31329211592674255\n",
      "Train: Epoch [23], Batch [921/938], Loss: 0.18130427598953247\n",
      "Train: Epoch [23], Batch [922/938], Loss: 0.39966461062431335\n",
      "Train: Epoch [23], Batch [923/938], Loss: 0.298753559589386\n",
      "Train: Epoch [23], Batch [924/938], Loss: 0.23931936919689178\n",
      "Train: Epoch [23], Batch [925/938], Loss: 0.3167474865913391\n",
      "Train: Epoch [23], Batch [926/938], Loss: 0.3968472480773926\n",
      "Train: Epoch [23], Batch [927/938], Loss: 0.2948545813560486\n",
      "Train: Epoch [23], Batch [928/938], Loss: 0.38124412298202515\n",
      "Train: Epoch [23], Batch [929/938], Loss: 0.3871248960494995\n",
      "Train: Epoch [23], Batch [930/938], Loss: 0.33709824085235596\n",
      "Train: Epoch [23], Batch [931/938], Loss: 0.463407427072525\n",
      "Train: Epoch [23], Batch [932/938], Loss: 0.40311774611473083\n",
      "Train: Epoch [23], Batch [933/938], Loss: 0.37150949239730835\n",
      "Train: Epoch [23], Batch [934/938], Loss: 0.456983745098114\n",
      "Train: Epoch [23], Batch [935/938], Loss: 0.32902616262435913\n",
      "Train: Epoch [23], Batch [936/938], Loss: 0.4995131194591522\n",
      "Train: Epoch [23], Batch [937/938], Loss: 0.4078112840652466\n",
      "Train: Epoch [23], Batch [938/938], Loss: 0.6850017309188843\n",
      "Accuracy of train set: 0.8623666666666666\n",
      "Validation: Epoch [23], Batch [1/938], Loss: 0.24219803512096405\n",
      "Validation: Epoch [23], Batch [2/938], Loss: 0.44214189052581787\n",
      "Validation: Epoch [23], Batch [3/938], Loss: 0.4152437448501587\n",
      "Validation: Epoch [23], Batch [4/938], Loss: 0.4034825563430786\n",
      "Validation: Epoch [23], Batch [5/938], Loss: 0.2970768213272095\n",
      "Validation: Epoch [23], Batch [6/938], Loss: 0.35043179988861084\n",
      "Validation: Epoch [23], Batch [7/938], Loss: 0.39036619663238525\n",
      "Validation: Epoch [23], Batch [8/938], Loss: 0.371177613735199\n",
      "Validation: Epoch [23], Batch [9/938], Loss: 0.3537684977054596\n",
      "Validation: Epoch [23], Batch [10/938], Loss: 0.35299843549728394\n",
      "Validation: Epoch [23], Batch [11/938], Loss: 0.23803336918354034\n",
      "Validation: Epoch [23], Batch [12/938], Loss: 0.3502594828605652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [13/938], Loss: 0.37836170196533203\n",
      "Validation: Epoch [23], Batch [14/938], Loss: 0.40146562457084656\n",
      "Validation: Epoch [23], Batch [15/938], Loss: 0.3077080547809601\n",
      "Validation: Epoch [23], Batch [16/938], Loss: 0.45222780108451843\n",
      "Validation: Epoch [23], Batch [17/938], Loss: 0.37440210580825806\n",
      "Validation: Epoch [23], Batch [18/938], Loss: 0.30672839283943176\n",
      "Validation: Epoch [23], Batch [19/938], Loss: 0.48760658502578735\n",
      "Validation: Epoch [23], Batch [20/938], Loss: 0.32303938269615173\n",
      "Validation: Epoch [23], Batch [21/938], Loss: 0.24471265077590942\n",
      "Validation: Epoch [23], Batch [22/938], Loss: 0.47844719886779785\n",
      "Validation: Epoch [23], Batch [23/938], Loss: 0.5773054957389832\n",
      "Validation: Epoch [23], Batch [24/938], Loss: 0.41963034868240356\n",
      "Validation: Epoch [23], Batch [25/938], Loss: 0.24496936798095703\n",
      "Validation: Epoch [23], Batch [26/938], Loss: 0.3914702832698822\n",
      "Validation: Epoch [23], Batch [27/938], Loss: 0.3866453766822815\n",
      "Validation: Epoch [23], Batch [28/938], Loss: 0.4192546010017395\n",
      "Validation: Epoch [23], Batch [29/938], Loss: 0.5046347975730896\n",
      "Validation: Epoch [23], Batch [30/938], Loss: 0.48763933777809143\n",
      "Validation: Epoch [23], Batch [31/938], Loss: 0.2805655598640442\n",
      "Validation: Epoch [23], Batch [32/938], Loss: 0.4319106340408325\n",
      "Validation: Epoch [23], Batch [33/938], Loss: 0.46916142106056213\n",
      "Validation: Epoch [23], Batch [34/938], Loss: 0.42879581451416016\n",
      "Validation: Epoch [23], Batch [35/938], Loss: 0.454260915517807\n",
      "Validation: Epoch [23], Batch [36/938], Loss: 0.4127282202243805\n",
      "Validation: Epoch [23], Batch [37/938], Loss: 0.46840372681617737\n",
      "Validation: Epoch [23], Batch [38/938], Loss: 0.4799131155014038\n",
      "Validation: Epoch [23], Batch [39/938], Loss: 0.20754748582839966\n",
      "Validation: Epoch [23], Batch [40/938], Loss: 0.4797740578651428\n",
      "Validation: Epoch [23], Batch [41/938], Loss: 0.4927810728549957\n",
      "Validation: Epoch [23], Batch [42/938], Loss: 0.2079968899488449\n",
      "Validation: Epoch [23], Batch [43/938], Loss: 0.2790799140930176\n",
      "Validation: Epoch [23], Batch [44/938], Loss: 0.39080286026000977\n",
      "Validation: Epoch [23], Batch [45/938], Loss: 0.42530557513237\n",
      "Validation: Epoch [23], Batch [46/938], Loss: 0.30789774656295776\n",
      "Validation: Epoch [23], Batch [47/938], Loss: 0.7012484073638916\n",
      "Validation: Epoch [23], Batch [48/938], Loss: 0.25905176997184753\n",
      "Validation: Epoch [23], Batch [49/938], Loss: 0.33551204204559326\n",
      "Validation: Epoch [23], Batch [50/938], Loss: 0.366188108921051\n",
      "Validation: Epoch [23], Batch [51/938], Loss: 0.4372681975364685\n",
      "Validation: Epoch [23], Batch [52/938], Loss: 0.46312129497528076\n",
      "Validation: Epoch [23], Batch [53/938], Loss: 0.3372485637664795\n",
      "Validation: Epoch [23], Batch [54/938], Loss: 0.3127821683883667\n",
      "Validation: Epoch [23], Batch [55/938], Loss: 0.32408300042152405\n",
      "Validation: Epoch [23], Batch [56/938], Loss: 0.4627249538898468\n",
      "Validation: Epoch [23], Batch [57/938], Loss: 0.47145238518714905\n",
      "Validation: Epoch [23], Batch [58/938], Loss: 0.16746190190315247\n",
      "Validation: Epoch [23], Batch [59/938], Loss: 0.42622458934783936\n",
      "Validation: Epoch [23], Batch [60/938], Loss: 0.38774779438972473\n",
      "Validation: Epoch [23], Batch [61/938], Loss: 0.40630093216896057\n",
      "Validation: Epoch [23], Batch [62/938], Loss: 0.28645890951156616\n",
      "Validation: Epoch [23], Batch [63/938], Loss: 0.41220128536224365\n",
      "Validation: Epoch [23], Batch [64/938], Loss: 0.3208794891834259\n",
      "Validation: Epoch [23], Batch [65/938], Loss: 0.22174391150474548\n",
      "Validation: Epoch [23], Batch [66/938], Loss: 0.4589782655239105\n",
      "Validation: Epoch [23], Batch [67/938], Loss: 0.4187496304512024\n",
      "Validation: Epoch [23], Batch [68/938], Loss: 0.5298529863357544\n",
      "Validation: Epoch [23], Batch [69/938], Loss: 0.29557520151138306\n",
      "Validation: Epoch [23], Batch [70/938], Loss: 0.4092506170272827\n",
      "Validation: Epoch [23], Batch [71/938], Loss: 0.6228699684143066\n",
      "Validation: Epoch [23], Batch [72/938], Loss: 0.37253642082214355\n",
      "Validation: Epoch [23], Batch [73/938], Loss: 0.5339661836624146\n",
      "Validation: Epoch [23], Batch [74/938], Loss: 0.3087286353111267\n",
      "Validation: Epoch [23], Batch [75/938], Loss: 0.37349873781204224\n",
      "Validation: Epoch [23], Batch [76/938], Loss: 0.3333430290222168\n",
      "Validation: Epoch [23], Batch [77/938], Loss: 0.20454934239387512\n",
      "Validation: Epoch [23], Batch [78/938], Loss: 0.3227238953113556\n",
      "Validation: Epoch [23], Batch [79/938], Loss: 0.3861038088798523\n",
      "Validation: Epoch [23], Batch [80/938], Loss: 0.45671331882476807\n",
      "Validation: Epoch [23], Batch [81/938], Loss: 0.2941299080848694\n",
      "Validation: Epoch [23], Batch [82/938], Loss: 0.24474181234836578\n",
      "Validation: Epoch [23], Batch [83/938], Loss: 0.45813944935798645\n",
      "Validation: Epoch [23], Batch [84/938], Loss: 0.3855617046356201\n",
      "Validation: Epoch [23], Batch [85/938], Loss: 0.38964056968688965\n",
      "Validation: Epoch [23], Batch [86/938], Loss: 0.2873722016811371\n",
      "Validation: Epoch [23], Batch [87/938], Loss: 0.2896069884300232\n",
      "Validation: Epoch [23], Batch [88/938], Loss: 0.43193519115448\n",
      "Validation: Epoch [23], Batch [89/938], Loss: 0.3308439254760742\n",
      "Validation: Epoch [23], Batch [90/938], Loss: 0.49281391501426697\n",
      "Validation: Epoch [23], Batch [91/938], Loss: 0.3476065397262573\n",
      "Validation: Epoch [23], Batch [92/938], Loss: 0.4621582627296448\n",
      "Validation: Epoch [23], Batch [93/938], Loss: 0.403717041015625\n",
      "Validation: Epoch [23], Batch [94/938], Loss: 0.25312307476997375\n",
      "Validation: Epoch [23], Batch [95/938], Loss: 0.3883794844150543\n",
      "Validation: Epoch [23], Batch [96/938], Loss: 0.22615545988082886\n",
      "Validation: Epoch [23], Batch [97/938], Loss: 0.3140498399734497\n",
      "Validation: Epoch [23], Batch [98/938], Loss: 0.46070122718811035\n",
      "Validation: Epoch [23], Batch [99/938], Loss: 0.42849230766296387\n",
      "Validation: Epoch [23], Batch [100/938], Loss: 0.4291515648365021\n",
      "Validation: Epoch [23], Batch [101/938], Loss: 0.4413810968399048\n",
      "Validation: Epoch [23], Batch [102/938], Loss: 0.37814706563949585\n",
      "Validation: Epoch [23], Batch [103/938], Loss: 0.2794601023197174\n",
      "Validation: Epoch [23], Batch [104/938], Loss: 0.3430861830711365\n",
      "Validation: Epoch [23], Batch [105/938], Loss: 0.4310948848724365\n",
      "Validation: Epoch [23], Batch [106/938], Loss: 0.42113053798675537\n",
      "Validation: Epoch [23], Batch [107/938], Loss: 0.540133535861969\n",
      "Validation: Epoch [23], Batch [108/938], Loss: 0.3097296953201294\n",
      "Validation: Epoch [23], Batch [109/938], Loss: 0.6209026575088501\n",
      "Validation: Epoch [23], Batch [110/938], Loss: 0.37973445653915405\n",
      "Validation: Epoch [23], Batch [111/938], Loss: 0.37067753076553345\n",
      "Validation: Epoch [23], Batch [112/938], Loss: 0.3442994952201843\n",
      "Validation: Epoch [23], Batch [113/938], Loss: 0.24670784175395966\n",
      "Validation: Epoch [23], Batch [114/938], Loss: 0.2836335301399231\n",
      "Validation: Epoch [23], Batch [115/938], Loss: 0.308161199092865\n",
      "Validation: Epoch [23], Batch [116/938], Loss: 0.4025324881076813\n",
      "Validation: Epoch [23], Batch [117/938], Loss: 0.6549003720283508\n",
      "Validation: Epoch [23], Batch [118/938], Loss: 0.30116719007492065\n",
      "Validation: Epoch [23], Batch [119/938], Loss: 0.5312082171440125\n",
      "Validation: Epoch [23], Batch [120/938], Loss: 0.21446330845355988\n",
      "Validation: Epoch [23], Batch [121/938], Loss: 0.3672858476638794\n",
      "Validation: Epoch [23], Batch [122/938], Loss: 0.28235095739364624\n",
      "Validation: Epoch [23], Batch [123/938], Loss: 0.3138037919998169\n",
      "Validation: Epoch [23], Batch [124/938], Loss: 0.4460977017879486\n",
      "Validation: Epoch [23], Batch [125/938], Loss: 0.29587119817733765\n",
      "Validation: Epoch [23], Batch [126/938], Loss: 0.36325836181640625\n",
      "Validation: Epoch [23], Batch [127/938], Loss: 0.370278924703598\n",
      "Validation: Epoch [23], Batch [128/938], Loss: 0.3520481586456299\n",
      "Validation: Epoch [23], Batch [129/938], Loss: 0.4976557195186615\n",
      "Validation: Epoch [23], Batch [130/938], Loss: 0.35270166397094727\n",
      "Validation: Epoch [23], Batch [131/938], Loss: 0.5577670335769653\n",
      "Validation: Epoch [23], Batch [132/938], Loss: 0.49891114234924316\n",
      "Validation: Epoch [23], Batch [133/938], Loss: 0.26993194222450256\n",
      "Validation: Epoch [23], Batch [134/938], Loss: 0.23569191992282867\n",
      "Validation: Epoch [23], Batch [135/938], Loss: 0.3513137102127075\n",
      "Validation: Epoch [23], Batch [136/938], Loss: 0.3714796304702759\n",
      "Validation: Epoch [23], Batch [137/938], Loss: 0.3933519721031189\n",
      "Validation: Epoch [23], Batch [138/938], Loss: 0.3351818323135376\n",
      "Validation: Epoch [23], Batch [139/938], Loss: 0.2812689244747162\n",
      "Validation: Epoch [23], Batch [140/938], Loss: 0.38426339626312256\n",
      "Validation: Epoch [23], Batch [141/938], Loss: 0.3692062497138977\n",
      "Validation: Epoch [23], Batch [142/938], Loss: 0.5178598165512085\n",
      "Validation: Epoch [23], Batch [143/938], Loss: 0.3739703595638275\n",
      "Validation: Epoch [23], Batch [144/938], Loss: 0.2512640953063965\n",
      "Validation: Epoch [23], Batch [145/938], Loss: 0.23013165593147278\n",
      "Validation: Epoch [23], Batch [146/938], Loss: 0.2939109206199646\n",
      "Validation: Epoch [23], Batch [147/938], Loss: 0.3029288351535797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [148/938], Loss: 0.4514020085334778\n",
      "Validation: Epoch [23], Batch [149/938], Loss: 0.45140373706817627\n",
      "Validation: Epoch [23], Batch [150/938], Loss: 0.3222315311431885\n",
      "Validation: Epoch [23], Batch [151/938], Loss: 0.371579110622406\n",
      "Validation: Epoch [23], Batch [152/938], Loss: 0.2667747735977173\n",
      "Validation: Epoch [23], Batch [153/938], Loss: 0.5589552521705627\n",
      "Validation: Epoch [23], Batch [154/938], Loss: 0.4737662971019745\n",
      "Validation: Epoch [23], Batch [155/938], Loss: 0.44171392917633057\n",
      "Validation: Epoch [23], Batch [156/938], Loss: 0.39023032784461975\n",
      "Validation: Epoch [23], Batch [157/938], Loss: 0.37832245230674744\n",
      "Validation: Epoch [23], Batch [158/938], Loss: 0.24743692576885223\n",
      "Validation: Epoch [23], Batch [159/938], Loss: 0.26385676860809326\n",
      "Validation: Epoch [23], Batch [160/938], Loss: 0.451320081949234\n",
      "Validation: Epoch [23], Batch [161/938], Loss: 0.33030760288238525\n",
      "Validation: Epoch [23], Batch [162/938], Loss: 0.5306965112686157\n",
      "Validation: Epoch [23], Batch [163/938], Loss: 0.3741306662559509\n",
      "Validation: Epoch [23], Batch [164/938], Loss: 0.46268177032470703\n",
      "Validation: Epoch [23], Batch [165/938], Loss: 0.37578484416007996\n",
      "Validation: Epoch [23], Batch [166/938], Loss: 0.37009644508361816\n",
      "Validation: Epoch [23], Batch [167/938], Loss: 0.23699361085891724\n",
      "Validation: Epoch [23], Batch [168/938], Loss: 0.43637266755104065\n",
      "Validation: Epoch [23], Batch [169/938], Loss: 0.34094488620758057\n",
      "Validation: Epoch [23], Batch [170/938], Loss: 0.3431578278541565\n",
      "Validation: Epoch [23], Batch [171/938], Loss: 0.20270812511444092\n",
      "Validation: Epoch [23], Batch [172/938], Loss: 0.3485563397407532\n",
      "Validation: Epoch [23], Batch [173/938], Loss: 0.45092085003852844\n",
      "Validation: Epoch [23], Batch [174/938], Loss: 0.48689648509025574\n",
      "Validation: Epoch [23], Batch [175/938], Loss: 0.17829126119613647\n",
      "Validation: Epoch [23], Batch [176/938], Loss: 0.3470602333545685\n",
      "Validation: Epoch [23], Batch [177/938], Loss: 0.26169443130493164\n",
      "Validation: Epoch [23], Batch [178/938], Loss: 0.2669481337070465\n",
      "Validation: Epoch [23], Batch [179/938], Loss: 0.30705729126930237\n",
      "Validation: Epoch [23], Batch [180/938], Loss: 0.3315114676952362\n",
      "Validation: Epoch [23], Batch [181/938], Loss: 0.4026244878768921\n",
      "Validation: Epoch [23], Batch [182/938], Loss: 0.31231585144996643\n",
      "Validation: Epoch [23], Batch [183/938], Loss: 0.41274499893188477\n",
      "Validation: Epoch [23], Batch [184/938], Loss: 0.3598215579986572\n",
      "Validation: Epoch [23], Batch [185/938], Loss: 0.30263209342956543\n",
      "Validation: Epoch [23], Batch [186/938], Loss: 0.3464641869068146\n",
      "Validation: Epoch [23], Batch [187/938], Loss: 0.30716896057128906\n",
      "Validation: Epoch [23], Batch [188/938], Loss: 0.3933144509792328\n",
      "Validation: Epoch [23], Batch [189/938], Loss: 0.3446136713027954\n",
      "Validation: Epoch [23], Batch [190/938], Loss: 0.4195224940776825\n",
      "Validation: Epoch [23], Batch [191/938], Loss: 0.5006557703018188\n",
      "Validation: Epoch [23], Batch [192/938], Loss: 0.2662418782711029\n",
      "Validation: Epoch [23], Batch [193/938], Loss: 0.4715205132961273\n",
      "Validation: Epoch [23], Batch [194/938], Loss: 0.4654301404953003\n",
      "Validation: Epoch [23], Batch [195/938], Loss: 0.33435630798339844\n",
      "Validation: Epoch [23], Batch [196/938], Loss: 0.4993293881416321\n",
      "Validation: Epoch [23], Batch [197/938], Loss: 0.20944619178771973\n",
      "Validation: Epoch [23], Batch [198/938], Loss: 0.6719232797622681\n",
      "Validation: Epoch [23], Batch [199/938], Loss: 0.4318283200263977\n",
      "Validation: Epoch [23], Batch [200/938], Loss: 0.326934278011322\n",
      "Validation: Epoch [23], Batch [201/938], Loss: 0.41900205612182617\n",
      "Validation: Epoch [23], Batch [202/938], Loss: 0.24633312225341797\n",
      "Validation: Epoch [23], Batch [203/938], Loss: 0.34475013613700867\n",
      "Validation: Epoch [23], Batch [204/938], Loss: 0.45129039883613586\n",
      "Validation: Epoch [23], Batch [205/938], Loss: 0.5751100778579712\n",
      "Validation: Epoch [23], Batch [206/938], Loss: 0.37128639221191406\n",
      "Validation: Epoch [23], Batch [207/938], Loss: 0.5530822277069092\n",
      "Validation: Epoch [23], Batch [208/938], Loss: 0.2972574234008789\n",
      "Validation: Epoch [23], Batch [209/938], Loss: 0.4228803217411041\n",
      "Validation: Epoch [23], Batch [210/938], Loss: 0.4060080051422119\n",
      "Validation: Epoch [23], Batch [211/938], Loss: 0.5582834482192993\n",
      "Validation: Epoch [23], Batch [212/938], Loss: 0.31942206621170044\n",
      "Validation: Epoch [23], Batch [213/938], Loss: 0.38177716732025146\n",
      "Validation: Epoch [23], Batch [214/938], Loss: 0.3299228250980377\n",
      "Validation: Epoch [23], Batch [215/938], Loss: 0.3705131411552429\n",
      "Validation: Epoch [23], Batch [216/938], Loss: 0.35685718059539795\n",
      "Validation: Epoch [23], Batch [217/938], Loss: 0.45534953474998474\n",
      "Validation: Epoch [23], Batch [218/938], Loss: 0.22974050045013428\n",
      "Validation: Epoch [23], Batch [219/938], Loss: 0.2387220710515976\n",
      "Validation: Epoch [23], Batch [220/938], Loss: 0.4041084051132202\n",
      "Validation: Epoch [23], Batch [221/938], Loss: 0.3798421025276184\n",
      "Validation: Epoch [23], Batch [222/938], Loss: 0.37285661697387695\n",
      "Validation: Epoch [23], Batch [223/938], Loss: 0.42041003704071045\n",
      "Validation: Epoch [23], Batch [224/938], Loss: 0.5070830583572388\n",
      "Validation: Epoch [23], Batch [225/938], Loss: 0.4258778989315033\n",
      "Validation: Epoch [23], Batch [226/938], Loss: 0.5951944589614868\n",
      "Validation: Epoch [23], Batch [227/938], Loss: 0.3787616789340973\n",
      "Validation: Epoch [23], Batch [228/938], Loss: 0.41624322533607483\n",
      "Validation: Epoch [23], Batch [229/938], Loss: 0.3190103769302368\n",
      "Validation: Epoch [23], Batch [230/938], Loss: 0.5088540315628052\n",
      "Validation: Epoch [23], Batch [231/938], Loss: 0.323638916015625\n",
      "Validation: Epoch [23], Batch [232/938], Loss: 0.28900575637817383\n",
      "Validation: Epoch [23], Batch [233/938], Loss: 0.2892199158668518\n",
      "Validation: Epoch [23], Batch [234/938], Loss: 0.3084834814071655\n",
      "Validation: Epoch [23], Batch [235/938], Loss: 0.328673779964447\n",
      "Validation: Epoch [23], Batch [236/938], Loss: 0.37926918268203735\n",
      "Validation: Epoch [23], Batch [237/938], Loss: 0.42479100823402405\n",
      "Validation: Epoch [23], Batch [238/938], Loss: 0.419779896736145\n",
      "Validation: Epoch [23], Batch [239/938], Loss: 0.4679511487483978\n",
      "Validation: Epoch [23], Batch [240/938], Loss: 0.2784808874130249\n",
      "Validation: Epoch [23], Batch [241/938], Loss: 0.3857724964618683\n",
      "Validation: Epoch [23], Batch [242/938], Loss: 0.31652361154556274\n",
      "Validation: Epoch [23], Batch [243/938], Loss: 0.3197762370109558\n",
      "Validation: Epoch [23], Batch [244/938], Loss: 0.4115251898765564\n",
      "Validation: Epoch [23], Batch [245/938], Loss: 0.3382176160812378\n",
      "Validation: Epoch [23], Batch [246/938], Loss: 0.46956518292427063\n",
      "Validation: Epoch [23], Batch [247/938], Loss: 0.45058688521385193\n",
      "Validation: Epoch [23], Batch [248/938], Loss: 0.34092485904693604\n",
      "Validation: Epoch [23], Batch [249/938], Loss: 0.30387353897094727\n",
      "Validation: Epoch [23], Batch [250/938], Loss: 0.23363611102104187\n",
      "Validation: Epoch [23], Batch [251/938], Loss: 0.23553574085235596\n",
      "Validation: Epoch [23], Batch [252/938], Loss: 0.30963921546936035\n",
      "Validation: Epoch [23], Batch [253/938], Loss: 0.3578570485115051\n",
      "Validation: Epoch [23], Batch [254/938], Loss: 0.2318473756313324\n",
      "Validation: Epoch [23], Batch [255/938], Loss: 0.38235560059547424\n",
      "Validation: Epoch [23], Batch [256/938], Loss: 0.5451661348342896\n",
      "Validation: Epoch [23], Batch [257/938], Loss: 0.3978044390678406\n",
      "Validation: Epoch [23], Batch [258/938], Loss: 0.3798743188381195\n",
      "Validation: Epoch [23], Batch [259/938], Loss: 0.3608901798725128\n",
      "Validation: Epoch [23], Batch [260/938], Loss: 0.4895714223384857\n",
      "Validation: Epoch [23], Batch [261/938], Loss: 0.5388087034225464\n",
      "Validation: Epoch [23], Batch [262/938], Loss: 0.3638284504413605\n",
      "Validation: Epoch [23], Batch [263/938], Loss: 0.34111863374710083\n",
      "Validation: Epoch [23], Batch [264/938], Loss: 0.5080047845840454\n",
      "Validation: Epoch [23], Batch [265/938], Loss: 0.26074519753456116\n",
      "Validation: Epoch [23], Batch [266/938], Loss: 0.3628900647163391\n",
      "Validation: Epoch [23], Batch [267/938], Loss: 0.30155375599861145\n",
      "Validation: Epoch [23], Batch [268/938], Loss: 0.37352749705314636\n",
      "Validation: Epoch [23], Batch [269/938], Loss: 0.5063157081604004\n",
      "Validation: Epoch [23], Batch [270/938], Loss: 0.3671098053455353\n",
      "Validation: Epoch [23], Batch [271/938], Loss: 0.40843290090560913\n",
      "Validation: Epoch [23], Batch [272/938], Loss: 0.4010634422302246\n",
      "Validation: Epoch [23], Batch [273/938], Loss: 0.29125651717185974\n",
      "Validation: Epoch [23], Batch [274/938], Loss: 0.40833795070648193\n",
      "Validation: Epoch [23], Batch [275/938], Loss: 0.2903551459312439\n",
      "Validation: Epoch [23], Batch [276/938], Loss: 0.4476461410522461\n",
      "Validation: Epoch [23], Batch [277/938], Loss: 0.3916478157043457\n",
      "Validation: Epoch [23], Batch [278/938], Loss: 0.3327803909778595\n",
      "Validation: Epoch [23], Batch [279/938], Loss: 0.4137203097343445\n",
      "Validation: Epoch [23], Batch [280/938], Loss: 0.4659559726715088\n",
      "Validation: Epoch [23], Batch [281/938], Loss: 0.4973105192184448\n",
      "Validation: Epoch [23], Batch [282/938], Loss: 0.3743307590484619\n",
      "Validation: Epoch [23], Batch [283/938], Loss: 0.5009010434150696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [284/938], Loss: 0.354917973279953\n",
      "Validation: Epoch [23], Batch [285/938], Loss: 0.39225929975509644\n",
      "Validation: Epoch [23], Batch [286/938], Loss: 0.5223037004470825\n",
      "Validation: Epoch [23], Batch [287/938], Loss: 0.3235897421836853\n",
      "Validation: Epoch [23], Batch [288/938], Loss: 0.4117167592048645\n",
      "Validation: Epoch [23], Batch [289/938], Loss: 0.30698680877685547\n",
      "Validation: Epoch [23], Batch [290/938], Loss: 0.3598816990852356\n",
      "Validation: Epoch [23], Batch [291/938], Loss: 0.33522260189056396\n",
      "Validation: Epoch [23], Batch [292/938], Loss: 0.4014908969402313\n",
      "Validation: Epoch [23], Batch [293/938], Loss: 0.5176979899406433\n",
      "Validation: Epoch [23], Batch [294/938], Loss: 0.40080398321151733\n",
      "Validation: Epoch [23], Batch [295/938], Loss: 0.4805096685886383\n",
      "Validation: Epoch [23], Batch [296/938], Loss: 0.4693848490715027\n",
      "Validation: Epoch [23], Batch [297/938], Loss: 0.4084310531616211\n",
      "Validation: Epoch [23], Batch [298/938], Loss: 0.35283035039901733\n",
      "Validation: Epoch [23], Batch [299/938], Loss: 0.4039793014526367\n",
      "Validation: Epoch [23], Batch [300/938], Loss: 0.3056616187095642\n",
      "Validation: Epoch [23], Batch [301/938], Loss: 0.37216916680336\n",
      "Validation: Epoch [23], Batch [302/938], Loss: 0.20056197047233582\n",
      "Validation: Epoch [23], Batch [303/938], Loss: 0.3184928297996521\n",
      "Validation: Epoch [23], Batch [304/938], Loss: 0.5893083810806274\n",
      "Validation: Epoch [23], Batch [305/938], Loss: 0.2991057336330414\n",
      "Validation: Epoch [23], Batch [306/938], Loss: 0.3404250741004944\n",
      "Validation: Epoch [23], Batch [307/938], Loss: 0.4149225354194641\n",
      "Validation: Epoch [23], Batch [308/938], Loss: 0.5355557203292847\n",
      "Validation: Epoch [23], Batch [309/938], Loss: 0.18958580493927002\n",
      "Validation: Epoch [23], Batch [310/938], Loss: 0.4403798580169678\n",
      "Validation: Epoch [23], Batch [311/938], Loss: 0.3082468509674072\n",
      "Validation: Epoch [23], Batch [312/938], Loss: 0.36063048243522644\n",
      "Validation: Epoch [23], Batch [313/938], Loss: 0.27179181575775146\n",
      "Validation: Epoch [23], Batch [314/938], Loss: 0.2736230194568634\n",
      "Validation: Epoch [23], Batch [315/938], Loss: 0.3757524788379669\n",
      "Validation: Epoch [23], Batch [316/938], Loss: 0.5472856163978577\n",
      "Validation: Epoch [23], Batch [317/938], Loss: 0.43209370970726013\n",
      "Validation: Epoch [23], Batch [318/938], Loss: 0.4289366602897644\n",
      "Validation: Epoch [23], Batch [319/938], Loss: 0.33679720759391785\n",
      "Validation: Epoch [23], Batch [320/938], Loss: 0.5304204225540161\n",
      "Validation: Epoch [23], Batch [321/938], Loss: 0.6173440217971802\n",
      "Validation: Epoch [23], Batch [322/938], Loss: 0.342713326215744\n",
      "Validation: Epoch [23], Batch [323/938], Loss: 0.23041963577270508\n",
      "Validation: Epoch [23], Batch [324/938], Loss: 0.31515979766845703\n",
      "Validation: Epoch [23], Batch [325/938], Loss: 0.33223825693130493\n",
      "Validation: Epoch [23], Batch [326/938], Loss: 0.3464602828025818\n",
      "Validation: Epoch [23], Batch [327/938], Loss: 0.4178813397884369\n",
      "Validation: Epoch [23], Batch [328/938], Loss: 0.33857792615890503\n",
      "Validation: Epoch [23], Batch [329/938], Loss: 0.3887981176376343\n",
      "Validation: Epoch [23], Batch [330/938], Loss: 0.5302802324295044\n",
      "Validation: Epoch [23], Batch [331/938], Loss: 0.2725534439086914\n",
      "Validation: Epoch [23], Batch [332/938], Loss: 0.3580910563468933\n",
      "Validation: Epoch [23], Batch [333/938], Loss: 0.45256122946739197\n",
      "Validation: Epoch [23], Batch [334/938], Loss: 0.2901078462600708\n",
      "Validation: Epoch [23], Batch [335/938], Loss: 0.4024798572063446\n",
      "Validation: Epoch [23], Batch [336/938], Loss: 0.32103896141052246\n",
      "Validation: Epoch [23], Batch [337/938], Loss: 0.5918147563934326\n",
      "Validation: Epoch [23], Batch [338/938], Loss: 0.3241247534751892\n",
      "Validation: Epoch [23], Batch [339/938], Loss: 0.3321772813796997\n",
      "Validation: Epoch [23], Batch [340/938], Loss: 0.3335500955581665\n",
      "Validation: Epoch [23], Batch [341/938], Loss: 0.27843552827835083\n",
      "Validation: Epoch [23], Batch [342/938], Loss: 0.39857369661331177\n",
      "Validation: Epoch [23], Batch [343/938], Loss: 0.3465816378593445\n",
      "Validation: Epoch [23], Batch [344/938], Loss: 0.363248348236084\n",
      "Validation: Epoch [23], Batch [345/938], Loss: 0.2652930021286011\n",
      "Validation: Epoch [23], Batch [346/938], Loss: 0.26325538754463196\n",
      "Validation: Epoch [23], Batch [347/938], Loss: 0.4210323691368103\n",
      "Validation: Epoch [23], Batch [348/938], Loss: 0.29372549057006836\n",
      "Validation: Epoch [23], Batch [349/938], Loss: 0.22071191668510437\n",
      "Validation: Epoch [23], Batch [350/938], Loss: 0.45614108443260193\n",
      "Validation: Epoch [23], Batch [351/938], Loss: 0.3543228507041931\n",
      "Validation: Epoch [23], Batch [352/938], Loss: 0.3873871862888336\n",
      "Validation: Epoch [23], Batch [353/938], Loss: 0.542607307434082\n",
      "Validation: Epoch [23], Batch [354/938], Loss: 0.5209411382675171\n",
      "Validation: Epoch [23], Batch [355/938], Loss: 0.4127906262874603\n",
      "Validation: Epoch [23], Batch [356/938], Loss: 0.3177676796913147\n",
      "Validation: Epoch [23], Batch [357/938], Loss: 0.3772783577442169\n",
      "Validation: Epoch [23], Batch [358/938], Loss: 0.3248606324195862\n",
      "Validation: Epoch [23], Batch [359/938], Loss: 0.41864851117134094\n",
      "Validation: Epoch [23], Batch [360/938], Loss: 0.32314765453338623\n",
      "Validation: Epoch [23], Batch [361/938], Loss: 0.43728959560394287\n",
      "Validation: Epoch [23], Batch [362/938], Loss: 0.2927556037902832\n",
      "Validation: Epoch [23], Batch [363/938], Loss: 0.44493186473846436\n",
      "Validation: Epoch [23], Batch [364/938], Loss: 0.5134221911430359\n",
      "Validation: Epoch [23], Batch [365/938], Loss: 0.4316287934780121\n",
      "Validation: Epoch [23], Batch [366/938], Loss: 0.5284488201141357\n",
      "Validation: Epoch [23], Batch [367/938], Loss: 0.3324067294597626\n",
      "Validation: Epoch [23], Batch [368/938], Loss: 0.3704421818256378\n",
      "Validation: Epoch [23], Batch [369/938], Loss: 0.3053548336029053\n",
      "Validation: Epoch [23], Batch [370/938], Loss: 0.21609722077846527\n",
      "Validation: Epoch [23], Batch [371/938], Loss: 0.2851201295852661\n",
      "Validation: Epoch [23], Batch [372/938], Loss: 0.39683935046195984\n",
      "Validation: Epoch [23], Batch [373/938], Loss: 0.348070353269577\n",
      "Validation: Epoch [23], Batch [374/938], Loss: 0.3015509843826294\n",
      "Validation: Epoch [23], Batch [375/938], Loss: 0.4049423933029175\n",
      "Validation: Epoch [23], Batch [376/938], Loss: 0.4665808379650116\n",
      "Validation: Epoch [23], Batch [377/938], Loss: 0.28850844502449036\n",
      "Validation: Epoch [23], Batch [378/938], Loss: 0.4894537329673767\n",
      "Validation: Epoch [23], Batch [379/938], Loss: 0.283098429441452\n",
      "Validation: Epoch [23], Batch [380/938], Loss: 0.40300530195236206\n",
      "Validation: Epoch [23], Batch [381/938], Loss: 0.45474180579185486\n",
      "Validation: Epoch [23], Batch [382/938], Loss: 0.5151232481002808\n",
      "Validation: Epoch [23], Batch [383/938], Loss: 0.4810165762901306\n",
      "Validation: Epoch [23], Batch [384/938], Loss: 0.4128878116607666\n",
      "Validation: Epoch [23], Batch [385/938], Loss: 0.4296557903289795\n",
      "Validation: Epoch [23], Batch [386/938], Loss: 0.3757079243659973\n",
      "Validation: Epoch [23], Batch [387/938], Loss: 0.24617356061935425\n",
      "Validation: Epoch [23], Batch [388/938], Loss: 0.34289100766181946\n",
      "Validation: Epoch [23], Batch [389/938], Loss: 0.4294728636741638\n",
      "Validation: Epoch [23], Batch [390/938], Loss: 0.6555231213569641\n",
      "Validation: Epoch [23], Batch [391/938], Loss: 0.33209651708602905\n",
      "Validation: Epoch [23], Batch [392/938], Loss: 0.36055994033813477\n",
      "Validation: Epoch [23], Batch [393/938], Loss: 0.7464215159416199\n",
      "Validation: Epoch [23], Batch [394/938], Loss: 0.4153905510902405\n",
      "Validation: Epoch [23], Batch [395/938], Loss: 0.3390633463859558\n",
      "Validation: Epoch [23], Batch [396/938], Loss: 0.4107573926448822\n",
      "Validation: Epoch [23], Batch [397/938], Loss: 0.42373210191726685\n",
      "Validation: Epoch [23], Batch [398/938], Loss: 0.39726758003234863\n",
      "Validation: Epoch [23], Batch [399/938], Loss: 0.37120145559310913\n",
      "Validation: Epoch [23], Batch [400/938], Loss: 0.3773221969604492\n",
      "Validation: Epoch [23], Batch [401/938], Loss: 0.23966412246227264\n",
      "Validation: Epoch [23], Batch [402/938], Loss: 0.2968003451824188\n",
      "Validation: Epoch [23], Batch [403/938], Loss: 0.3142985999584198\n",
      "Validation: Epoch [23], Batch [404/938], Loss: 0.29317671060562134\n",
      "Validation: Epoch [23], Batch [405/938], Loss: 0.35962074995040894\n",
      "Validation: Epoch [23], Batch [406/938], Loss: 0.2985008656978607\n",
      "Validation: Epoch [23], Batch [407/938], Loss: 0.48471900820732117\n",
      "Validation: Epoch [23], Batch [408/938], Loss: 0.5323846340179443\n",
      "Validation: Epoch [23], Batch [409/938], Loss: 0.3661159873008728\n",
      "Validation: Epoch [23], Batch [410/938], Loss: 0.33017629384994507\n",
      "Validation: Epoch [23], Batch [411/938], Loss: 0.43242162466049194\n",
      "Validation: Epoch [23], Batch [412/938], Loss: 0.45256364345550537\n",
      "Validation: Epoch [23], Batch [413/938], Loss: 0.3174923062324524\n",
      "Validation: Epoch [23], Batch [414/938], Loss: 0.4700942635536194\n",
      "Validation: Epoch [23], Batch [415/938], Loss: 0.385632187128067\n",
      "Validation: Epoch [23], Batch [416/938], Loss: 0.4247138500213623\n",
      "Validation: Epoch [23], Batch [417/938], Loss: 0.34047043323516846\n",
      "Validation: Epoch [23], Batch [418/938], Loss: 0.32951873540878296\n",
      "Validation: Epoch [23], Batch [419/938], Loss: 0.4026682376861572\n",
      "Validation: Epoch [23], Batch [420/938], Loss: 0.36590057611465454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [421/938], Loss: 0.4204789996147156\n",
      "Validation: Epoch [23], Batch [422/938], Loss: 0.29103249311447144\n",
      "Validation: Epoch [23], Batch [423/938], Loss: 0.48126399517059326\n",
      "Validation: Epoch [23], Batch [424/938], Loss: 0.39839744567871094\n",
      "Validation: Epoch [23], Batch [425/938], Loss: 0.2581385374069214\n",
      "Validation: Epoch [23], Batch [426/938], Loss: 0.2363981306552887\n",
      "Validation: Epoch [23], Batch [427/938], Loss: 0.7512983679771423\n",
      "Validation: Epoch [23], Batch [428/938], Loss: 0.23729559779167175\n",
      "Validation: Epoch [23], Batch [429/938], Loss: 0.272766649723053\n",
      "Validation: Epoch [23], Batch [430/938], Loss: 0.30887675285339355\n",
      "Validation: Epoch [23], Batch [431/938], Loss: 0.5147758722305298\n",
      "Validation: Epoch [23], Batch [432/938], Loss: 0.37582528591156006\n",
      "Validation: Epoch [23], Batch [433/938], Loss: 0.38697904348373413\n",
      "Validation: Epoch [23], Batch [434/938], Loss: 0.3374052047729492\n",
      "Validation: Epoch [23], Batch [435/938], Loss: 0.6058043837547302\n",
      "Validation: Epoch [23], Batch [436/938], Loss: 0.3695371747016907\n",
      "Validation: Epoch [23], Batch [437/938], Loss: 0.26833459734916687\n",
      "Validation: Epoch [23], Batch [438/938], Loss: 0.32433557510375977\n",
      "Validation: Epoch [23], Batch [439/938], Loss: 0.31179678440093994\n",
      "Validation: Epoch [23], Batch [440/938], Loss: 0.3925592303276062\n",
      "Validation: Epoch [23], Batch [441/938], Loss: 0.25736796855926514\n",
      "Validation: Epoch [23], Batch [442/938], Loss: 0.38046759366989136\n",
      "Validation: Epoch [23], Batch [443/938], Loss: 0.39691808819770813\n",
      "Validation: Epoch [23], Batch [444/938], Loss: 0.47535133361816406\n",
      "Validation: Epoch [23], Batch [445/938], Loss: 0.18990720808506012\n",
      "Validation: Epoch [23], Batch [446/938], Loss: 0.3428112864494324\n",
      "Validation: Epoch [23], Batch [447/938], Loss: 0.31448450684547424\n",
      "Validation: Epoch [23], Batch [448/938], Loss: 0.3224577307701111\n",
      "Validation: Epoch [23], Batch [449/938], Loss: 0.3548188805580139\n",
      "Validation: Epoch [23], Batch [450/938], Loss: 0.4471672475337982\n",
      "Validation: Epoch [23], Batch [451/938], Loss: 0.43223270773887634\n",
      "Validation: Epoch [23], Batch [452/938], Loss: 0.4321262240409851\n",
      "Validation: Epoch [23], Batch [453/938], Loss: 0.39426809549331665\n",
      "Validation: Epoch [23], Batch [454/938], Loss: 0.2902304530143738\n",
      "Validation: Epoch [23], Batch [455/938], Loss: 0.515276312828064\n",
      "Validation: Epoch [23], Batch [456/938], Loss: 0.27947095036506653\n",
      "Validation: Epoch [23], Batch [457/938], Loss: 0.2675074338912964\n",
      "Validation: Epoch [23], Batch [458/938], Loss: 0.3444247245788574\n",
      "Validation: Epoch [23], Batch [459/938], Loss: 0.3581060469150543\n",
      "Validation: Epoch [23], Batch [460/938], Loss: 0.4027201235294342\n",
      "Validation: Epoch [23], Batch [461/938], Loss: 0.365630567073822\n",
      "Validation: Epoch [23], Batch [462/938], Loss: 0.6040942668914795\n",
      "Validation: Epoch [23], Batch [463/938], Loss: 0.29028788208961487\n",
      "Validation: Epoch [23], Batch [464/938], Loss: 0.28746843338012695\n",
      "Validation: Epoch [23], Batch [465/938], Loss: 0.30918818712234497\n",
      "Validation: Epoch [23], Batch [466/938], Loss: 0.30662915110588074\n",
      "Validation: Epoch [23], Batch [467/938], Loss: 0.3895297944545746\n",
      "Validation: Epoch [23], Batch [468/938], Loss: 0.3368305265903473\n",
      "Validation: Epoch [23], Batch [469/938], Loss: 0.28647249937057495\n",
      "Validation: Epoch [23], Batch [470/938], Loss: 0.3496927320957184\n",
      "Validation: Epoch [23], Batch [471/938], Loss: 0.28167930245399475\n",
      "Validation: Epoch [23], Batch [472/938], Loss: 0.3460804224014282\n",
      "Validation: Epoch [23], Batch [473/938], Loss: 0.28101465106010437\n",
      "Validation: Epoch [23], Batch [474/938], Loss: 0.5910545587539673\n",
      "Validation: Epoch [23], Batch [475/938], Loss: 0.6131747961044312\n",
      "Validation: Epoch [23], Batch [476/938], Loss: 0.45240098237991333\n",
      "Validation: Epoch [23], Batch [477/938], Loss: 0.23582258820533752\n",
      "Validation: Epoch [23], Batch [478/938], Loss: 0.4689977169036865\n",
      "Validation: Epoch [23], Batch [479/938], Loss: 0.4640789330005646\n",
      "Validation: Epoch [23], Batch [480/938], Loss: 0.1989649385213852\n",
      "Validation: Epoch [23], Batch [481/938], Loss: 0.2112140655517578\n",
      "Validation: Epoch [23], Batch [482/938], Loss: 0.516701340675354\n",
      "Validation: Epoch [23], Batch [483/938], Loss: 0.4034474492073059\n",
      "Validation: Epoch [23], Batch [484/938], Loss: 0.3202018439769745\n",
      "Validation: Epoch [23], Batch [485/938], Loss: 0.4709089994430542\n",
      "Validation: Epoch [23], Batch [486/938], Loss: 0.30878567695617676\n",
      "Validation: Epoch [23], Batch [487/938], Loss: 0.3658260107040405\n",
      "Validation: Epoch [23], Batch [488/938], Loss: 0.3177182078361511\n",
      "Validation: Epoch [23], Batch [489/938], Loss: 0.3154681622982025\n",
      "Validation: Epoch [23], Batch [490/938], Loss: 0.2977534532546997\n",
      "Validation: Epoch [23], Batch [491/938], Loss: 0.2810159921646118\n",
      "Validation: Epoch [23], Batch [492/938], Loss: 0.32903051376342773\n",
      "Validation: Epoch [23], Batch [493/938], Loss: 0.31612443923950195\n",
      "Validation: Epoch [23], Batch [494/938], Loss: 0.4745465815067291\n",
      "Validation: Epoch [23], Batch [495/938], Loss: 0.4848988652229309\n",
      "Validation: Epoch [23], Batch [496/938], Loss: 0.33895277976989746\n",
      "Validation: Epoch [23], Batch [497/938], Loss: 0.5479942560195923\n",
      "Validation: Epoch [23], Batch [498/938], Loss: 0.38968002796173096\n",
      "Validation: Epoch [23], Batch [499/938], Loss: 0.48765620589256287\n",
      "Validation: Epoch [23], Batch [500/938], Loss: 0.3044966161251068\n",
      "Validation: Epoch [23], Batch [501/938], Loss: 0.44879400730133057\n",
      "Validation: Epoch [23], Batch [502/938], Loss: 0.27887704968452454\n",
      "Validation: Epoch [23], Batch [503/938], Loss: 0.4982684254646301\n",
      "Validation: Epoch [23], Batch [504/938], Loss: 0.2876579165458679\n",
      "Validation: Epoch [23], Batch [505/938], Loss: 0.5358198881149292\n",
      "Validation: Epoch [23], Batch [506/938], Loss: 0.48287808895111084\n",
      "Validation: Epoch [23], Batch [507/938], Loss: 0.45091021060943604\n",
      "Validation: Epoch [23], Batch [508/938], Loss: 0.3485853672027588\n",
      "Validation: Epoch [23], Batch [509/938], Loss: 0.2917464077472687\n",
      "Validation: Epoch [23], Batch [510/938], Loss: 0.3731131851673126\n",
      "Validation: Epoch [23], Batch [511/938], Loss: 0.45476117730140686\n",
      "Validation: Epoch [23], Batch [512/938], Loss: 0.3113979995250702\n",
      "Validation: Epoch [23], Batch [513/938], Loss: 0.2644805908203125\n",
      "Validation: Epoch [23], Batch [514/938], Loss: 0.22469300031661987\n",
      "Validation: Epoch [23], Batch [515/938], Loss: 0.264385461807251\n",
      "Validation: Epoch [23], Batch [516/938], Loss: 0.3721369504928589\n",
      "Validation: Epoch [23], Batch [517/938], Loss: 0.44115450978279114\n",
      "Validation: Epoch [23], Batch [518/938], Loss: 0.4482197165489197\n",
      "Validation: Epoch [23], Batch [519/938], Loss: 0.3731328845024109\n",
      "Validation: Epoch [23], Batch [520/938], Loss: 0.2159425914287567\n",
      "Validation: Epoch [23], Batch [521/938], Loss: 0.5454662442207336\n",
      "Validation: Epoch [23], Batch [522/938], Loss: 0.27981480956077576\n",
      "Validation: Epoch [23], Batch [523/938], Loss: 0.48661932349205017\n",
      "Validation: Epoch [23], Batch [524/938], Loss: 0.3945515751838684\n",
      "Validation: Epoch [23], Batch [525/938], Loss: 0.3698351979255676\n",
      "Validation: Epoch [23], Batch [526/938], Loss: 0.3923225402832031\n",
      "Validation: Epoch [23], Batch [527/938], Loss: 0.47928544878959656\n",
      "Validation: Epoch [23], Batch [528/938], Loss: 0.2773118317127228\n",
      "Validation: Epoch [23], Batch [529/938], Loss: 0.34042051434516907\n",
      "Validation: Epoch [23], Batch [530/938], Loss: 0.30472898483276367\n",
      "Validation: Epoch [23], Batch [531/938], Loss: 0.38937002420425415\n",
      "Validation: Epoch [23], Batch [532/938], Loss: 0.3342018127441406\n",
      "Validation: Epoch [23], Batch [533/938], Loss: 0.47628501057624817\n",
      "Validation: Epoch [23], Batch [534/938], Loss: 0.26345890760421753\n",
      "Validation: Epoch [23], Batch [535/938], Loss: 0.3127947449684143\n",
      "Validation: Epoch [23], Batch [536/938], Loss: 0.46804046630859375\n",
      "Validation: Epoch [23], Batch [537/938], Loss: 0.2748434543609619\n",
      "Validation: Epoch [23], Batch [538/938], Loss: 0.27888983488082886\n",
      "Validation: Epoch [23], Batch [539/938], Loss: 0.3704190254211426\n",
      "Validation: Epoch [23], Batch [540/938], Loss: 0.3361741900444031\n",
      "Validation: Epoch [23], Batch [541/938], Loss: 0.31119346618652344\n",
      "Validation: Epoch [23], Batch [542/938], Loss: 0.3341969847679138\n",
      "Validation: Epoch [23], Batch [543/938], Loss: 0.2115681767463684\n",
      "Validation: Epoch [23], Batch [544/938], Loss: 0.29881635308265686\n",
      "Validation: Epoch [23], Batch [545/938], Loss: 0.5178036689758301\n",
      "Validation: Epoch [23], Batch [546/938], Loss: 0.3175683319568634\n",
      "Validation: Epoch [23], Batch [547/938], Loss: 0.25393518805503845\n",
      "Validation: Epoch [23], Batch [548/938], Loss: 0.3499080538749695\n",
      "Validation: Epoch [23], Batch [549/938], Loss: 0.42653894424438477\n",
      "Validation: Epoch [23], Batch [550/938], Loss: 0.39351686835289\n",
      "Validation: Epoch [23], Batch [551/938], Loss: 0.4721747934818268\n",
      "Validation: Epoch [23], Batch [552/938], Loss: 0.3674905598163605\n",
      "Validation: Epoch [23], Batch [553/938], Loss: 0.4374869465827942\n",
      "Validation: Epoch [23], Batch [554/938], Loss: 0.33283743262290955\n",
      "Validation: Epoch [23], Batch [555/938], Loss: 0.3531104028224945\n",
      "Validation: Epoch [23], Batch [556/938], Loss: 0.35339462757110596\n",
      "Validation: Epoch [23], Batch [557/938], Loss: 0.3661072254180908\n",
      "Validation: Epoch [23], Batch [558/938], Loss: 0.34617382287979126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [559/938], Loss: 0.5215585827827454\n",
      "Validation: Epoch [23], Batch [560/938], Loss: 0.4751756191253662\n",
      "Validation: Epoch [23], Batch [561/938], Loss: 0.40293818712234497\n",
      "Validation: Epoch [23], Batch [562/938], Loss: 0.5584102869033813\n",
      "Validation: Epoch [23], Batch [563/938], Loss: 0.3423377275466919\n",
      "Validation: Epoch [23], Batch [564/938], Loss: 0.26826608180999756\n",
      "Validation: Epoch [23], Batch [565/938], Loss: 0.32764095067977905\n",
      "Validation: Epoch [23], Batch [566/938], Loss: 0.35950493812561035\n",
      "Validation: Epoch [23], Batch [567/938], Loss: 0.3074461817741394\n",
      "Validation: Epoch [23], Batch [568/938], Loss: 0.4477385878562927\n",
      "Validation: Epoch [23], Batch [569/938], Loss: 0.36554187536239624\n",
      "Validation: Epoch [23], Batch [570/938], Loss: 0.3291359841823578\n",
      "Validation: Epoch [23], Batch [571/938], Loss: 0.2136799395084381\n",
      "Validation: Epoch [23], Batch [572/938], Loss: 0.38011667132377625\n",
      "Validation: Epoch [23], Batch [573/938], Loss: 0.448070764541626\n",
      "Validation: Epoch [23], Batch [574/938], Loss: 0.5512938499450684\n",
      "Validation: Epoch [23], Batch [575/938], Loss: 0.3808271288871765\n",
      "Validation: Epoch [23], Batch [576/938], Loss: 0.4368860721588135\n",
      "Validation: Epoch [23], Batch [577/938], Loss: 0.47596195340156555\n",
      "Validation: Epoch [23], Batch [578/938], Loss: 0.4065968990325928\n",
      "Validation: Epoch [23], Batch [579/938], Loss: 0.6066452264785767\n",
      "Validation: Epoch [23], Batch [580/938], Loss: 0.28898686170578003\n",
      "Validation: Epoch [23], Batch [581/938], Loss: 0.36392903327941895\n",
      "Validation: Epoch [23], Batch [582/938], Loss: 0.3309853672981262\n",
      "Validation: Epoch [23], Batch [583/938], Loss: 0.34655284881591797\n",
      "Validation: Epoch [23], Batch [584/938], Loss: 0.41897261142730713\n",
      "Validation: Epoch [23], Batch [585/938], Loss: 0.3230830430984497\n",
      "Validation: Epoch [23], Batch [586/938], Loss: 0.4625304341316223\n",
      "Validation: Epoch [23], Batch [587/938], Loss: 0.5934057831764221\n",
      "Validation: Epoch [23], Batch [588/938], Loss: 0.3271148204803467\n",
      "Validation: Epoch [23], Batch [589/938], Loss: 0.18414777517318726\n",
      "Validation: Epoch [23], Batch [590/938], Loss: 0.40231508016586304\n",
      "Validation: Epoch [23], Batch [591/938], Loss: 0.21427220106124878\n",
      "Validation: Epoch [23], Batch [592/938], Loss: 0.25582343339920044\n",
      "Validation: Epoch [23], Batch [593/938], Loss: 0.30475103855133057\n",
      "Validation: Epoch [23], Batch [594/938], Loss: 0.5033010244369507\n",
      "Validation: Epoch [23], Batch [595/938], Loss: 0.43973058462142944\n",
      "Validation: Epoch [23], Batch [596/938], Loss: 0.44811415672302246\n",
      "Validation: Epoch [23], Batch [597/938], Loss: 0.22213777899742126\n",
      "Validation: Epoch [23], Batch [598/938], Loss: 0.3781920075416565\n",
      "Validation: Epoch [23], Batch [599/938], Loss: 0.3526577353477478\n",
      "Validation: Epoch [23], Batch [600/938], Loss: 0.41162002086639404\n",
      "Validation: Epoch [23], Batch [601/938], Loss: 0.3742368817329407\n",
      "Validation: Epoch [23], Batch [602/938], Loss: 0.46660685539245605\n",
      "Validation: Epoch [23], Batch [603/938], Loss: 0.4545693099498749\n",
      "Validation: Epoch [23], Batch [604/938], Loss: 0.5340065956115723\n",
      "Validation: Epoch [23], Batch [605/938], Loss: 0.3101116418838501\n",
      "Validation: Epoch [23], Batch [606/938], Loss: 0.47760745882987976\n",
      "Validation: Epoch [23], Batch [607/938], Loss: 0.3637542128562927\n",
      "Validation: Epoch [23], Batch [608/938], Loss: 0.4136148691177368\n",
      "Validation: Epoch [23], Batch [609/938], Loss: 0.3484088182449341\n",
      "Validation: Epoch [23], Batch [610/938], Loss: 0.23518799245357513\n",
      "Validation: Epoch [23], Batch [611/938], Loss: 0.4335484802722931\n",
      "Validation: Epoch [23], Batch [612/938], Loss: 0.36763545870780945\n",
      "Validation: Epoch [23], Batch [613/938], Loss: 0.36120203137397766\n",
      "Validation: Epoch [23], Batch [614/938], Loss: 0.4280063509941101\n",
      "Validation: Epoch [23], Batch [615/938], Loss: 0.3717533349990845\n",
      "Validation: Epoch [23], Batch [616/938], Loss: 0.5522245764732361\n",
      "Validation: Epoch [23], Batch [617/938], Loss: 0.4737236201763153\n",
      "Validation: Epoch [23], Batch [618/938], Loss: 0.2673519253730774\n",
      "Validation: Epoch [23], Batch [619/938], Loss: 0.2963595688343048\n",
      "Validation: Epoch [23], Batch [620/938], Loss: 0.3467610478401184\n",
      "Validation: Epoch [23], Batch [621/938], Loss: 0.3962530493736267\n",
      "Validation: Epoch [23], Batch [622/938], Loss: 0.49012961983680725\n",
      "Validation: Epoch [23], Batch [623/938], Loss: 0.39620327949523926\n",
      "Validation: Epoch [23], Batch [624/938], Loss: 0.4246455729007721\n",
      "Validation: Epoch [23], Batch [625/938], Loss: 0.32210031151771545\n",
      "Validation: Epoch [23], Batch [626/938], Loss: 0.2349356859922409\n",
      "Validation: Epoch [23], Batch [627/938], Loss: 0.2112986147403717\n",
      "Validation: Epoch [23], Batch [628/938], Loss: 0.3480857014656067\n",
      "Validation: Epoch [23], Batch [629/938], Loss: 0.46331629157066345\n",
      "Validation: Epoch [23], Batch [630/938], Loss: 0.34839677810668945\n",
      "Validation: Epoch [23], Batch [631/938], Loss: 0.2706342935562134\n",
      "Validation: Epoch [23], Batch [632/938], Loss: 0.20912756025791168\n",
      "Validation: Epoch [23], Batch [633/938], Loss: 0.2488512396812439\n",
      "Validation: Epoch [23], Batch [634/938], Loss: 0.33451926708221436\n",
      "Validation: Epoch [23], Batch [635/938], Loss: 0.4882202446460724\n",
      "Validation: Epoch [23], Batch [636/938], Loss: 0.27533113956451416\n",
      "Validation: Epoch [23], Batch [637/938], Loss: 0.29197508096694946\n",
      "Validation: Epoch [23], Batch [638/938], Loss: 0.27527159452438354\n",
      "Validation: Epoch [23], Batch [639/938], Loss: 0.23104853928089142\n",
      "Validation: Epoch [23], Batch [640/938], Loss: 0.38204121589660645\n",
      "Validation: Epoch [23], Batch [641/938], Loss: 0.3604544401168823\n",
      "Validation: Epoch [23], Batch [642/938], Loss: 0.37456226348876953\n",
      "Validation: Epoch [23], Batch [643/938], Loss: 0.35501959919929504\n",
      "Validation: Epoch [23], Batch [644/938], Loss: 0.3962390720844269\n",
      "Validation: Epoch [23], Batch [645/938], Loss: 0.21685701608657837\n",
      "Validation: Epoch [23], Batch [646/938], Loss: 0.16813701391220093\n",
      "Validation: Epoch [23], Batch [647/938], Loss: 0.2313307225704193\n",
      "Validation: Epoch [23], Batch [648/938], Loss: 0.20076550543308258\n",
      "Validation: Epoch [23], Batch [649/938], Loss: 0.3524003028869629\n",
      "Validation: Epoch [23], Batch [650/938], Loss: 0.4125784933567047\n",
      "Validation: Epoch [23], Batch [651/938], Loss: 0.3747390806674957\n",
      "Validation: Epoch [23], Batch [652/938], Loss: 0.35489994287490845\n",
      "Validation: Epoch [23], Batch [653/938], Loss: 0.4860053062438965\n",
      "Validation: Epoch [23], Batch [654/938], Loss: 0.3843480348587036\n",
      "Validation: Epoch [23], Batch [655/938], Loss: 0.7683652639389038\n",
      "Validation: Epoch [23], Batch [656/938], Loss: 0.4203478693962097\n",
      "Validation: Epoch [23], Batch [657/938], Loss: 0.4232727885246277\n",
      "Validation: Epoch [23], Batch [658/938], Loss: 0.37197768688201904\n",
      "Validation: Epoch [23], Batch [659/938], Loss: 0.38692933320999146\n",
      "Validation: Epoch [23], Batch [660/938], Loss: 0.4443046748638153\n",
      "Validation: Epoch [23], Batch [661/938], Loss: 0.3849869668483734\n",
      "Validation: Epoch [23], Batch [662/938], Loss: 0.2729533910751343\n",
      "Validation: Epoch [23], Batch [663/938], Loss: 0.35372596979141235\n",
      "Validation: Epoch [23], Batch [664/938], Loss: 0.4985775649547577\n",
      "Validation: Epoch [23], Batch [665/938], Loss: 0.3739181458950043\n",
      "Validation: Epoch [23], Batch [666/938], Loss: 0.3976229727268219\n",
      "Validation: Epoch [23], Batch [667/938], Loss: 0.42275136709213257\n",
      "Validation: Epoch [23], Batch [668/938], Loss: 0.4117548167705536\n",
      "Validation: Epoch [23], Batch [669/938], Loss: 0.4560104310512543\n",
      "Validation: Epoch [23], Batch [670/938], Loss: 0.21674570441246033\n",
      "Validation: Epoch [23], Batch [671/938], Loss: 0.6042305827140808\n",
      "Validation: Epoch [23], Batch [672/938], Loss: 0.41443657875061035\n",
      "Validation: Epoch [23], Batch [673/938], Loss: 0.35320019721984863\n",
      "Validation: Epoch [23], Batch [674/938], Loss: 0.4320752024650574\n",
      "Validation: Epoch [23], Batch [675/938], Loss: 0.381273090839386\n",
      "Validation: Epoch [23], Batch [676/938], Loss: 0.3078334629535675\n",
      "Validation: Epoch [23], Batch [677/938], Loss: 0.3833509683609009\n",
      "Validation: Epoch [23], Batch [678/938], Loss: 0.43417370319366455\n",
      "Validation: Epoch [23], Batch [679/938], Loss: 0.2705138921737671\n",
      "Validation: Epoch [23], Batch [680/938], Loss: 0.5742490291595459\n",
      "Validation: Epoch [23], Batch [681/938], Loss: 0.28634902834892273\n",
      "Validation: Epoch [23], Batch [682/938], Loss: 0.39461836218833923\n",
      "Validation: Epoch [23], Batch [683/938], Loss: 0.39931875467300415\n",
      "Validation: Epoch [23], Batch [684/938], Loss: 0.37768474221229553\n",
      "Validation: Epoch [23], Batch [685/938], Loss: 0.36729729175567627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [686/938], Loss: 0.6725265979766846\n",
      "Validation: Epoch [23], Batch [687/938], Loss: 0.3147125542163849\n",
      "Validation: Epoch [23], Batch [688/938], Loss: 0.30096763372421265\n",
      "Validation: Epoch [23], Batch [689/938], Loss: 0.43464452028274536\n",
      "Validation: Epoch [23], Batch [690/938], Loss: 0.3588188886642456\n",
      "Validation: Epoch [23], Batch [691/938], Loss: 0.4008905589580536\n",
      "Validation: Epoch [23], Batch [692/938], Loss: 0.4834989905357361\n",
      "Validation: Epoch [23], Batch [693/938], Loss: 0.33503007888793945\n",
      "Validation: Epoch [23], Batch [694/938], Loss: 0.2666817009449005\n",
      "Validation: Epoch [23], Batch [695/938], Loss: 0.32730090618133545\n",
      "Validation: Epoch [23], Batch [696/938], Loss: 0.3354841470718384\n",
      "Validation: Epoch [23], Batch [697/938], Loss: 0.47210630774497986\n",
      "Validation: Epoch [23], Batch [698/938], Loss: 0.3347817659378052\n",
      "Validation: Epoch [23], Batch [699/938], Loss: 0.22236685454845428\n",
      "Validation: Epoch [23], Batch [700/938], Loss: 0.32929161190986633\n",
      "Validation: Epoch [23], Batch [701/938], Loss: 0.33730265498161316\n",
      "Validation: Epoch [23], Batch [702/938], Loss: 0.3734668791294098\n",
      "Validation: Epoch [23], Batch [703/938], Loss: 0.5096225738525391\n",
      "Validation: Epoch [23], Batch [704/938], Loss: 0.4548072814941406\n",
      "Validation: Epoch [23], Batch [705/938], Loss: 0.27581167221069336\n",
      "Validation: Epoch [23], Batch [706/938], Loss: 0.42716169357299805\n",
      "Validation: Epoch [23], Batch [707/938], Loss: 0.2312193661928177\n",
      "Validation: Epoch [23], Batch [708/938], Loss: 0.5499899387359619\n",
      "Validation: Epoch [23], Batch [709/938], Loss: 0.4048369526863098\n",
      "Validation: Epoch [23], Batch [710/938], Loss: 0.3721766471862793\n",
      "Validation: Epoch [23], Batch [711/938], Loss: 0.11510683596134186\n",
      "Validation: Epoch [23], Batch [712/938], Loss: 0.3371250629425049\n",
      "Validation: Epoch [23], Batch [713/938], Loss: 0.4835163354873657\n",
      "Validation: Epoch [23], Batch [714/938], Loss: 0.18182337284088135\n",
      "Validation: Epoch [23], Batch [715/938], Loss: 0.2769742012023926\n",
      "Validation: Epoch [23], Batch [716/938], Loss: 0.231731116771698\n",
      "Validation: Epoch [23], Batch [717/938], Loss: 0.26428401470184326\n",
      "Validation: Epoch [23], Batch [718/938], Loss: 0.31611210107803345\n",
      "Validation: Epoch [23], Batch [719/938], Loss: 0.4882076382637024\n",
      "Validation: Epoch [23], Batch [720/938], Loss: 0.4088163673877716\n",
      "Validation: Epoch [23], Batch [721/938], Loss: 0.3242812156677246\n",
      "Validation: Epoch [23], Batch [722/938], Loss: 0.3511078953742981\n",
      "Validation: Epoch [23], Batch [723/938], Loss: 0.27814286947250366\n",
      "Validation: Epoch [23], Batch [724/938], Loss: 0.45927998423576355\n",
      "Validation: Epoch [23], Batch [725/938], Loss: 0.3059001863002777\n",
      "Validation: Epoch [23], Batch [726/938], Loss: 0.3446773588657379\n",
      "Validation: Epoch [23], Batch [727/938], Loss: 0.4958269000053406\n",
      "Validation: Epoch [23], Batch [728/938], Loss: 0.3173324465751648\n",
      "Validation: Epoch [23], Batch [729/938], Loss: 0.41074973344802856\n",
      "Validation: Epoch [23], Batch [730/938], Loss: 0.25575339794158936\n",
      "Validation: Epoch [23], Batch [731/938], Loss: 0.518363356590271\n",
      "Validation: Epoch [23], Batch [732/938], Loss: 0.5079476833343506\n",
      "Validation: Epoch [23], Batch [733/938], Loss: 0.29970741271972656\n",
      "Validation: Epoch [23], Batch [734/938], Loss: 0.31798937916755676\n",
      "Validation: Epoch [23], Batch [735/938], Loss: 0.3742377758026123\n",
      "Validation: Epoch [23], Batch [736/938], Loss: 0.3664764165878296\n",
      "Validation: Epoch [23], Batch [737/938], Loss: 0.3891727328300476\n",
      "Validation: Epoch [23], Batch [738/938], Loss: 0.3758305311203003\n",
      "Validation: Epoch [23], Batch [739/938], Loss: 0.2725656032562256\n",
      "Validation: Epoch [23], Batch [740/938], Loss: 0.4177566170692444\n",
      "Validation: Epoch [23], Batch [741/938], Loss: 0.3789994716644287\n",
      "Validation: Epoch [23], Batch [742/938], Loss: 0.38813674449920654\n",
      "Validation: Epoch [23], Batch [743/938], Loss: 0.5833390355110168\n",
      "Validation: Epoch [23], Batch [744/938], Loss: 0.26100409030914307\n",
      "Validation: Epoch [23], Batch [745/938], Loss: 0.4455825090408325\n",
      "Validation: Epoch [23], Batch [746/938], Loss: 0.22025242447853088\n",
      "Validation: Epoch [23], Batch [747/938], Loss: 0.49432572722435\n",
      "Validation: Epoch [23], Batch [748/938], Loss: 0.32876965403556824\n",
      "Validation: Epoch [23], Batch [749/938], Loss: 0.3686596751213074\n",
      "Validation: Epoch [23], Batch [750/938], Loss: 0.3943604826927185\n",
      "Validation: Epoch [23], Batch [751/938], Loss: 0.4790545105934143\n",
      "Validation: Epoch [23], Batch [752/938], Loss: 0.28087881207466125\n",
      "Validation: Epoch [23], Batch [753/938], Loss: 0.30093738436698914\n",
      "Validation: Epoch [23], Batch [754/938], Loss: 0.4166299104690552\n",
      "Validation: Epoch [23], Batch [755/938], Loss: 0.48508360981941223\n",
      "Validation: Epoch [23], Batch [756/938], Loss: 0.23437094688415527\n",
      "Validation: Epoch [23], Batch [757/938], Loss: 0.3658517003059387\n",
      "Validation: Epoch [23], Batch [758/938], Loss: 0.380566269159317\n",
      "Validation: Epoch [23], Batch [759/938], Loss: 0.22185030579566956\n",
      "Validation: Epoch [23], Batch [760/938], Loss: 0.501095175743103\n",
      "Validation: Epoch [23], Batch [761/938], Loss: 0.3731953501701355\n",
      "Validation: Epoch [23], Batch [762/938], Loss: 0.3272705078125\n",
      "Validation: Epoch [23], Batch [763/938], Loss: 0.2646859288215637\n",
      "Validation: Epoch [23], Batch [764/938], Loss: 0.46936267614364624\n",
      "Validation: Epoch [23], Batch [765/938], Loss: 0.4001070261001587\n",
      "Validation: Epoch [23], Batch [766/938], Loss: 0.5885293483734131\n",
      "Validation: Epoch [23], Batch [767/938], Loss: 0.2620236873626709\n",
      "Validation: Epoch [23], Batch [768/938], Loss: 0.30215078592300415\n",
      "Validation: Epoch [23], Batch [769/938], Loss: 0.5089486837387085\n",
      "Validation: Epoch [23], Batch [770/938], Loss: 0.48869338631629944\n",
      "Validation: Epoch [23], Batch [771/938], Loss: 0.43318384885787964\n",
      "Validation: Epoch [23], Batch [772/938], Loss: 0.37820810079574585\n",
      "Validation: Epoch [23], Batch [773/938], Loss: 0.3356410562992096\n",
      "Validation: Epoch [23], Batch [774/938], Loss: 0.35141175985336304\n",
      "Validation: Epoch [23], Batch [775/938], Loss: 0.4505664110183716\n",
      "Validation: Epoch [23], Batch [776/938], Loss: 0.3624929189682007\n",
      "Validation: Epoch [23], Batch [777/938], Loss: 0.45491039752960205\n",
      "Validation: Epoch [23], Batch [778/938], Loss: 0.29469141364097595\n",
      "Validation: Epoch [23], Batch [779/938], Loss: 0.43980908393859863\n",
      "Validation: Epoch [23], Batch [780/938], Loss: 0.26957035064697266\n",
      "Validation: Epoch [23], Batch [781/938], Loss: 0.36543554067611694\n",
      "Validation: Epoch [23], Batch [782/938], Loss: 0.3374352753162384\n",
      "Validation: Epoch [23], Batch [783/938], Loss: 0.32970401644706726\n",
      "Validation: Epoch [23], Batch [784/938], Loss: 0.18708696961402893\n",
      "Validation: Epoch [23], Batch [785/938], Loss: 0.2995885908603668\n",
      "Validation: Epoch [23], Batch [786/938], Loss: 0.46787792444229126\n",
      "Validation: Epoch [23], Batch [787/938], Loss: 0.3447384834289551\n",
      "Validation: Epoch [23], Batch [788/938], Loss: 0.23712725937366486\n",
      "Validation: Epoch [23], Batch [789/938], Loss: 0.32221949100494385\n",
      "Validation: Epoch [23], Batch [790/938], Loss: 0.31914007663726807\n",
      "Validation: Epoch [23], Batch [791/938], Loss: 0.45460548996925354\n",
      "Validation: Epoch [23], Batch [792/938], Loss: 0.4076051414012909\n",
      "Validation: Epoch [23], Batch [793/938], Loss: 0.3759162425994873\n",
      "Validation: Epoch [23], Batch [794/938], Loss: 0.25784075260162354\n",
      "Validation: Epoch [23], Batch [795/938], Loss: 0.3295334279537201\n",
      "Validation: Epoch [23], Batch [796/938], Loss: 0.25103235244750977\n",
      "Validation: Epoch [23], Batch [797/938], Loss: 0.3440288007259369\n",
      "Validation: Epoch [23], Batch [798/938], Loss: 0.23370766639709473\n",
      "Validation: Epoch [23], Batch [799/938], Loss: 0.5068028569221497\n",
      "Validation: Epoch [23], Batch [800/938], Loss: 0.6130459308624268\n",
      "Validation: Epoch [23], Batch [801/938], Loss: 0.3520946502685547\n",
      "Validation: Epoch [23], Batch [802/938], Loss: 0.5423738360404968\n",
      "Validation: Epoch [23], Batch [803/938], Loss: 0.3270561695098877\n",
      "Validation: Epoch [23], Batch [804/938], Loss: 0.4168389141559601\n",
      "Validation: Epoch [23], Batch [805/938], Loss: 0.3662012815475464\n",
      "Validation: Epoch [23], Batch [806/938], Loss: 0.23921257257461548\n",
      "Validation: Epoch [23], Batch [807/938], Loss: 0.32925909757614136\n",
      "Validation: Epoch [23], Batch [808/938], Loss: 0.25070324540138245\n",
      "Validation: Epoch [23], Batch [809/938], Loss: 0.27851402759552\n",
      "Validation: Epoch [23], Batch [810/938], Loss: 0.38160204887390137\n",
      "Validation: Epoch [23], Batch [811/938], Loss: 0.5100628137588501\n",
      "Validation: Epoch [23], Batch [812/938], Loss: 0.319013774394989\n",
      "Validation: Epoch [23], Batch [813/938], Loss: 0.40172916650772095\n",
      "Validation: Epoch [23], Batch [814/938], Loss: 0.7296019792556763\n",
      "Validation: Epoch [23], Batch [815/938], Loss: 0.2978941798210144\n",
      "Validation: Epoch [23], Batch [816/938], Loss: 0.4619864225387573\n",
      "Validation: Epoch [23], Batch [817/938], Loss: 0.3113953471183777\n",
      "Validation: Epoch [23], Batch [818/938], Loss: 0.42407989501953125\n",
      "Validation: Epoch [23], Batch [819/938], Loss: 0.32485121488571167\n",
      "Validation: Epoch [23], Batch [820/938], Loss: 0.3536731004714966\n",
      "Validation: Epoch [23], Batch [821/938], Loss: 0.47010624408721924\n",
      "Validation: Epoch [23], Batch [822/938], Loss: 0.32883554697036743\n",
      "Validation: Epoch [23], Batch [823/938], Loss: 0.23037685453891754\n",
      "Validation: Epoch [23], Batch [824/938], Loss: 0.17821982502937317\n",
      "Validation: Epoch [23], Batch [825/938], Loss: 0.2277795821428299\n",
      "Validation: Epoch [23], Batch [826/938], Loss: 0.32302528619766235\n",
      "Validation: Epoch [23], Batch [827/938], Loss: 0.5041937232017517\n",
      "Validation: Epoch [23], Batch [828/938], Loss: 0.3194608688354492\n",
      "Validation: Epoch [23], Batch [829/938], Loss: 0.36934030055999756\n",
      "Validation: Epoch [23], Batch [830/938], Loss: 0.40821439027786255\n",
      "Validation: Epoch [23], Batch [831/938], Loss: 0.4891371726989746\n",
      "Validation: Epoch [23], Batch [832/938], Loss: 0.5564459562301636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [23], Batch [833/938], Loss: 0.37988853454589844\n",
      "Validation: Epoch [23], Batch [834/938], Loss: 0.346119225025177\n",
      "Validation: Epoch [23], Batch [835/938], Loss: 0.29058998823165894\n",
      "Validation: Epoch [23], Batch [836/938], Loss: 0.22692561149597168\n",
      "Validation: Epoch [23], Batch [837/938], Loss: 0.3387405574321747\n",
      "Validation: Epoch [23], Batch [838/938], Loss: 0.22114625573158264\n",
      "Validation: Epoch [23], Batch [839/938], Loss: 0.445759654045105\n",
      "Validation: Epoch [23], Batch [840/938], Loss: 0.3548581600189209\n",
      "Validation: Epoch [23], Batch [841/938], Loss: 0.3823012113571167\n",
      "Validation: Epoch [23], Batch [842/938], Loss: 0.45697852969169617\n",
      "Validation: Epoch [23], Batch [843/938], Loss: 0.6342760324478149\n",
      "Validation: Epoch [23], Batch [844/938], Loss: 0.32523858547210693\n",
      "Validation: Epoch [23], Batch [845/938], Loss: 0.2956005930900574\n",
      "Validation: Epoch [23], Batch [846/938], Loss: 0.3770226836204529\n",
      "Validation: Epoch [23], Batch [847/938], Loss: 0.4147604703903198\n",
      "Validation: Epoch [23], Batch [848/938], Loss: 0.30190736055374146\n",
      "Validation: Epoch [23], Batch [849/938], Loss: 0.281766414642334\n",
      "Validation: Epoch [23], Batch [850/938], Loss: 0.32286307215690613\n",
      "Validation: Epoch [23], Batch [851/938], Loss: 0.49840641021728516\n",
      "Validation: Epoch [23], Batch [852/938], Loss: 0.2666521966457367\n",
      "Validation: Epoch [23], Batch [853/938], Loss: 0.5539494752883911\n",
      "Validation: Epoch [23], Batch [854/938], Loss: 0.3614473342895508\n",
      "Validation: Epoch [23], Batch [855/938], Loss: 0.28027698397636414\n",
      "Validation: Epoch [23], Batch [856/938], Loss: 0.45447424054145813\n",
      "Validation: Epoch [23], Batch [857/938], Loss: 0.273845374584198\n",
      "Validation: Epoch [23], Batch [858/938], Loss: 0.26551488041877747\n",
      "Validation: Epoch [23], Batch [859/938], Loss: 0.2486531138420105\n",
      "Validation: Epoch [23], Batch [860/938], Loss: 0.3617593050003052\n",
      "Validation: Epoch [23], Batch [861/938], Loss: 0.3592582643032074\n",
      "Validation: Epoch [23], Batch [862/938], Loss: 0.2898792028427124\n",
      "Validation: Epoch [23], Batch [863/938], Loss: 0.6092020869255066\n",
      "Validation: Epoch [23], Batch [864/938], Loss: 0.2570892572402954\n",
      "Validation: Epoch [23], Batch [865/938], Loss: 0.42066991329193115\n",
      "Validation: Epoch [23], Batch [866/938], Loss: 0.503105103969574\n",
      "Validation: Epoch [23], Batch [867/938], Loss: 0.34268298745155334\n",
      "Validation: Epoch [23], Batch [868/938], Loss: 0.45838528871536255\n",
      "Validation: Epoch [23], Batch [869/938], Loss: 0.44448450207710266\n",
      "Validation: Epoch [23], Batch [870/938], Loss: 0.4132485091686249\n",
      "Validation: Epoch [23], Batch [871/938], Loss: 0.28997302055358887\n",
      "Validation: Epoch [23], Batch [872/938], Loss: 0.2325410097837448\n",
      "Validation: Epoch [23], Batch [873/938], Loss: 0.3496721684932709\n",
      "Validation: Epoch [23], Batch [874/938], Loss: 0.311452180147171\n",
      "Validation: Epoch [23], Batch [875/938], Loss: 0.37703633308410645\n",
      "Validation: Epoch [23], Batch [876/938], Loss: 0.38387244939804077\n",
      "Validation: Epoch [23], Batch [877/938], Loss: 0.38600385189056396\n",
      "Validation: Epoch [23], Batch [878/938], Loss: 0.4079401195049286\n",
      "Validation: Epoch [23], Batch [879/938], Loss: 0.3978514075279236\n",
      "Validation: Epoch [23], Batch [880/938], Loss: 0.22564080357551575\n",
      "Validation: Epoch [23], Batch [881/938], Loss: 0.342637300491333\n",
      "Validation: Epoch [23], Batch [882/938], Loss: 0.2842729389667511\n",
      "Validation: Epoch [23], Batch [883/938], Loss: 0.6170368194580078\n",
      "Validation: Epoch [23], Batch [884/938], Loss: 0.41139400005340576\n",
      "Validation: Epoch [23], Batch [885/938], Loss: 0.29614678025245667\n",
      "Validation: Epoch [23], Batch [886/938], Loss: 0.38522177934646606\n",
      "Validation: Epoch [23], Batch [887/938], Loss: 0.4318951964378357\n",
      "Validation: Epoch [23], Batch [888/938], Loss: 0.48868274688720703\n",
      "Validation: Epoch [23], Batch [889/938], Loss: 0.3811115026473999\n",
      "Validation: Epoch [23], Batch [890/938], Loss: 0.37472236156463623\n",
      "Validation: Epoch [23], Batch [891/938], Loss: 0.39737749099731445\n",
      "Validation: Epoch [23], Batch [892/938], Loss: 0.4154711365699768\n",
      "Validation: Epoch [23], Batch [893/938], Loss: 0.3105464577674866\n",
      "Validation: Epoch [23], Batch [894/938], Loss: 0.30645889043807983\n",
      "Validation: Epoch [23], Batch [895/938], Loss: 0.31477460265159607\n",
      "Validation: Epoch [23], Batch [896/938], Loss: 0.2923826277256012\n",
      "Validation: Epoch [23], Batch [897/938], Loss: 0.40587466955184937\n",
      "Validation: Epoch [23], Batch [898/938], Loss: 0.42486435174942017\n",
      "Validation: Epoch [23], Batch [899/938], Loss: 0.3531753420829773\n",
      "Validation: Epoch [23], Batch [900/938], Loss: 0.37672778964042664\n",
      "Validation: Epoch [23], Batch [901/938], Loss: 0.44433680176734924\n",
      "Validation: Epoch [23], Batch [902/938], Loss: 0.23497581481933594\n",
      "Validation: Epoch [23], Batch [903/938], Loss: 0.5053770542144775\n",
      "Validation: Epoch [23], Batch [904/938], Loss: 0.48109662532806396\n",
      "Validation: Epoch [23], Batch [905/938], Loss: 0.5836591720581055\n",
      "Validation: Epoch [23], Batch [906/938], Loss: 0.407855361700058\n",
      "Validation: Epoch [23], Batch [907/938], Loss: 0.4640454947948456\n",
      "Validation: Epoch [23], Batch [908/938], Loss: 0.4111802279949188\n",
      "Validation: Epoch [23], Batch [909/938], Loss: 0.35083937644958496\n",
      "Validation: Epoch [23], Batch [910/938], Loss: 0.3435225486755371\n",
      "Validation: Epoch [23], Batch [911/938], Loss: 0.2945919632911682\n",
      "Validation: Epoch [23], Batch [912/938], Loss: 0.37534794211387634\n",
      "Validation: Epoch [23], Batch [913/938], Loss: 0.3536126911640167\n",
      "Validation: Epoch [23], Batch [914/938], Loss: 0.3726063072681427\n",
      "Validation: Epoch [23], Batch [915/938], Loss: 0.3957121968269348\n",
      "Validation: Epoch [23], Batch [916/938], Loss: 0.45204317569732666\n",
      "Validation: Epoch [23], Batch [917/938], Loss: 0.2332153022289276\n",
      "Validation: Epoch [23], Batch [918/938], Loss: 0.5284807085990906\n",
      "Validation: Epoch [23], Batch [919/938], Loss: 0.3178814947605133\n",
      "Validation: Epoch [23], Batch [920/938], Loss: 0.22138458490371704\n",
      "Validation: Epoch [23], Batch [921/938], Loss: 0.4488772451877594\n",
      "Validation: Epoch [23], Batch [922/938], Loss: 0.33329522609710693\n",
      "Validation: Epoch [23], Batch [923/938], Loss: 0.24145913124084473\n",
      "Validation: Epoch [23], Batch [924/938], Loss: 0.32545411586761475\n",
      "Validation: Epoch [23], Batch [925/938], Loss: 0.3718704283237457\n",
      "Validation: Epoch [23], Batch [926/938], Loss: 0.5482631325721741\n",
      "Validation: Epoch [23], Batch [927/938], Loss: 0.36597371101379395\n",
      "Validation: Epoch [23], Batch [928/938], Loss: 0.28010839223861694\n",
      "Validation: Epoch [23], Batch [929/938], Loss: 0.35412728786468506\n",
      "Validation: Epoch [23], Batch [930/938], Loss: 0.3587871789932251\n",
      "Validation: Epoch [23], Batch [931/938], Loss: 0.5169737339019775\n",
      "Validation: Epoch [23], Batch [932/938], Loss: 0.3068234324455261\n",
      "Validation: Epoch [23], Batch [933/938], Loss: 0.3601085841655731\n",
      "Validation: Epoch [23], Batch [934/938], Loss: 0.3421095609664917\n",
      "Validation: Epoch [23], Batch [935/938], Loss: 0.37463319301605225\n",
      "Validation: Epoch [23], Batch [936/938], Loss: 0.3699759542942047\n",
      "Validation: Epoch [23], Batch [937/938], Loss: 0.356444776058197\n",
      "Validation: Epoch [23], Batch [938/938], Loss: 0.46501556038856506\n",
      "Accuracy of test set: 0.8662833333333333\n",
      "Train: Epoch [24], Batch [1/938], Loss: 0.4371861219406128\n",
      "Train: Epoch [24], Batch [2/938], Loss: 0.582830548286438\n",
      "Train: Epoch [24], Batch [3/938], Loss: 0.5976266264915466\n",
      "Train: Epoch [24], Batch [4/938], Loss: 0.38713887333869934\n",
      "Train: Epoch [24], Batch [5/938], Loss: 0.34474095702171326\n",
      "Train: Epoch [24], Batch [6/938], Loss: 0.3791433870792389\n",
      "Train: Epoch [24], Batch [7/938], Loss: 0.640204906463623\n",
      "Train: Epoch [24], Batch [8/938], Loss: 0.3886904716491699\n",
      "Train: Epoch [24], Batch [9/938], Loss: 0.28378865122795105\n",
      "Train: Epoch [24], Batch [10/938], Loss: 0.40889546275138855\n",
      "Train: Epoch [24], Batch [11/938], Loss: 0.37853115797042847\n",
      "Train: Epoch [24], Batch [12/938], Loss: 0.6385540962219238\n",
      "Train: Epoch [24], Batch [13/938], Loss: 0.3541177213191986\n",
      "Train: Epoch [24], Batch [14/938], Loss: 0.30441880226135254\n",
      "Train: Epoch [24], Batch [15/938], Loss: 0.22514019906520844\n",
      "Train: Epoch [24], Batch [16/938], Loss: 0.3278285264968872\n",
      "Train: Epoch [24], Batch [17/938], Loss: 0.33476153016090393\n",
      "Train: Epoch [24], Batch [18/938], Loss: 0.720319390296936\n",
      "Train: Epoch [24], Batch [19/938], Loss: 0.2706555426120758\n",
      "Train: Epoch [24], Batch [20/938], Loss: 0.2761247158050537\n",
      "Train: Epoch [24], Batch [21/938], Loss: 0.29189610481262207\n",
      "Train: Epoch [24], Batch [22/938], Loss: 0.4454977810382843\n",
      "Train: Epoch [24], Batch [23/938], Loss: 0.5373935699462891\n",
      "Train: Epoch [24], Batch [24/938], Loss: 0.33415472507476807\n",
      "Train: Epoch [24], Batch [25/938], Loss: 0.615836501121521\n",
      "Train: Epoch [24], Batch [26/938], Loss: 0.3666391372680664\n",
      "Train: Epoch [24], Batch [27/938], Loss: 0.40598922967910767\n",
      "Train: Epoch [24], Batch [28/938], Loss: 0.3613404631614685\n",
      "Train: Epoch [24], Batch [29/938], Loss: 0.4203016757965088\n",
      "Train: Epoch [24], Batch [30/938], Loss: 0.38493064045906067\n",
      "Train: Epoch [24], Batch [31/938], Loss: 0.2447492480278015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [32/938], Loss: 0.2947617173194885\n",
      "Train: Epoch [24], Batch [33/938], Loss: 0.39799341559410095\n",
      "Train: Epoch [24], Batch [34/938], Loss: 0.4489556550979614\n",
      "Train: Epoch [24], Batch [35/938], Loss: 0.6017026901245117\n",
      "Train: Epoch [24], Batch [36/938], Loss: 0.3881348967552185\n",
      "Train: Epoch [24], Batch [37/938], Loss: 0.3968857526779175\n",
      "Train: Epoch [24], Batch [38/938], Loss: 0.5329949855804443\n",
      "Train: Epoch [24], Batch [39/938], Loss: 0.37913769483566284\n",
      "Train: Epoch [24], Batch [40/938], Loss: 0.3826006054878235\n",
      "Train: Epoch [24], Batch [41/938], Loss: 0.4655388593673706\n",
      "Train: Epoch [24], Batch [42/938], Loss: 0.5279898643493652\n",
      "Train: Epoch [24], Batch [43/938], Loss: 0.36660462617874146\n",
      "Train: Epoch [24], Batch [44/938], Loss: 0.4733336567878723\n",
      "Train: Epoch [24], Batch [45/938], Loss: 0.34307390451431274\n",
      "Train: Epoch [24], Batch [46/938], Loss: 0.31067705154418945\n",
      "Train: Epoch [24], Batch [47/938], Loss: 0.24107545614242554\n",
      "Train: Epoch [24], Batch [48/938], Loss: 0.30196407437324524\n",
      "Train: Epoch [24], Batch [49/938], Loss: 0.2592196464538574\n",
      "Train: Epoch [24], Batch [50/938], Loss: 0.3600791096687317\n",
      "Train: Epoch [24], Batch [51/938], Loss: 0.24172496795654297\n",
      "Train: Epoch [24], Batch [52/938], Loss: 0.22229458391666412\n",
      "Train: Epoch [24], Batch [53/938], Loss: 0.33866626024246216\n",
      "Train: Epoch [24], Batch [54/938], Loss: 0.40957728028297424\n",
      "Train: Epoch [24], Batch [55/938], Loss: 0.44058912992477417\n",
      "Train: Epoch [24], Batch [56/938], Loss: 0.6793373823165894\n",
      "Train: Epoch [24], Batch [57/938], Loss: 0.33078569173812866\n",
      "Train: Epoch [24], Batch [58/938], Loss: 0.47517478466033936\n",
      "Train: Epoch [24], Batch [59/938], Loss: 0.2597895860671997\n",
      "Train: Epoch [24], Batch [60/938], Loss: 0.31411212682724\n",
      "Train: Epoch [24], Batch [61/938], Loss: 0.46116888523101807\n",
      "Train: Epoch [24], Batch [62/938], Loss: 0.6630725860595703\n",
      "Train: Epoch [24], Batch [63/938], Loss: 0.3625989258289337\n",
      "Train: Epoch [24], Batch [64/938], Loss: 0.49469175934791565\n",
      "Train: Epoch [24], Batch [65/938], Loss: 0.36530759930610657\n",
      "Train: Epoch [24], Batch [66/938], Loss: 0.3410416841506958\n",
      "Train: Epoch [24], Batch [67/938], Loss: 0.3266826868057251\n",
      "Train: Epoch [24], Batch [68/938], Loss: 0.43167558312416077\n",
      "Train: Epoch [24], Batch [69/938], Loss: 0.23905956745147705\n",
      "Train: Epoch [24], Batch [70/938], Loss: 0.35952460765838623\n",
      "Train: Epoch [24], Batch [71/938], Loss: 0.5546504259109497\n",
      "Train: Epoch [24], Batch [72/938], Loss: 0.45421016216278076\n",
      "Train: Epoch [24], Batch [73/938], Loss: 0.3559015691280365\n",
      "Train: Epoch [24], Batch [74/938], Loss: 0.24837246537208557\n",
      "Train: Epoch [24], Batch [75/938], Loss: 0.3848162889480591\n",
      "Train: Epoch [24], Batch [76/938], Loss: 0.3443738520145416\n",
      "Train: Epoch [24], Batch [77/938], Loss: 0.44671934843063354\n",
      "Train: Epoch [24], Batch [78/938], Loss: 0.3359881639480591\n",
      "Train: Epoch [24], Batch [79/938], Loss: 0.34571826457977295\n",
      "Train: Epoch [24], Batch [80/938], Loss: 0.2597709000110626\n",
      "Train: Epoch [24], Batch [81/938], Loss: 0.6270556449890137\n",
      "Train: Epoch [24], Batch [82/938], Loss: 0.41477876901626587\n",
      "Train: Epoch [24], Batch [83/938], Loss: 0.18958762288093567\n",
      "Train: Epoch [24], Batch [84/938], Loss: 0.34830164909362793\n",
      "Train: Epoch [24], Batch [85/938], Loss: 0.326373815536499\n",
      "Train: Epoch [24], Batch [86/938], Loss: 0.37173354625701904\n",
      "Train: Epoch [24], Batch [87/938], Loss: 0.5498088598251343\n",
      "Train: Epoch [24], Batch [88/938], Loss: 0.29728561639785767\n",
      "Train: Epoch [24], Batch [89/938], Loss: 0.34274113178253174\n",
      "Train: Epoch [24], Batch [90/938], Loss: 0.5897306203842163\n",
      "Train: Epoch [24], Batch [91/938], Loss: 0.48536595702171326\n",
      "Train: Epoch [24], Batch [92/938], Loss: 0.3461449146270752\n",
      "Train: Epoch [24], Batch [93/938], Loss: 0.5427993535995483\n",
      "Train: Epoch [24], Batch [94/938], Loss: 0.3227714002132416\n",
      "Train: Epoch [24], Batch [95/938], Loss: 0.2997646927833557\n",
      "Train: Epoch [24], Batch [96/938], Loss: 0.23704808950424194\n",
      "Train: Epoch [24], Batch [97/938], Loss: 0.36294952034950256\n",
      "Train: Epoch [24], Batch [98/938], Loss: 0.4535709321498871\n",
      "Train: Epoch [24], Batch [99/938], Loss: 0.40594756603240967\n",
      "Train: Epoch [24], Batch [100/938], Loss: 0.46768492460250854\n",
      "Train: Epoch [24], Batch [101/938], Loss: 0.18998916447162628\n",
      "Train: Epoch [24], Batch [102/938], Loss: 0.2945210933685303\n",
      "Train: Epoch [24], Batch [103/938], Loss: 0.39881736040115356\n",
      "Train: Epoch [24], Batch [104/938], Loss: 0.34153616428375244\n",
      "Train: Epoch [24], Batch [105/938], Loss: 0.4456709027290344\n",
      "Train: Epoch [24], Batch [106/938], Loss: 0.35791951417922974\n",
      "Train: Epoch [24], Batch [107/938], Loss: 0.44950953125953674\n",
      "Train: Epoch [24], Batch [108/938], Loss: 0.3275109827518463\n",
      "Train: Epoch [24], Batch [109/938], Loss: 0.2568128705024719\n",
      "Train: Epoch [24], Batch [110/938], Loss: 0.3594614267349243\n",
      "Train: Epoch [24], Batch [111/938], Loss: 0.4414758086204529\n",
      "Train: Epoch [24], Batch [112/938], Loss: 0.3995283842086792\n",
      "Train: Epoch [24], Batch [113/938], Loss: 0.4271140992641449\n",
      "Train: Epoch [24], Batch [114/938], Loss: 0.29078739881515503\n",
      "Train: Epoch [24], Batch [115/938], Loss: 0.4189056158065796\n",
      "Train: Epoch [24], Batch [116/938], Loss: 0.5088863372802734\n",
      "Train: Epoch [24], Batch [117/938], Loss: 0.38078397512435913\n",
      "Train: Epoch [24], Batch [118/938], Loss: 0.32947835326194763\n",
      "Train: Epoch [24], Batch [119/938], Loss: 0.2961028516292572\n",
      "Train: Epoch [24], Batch [120/938], Loss: 0.44388657808303833\n",
      "Train: Epoch [24], Batch [121/938], Loss: 0.3535057306289673\n",
      "Train: Epoch [24], Batch [122/938], Loss: 0.3819025456905365\n",
      "Train: Epoch [24], Batch [123/938], Loss: 0.3294826149940491\n",
      "Train: Epoch [24], Batch [124/938], Loss: 0.33172741532325745\n",
      "Train: Epoch [24], Batch [125/938], Loss: 0.3022204041481018\n",
      "Train: Epoch [24], Batch [126/938], Loss: 0.19607998430728912\n",
      "Train: Epoch [24], Batch [127/938], Loss: 0.35725483298301697\n",
      "Train: Epoch [24], Batch [128/938], Loss: 0.3323701322078705\n",
      "Train: Epoch [24], Batch [129/938], Loss: 0.35626915097236633\n",
      "Train: Epoch [24], Batch [130/938], Loss: 0.3046707510948181\n",
      "Train: Epoch [24], Batch [131/938], Loss: 0.24829107522964478\n",
      "Train: Epoch [24], Batch [132/938], Loss: 0.4738742709159851\n",
      "Train: Epoch [24], Batch [133/938], Loss: 0.4002486765384674\n",
      "Train: Epoch [24], Batch [134/938], Loss: 0.41866281628608704\n",
      "Train: Epoch [24], Batch [135/938], Loss: 0.5213079452514648\n",
      "Train: Epoch [24], Batch [136/938], Loss: 0.3021509647369385\n",
      "Train: Epoch [24], Batch [137/938], Loss: 0.34182363748550415\n",
      "Train: Epoch [24], Batch [138/938], Loss: 0.2870817482471466\n",
      "Train: Epoch [24], Batch [139/938], Loss: 0.36255595088005066\n",
      "Train: Epoch [24], Batch [140/938], Loss: 0.45299798250198364\n",
      "Train: Epoch [24], Batch [141/938], Loss: 0.47342443466186523\n",
      "Train: Epoch [24], Batch [142/938], Loss: 0.4334920644760132\n",
      "Train: Epoch [24], Batch [143/938], Loss: 0.5646020174026489\n",
      "Train: Epoch [24], Batch [144/938], Loss: 0.34412431716918945\n",
      "Train: Epoch [24], Batch [145/938], Loss: 0.23030564188957214\n",
      "Train: Epoch [24], Batch [146/938], Loss: 0.3820783495903015\n",
      "Train: Epoch [24], Batch [147/938], Loss: 0.3695228099822998\n",
      "Train: Epoch [24], Batch [148/938], Loss: 0.2352333813905716\n",
      "Train: Epoch [24], Batch [149/938], Loss: 0.2460486739873886\n",
      "Train: Epoch [24], Batch [150/938], Loss: 0.44836291670799255\n",
      "Train: Epoch [24], Batch [151/938], Loss: 0.3900317847728729\n",
      "Train: Epoch [24], Batch [152/938], Loss: 0.40142032504081726\n",
      "Train: Epoch [24], Batch [153/938], Loss: 0.7003287672996521\n",
      "Train: Epoch [24], Batch [154/938], Loss: 0.38162797689437866\n",
      "Train: Epoch [24], Batch [155/938], Loss: 0.353711873292923\n",
      "Train: Epoch [24], Batch [156/938], Loss: 0.45434054732322693\n",
      "Train: Epoch [24], Batch [157/938], Loss: 0.24633333086967468\n",
      "Train: Epoch [24], Batch [158/938], Loss: 0.504000186920166\n",
      "Train: Epoch [24], Batch [159/938], Loss: 0.4318356513977051\n",
      "Train: Epoch [24], Batch [160/938], Loss: 0.40973958373069763\n",
      "Train: Epoch [24], Batch [161/938], Loss: 0.3784101605415344\n",
      "Train: Epoch [24], Batch [162/938], Loss: 0.32291895151138306\n",
      "Train: Epoch [24], Batch [163/938], Loss: 0.30192503333091736\n",
      "Train: Epoch [24], Batch [164/938], Loss: 0.3612147867679596\n",
      "Train: Epoch [24], Batch [165/938], Loss: 0.42397749423980713\n",
      "Train: Epoch [24], Batch [166/938], Loss: 0.27277156710624695\n",
      "Train: Epoch [24], Batch [167/938], Loss: 0.2746756076812744\n",
      "Train: Epoch [24], Batch [168/938], Loss: 0.2538135051727295\n",
      "Train: Epoch [24], Batch [169/938], Loss: 0.4467252492904663\n",
      "Train: Epoch [24], Batch [170/938], Loss: 0.4958369731903076\n",
      "Train: Epoch [24], Batch [171/938], Loss: 0.338354229927063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [172/938], Loss: 0.349151074886322\n",
      "Train: Epoch [24], Batch [173/938], Loss: 0.43588870763778687\n",
      "Train: Epoch [24], Batch [174/938], Loss: 0.25779056549072266\n",
      "Train: Epoch [24], Batch [175/938], Loss: 0.32827699184417725\n",
      "Train: Epoch [24], Batch [176/938], Loss: 0.43111565709114075\n",
      "Train: Epoch [24], Batch [177/938], Loss: 0.34130674600601196\n",
      "Train: Epoch [24], Batch [178/938], Loss: 0.2898046374320984\n",
      "Train: Epoch [24], Batch [179/938], Loss: 0.5230701565742493\n",
      "Train: Epoch [24], Batch [180/938], Loss: 0.5388302803039551\n",
      "Train: Epoch [24], Batch [181/938], Loss: 0.5148733258247375\n",
      "Train: Epoch [24], Batch [182/938], Loss: 0.4686424136161804\n",
      "Train: Epoch [24], Batch [183/938], Loss: 0.4333484172821045\n",
      "Train: Epoch [24], Batch [184/938], Loss: 0.274728387594223\n",
      "Train: Epoch [24], Batch [185/938], Loss: 0.31140291690826416\n",
      "Train: Epoch [24], Batch [186/938], Loss: 0.5320934653282166\n",
      "Train: Epoch [24], Batch [187/938], Loss: 0.2124660760164261\n",
      "Train: Epoch [24], Batch [188/938], Loss: 0.3034329116344452\n",
      "Train: Epoch [24], Batch [189/938], Loss: 0.2800142168998718\n",
      "Train: Epoch [24], Batch [190/938], Loss: 0.3301669955253601\n",
      "Train: Epoch [24], Batch [191/938], Loss: 0.25696298480033875\n",
      "Train: Epoch [24], Batch [192/938], Loss: 0.38520681858062744\n",
      "Train: Epoch [24], Batch [193/938], Loss: 0.2888985276222229\n",
      "Train: Epoch [24], Batch [194/938], Loss: 0.24318893253803253\n",
      "Train: Epoch [24], Batch [195/938], Loss: 0.35591864585876465\n",
      "Train: Epoch [24], Batch [196/938], Loss: 0.38713157176971436\n",
      "Train: Epoch [24], Batch [197/938], Loss: 0.42021289467811584\n",
      "Train: Epoch [24], Batch [198/938], Loss: 0.2016732096672058\n",
      "Train: Epoch [24], Batch [199/938], Loss: 0.7663930654525757\n",
      "Train: Epoch [24], Batch [200/938], Loss: 0.4174288213253021\n",
      "Train: Epoch [24], Batch [201/938], Loss: 0.3925485610961914\n",
      "Train: Epoch [24], Batch [202/938], Loss: 0.49079468846321106\n",
      "Train: Epoch [24], Batch [203/938], Loss: 0.4026812016963959\n",
      "Train: Epoch [24], Batch [204/938], Loss: 0.4477745592594147\n",
      "Train: Epoch [24], Batch [205/938], Loss: 0.4428560137748718\n",
      "Train: Epoch [24], Batch [206/938], Loss: 0.34832024574279785\n",
      "Train: Epoch [24], Batch [207/938], Loss: 0.27177566289901733\n",
      "Train: Epoch [24], Batch [208/938], Loss: 0.4142514169216156\n",
      "Train: Epoch [24], Batch [209/938], Loss: 0.49386996030807495\n",
      "Train: Epoch [24], Batch [210/938], Loss: 0.2994317412376404\n",
      "Train: Epoch [24], Batch [211/938], Loss: 0.3821938633918762\n",
      "Train: Epoch [24], Batch [212/938], Loss: 0.41701188683509827\n",
      "Train: Epoch [24], Batch [213/938], Loss: 0.45787426829338074\n",
      "Train: Epoch [24], Batch [214/938], Loss: 0.350752592086792\n",
      "Train: Epoch [24], Batch [215/938], Loss: 0.3277967870235443\n",
      "Train: Epoch [24], Batch [216/938], Loss: 0.2768089473247528\n",
      "Train: Epoch [24], Batch [217/938], Loss: 0.2751370370388031\n",
      "Train: Epoch [24], Batch [218/938], Loss: 0.2584269046783447\n",
      "Train: Epoch [24], Batch [219/938], Loss: 0.378515362739563\n",
      "Train: Epoch [24], Batch [220/938], Loss: 0.4646967053413391\n",
      "Train: Epoch [24], Batch [221/938], Loss: 0.30913835763931274\n",
      "Train: Epoch [24], Batch [222/938], Loss: 0.35526835918426514\n",
      "Train: Epoch [24], Batch [223/938], Loss: 0.5714411735534668\n",
      "Train: Epoch [24], Batch [224/938], Loss: 0.2733161747455597\n",
      "Train: Epoch [24], Batch [225/938], Loss: 0.551877498626709\n",
      "Train: Epoch [24], Batch [226/938], Loss: 0.17886939644813538\n",
      "Train: Epoch [24], Batch [227/938], Loss: 0.3963364362716675\n",
      "Train: Epoch [24], Batch [228/938], Loss: 0.4683229923248291\n",
      "Train: Epoch [24], Batch [229/938], Loss: 0.4180404543876648\n",
      "Train: Epoch [24], Batch [230/938], Loss: 0.27600833773612976\n",
      "Train: Epoch [24], Batch [231/938], Loss: 0.48345401883125305\n",
      "Train: Epoch [24], Batch [232/938], Loss: 0.4048420786857605\n",
      "Train: Epoch [24], Batch [233/938], Loss: 0.35778558254241943\n",
      "Train: Epoch [24], Batch [234/938], Loss: 0.5127478837966919\n",
      "Train: Epoch [24], Batch [235/938], Loss: 0.3148494362831116\n",
      "Train: Epoch [24], Batch [236/938], Loss: 0.3865068852901459\n",
      "Train: Epoch [24], Batch [237/938], Loss: 0.3396209478378296\n",
      "Train: Epoch [24], Batch [238/938], Loss: 0.31141722202301025\n",
      "Train: Epoch [24], Batch [239/938], Loss: 0.2949131429195404\n",
      "Train: Epoch [24], Batch [240/938], Loss: 0.3139629364013672\n",
      "Train: Epoch [24], Batch [241/938], Loss: 0.4308384954929352\n",
      "Train: Epoch [24], Batch [242/938], Loss: 0.33250683546066284\n",
      "Train: Epoch [24], Batch [243/938], Loss: 0.4445594251155853\n",
      "Train: Epoch [24], Batch [244/938], Loss: 0.2960437834262848\n",
      "Train: Epoch [24], Batch [245/938], Loss: 0.41592395305633545\n",
      "Train: Epoch [24], Batch [246/938], Loss: 0.3509449362754822\n",
      "Train: Epoch [24], Batch [247/938], Loss: 0.4059566855430603\n",
      "Train: Epoch [24], Batch [248/938], Loss: 0.4184386134147644\n",
      "Train: Epoch [24], Batch [249/938], Loss: 0.5283366441726685\n",
      "Train: Epoch [24], Batch [250/938], Loss: 0.3105825185775757\n",
      "Train: Epoch [24], Batch [251/938], Loss: 0.34950876235961914\n",
      "Train: Epoch [24], Batch [252/938], Loss: 0.35775643587112427\n",
      "Train: Epoch [24], Batch [253/938], Loss: 0.3861118257045746\n",
      "Train: Epoch [24], Batch [254/938], Loss: 0.35158562660217285\n",
      "Train: Epoch [24], Batch [255/938], Loss: 0.42267632484436035\n",
      "Train: Epoch [24], Batch [256/938], Loss: 0.45245373249053955\n",
      "Train: Epoch [24], Batch [257/938], Loss: 0.30510127544403076\n",
      "Train: Epoch [24], Batch [258/938], Loss: 0.43449997901916504\n",
      "Train: Epoch [24], Batch [259/938], Loss: 0.4119972586631775\n",
      "Train: Epoch [24], Batch [260/938], Loss: 0.2600353956222534\n",
      "Train: Epoch [24], Batch [261/938], Loss: 0.4671587347984314\n",
      "Train: Epoch [24], Batch [262/938], Loss: 0.29380008578300476\n",
      "Train: Epoch [24], Batch [263/938], Loss: 0.34063827991485596\n",
      "Train: Epoch [24], Batch [264/938], Loss: 0.3836331367492676\n",
      "Train: Epoch [24], Batch [265/938], Loss: 0.4207759499549866\n",
      "Train: Epoch [24], Batch [266/938], Loss: 0.3803136944770813\n",
      "Train: Epoch [24], Batch [267/938], Loss: 0.4058459997177124\n",
      "Train: Epoch [24], Batch [268/938], Loss: 0.43016284704208374\n",
      "Train: Epoch [24], Batch [269/938], Loss: 0.3834620714187622\n",
      "Train: Epoch [24], Batch [270/938], Loss: 0.26402705907821655\n",
      "Train: Epoch [24], Batch [271/938], Loss: 0.34138891100883484\n",
      "Train: Epoch [24], Batch [272/938], Loss: 0.2590792775154114\n",
      "Train: Epoch [24], Batch [273/938], Loss: 0.48635709285736084\n",
      "Train: Epoch [24], Batch [274/938], Loss: 0.483886182308197\n",
      "Train: Epoch [24], Batch [275/938], Loss: 0.3825379014015198\n",
      "Train: Epoch [24], Batch [276/938], Loss: 0.4428555369377136\n",
      "Train: Epoch [24], Batch [277/938], Loss: 0.21231725811958313\n",
      "Train: Epoch [24], Batch [278/938], Loss: 0.4145698845386505\n",
      "Train: Epoch [24], Batch [279/938], Loss: 0.32965898513793945\n",
      "Train: Epoch [24], Batch [280/938], Loss: 0.4763946533203125\n",
      "Train: Epoch [24], Batch [281/938], Loss: 0.5473619103431702\n",
      "Train: Epoch [24], Batch [282/938], Loss: 0.4083116054534912\n",
      "Train: Epoch [24], Batch [283/938], Loss: 0.3348754644393921\n",
      "Train: Epoch [24], Batch [284/938], Loss: 0.4870743155479431\n",
      "Train: Epoch [24], Batch [285/938], Loss: 0.35223960876464844\n",
      "Train: Epoch [24], Batch [286/938], Loss: 0.3115230202674866\n",
      "Train: Epoch [24], Batch [287/938], Loss: 0.4625614285469055\n",
      "Train: Epoch [24], Batch [288/938], Loss: 0.41865503787994385\n",
      "Train: Epoch [24], Batch [289/938], Loss: 0.1882837861776352\n",
      "Train: Epoch [24], Batch [290/938], Loss: 0.6031039953231812\n",
      "Train: Epoch [24], Batch [291/938], Loss: 0.5024596452713013\n",
      "Train: Epoch [24], Batch [292/938], Loss: 0.34196752309799194\n",
      "Train: Epoch [24], Batch [293/938], Loss: 0.3711630702018738\n",
      "Train: Epoch [24], Batch [294/938], Loss: 0.21594199538230896\n",
      "Train: Epoch [24], Batch [295/938], Loss: 0.3983611464500427\n",
      "Train: Epoch [24], Batch [296/938], Loss: 0.41774049401283264\n",
      "Train: Epoch [24], Batch [297/938], Loss: 0.44705894589424133\n",
      "Train: Epoch [24], Batch [298/938], Loss: 0.22233621776103973\n",
      "Train: Epoch [24], Batch [299/938], Loss: 0.6271811723709106\n",
      "Train: Epoch [24], Batch [300/938], Loss: 0.33617401123046875\n",
      "Train: Epoch [24], Batch [301/938], Loss: 0.3123568296432495\n",
      "Train: Epoch [24], Batch [302/938], Loss: 0.5138789415359497\n",
      "Train: Epoch [24], Batch [303/938], Loss: 0.25217297673225403\n",
      "Train: Epoch [24], Batch [304/938], Loss: 0.5350337624549866\n",
      "Train: Epoch [24], Batch [305/938], Loss: 0.45608317852020264\n",
      "Train: Epoch [24], Batch [306/938], Loss: 0.3933277726173401\n",
      "Train: Epoch [24], Batch [307/938], Loss: 0.36903175711631775\n",
      "Train: Epoch [24], Batch [308/938], Loss: 0.5387349128723145\n",
      "Train: Epoch [24], Batch [309/938], Loss: 0.20652486383914948\n",
      "Train: Epoch [24], Batch [310/938], Loss: 0.4161665141582489\n",
      "Train: Epoch [24], Batch [311/938], Loss: 0.5340451002120972\n",
      "Train: Epoch [24], Batch [312/938], Loss: 0.35363516211509705\n",
      "Train: Epoch [24], Batch [313/938], Loss: 0.38614794611930847\n",
      "Train: Epoch [24], Batch [314/938], Loss: 0.5539361238479614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [315/938], Loss: 0.2907385230064392\n",
      "Train: Epoch [24], Batch [316/938], Loss: 0.32280445098876953\n",
      "Train: Epoch [24], Batch [317/938], Loss: 0.5190977454185486\n",
      "Train: Epoch [24], Batch [318/938], Loss: 0.4000324606895447\n",
      "Train: Epoch [24], Batch [319/938], Loss: 0.24795697629451752\n",
      "Train: Epoch [24], Batch [320/938], Loss: 0.3225625455379486\n",
      "Train: Epoch [24], Batch [321/938], Loss: 0.6129473447799683\n",
      "Train: Epoch [24], Batch [322/938], Loss: 0.32898855209350586\n",
      "Train: Epoch [24], Batch [323/938], Loss: 0.40724483132362366\n",
      "Train: Epoch [24], Batch [324/938], Loss: 0.22622887790203094\n",
      "Train: Epoch [24], Batch [325/938], Loss: 0.3647730350494385\n",
      "Train: Epoch [24], Batch [326/938], Loss: 0.4199824035167694\n",
      "Train: Epoch [24], Batch [327/938], Loss: 0.3330977261066437\n",
      "Train: Epoch [24], Batch [328/938], Loss: 0.3182969391345978\n",
      "Train: Epoch [24], Batch [329/938], Loss: 0.5105742812156677\n",
      "Train: Epoch [24], Batch [330/938], Loss: 0.3491937816143036\n",
      "Train: Epoch [24], Batch [331/938], Loss: 0.4889412522315979\n",
      "Train: Epoch [24], Batch [332/938], Loss: 0.41691598296165466\n",
      "Train: Epoch [24], Batch [333/938], Loss: 0.3361798822879791\n",
      "Train: Epoch [24], Batch [334/938], Loss: 0.3308984935283661\n",
      "Train: Epoch [24], Batch [335/938], Loss: 0.30973634123802185\n",
      "Train: Epoch [24], Batch [336/938], Loss: 0.5171060562133789\n",
      "Train: Epoch [24], Batch [337/938], Loss: 0.26912879943847656\n",
      "Train: Epoch [24], Batch [338/938], Loss: 0.36420685052871704\n",
      "Train: Epoch [24], Batch [339/938], Loss: 0.44318050146102905\n",
      "Train: Epoch [24], Batch [340/938], Loss: 0.4618448317050934\n",
      "Train: Epoch [24], Batch [341/938], Loss: 0.45841890573501587\n",
      "Train: Epoch [24], Batch [342/938], Loss: 0.38664910197257996\n",
      "Train: Epoch [24], Batch [343/938], Loss: 0.3666985034942627\n",
      "Train: Epoch [24], Batch [344/938], Loss: 0.38407278060913086\n",
      "Train: Epoch [24], Batch [345/938], Loss: 0.4489706754684448\n",
      "Train: Epoch [24], Batch [346/938], Loss: 0.28085964918136597\n",
      "Train: Epoch [24], Batch [347/938], Loss: 0.308226078748703\n",
      "Train: Epoch [24], Batch [348/938], Loss: 0.40743452310562134\n",
      "Train: Epoch [24], Batch [349/938], Loss: 0.31826967000961304\n",
      "Train: Epoch [24], Batch [350/938], Loss: 0.2823588252067566\n",
      "Train: Epoch [24], Batch [351/938], Loss: 0.4228435158729553\n",
      "Train: Epoch [24], Batch [352/938], Loss: 0.35475796461105347\n",
      "Train: Epoch [24], Batch [353/938], Loss: 0.4510493576526642\n",
      "Train: Epoch [24], Batch [354/938], Loss: 0.41144758462905884\n",
      "Train: Epoch [24], Batch [355/938], Loss: 0.2517375946044922\n",
      "Train: Epoch [24], Batch [356/938], Loss: 0.31570708751678467\n",
      "Train: Epoch [24], Batch [357/938], Loss: 0.48203930258750916\n",
      "Train: Epoch [24], Batch [358/938], Loss: 0.28835010528564453\n",
      "Train: Epoch [24], Batch [359/938], Loss: 0.3660910129547119\n",
      "Train: Epoch [24], Batch [360/938], Loss: 0.3140539824962616\n",
      "Train: Epoch [24], Batch [361/938], Loss: 0.41225022077560425\n",
      "Train: Epoch [24], Batch [362/938], Loss: 0.5575829744338989\n",
      "Train: Epoch [24], Batch [363/938], Loss: 0.3596729636192322\n",
      "Train: Epoch [24], Batch [364/938], Loss: 0.47536882758140564\n",
      "Train: Epoch [24], Batch [365/938], Loss: 0.37903815507888794\n",
      "Train: Epoch [24], Batch [366/938], Loss: 0.6073785424232483\n",
      "Train: Epoch [24], Batch [367/938], Loss: 0.304961621761322\n",
      "Train: Epoch [24], Batch [368/938], Loss: 0.4692913293838501\n",
      "Train: Epoch [24], Batch [369/938], Loss: 0.42548519372940063\n",
      "Train: Epoch [24], Batch [370/938], Loss: 0.5474542379379272\n",
      "Train: Epoch [24], Batch [371/938], Loss: 0.2899633049964905\n",
      "Train: Epoch [24], Batch [372/938], Loss: 0.4172326624393463\n",
      "Train: Epoch [24], Batch [373/938], Loss: 0.3221861720085144\n",
      "Train: Epoch [24], Batch [374/938], Loss: 0.24049463868141174\n",
      "Train: Epoch [24], Batch [375/938], Loss: 0.3279991149902344\n",
      "Train: Epoch [24], Batch [376/938], Loss: 0.32599368691444397\n",
      "Train: Epoch [24], Batch [377/938], Loss: 0.34745901823043823\n",
      "Train: Epoch [24], Batch [378/938], Loss: 0.6751270294189453\n",
      "Train: Epoch [24], Batch [379/938], Loss: 0.25745493173599243\n",
      "Train: Epoch [24], Batch [380/938], Loss: 0.35037297010421753\n",
      "Train: Epoch [24], Batch [381/938], Loss: 0.298676073551178\n",
      "Train: Epoch [24], Batch [382/938], Loss: 0.31120312213897705\n",
      "Train: Epoch [24], Batch [383/938], Loss: 0.3516331911087036\n",
      "Train: Epoch [24], Batch [384/938], Loss: 0.3807184398174286\n",
      "Train: Epoch [24], Batch [385/938], Loss: 0.37189269065856934\n",
      "Train: Epoch [24], Batch [386/938], Loss: 0.35428327322006226\n",
      "Train: Epoch [24], Batch [387/938], Loss: 0.3116762638092041\n",
      "Train: Epoch [24], Batch [388/938], Loss: 0.5133589506149292\n",
      "Train: Epoch [24], Batch [389/938], Loss: 0.2638120949268341\n",
      "Train: Epoch [24], Batch [390/938], Loss: 0.2143392562866211\n",
      "Train: Epoch [24], Batch [391/938], Loss: 0.3036099970340729\n",
      "Train: Epoch [24], Batch [392/938], Loss: 0.36583197116851807\n",
      "Train: Epoch [24], Batch [393/938], Loss: 0.38467714190483093\n",
      "Train: Epoch [24], Batch [394/938], Loss: 0.32922667264938354\n",
      "Train: Epoch [24], Batch [395/938], Loss: 0.440110981464386\n",
      "Train: Epoch [24], Batch [396/938], Loss: 0.356859028339386\n",
      "Train: Epoch [24], Batch [397/938], Loss: 0.4329589605331421\n",
      "Train: Epoch [24], Batch [398/938], Loss: 0.2812412977218628\n",
      "Train: Epoch [24], Batch [399/938], Loss: 0.2789754867553711\n",
      "Train: Epoch [24], Batch [400/938], Loss: 0.424996554851532\n",
      "Train: Epoch [24], Batch [401/938], Loss: 0.2613505721092224\n",
      "Train: Epoch [24], Batch [402/938], Loss: 0.4172096848487854\n",
      "Train: Epoch [24], Batch [403/938], Loss: 0.7256200313568115\n",
      "Train: Epoch [24], Batch [404/938], Loss: 0.44397643208503723\n",
      "Train: Epoch [24], Batch [405/938], Loss: 0.36073529720306396\n",
      "Train: Epoch [24], Batch [406/938], Loss: 0.5299592614173889\n",
      "Train: Epoch [24], Batch [407/938], Loss: 0.3147552013397217\n",
      "Train: Epoch [24], Batch [408/938], Loss: 0.4677841067314148\n",
      "Train: Epoch [24], Batch [409/938], Loss: 0.3064652979373932\n",
      "Train: Epoch [24], Batch [410/938], Loss: 0.3017177879810333\n",
      "Train: Epoch [24], Batch [411/938], Loss: 0.40929388999938965\n",
      "Train: Epoch [24], Batch [412/938], Loss: 0.2970335781574249\n",
      "Train: Epoch [24], Batch [413/938], Loss: 0.31925415992736816\n",
      "Train: Epoch [24], Batch [414/938], Loss: 0.38949161767959595\n",
      "Train: Epoch [24], Batch [415/938], Loss: 0.4766514003276825\n",
      "Train: Epoch [24], Batch [416/938], Loss: 0.4411994218826294\n",
      "Train: Epoch [24], Batch [417/938], Loss: 0.27457985281944275\n",
      "Train: Epoch [24], Batch [418/938], Loss: 0.37098410725593567\n",
      "Train: Epoch [24], Batch [419/938], Loss: 0.26955854892730713\n",
      "Train: Epoch [24], Batch [420/938], Loss: 0.2508631944656372\n",
      "Train: Epoch [24], Batch [421/938], Loss: 0.2369595170021057\n",
      "Train: Epoch [24], Batch [422/938], Loss: 0.3269686698913574\n",
      "Train: Epoch [24], Batch [423/938], Loss: 0.4629303216934204\n",
      "Train: Epoch [24], Batch [424/938], Loss: 0.38247621059417725\n",
      "Train: Epoch [24], Batch [425/938], Loss: 0.5135692358016968\n",
      "Train: Epoch [24], Batch [426/938], Loss: 0.2716079354286194\n",
      "Train: Epoch [24], Batch [427/938], Loss: 0.3437979221343994\n",
      "Train: Epoch [24], Batch [428/938], Loss: 0.5427208542823792\n",
      "Train: Epoch [24], Batch [429/938], Loss: 0.3921359181404114\n",
      "Train: Epoch [24], Batch [430/938], Loss: 0.40446656942367554\n",
      "Train: Epoch [24], Batch [431/938], Loss: 0.3639248013496399\n",
      "Train: Epoch [24], Batch [432/938], Loss: 0.44795340299606323\n",
      "Train: Epoch [24], Batch [433/938], Loss: 0.3545844554901123\n",
      "Train: Epoch [24], Batch [434/938], Loss: 0.48757511377334595\n",
      "Train: Epoch [24], Batch [435/938], Loss: 0.5005410313606262\n",
      "Train: Epoch [24], Batch [436/938], Loss: 0.40278157591819763\n",
      "Train: Epoch [24], Batch [437/938], Loss: 0.47945553064346313\n",
      "Train: Epoch [24], Batch [438/938], Loss: 0.3827173709869385\n",
      "Train: Epoch [24], Batch [439/938], Loss: 0.4215683341026306\n",
      "Train: Epoch [24], Batch [440/938], Loss: 0.4908654987812042\n",
      "Train: Epoch [24], Batch [441/938], Loss: 0.4016726315021515\n",
      "Train: Epoch [24], Batch [442/938], Loss: 0.28537851572036743\n",
      "Train: Epoch [24], Batch [443/938], Loss: 0.44770997762680054\n",
      "Train: Epoch [24], Batch [444/938], Loss: 0.5826624631881714\n",
      "Train: Epoch [24], Batch [445/938], Loss: 0.34913182258605957\n",
      "Train: Epoch [24], Batch [446/938], Loss: 0.4256490468978882\n",
      "Train: Epoch [24], Batch [447/938], Loss: 0.5072370767593384\n",
      "Train: Epoch [24], Batch [448/938], Loss: 0.4370075464248657\n",
      "Train: Epoch [24], Batch [449/938], Loss: 0.31644368171691895\n",
      "Train: Epoch [24], Batch [450/938], Loss: 0.39361774921417236\n",
      "Train: Epoch [24], Batch [451/938], Loss: 0.3634057343006134\n",
      "Train: Epoch [24], Batch [452/938], Loss: 0.4774527847766876\n",
      "Train: Epoch [24], Batch [453/938], Loss: 0.25474071502685547\n",
      "Train: Epoch [24], Batch [454/938], Loss: 0.35545021295547485\n",
      "Train: Epoch [24], Batch [455/938], Loss: 0.5053057670593262\n",
      "Train: Epoch [24], Batch [456/938], Loss: 0.5194811820983887\n",
      "Train: Epoch [24], Batch [457/938], Loss: 0.4243866205215454\n",
      "Train: Epoch [24], Batch [458/938], Loss: 0.5697191953659058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [459/938], Loss: 0.3917900323867798\n",
      "Train: Epoch [24], Batch [460/938], Loss: 0.49678146839141846\n",
      "Train: Epoch [24], Batch [461/938], Loss: 0.2912891209125519\n",
      "Train: Epoch [24], Batch [462/938], Loss: 0.3036230206489563\n",
      "Train: Epoch [24], Batch [463/938], Loss: 0.34188514947891235\n",
      "Train: Epoch [24], Batch [464/938], Loss: 0.22877347469329834\n",
      "Train: Epoch [24], Batch [465/938], Loss: 0.41188254952430725\n",
      "Train: Epoch [24], Batch [466/938], Loss: 0.3078456521034241\n",
      "Train: Epoch [24], Batch [467/938], Loss: 0.28574275970458984\n",
      "Train: Epoch [24], Batch [468/938], Loss: 0.41244733333587646\n",
      "Train: Epoch [24], Batch [469/938], Loss: 0.3757510185241699\n",
      "Train: Epoch [24], Batch [470/938], Loss: 0.34390151500701904\n",
      "Train: Epoch [24], Batch [471/938], Loss: 0.34372639656066895\n",
      "Train: Epoch [24], Batch [472/938], Loss: 0.27140817046165466\n",
      "Train: Epoch [24], Batch [473/938], Loss: 0.42084643244743347\n",
      "Train: Epoch [24], Batch [474/938], Loss: 0.613181471824646\n",
      "Train: Epoch [24], Batch [475/938], Loss: 0.40366271138191223\n",
      "Train: Epoch [24], Batch [476/938], Loss: 0.41218844056129456\n",
      "Train: Epoch [24], Batch [477/938], Loss: 0.27337583899497986\n",
      "Train: Epoch [24], Batch [478/938], Loss: 0.43561968207359314\n",
      "Train: Epoch [24], Batch [479/938], Loss: 0.3368445038795471\n",
      "Train: Epoch [24], Batch [480/938], Loss: 0.29251566529273987\n",
      "Train: Epoch [24], Batch [481/938], Loss: 0.18101637065410614\n",
      "Train: Epoch [24], Batch [482/938], Loss: 0.30950045585632324\n",
      "Train: Epoch [24], Batch [483/938], Loss: 0.32604867219924927\n",
      "Train: Epoch [24], Batch [484/938], Loss: 0.350546658039093\n",
      "Train: Epoch [24], Batch [485/938], Loss: 0.442230224609375\n",
      "Train: Epoch [24], Batch [486/938], Loss: 0.3790414035320282\n",
      "Train: Epoch [24], Batch [487/938], Loss: 0.5715501308441162\n",
      "Train: Epoch [24], Batch [488/938], Loss: 0.3651444911956787\n",
      "Train: Epoch [24], Batch [489/938], Loss: 0.26958519220352173\n",
      "Train: Epoch [24], Batch [490/938], Loss: 0.3338618874549866\n",
      "Train: Epoch [24], Batch [491/938], Loss: 0.4437977969646454\n",
      "Train: Epoch [24], Batch [492/938], Loss: 0.5947966575622559\n",
      "Train: Epoch [24], Batch [493/938], Loss: 0.5065149664878845\n",
      "Train: Epoch [24], Batch [494/938], Loss: 0.5319641828536987\n",
      "Train: Epoch [24], Batch [495/938], Loss: 0.32639601826667786\n",
      "Train: Epoch [24], Batch [496/938], Loss: 0.5337290167808533\n",
      "Train: Epoch [24], Batch [497/938], Loss: 0.4178122878074646\n",
      "Train: Epoch [24], Batch [498/938], Loss: 0.29885953664779663\n",
      "Train: Epoch [24], Batch [499/938], Loss: 0.4959924817085266\n",
      "Train: Epoch [24], Batch [500/938], Loss: 0.3362135887145996\n",
      "Train: Epoch [24], Batch [501/938], Loss: 0.5258341431617737\n",
      "Train: Epoch [24], Batch [502/938], Loss: 0.4788636863231659\n",
      "Train: Epoch [24], Batch [503/938], Loss: 0.29679417610168457\n",
      "Train: Epoch [24], Batch [504/938], Loss: 0.196855366230011\n",
      "Train: Epoch [24], Batch [505/938], Loss: 0.623101532459259\n",
      "Train: Epoch [24], Batch [506/938], Loss: 0.35557064414024353\n",
      "Train: Epoch [24], Batch [507/938], Loss: 0.25050127506256104\n",
      "Train: Epoch [24], Batch [508/938], Loss: 0.42455482482910156\n",
      "Train: Epoch [24], Batch [509/938], Loss: 0.3422081470489502\n",
      "Train: Epoch [24], Batch [510/938], Loss: 0.5170609354972839\n",
      "Train: Epoch [24], Batch [511/938], Loss: 0.4654175937175751\n",
      "Train: Epoch [24], Batch [512/938], Loss: 0.49317270517349243\n",
      "Train: Epoch [24], Batch [513/938], Loss: 0.3132855296134949\n",
      "Train: Epoch [24], Batch [514/938], Loss: 0.5452878475189209\n",
      "Train: Epoch [24], Batch [515/938], Loss: 0.27626827359199524\n",
      "Train: Epoch [24], Batch [516/938], Loss: 0.27776065468788147\n",
      "Train: Epoch [24], Batch [517/938], Loss: 0.5948219299316406\n",
      "Train: Epoch [24], Batch [518/938], Loss: 0.30201634764671326\n",
      "Train: Epoch [24], Batch [519/938], Loss: 0.2029097080230713\n",
      "Train: Epoch [24], Batch [520/938], Loss: 0.34055882692337036\n",
      "Train: Epoch [24], Batch [521/938], Loss: 0.435486376285553\n",
      "Train: Epoch [24], Batch [522/938], Loss: 0.2366156280040741\n",
      "Train: Epoch [24], Batch [523/938], Loss: 0.3139263391494751\n",
      "Train: Epoch [24], Batch [524/938], Loss: 0.31492167711257935\n",
      "Train: Epoch [24], Batch [525/938], Loss: 0.38275450468063354\n",
      "Train: Epoch [24], Batch [526/938], Loss: 0.3631740212440491\n",
      "Train: Epoch [24], Batch [527/938], Loss: 0.4887966513633728\n",
      "Train: Epoch [24], Batch [528/938], Loss: 0.4267318546772003\n",
      "Train: Epoch [24], Batch [529/938], Loss: 0.5586812496185303\n",
      "Train: Epoch [24], Batch [530/938], Loss: 0.5128334760665894\n",
      "Train: Epoch [24], Batch [531/938], Loss: 0.5825603008270264\n",
      "Train: Epoch [24], Batch [532/938], Loss: 0.5159741044044495\n",
      "Train: Epoch [24], Batch [533/938], Loss: 0.44620364904403687\n",
      "Train: Epoch [24], Batch [534/938], Loss: 0.21294184029102325\n",
      "Train: Epoch [24], Batch [535/938], Loss: 0.22476665675640106\n",
      "Train: Epoch [24], Batch [536/938], Loss: 0.24373206496238708\n",
      "Train: Epoch [24], Batch [537/938], Loss: 0.3332405090332031\n",
      "Train: Epoch [24], Batch [538/938], Loss: 0.26778727769851685\n",
      "Train: Epoch [24], Batch [539/938], Loss: 0.40632468461990356\n",
      "Train: Epoch [24], Batch [540/938], Loss: 0.3630542755126953\n",
      "Train: Epoch [24], Batch [541/938], Loss: 0.2588155269622803\n",
      "Train: Epoch [24], Batch [542/938], Loss: 0.31148776412010193\n",
      "Train: Epoch [24], Batch [543/938], Loss: 0.2850472927093506\n",
      "Train: Epoch [24], Batch [544/938], Loss: 0.24427580833435059\n",
      "Train: Epoch [24], Batch [545/938], Loss: 0.37545502185821533\n",
      "Train: Epoch [24], Batch [546/938], Loss: 0.36496520042419434\n",
      "Train: Epoch [24], Batch [547/938], Loss: 0.33051273226737976\n",
      "Train: Epoch [24], Batch [548/938], Loss: 0.3568039536476135\n",
      "Train: Epoch [24], Batch [549/938], Loss: 0.21446499228477478\n",
      "Train: Epoch [24], Batch [550/938], Loss: 0.35140472650527954\n",
      "Train: Epoch [24], Batch [551/938], Loss: 0.298922061920166\n",
      "Train: Epoch [24], Batch [552/938], Loss: 0.31937670707702637\n",
      "Train: Epoch [24], Batch [553/938], Loss: 0.42597144842147827\n",
      "Train: Epoch [24], Batch [554/938], Loss: 0.39406895637512207\n",
      "Train: Epoch [24], Batch [555/938], Loss: 0.579015851020813\n",
      "Train: Epoch [24], Batch [556/938], Loss: 0.35122185945510864\n",
      "Train: Epoch [24], Batch [557/938], Loss: 0.37576520442962646\n",
      "Train: Epoch [24], Batch [558/938], Loss: 0.3958585858345032\n",
      "Train: Epoch [24], Batch [559/938], Loss: 0.4200529158115387\n",
      "Train: Epoch [24], Batch [560/938], Loss: 0.34572187066078186\n",
      "Train: Epoch [24], Batch [561/938], Loss: 0.3139083981513977\n",
      "Train: Epoch [24], Batch [562/938], Loss: 0.33514294028282166\n",
      "Train: Epoch [24], Batch [563/938], Loss: 0.2646501958370209\n",
      "Train: Epoch [24], Batch [564/938], Loss: 0.2800138592720032\n",
      "Train: Epoch [24], Batch [565/938], Loss: 0.43236860632896423\n",
      "Train: Epoch [24], Batch [566/938], Loss: 0.2958129048347473\n",
      "Train: Epoch [24], Batch [567/938], Loss: 0.31385794281959534\n",
      "Train: Epoch [24], Batch [568/938], Loss: 0.28991395235061646\n",
      "Train: Epoch [24], Batch [569/938], Loss: 0.3965684771537781\n",
      "Train: Epoch [24], Batch [570/938], Loss: 0.3453591465950012\n",
      "Train: Epoch [24], Batch [571/938], Loss: 0.4054664969444275\n",
      "Train: Epoch [24], Batch [572/938], Loss: 0.3209884762763977\n",
      "Train: Epoch [24], Batch [573/938], Loss: 0.2005508989095688\n",
      "Train: Epoch [24], Batch [574/938], Loss: 0.33865267038345337\n",
      "Train: Epoch [24], Batch [575/938], Loss: 0.43303024768829346\n",
      "Train: Epoch [24], Batch [576/938], Loss: 0.26576802134513855\n",
      "Train: Epoch [24], Batch [577/938], Loss: 0.5286935567855835\n",
      "Train: Epoch [24], Batch [578/938], Loss: 0.2118578851222992\n",
      "Train: Epoch [24], Batch [579/938], Loss: 0.4200272560119629\n",
      "Train: Epoch [24], Batch [580/938], Loss: 0.38631635904312134\n",
      "Train: Epoch [24], Batch [581/938], Loss: 0.27674630284309387\n",
      "Train: Epoch [24], Batch [582/938], Loss: 0.4024898409843445\n",
      "Train: Epoch [24], Batch [583/938], Loss: 0.5129477977752686\n",
      "Train: Epoch [24], Batch [584/938], Loss: 0.2838246524333954\n",
      "Train: Epoch [24], Batch [585/938], Loss: 0.3814660906791687\n",
      "Train: Epoch [24], Batch [586/938], Loss: 0.31586629152297974\n",
      "Train: Epoch [24], Batch [587/938], Loss: 0.24429050087928772\n",
      "Train: Epoch [24], Batch [588/938], Loss: 0.2842083275318146\n",
      "Train: Epoch [24], Batch [589/938], Loss: 0.45620325207710266\n",
      "Train: Epoch [24], Batch [590/938], Loss: 0.4768405258655548\n",
      "Train: Epoch [24], Batch [591/938], Loss: 0.4603434205055237\n",
      "Train: Epoch [24], Batch [592/938], Loss: 0.12338265776634216\n",
      "Train: Epoch [24], Batch [593/938], Loss: 0.3343438506126404\n",
      "Train: Epoch [24], Batch [594/938], Loss: 0.2967984080314636\n",
      "Train: Epoch [24], Batch [595/938], Loss: 0.47937506437301636\n",
      "Train: Epoch [24], Batch [596/938], Loss: 0.44879114627838135\n",
      "Train: Epoch [24], Batch [597/938], Loss: 0.3278260827064514\n",
      "Train: Epoch [24], Batch [598/938], Loss: 0.33646389842033386\n",
      "Train: Epoch [24], Batch [599/938], Loss: 0.5633289217948914\n",
      "Train: Epoch [24], Batch [600/938], Loss: 0.49775463342666626\n",
      "Train: Epoch [24], Batch [601/938], Loss: 0.5404450297355652\n",
      "Train: Epoch [24], Batch [602/938], Loss: 0.2426469326019287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [603/938], Loss: 0.4169367849826813\n",
      "Train: Epoch [24], Batch [604/938], Loss: 0.39188480377197266\n",
      "Train: Epoch [24], Batch [605/938], Loss: 0.3026549220085144\n",
      "Train: Epoch [24], Batch [606/938], Loss: 0.32938867807388306\n",
      "Train: Epoch [24], Batch [607/938], Loss: 0.5678527355194092\n",
      "Train: Epoch [24], Batch [608/938], Loss: 0.32647016644477844\n",
      "Train: Epoch [24], Batch [609/938], Loss: 0.49329301714897156\n",
      "Train: Epoch [24], Batch [610/938], Loss: 0.2622303366661072\n",
      "Train: Epoch [24], Batch [611/938], Loss: 0.3647582530975342\n",
      "Train: Epoch [24], Batch [612/938], Loss: 0.3153674304485321\n",
      "Train: Epoch [24], Batch [613/938], Loss: 0.3619855046272278\n",
      "Train: Epoch [24], Batch [614/938], Loss: 0.38085097074508667\n",
      "Train: Epoch [24], Batch [615/938], Loss: 0.502816915512085\n",
      "Train: Epoch [24], Batch [616/938], Loss: 0.43235379457473755\n",
      "Train: Epoch [24], Batch [617/938], Loss: 0.3478322923183441\n",
      "Train: Epoch [24], Batch [618/938], Loss: 0.3593253195285797\n",
      "Train: Epoch [24], Batch [619/938], Loss: 0.28793588280677795\n",
      "Train: Epoch [24], Batch [620/938], Loss: 0.3114941716194153\n",
      "Train: Epoch [24], Batch [621/938], Loss: 0.4340323805809021\n",
      "Train: Epoch [24], Batch [622/938], Loss: 0.4151362180709839\n",
      "Train: Epoch [24], Batch [623/938], Loss: 0.49294182658195496\n",
      "Train: Epoch [24], Batch [624/938], Loss: 0.3932906687259674\n",
      "Train: Epoch [24], Batch [625/938], Loss: 0.4463614225387573\n",
      "Train: Epoch [24], Batch [626/938], Loss: 0.43638306856155396\n",
      "Train: Epoch [24], Batch [627/938], Loss: 0.35561811923980713\n",
      "Train: Epoch [24], Batch [628/938], Loss: 0.36410534381866455\n",
      "Train: Epoch [24], Batch [629/938], Loss: 0.22834719717502594\n",
      "Train: Epoch [24], Batch [630/938], Loss: 0.26498010754585266\n",
      "Train: Epoch [24], Batch [631/938], Loss: 0.3972328305244446\n",
      "Train: Epoch [24], Batch [632/938], Loss: 0.418641597032547\n",
      "Train: Epoch [24], Batch [633/938], Loss: 0.36910128593444824\n",
      "Train: Epoch [24], Batch [634/938], Loss: 0.46757644414901733\n",
      "Train: Epoch [24], Batch [635/938], Loss: 0.45670875906944275\n",
      "Train: Epoch [24], Batch [636/938], Loss: 0.4656972885131836\n",
      "Train: Epoch [24], Batch [637/938], Loss: 0.482543408870697\n",
      "Train: Epoch [24], Batch [638/938], Loss: 0.3749281167984009\n",
      "Train: Epoch [24], Batch [639/938], Loss: 0.3321911096572876\n",
      "Train: Epoch [24], Batch [640/938], Loss: 0.18173068761825562\n",
      "Train: Epoch [24], Batch [641/938], Loss: 0.3921949565410614\n",
      "Train: Epoch [24], Batch [642/938], Loss: 0.254952609539032\n",
      "Train: Epoch [24], Batch [643/938], Loss: 0.41335147619247437\n",
      "Train: Epoch [24], Batch [644/938], Loss: 0.45229896903038025\n",
      "Train: Epoch [24], Batch [645/938], Loss: 0.5143336057662964\n",
      "Train: Epoch [24], Batch [646/938], Loss: 0.2473442703485489\n",
      "Train: Epoch [24], Batch [647/938], Loss: 0.5110663175582886\n",
      "Train: Epoch [24], Batch [648/938], Loss: 0.32219114899635315\n",
      "Train: Epoch [24], Batch [649/938], Loss: 0.5669596791267395\n",
      "Train: Epoch [24], Batch [650/938], Loss: 0.3328985571861267\n",
      "Train: Epoch [24], Batch [651/938], Loss: 0.335064560174942\n",
      "Train: Epoch [24], Batch [652/938], Loss: 0.5573812127113342\n",
      "Train: Epoch [24], Batch [653/938], Loss: 0.5551701188087463\n",
      "Train: Epoch [24], Batch [654/938], Loss: 0.2810571789741516\n",
      "Train: Epoch [24], Batch [655/938], Loss: 0.3952508866786957\n",
      "Train: Epoch [24], Batch [656/938], Loss: 0.4860823154449463\n",
      "Train: Epoch [24], Batch [657/938], Loss: 0.3657163381576538\n",
      "Train: Epoch [24], Batch [658/938], Loss: 0.32628241181373596\n",
      "Train: Epoch [24], Batch [659/938], Loss: 0.3147110044956207\n",
      "Train: Epoch [24], Batch [660/938], Loss: 0.3064287602901459\n",
      "Train: Epoch [24], Batch [661/938], Loss: 0.397722065448761\n",
      "Train: Epoch [24], Batch [662/938], Loss: 0.3174452781677246\n",
      "Train: Epoch [24], Batch [663/938], Loss: 0.32864996790885925\n",
      "Train: Epoch [24], Batch [664/938], Loss: 0.4725649952888489\n",
      "Train: Epoch [24], Batch [665/938], Loss: 0.3199149966239929\n",
      "Train: Epoch [24], Batch [666/938], Loss: 0.4824374318122864\n",
      "Train: Epoch [24], Batch [667/938], Loss: 0.263679176568985\n",
      "Train: Epoch [24], Batch [668/938], Loss: 0.44429075717926025\n",
      "Train: Epoch [24], Batch [669/938], Loss: 0.3225138783454895\n",
      "Train: Epoch [24], Batch [670/938], Loss: 0.25088614225387573\n",
      "Train: Epoch [24], Batch [671/938], Loss: 0.35061463713645935\n",
      "Train: Epoch [24], Batch [672/938], Loss: 0.4717988967895508\n",
      "Train: Epoch [24], Batch [673/938], Loss: 0.31815817952156067\n",
      "Train: Epoch [24], Batch [674/938], Loss: 0.5306181907653809\n",
      "Train: Epoch [24], Batch [675/938], Loss: 0.5990922451019287\n",
      "Train: Epoch [24], Batch [676/938], Loss: 0.28020671010017395\n",
      "Train: Epoch [24], Batch [677/938], Loss: 0.3804559111595154\n",
      "Train: Epoch [24], Batch [678/938], Loss: 0.4867616891860962\n",
      "Train: Epoch [24], Batch [679/938], Loss: 0.44342929124832153\n",
      "Train: Epoch [24], Batch [680/938], Loss: 0.35543590784072876\n",
      "Train: Epoch [24], Batch [681/938], Loss: 0.3441060781478882\n",
      "Train: Epoch [24], Batch [682/938], Loss: 0.5898860096931458\n",
      "Train: Epoch [24], Batch [683/938], Loss: 0.5274688005447388\n",
      "Train: Epoch [24], Batch [684/938], Loss: 0.18675893545150757\n",
      "Train: Epoch [24], Batch [685/938], Loss: 0.3219933807849884\n",
      "Train: Epoch [24], Batch [686/938], Loss: 0.34926068782806396\n",
      "Train: Epoch [24], Batch [687/938], Loss: 0.6415642499923706\n",
      "Train: Epoch [24], Batch [688/938], Loss: 0.3923805058002472\n",
      "Train: Epoch [24], Batch [689/938], Loss: 0.19255460798740387\n",
      "Train: Epoch [24], Batch [690/938], Loss: 0.2291739583015442\n",
      "Train: Epoch [24], Batch [691/938], Loss: 0.40483176708221436\n",
      "Train: Epoch [24], Batch [692/938], Loss: 0.5621081590652466\n",
      "Train: Epoch [24], Batch [693/938], Loss: 0.3537350296974182\n",
      "Train: Epoch [24], Batch [694/938], Loss: 0.42239025235176086\n",
      "Train: Epoch [24], Batch [695/938], Loss: 0.5271982550621033\n",
      "Train: Epoch [24], Batch [696/938], Loss: 0.3082349896430969\n",
      "Train: Epoch [24], Batch [697/938], Loss: 0.468172550201416\n",
      "Train: Epoch [24], Batch [698/938], Loss: 0.3908621668815613\n",
      "Train: Epoch [24], Batch [699/938], Loss: 0.4280950427055359\n",
      "Train: Epoch [24], Batch [700/938], Loss: 0.5269120931625366\n",
      "Train: Epoch [24], Batch [701/938], Loss: 0.3815184235572815\n",
      "Train: Epoch [24], Batch [702/938], Loss: 0.4231221675872803\n",
      "Train: Epoch [24], Batch [703/938], Loss: 0.294657438993454\n",
      "Train: Epoch [24], Batch [704/938], Loss: 0.15302255749702454\n",
      "Train: Epoch [24], Batch [705/938], Loss: 0.3417508602142334\n",
      "Train: Epoch [24], Batch [706/938], Loss: 0.2599330544471741\n",
      "Train: Epoch [24], Batch [707/938], Loss: 0.23227927088737488\n",
      "Train: Epoch [24], Batch [708/938], Loss: 0.3275395929813385\n",
      "Train: Epoch [24], Batch [709/938], Loss: 0.3686984181404114\n",
      "Train: Epoch [24], Batch [710/938], Loss: 0.6322513222694397\n",
      "Train: Epoch [24], Batch [711/938], Loss: 0.41249027848243713\n",
      "Train: Epoch [24], Batch [712/938], Loss: 0.4468849301338196\n",
      "Train: Epoch [24], Batch [713/938], Loss: 0.3928312361240387\n",
      "Train: Epoch [24], Batch [714/938], Loss: 0.38860705494880676\n",
      "Train: Epoch [24], Batch [715/938], Loss: 0.4164034128189087\n",
      "Train: Epoch [24], Batch [716/938], Loss: 0.34575793147087097\n",
      "Train: Epoch [24], Batch [717/938], Loss: 0.3479074239730835\n",
      "Train: Epoch [24], Batch [718/938], Loss: 0.3741210997104645\n",
      "Train: Epoch [24], Batch [719/938], Loss: 0.450786292552948\n",
      "Train: Epoch [24], Batch [720/938], Loss: 0.18722376227378845\n",
      "Train: Epoch [24], Batch [721/938], Loss: 0.3719794452190399\n",
      "Train: Epoch [24], Batch [722/938], Loss: 0.3869576156139374\n",
      "Train: Epoch [24], Batch [723/938], Loss: 0.27757149934768677\n",
      "Train: Epoch [24], Batch [724/938], Loss: 0.42116856575012207\n",
      "Train: Epoch [24], Batch [725/938], Loss: 0.4620441198348999\n",
      "Train: Epoch [24], Batch [726/938], Loss: 0.29447123408317566\n",
      "Train: Epoch [24], Batch [727/938], Loss: 0.5132087469100952\n",
      "Train: Epoch [24], Batch [728/938], Loss: 0.4381140470504761\n",
      "Train: Epoch [24], Batch [729/938], Loss: 0.28628528118133545\n",
      "Train: Epoch [24], Batch [730/938], Loss: 0.19092941284179688\n",
      "Train: Epoch [24], Batch [731/938], Loss: 0.4109334647655487\n",
      "Train: Epoch [24], Batch [732/938], Loss: 0.3375946879386902\n",
      "Train: Epoch [24], Batch [733/938], Loss: 0.4809005856513977\n",
      "Train: Epoch [24], Batch [734/938], Loss: 0.24132347106933594\n",
      "Train: Epoch [24], Batch [735/938], Loss: 0.19559313356876373\n",
      "Train: Epoch [24], Batch [736/938], Loss: 0.36076799035072327\n",
      "Train: Epoch [24], Batch [737/938], Loss: 0.5335906744003296\n",
      "Train: Epoch [24], Batch [738/938], Loss: 0.5651476383209229\n",
      "Train: Epoch [24], Batch [739/938], Loss: 0.3653064966201782\n",
      "Train: Epoch [24], Batch [740/938], Loss: 0.4321203827857971\n",
      "Train: Epoch [24], Batch [741/938], Loss: 0.47816890478134155\n",
      "Train: Epoch [24], Batch [742/938], Loss: 0.2191775143146515\n",
      "Train: Epoch [24], Batch [743/938], Loss: 0.4401858150959015\n",
      "Train: Epoch [24], Batch [744/938], Loss: 0.4282216727733612\n",
      "Train: Epoch [24], Batch [745/938], Loss: 0.46699807047843933\n",
      "Train: Epoch [24], Batch [746/938], Loss: 0.36465346813201904\n",
      "Train: Epoch [24], Batch [747/938], Loss: 0.2661628723144531\n",
      "Train: Epoch [24], Batch [748/938], Loss: 0.33348003029823303\n",
      "Train: Epoch [24], Batch [749/938], Loss: 0.4039265811443329\n",
      "Train: Epoch [24], Batch [750/938], Loss: 0.3545418977737427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [751/938], Loss: 0.44084641337394714\n",
      "Train: Epoch [24], Batch [752/938], Loss: 0.40545326471328735\n",
      "Train: Epoch [24], Batch [753/938], Loss: 0.4470447301864624\n",
      "Train: Epoch [24], Batch [754/938], Loss: 0.22391247749328613\n",
      "Train: Epoch [24], Batch [755/938], Loss: 0.3588198125362396\n",
      "Train: Epoch [24], Batch [756/938], Loss: 0.38651514053344727\n",
      "Train: Epoch [24], Batch [757/938], Loss: 0.3178982436656952\n",
      "Train: Epoch [24], Batch [758/938], Loss: 0.38495078682899475\n",
      "Train: Epoch [24], Batch [759/938], Loss: 0.3228834271430969\n",
      "Train: Epoch [24], Batch [760/938], Loss: 0.380196213722229\n",
      "Train: Epoch [24], Batch [761/938], Loss: 0.41646018624305725\n",
      "Train: Epoch [24], Batch [762/938], Loss: 0.3265158534049988\n",
      "Train: Epoch [24], Batch [763/938], Loss: 0.30066096782684326\n",
      "Train: Epoch [24], Batch [764/938], Loss: 0.3785504102706909\n",
      "Train: Epoch [24], Batch [765/938], Loss: 0.3305305242538452\n",
      "Train: Epoch [24], Batch [766/938], Loss: 0.4468033015727997\n",
      "Train: Epoch [24], Batch [767/938], Loss: 0.3997456133365631\n",
      "Train: Epoch [24], Batch [768/938], Loss: 0.4258681535720825\n",
      "Train: Epoch [24], Batch [769/938], Loss: 0.5817601680755615\n",
      "Train: Epoch [24], Batch [770/938], Loss: 0.3102130889892578\n",
      "Train: Epoch [24], Batch [771/938], Loss: 0.34452593326568604\n",
      "Train: Epoch [24], Batch [772/938], Loss: 0.6072863340377808\n",
      "Train: Epoch [24], Batch [773/938], Loss: 0.2581414580345154\n",
      "Train: Epoch [24], Batch [774/938], Loss: 0.2805712819099426\n",
      "Train: Epoch [24], Batch [775/938], Loss: 0.4062674045562744\n",
      "Train: Epoch [24], Batch [776/938], Loss: 0.3095467686653137\n",
      "Train: Epoch [24], Batch [777/938], Loss: 0.39403676986694336\n",
      "Train: Epoch [24], Batch [778/938], Loss: 0.4512258768081665\n",
      "Train: Epoch [24], Batch [779/938], Loss: 0.4873383641242981\n",
      "Train: Epoch [24], Batch [780/938], Loss: 0.4599222242832184\n",
      "Train: Epoch [24], Batch [781/938], Loss: 0.31674379110336304\n",
      "Train: Epoch [24], Batch [782/938], Loss: 0.19620677828788757\n",
      "Train: Epoch [24], Batch [783/938], Loss: 0.3961739242076874\n",
      "Train: Epoch [24], Batch [784/938], Loss: 0.4288162887096405\n",
      "Train: Epoch [24], Batch [785/938], Loss: 0.3412742018699646\n",
      "Train: Epoch [24], Batch [786/938], Loss: 0.4775087833404541\n",
      "Train: Epoch [24], Batch [787/938], Loss: 0.3495522141456604\n",
      "Train: Epoch [24], Batch [788/938], Loss: 0.2951384484767914\n",
      "Train: Epoch [24], Batch [789/938], Loss: 0.36092332005500793\n",
      "Train: Epoch [24], Batch [790/938], Loss: 0.286165714263916\n",
      "Train: Epoch [24], Batch [791/938], Loss: 0.35721153020858765\n",
      "Train: Epoch [24], Batch [792/938], Loss: 0.3439154624938965\n",
      "Train: Epoch [24], Batch [793/938], Loss: 0.47547799348831177\n",
      "Train: Epoch [24], Batch [794/938], Loss: 0.49234238266944885\n",
      "Train: Epoch [24], Batch [795/938], Loss: 0.3599095344543457\n",
      "Train: Epoch [24], Batch [796/938], Loss: 0.3936029076576233\n",
      "Train: Epoch [24], Batch [797/938], Loss: 0.3214961886405945\n",
      "Train: Epoch [24], Batch [798/938], Loss: 0.2378252148628235\n",
      "Train: Epoch [24], Batch [799/938], Loss: 0.352300226688385\n",
      "Train: Epoch [24], Batch [800/938], Loss: 0.48705655336380005\n",
      "Train: Epoch [24], Batch [801/938], Loss: 0.3556700348854065\n",
      "Train: Epoch [24], Batch [802/938], Loss: 0.41114580631256104\n",
      "Train: Epoch [24], Batch [803/938], Loss: 0.42119109630584717\n",
      "Train: Epoch [24], Batch [804/938], Loss: 0.3525677025318146\n",
      "Train: Epoch [24], Batch [805/938], Loss: 0.2997578978538513\n",
      "Train: Epoch [24], Batch [806/938], Loss: 0.2560848891735077\n",
      "Train: Epoch [24], Batch [807/938], Loss: 0.39493852853775024\n",
      "Train: Epoch [24], Batch [808/938], Loss: 0.39851441979408264\n",
      "Train: Epoch [24], Batch [809/938], Loss: 0.46857941150665283\n",
      "Train: Epoch [24], Batch [810/938], Loss: 0.45990121364593506\n",
      "Train: Epoch [24], Batch [811/938], Loss: 0.2532181739807129\n",
      "Train: Epoch [24], Batch [812/938], Loss: 0.380450576543808\n",
      "Train: Epoch [24], Batch [813/938], Loss: 0.23655660450458527\n",
      "Train: Epoch [24], Batch [814/938], Loss: 0.42097535729408264\n",
      "Train: Epoch [24], Batch [815/938], Loss: 0.5883959531784058\n",
      "Train: Epoch [24], Batch [816/938], Loss: 0.4502944052219391\n",
      "Train: Epoch [24], Batch [817/938], Loss: 0.3220439553260803\n",
      "Train: Epoch [24], Batch [818/938], Loss: 0.2632408142089844\n",
      "Train: Epoch [24], Batch [819/938], Loss: 0.45574134588241577\n",
      "Train: Epoch [24], Batch [820/938], Loss: 0.42872706055641174\n",
      "Train: Epoch [24], Batch [821/938], Loss: 0.4048689305782318\n",
      "Train: Epoch [24], Batch [822/938], Loss: 0.3074687719345093\n",
      "Train: Epoch [24], Batch [823/938], Loss: 0.3870365023612976\n",
      "Train: Epoch [24], Batch [824/938], Loss: 0.30619877576828003\n",
      "Train: Epoch [24], Batch [825/938], Loss: 0.3646039068698883\n",
      "Train: Epoch [24], Batch [826/938], Loss: 0.257658988237381\n",
      "Train: Epoch [24], Batch [827/938], Loss: 0.34072166681289673\n",
      "Train: Epoch [24], Batch [828/938], Loss: 0.26228854060173035\n",
      "Train: Epoch [24], Batch [829/938], Loss: 0.4551677405834198\n",
      "Train: Epoch [24], Batch [830/938], Loss: 0.4634518623352051\n",
      "Train: Epoch [24], Batch [831/938], Loss: 0.4279922544956207\n",
      "Train: Epoch [24], Batch [832/938], Loss: 0.3770976960659027\n",
      "Train: Epoch [24], Batch [833/938], Loss: 0.26822322607040405\n",
      "Train: Epoch [24], Batch [834/938], Loss: 0.37936151027679443\n",
      "Train: Epoch [24], Batch [835/938], Loss: 0.2520095109939575\n",
      "Train: Epoch [24], Batch [836/938], Loss: 0.6951610445976257\n",
      "Train: Epoch [24], Batch [837/938], Loss: 0.39166367053985596\n",
      "Train: Epoch [24], Batch [838/938], Loss: 0.48251837491989136\n",
      "Train: Epoch [24], Batch [839/938], Loss: 0.3500404953956604\n",
      "Train: Epoch [24], Batch [840/938], Loss: 0.22422397136688232\n",
      "Train: Epoch [24], Batch [841/938], Loss: 0.21162280440330505\n",
      "Train: Epoch [24], Batch [842/938], Loss: 0.30288365483283997\n",
      "Train: Epoch [24], Batch [843/938], Loss: 0.4295153021812439\n",
      "Train: Epoch [24], Batch [844/938], Loss: 0.4491996169090271\n",
      "Train: Epoch [24], Batch [845/938], Loss: 0.34767651557922363\n",
      "Train: Epoch [24], Batch [846/938], Loss: 0.2509899437427521\n",
      "Train: Epoch [24], Batch [847/938], Loss: 0.43060556054115295\n",
      "Train: Epoch [24], Batch [848/938], Loss: 0.49847447872161865\n",
      "Train: Epoch [24], Batch [849/938], Loss: 0.498491108417511\n",
      "Train: Epoch [24], Batch [850/938], Loss: 0.48908087611198425\n",
      "Train: Epoch [24], Batch [851/938], Loss: 0.5105430483818054\n",
      "Train: Epoch [24], Batch [852/938], Loss: 0.3192906975746155\n",
      "Train: Epoch [24], Batch [853/938], Loss: 0.2761749029159546\n",
      "Train: Epoch [24], Batch [854/938], Loss: 0.258340984582901\n",
      "Train: Epoch [24], Batch [855/938], Loss: 0.465762197971344\n",
      "Train: Epoch [24], Batch [856/938], Loss: 0.2806905210018158\n",
      "Train: Epoch [24], Batch [857/938], Loss: 0.8043084740638733\n",
      "Train: Epoch [24], Batch [858/938], Loss: 0.3524817228317261\n",
      "Train: Epoch [24], Batch [859/938], Loss: 0.3833490014076233\n",
      "Train: Epoch [24], Batch [860/938], Loss: 0.3886352777481079\n",
      "Train: Epoch [24], Batch [861/938], Loss: 0.25453999638557434\n",
      "Train: Epoch [24], Batch [862/938], Loss: 0.3448570966720581\n",
      "Train: Epoch [24], Batch [863/938], Loss: 0.4791019558906555\n",
      "Train: Epoch [24], Batch [864/938], Loss: 0.5647114515304565\n",
      "Train: Epoch [24], Batch [865/938], Loss: 0.41259780526161194\n",
      "Train: Epoch [24], Batch [866/938], Loss: 0.3778752386569977\n",
      "Train: Epoch [24], Batch [867/938], Loss: 0.2700987756252289\n",
      "Train: Epoch [24], Batch [868/938], Loss: 0.4595467746257782\n",
      "Train: Epoch [24], Batch [869/938], Loss: 0.45909610390663147\n",
      "Train: Epoch [24], Batch [870/938], Loss: 0.30191802978515625\n",
      "Train: Epoch [24], Batch [871/938], Loss: 0.4771653413772583\n",
      "Train: Epoch [24], Batch [872/938], Loss: 0.3923163414001465\n",
      "Train: Epoch [24], Batch [873/938], Loss: 0.31597650051116943\n",
      "Train: Epoch [24], Batch [874/938], Loss: 0.2967408299446106\n",
      "Train: Epoch [24], Batch [875/938], Loss: 0.19553142786026\n",
      "Train: Epoch [24], Batch [876/938], Loss: 0.29658043384552\n",
      "Train: Epoch [24], Batch [877/938], Loss: 0.4241178035736084\n",
      "Train: Epoch [24], Batch [878/938], Loss: 0.4710773229598999\n",
      "Train: Epoch [24], Batch [879/938], Loss: 0.48132723569869995\n",
      "Train: Epoch [24], Batch [880/938], Loss: 0.2915262281894684\n",
      "Train: Epoch [24], Batch [881/938], Loss: 0.4527026116847992\n",
      "Train: Epoch [24], Batch [882/938], Loss: 0.3967110216617584\n",
      "Train: Epoch [24], Batch [883/938], Loss: 0.5565104484558105\n",
      "Train: Epoch [24], Batch [884/938], Loss: 0.4270249009132385\n",
      "Train: Epoch [24], Batch [885/938], Loss: 0.3054850101470947\n",
      "Train: Epoch [24], Batch [886/938], Loss: 0.3626604676246643\n",
      "Train: Epoch [24], Batch [887/938], Loss: 0.2771846055984497\n",
      "Train: Epoch [24], Batch [888/938], Loss: 0.6857186555862427\n",
      "Train: Epoch [24], Batch [889/938], Loss: 0.3899475932121277\n",
      "Train: Epoch [24], Batch [890/938], Loss: 0.37008535861968994\n",
      "Train: Epoch [24], Batch [891/938], Loss: 0.3865810036659241\n",
      "Train: Epoch [24], Batch [892/938], Loss: 0.4217033088207245\n",
      "Train: Epoch [24], Batch [893/938], Loss: 0.2980636656284332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [24], Batch [894/938], Loss: 0.31172430515289307\n",
      "Train: Epoch [24], Batch [895/938], Loss: 0.3560788631439209\n",
      "Train: Epoch [24], Batch [896/938], Loss: 0.4672285318374634\n",
      "Train: Epoch [24], Batch [897/938], Loss: 0.6462653875350952\n",
      "Train: Epoch [24], Batch [898/938], Loss: 0.48876941204071045\n",
      "Train: Epoch [24], Batch [899/938], Loss: 0.3172725737094879\n",
      "Train: Epoch [24], Batch [900/938], Loss: 0.544777512550354\n",
      "Train: Epoch [24], Batch [901/938], Loss: 0.3293812572956085\n",
      "Train: Epoch [24], Batch [902/938], Loss: 0.4171646237373352\n",
      "Train: Epoch [24], Batch [903/938], Loss: 0.314397394657135\n",
      "Train: Epoch [24], Batch [904/938], Loss: 0.3203743100166321\n",
      "Train: Epoch [24], Batch [905/938], Loss: 0.3465631604194641\n",
      "Train: Epoch [24], Batch [906/938], Loss: 0.2034008502960205\n",
      "Train: Epoch [24], Batch [907/938], Loss: 0.3737533688545227\n",
      "Train: Epoch [24], Batch [908/938], Loss: 0.41636574268341064\n",
      "Train: Epoch [24], Batch [909/938], Loss: 0.3739578127861023\n",
      "Train: Epoch [24], Batch [910/938], Loss: 0.27383238077163696\n",
      "Train: Epoch [24], Batch [911/938], Loss: 0.5492348670959473\n",
      "Train: Epoch [24], Batch [912/938], Loss: 0.30410218238830566\n",
      "Train: Epoch [24], Batch [913/938], Loss: 0.49043500423431396\n",
      "Train: Epoch [24], Batch [914/938], Loss: 0.3169513940811157\n",
      "Train: Epoch [24], Batch [915/938], Loss: 0.2188974916934967\n",
      "Train: Epoch [24], Batch [916/938], Loss: 0.6146742105484009\n",
      "Train: Epoch [24], Batch [917/938], Loss: 0.28670716285705566\n",
      "Train: Epoch [24], Batch [918/938], Loss: 0.39607489109039307\n",
      "Train: Epoch [24], Batch [919/938], Loss: 0.33386659622192383\n",
      "Train: Epoch [24], Batch [920/938], Loss: 0.42769384384155273\n",
      "Train: Epoch [24], Batch [921/938], Loss: 0.38969486951828003\n",
      "Train: Epoch [24], Batch [922/938], Loss: 0.5099612474441528\n",
      "Train: Epoch [24], Batch [923/938], Loss: 0.4410907030105591\n",
      "Train: Epoch [24], Batch [924/938], Loss: 0.32893165946006775\n",
      "Train: Epoch [24], Batch [925/938], Loss: 0.34231090545654297\n",
      "Train: Epoch [24], Batch [926/938], Loss: 0.36441171169281006\n",
      "Train: Epoch [24], Batch [927/938], Loss: 0.28696370124816895\n",
      "Train: Epoch [24], Batch [928/938], Loss: 0.4158889651298523\n",
      "Train: Epoch [24], Batch [929/938], Loss: 0.4761228561401367\n",
      "Train: Epoch [24], Batch [930/938], Loss: 0.4688951075077057\n",
      "Train: Epoch [24], Batch [931/938], Loss: 0.47495728731155396\n",
      "Train: Epoch [24], Batch [932/938], Loss: 0.4143911898136139\n",
      "Train: Epoch [24], Batch [933/938], Loss: 0.4597390592098236\n",
      "Train: Epoch [24], Batch [934/938], Loss: 0.32846587896347046\n",
      "Train: Epoch [24], Batch [935/938], Loss: 0.26669222116470337\n",
      "Train: Epoch [24], Batch [936/938], Loss: 0.3271945118904114\n",
      "Train: Epoch [24], Batch [937/938], Loss: 0.37305229902267456\n",
      "Train: Epoch [24], Batch [938/938], Loss: 0.4671587646007538\n",
      "Accuracy of train set: 0.8649666666666667\n",
      "Validation: Epoch [24], Batch [1/938], Loss: 0.2637433707714081\n",
      "Validation: Epoch [24], Batch [2/938], Loss: 0.3034396171569824\n",
      "Validation: Epoch [24], Batch [3/938], Loss: 0.3972083032131195\n",
      "Validation: Epoch [24], Batch [4/938], Loss: 0.3778252601623535\n",
      "Validation: Epoch [24], Batch [5/938], Loss: 0.3970082402229309\n",
      "Validation: Epoch [24], Batch [6/938], Loss: 0.4867500066757202\n",
      "Validation: Epoch [24], Batch [7/938], Loss: 0.4474940299987793\n",
      "Validation: Epoch [24], Batch [8/938], Loss: 0.3905786871910095\n",
      "Validation: Epoch [24], Batch [9/938], Loss: 0.2745242416858673\n",
      "Validation: Epoch [24], Batch [10/938], Loss: 0.33023059368133545\n",
      "Validation: Epoch [24], Batch [11/938], Loss: 0.5195633172988892\n",
      "Validation: Epoch [24], Batch [12/938], Loss: 0.3502867519855499\n",
      "Validation: Epoch [24], Batch [13/938], Loss: 0.5752040147781372\n",
      "Validation: Epoch [24], Batch [14/938], Loss: 0.2409209907054901\n",
      "Validation: Epoch [24], Batch [15/938], Loss: 0.15761446952819824\n",
      "Validation: Epoch [24], Batch [16/938], Loss: 0.24800148606300354\n",
      "Validation: Epoch [24], Batch [17/938], Loss: 0.2740192413330078\n",
      "Validation: Epoch [24], Batch [18/938], Loss: 0.3968309760093689\n",
      "Validation: Epoch [24], Batch [19/938], Loss: 0.4517972469329834\n",
      "Validation: Epoch [24], Batch [20/938], Loss: 0.44174087047576904\n",
      "Validation: Epoch [24], Batch [21/938], Loss: 0.30096977949142456\n",
      "Validation: Epoch [24], Batch [22/938], Loss: 0.413967102766037\n",
      "Validation: Epoch [24], Batch [23/938], Loss: 0.23518478870391846\n",
      "Validation: Epoch [24], Batch [24/938], Loss: 0.3445148169994354\n",
      "Validation: Epoch [24], Batch [25/938], Loss: 0.4808098375797272\n",
      "Validation: Epoch [24], Batch [26/938], Loss: 0.4918857216835022\n",
      "Validation: Epoch [24], Batch [27/938], Loss: 0.3419719338417053\n",
      "Validation: Epoch [24], Batch [28/938], Loss: 0.3608134984970093\n",
      "Validation: Epoch [24], Batch [29/938], Loss: 0.48244136571884155\n",
      "Validation: Epoch [24], Batch [30/938], Loss: 0.45586127042770386\n",
      "Validation: Epoch [24], Batch [31/938], Loss: 0.2957721948623657\n",
      "Validation: Epoch [24], Batch [32/938], Loss: 0.5444288849830627\n",
      "Validation: Epoch [24], Batch [33/938], Loss: 0.34952762722969055\n",
      "Validation: Epoch [24], Batch [34/938], Loss: 0.35126543045043945\n",
      "Validation: Epoch [24], Batch [35/938], Loss: 0.37548989057540894\n",
      "Validation: Epoch [24], Batch [36/938], Loss: 0.280764102935791\n",
      "Validation: Epoch [24], Batch [37/938], Loss: 0.22800372540950775\n",
      "Validation: Epoch [24], Batch [38/938], Loss: 0.6250160932540894\n",
      "Validation: Epoch [24], Batch [39/938], Loss: 0.6152616739273071\n",
      "Validation: Epoch [24], Batch [40/938], Loss: 0.41193732619285583\n",
      "Validation: Epoch [24], Batch [41/938], Loss: 0.37857747077941895\n",
      "Validation: Epoch [24], Batch [42/938], Loss: 0.2788945734500885\n",
      "Validation: Epoch [24], Batch [43/938], Loss: 0.35868018865585327\n",
      "Validation: Epoch [24], Batch [44/938], Loss: 0.46552085876464844\n",
      "Validation: Epoch [24], Batch [45/938], Loss: 0.367722749710083\n",
      "Validation: Epoch [24], Batch [46/938], Loss: 0.3530117869377136\n",
      "Validation: Epoch [24], Batch [47/938], Loss: 0.4572402834892273\n",
      "Validation: Epoch [24], Batch [48/938], Loss: 0.2810788154602051\n",
      "Validation: Epoch [24], Batch [49/938], Loss: 0.3225673735141754\n",
      "Validation: Epoch [24], Batch [50/938], Loss: 0.26753470301628113\n",
      "Validation: Epoch [24], Batch [51/938], Loss: 0.5212860703468323\n",
      "Validation: Epoch [24], Batch [52/938], Loss: 0.32363200187683105\n",
      "Validation: Epoch [24], Batch [53/938], Loss: 0.4175194501876831\n",
      "Validation: Epoch [24], Batch [54/938], Loss: 0.19437552988529205\n",
      "Validation: Epoch [24], Batch [55/938], Loss: 0.5037522315979004\n",
      "Validation: Epoch [24], Batch [56/938], Loss: 0.3564162850379944\n",
      "Validation: Epoch [24], Batch [57/938], Loss: 0.308854877948761\n",
      "Validation: Epoch [24], Batch [58/938], Loss: 0.4444948136806488\n",
      "Validation: Epoch [24], Batch [59/938], Loss: 0.4849417209625244\n",
      "Validation: Epoch [24], Batch [60/938], Loss: 0.35048794746398926\n",
      "Validation: Epoch [24], Batch [61/938], Loss: 0.33598771691322327\n",
      "Validation: Epoch [24], Batch [62/938], Loss: 0.35412389039993286\n",
      "Validation: Epoch [24], Batch [63/938], Loss: 0.27355706691741943\n",
      "Validation: Epoch [24], Batch [64/938], Loss: 0.4854075312614441\n",
      "Validation: Epoch [24], Batch [65/938], Loss: 0.40253645181655884\n",
      "Validation: Epoch [24], Batch [66/938], Loss: 0.36795276403427124\n",
      "Validation: Epoch [24], Batch [67/938], Loss: 0.47224512696266174\n",
      "Validation: Epoch [24], Batch [68/938], Loss: 0.34591570496559143\n",
      "Validation: Epoch [24], Batch [69/938], Loss: 0.3095930814743042\n",
      "Validation: Epoch [24], Batch [70/938], Loss: 0.2790907323360443\n",
      "Validation: Epoch [24], Batch [71/938], Loss: 0.5170517563819885\n",
      "Validation: Epoch [24], Batch [72/938], Loss: 0.2864043414592743\n",
      "Validation: Epoch [24], Batch [73/938], Loss: 0.2940349280834198\n",
      "Validation: Epoch [24], Batch [74/938], Loss: 0.31302884221076965\n",
      "Validation: Epoch [24], Batch [75/938], Loss: 0.2298794388771057\n",
      "Validation: Epoch [24], Batch [76/938], Loss: 0.3707698881626129\n",
      "Validation: Epoch [24], Batch [77/938], Loss: 0.3517114520072937\n",
      "Validation: Epoch [24], Batch [78/938], Loss: 0.35454970598220825\n",
      "Validation: Epoch [24], Batch [79/938], Loss: 0.36534634232521057\n",
      "Validation: Epoch [24], Batch [80/938], Loss: 0.27821147441864014\n",
      "Validation: Epoch [24], Batch [81/938], Loss: 0.4092544913291931\n",
      "Validation: Epoch [24], Batch [82/938], Loss: 0.32716554403305054\n",
      "Validation: Epoch [24], Batch [83/938], Loss: 0.43937456607818604\n",
      "Validation: Epoch [24], Batch [84/938], Loss: 0.4334968328475952\n",
      "Validation: Epoch [24], Batch [85/938], Loss: 0.44418054819107056\n",
      "Validation: Epoch [24], Batch [86/938], Loss: 0.30271485447883606\n",
      "Validation: Epoch [24], Batch [87/938], Loss: 0.36134859919548035\n",
      "Validation: Epoch [24], Batch [88/938], Loss: 0.31233641505241394\n",
      "Validation: Epoch [24], Batch [89/938], Loss: 0.41457533836364746\n",
      "Validation: Epoch [24], Batch [90/938], Loss: 0.3322848081588745\n",
      "Validation: Epoch [24], Batch [91/938], Loss: 0.2834278345108032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [92/938], Loss: 0.32957983016967773\n",
      "Validation: Epoch [24], Batch [93/938], Loss: 0.3852587342262268\n",
      "Validation: Epoch [24], Batch [94/938], Loss: 0.46971672773361206\n",
      "Validation: Epoch [24], Batch [95/938], Loss: 0.32718539237976074\n",
      "Validation: Epoch [24], Batch [96/938], Loss: 0.3661600351333618\n",
      "Validation: Epoch [24], Batch [97/938], Loss: 0.25628694891929626\n",
      "Validation: Epoch [24], Batch [98/938], Loss: 0.3281000554561615\n",
      "Validation: Epoch [24], Batch [99/938], Loss: 0.4668349027633667\n",
      "Validation: Epoch [24], Batch [100/938], Loss: 0.3010227084159851\n",
      "Validation: Epoch [24], Batch [101/938], Loss: 0.42933982610702515\n",
      "Validation: Epoch [24], Batch [102/938], Loss: 0.40223994851112366\n",
      "Validation: Epoch [24], Batch [103/938], Loss: 0.4549020230770111\n",
      "Validation: Epoch [24], Batch [104/938], Loss: 0.4052446484565735\n",
      "Validation: Epoch [24], Batch [105/938], Loss: 0.4690549969673157\n",
      "Validation: Epoch [24], Batch [106/938], Loss: 0.552636981010437\n",
      "Validation: Epoch [24], Batch [107/938], Loss: 0.3062109649181366\n",
      "Validation: Epoch [24], Batch [108/938], Loss: 0.5069497227668762\n",
      "Validation: Epoch [24], Batch [109/938], Loss: 0.39963459968566895\n",
      "Validation: Epoch [24], Batch [110/938], Loss: 0.2930315136909485\n",
      "Validation: Epoch [24], Batch [111/938], Loss: 0.3432275652885437\n",
      "Validation: Epoch [24], Batch [112/938], Loss: 0.31546750664711\n",
      "Validation: Epoch [24], Batch [113/938], Loss: 0.3405742645263672\n",
      "Validation: Epoch [24], Batch [114/938], Loss: 0.3033057749271393\n",
      "Validation: Epoch [24], Batch [115/938], Loss: 0.45596593618392944\n",
      "Validation: Epoch [24], Batch [116/938], Loss: 0.5451289415359497\n",
      "Validation: Epoch [24], Batch [117/938], Loss: 0.4180230498313904\n",
      "Validation: Epoch [24], Batch [118/938], Loss: 0.2969370186328888\n",
      "Validation: Epoch [24], Batch [119/938], Loss: 0.24498635530471802\n",
      "Validation: Epoch [24], Batch [120/938], Loss: 0.336961567401886\n",
      "Validation: Epoch [24], Batch [121/938], Loss: 0.4188847541809082\n",
      "Validation: Epoch [24], Batch [122/938], Loss: 0.22549691796302795\n",
      "Validation: Epoch [24], Batch [123/938], Loss: 0.4372978210449219\n",
      "Validation: Epoch [24], Batch [124/938], Loss: 0.47992753982543945\n",
      "Validation: Epoch [24], Batch [125/938], Loss: 0.3032194674015045\n",
      "Validation: Epoch [24], Batch [126/938], Loss: 0.4620610773563385\n",
      "Validation: Epoch [24], Batch [127/938], Loss: 0.46089106798171997\n",
      "Validation: Epoch [24], Batch [128/938], Loss: 0.3753099739551544\n",
      "Validation: Epoch [24], Batch [129/938], Loss: 0.3799135684967041\n",
      "Validation: Epoch [24], Batch [130/938], Loss: 0.43552494049072266\n",
      "Validation: Epoch [24], Batch [131/938], Loss: 0.3973660171031952\n",
      "Validation: Epoch [24], Batch [132/938], Loss: 0.5470371246337891\n",
      "Validation: Epoch [24], Batch [133/938], Loss: 0.3735714554786682\n",
      "Validation: Epoch [24], Batch [134/938], Loss: 0.21750608086585999\n",
      "Validation: Epoch [24], Batch [135/938], Loss: 0.2393328994512558\n",
      "Validation: Epoch [24], Batch [136/938], Loss: 0.4492069482803345\n",
      "Validation: Epoch [24], Batch [137/938], Loss: 0.43430817127227783\n",
      "Validation: Epoch [24], Batch [138/938], Loss: 0.3170400559902191\n",
      "Validation: Epoch [24], Batch [139/938], Loss: 0.4480254054069519\n",
      "Validation: Epoch [24], Batch [140/938], Loss: 0.513007640838623\n",
      "Validation: Epoch [24], Batch [141/938], Loss: 0.17867077887058258\n",
      "Validation: Epoch [24], Batch [142/938], Loss: 0.3659168481826782\n",
      "Validation: Epoch [24], Batch [143/938], Loss: 0.6651045083999634\n",
      "Validation: Epoch [24], Batch [144/938], Loss: 0.30996793508529663\n",
      "Validation: Epoch [24], Batch [145/938], Loss: 0.34941786527633667\n",
      "Validation: Epoch [24], Batch [146/938], Loss: 0.36760979890823364\n",
      "Validation: Epoch [24], Batch [147/938], Loss: 0.4444144070148468\n",
      "Validation: Epoch [24], Batch [148/938], Loss: 0.23172414302825928\n",
      "Validation: Epoch [24], Batch [149/938], Loss: 0.19505609571933746\n",
      "Validation: Epoch [24], Batch [150/938], Loss: 0.5085738897323608\n",
      "Validation: Epoch [24], Batch [151/938], Loss: 0.3496139943599701\n",
      "Validation: Epoch [24], Batch [152/938], Loss: 0.430477499961853\n",
      "Validation: Epoch [24], Batch [153/938], Loss: 0.4482596516609192\n",
      "Validation: Epoch [24], Batch [154/938], Loss: 0.4277959167957306\n",
      "Validation: Epoch [24], Batch [155/938], Loss: 0.4390469193458557\n",
      "Validation: Epoch [24], Batch [156/938], Loss: 0.3992883861064911\n",
      "Validation: Epoch [24], Batch [157/938], Loss: 0.31340330839157104\n",
      "Validation: Epoch [24], Batch [158/938], Loss: 0.3961471915245056\n",
      "Validation: Epoch [24], Batch [159/938], Loss: 0.46741652488708496\n",
      "Validation: Epoch [24], Batch [160/938], Loss: 0.3247230350971222\n",
      "Validation: Epoch [24], Batch [161/938], Loss: 0.25393375754356384\n",
      "Validation: Epoch [24], Batch [162/938], Loss: 0.2713759243488312\n",
      "Validation: Epoch [24], Batch [163/938], Loss: 0.279607355594635\n",
      "Validation: Epoch [24], Batch [164/938], Loss: 0.24877101182937622\n",
      "Validation: Epoch [24], Batch [165/938], Loss: 0.4807160198688507\n",
      "Validation: Epoch [24], Batch [166/938], Loss: 0.5220921039581299\n",
      "Validation: Epoch [24], Batch [167/938], Loss: 0.41679129004478455\n",
      "Validation: Epoch [24], Batch [168/938], Loss: 0.24157997965812683\n",
      "Validation: Epoch [24], Batch [169/938], Loss: 0.31382760405540466\n",
      "Validation: Epoch [24], Batch [170/938], Loss: 0.384876012802124\n",
      "Validation: Epoch [24], Batch [171/938], Loss: 0.3722901940345764\n",
      "Validation: Epoch [24], Batch [172/938], Loss: 0.49569547176361084\n",
      "Validation: Epoch [24], Batch [173/938], Loss: 0.4262953996658325\n",
      "Validation: Epoch [24], Batch [174/938], Loss: 0.3763572871685028\n",
      "Validation: Epoch [24], Batch [175/938], Loss: 0.32936757802963257\n",
      "Validation: Epoch [24], Batch [176/938], Loss: 0.43511879444122314\n",
      "Validation: Epoch [24], Batch [177/938], Loss: 0.2862285077571869\n",
      "Validation: Epoch [24], Batch [178/938], Loss: 0.4541844427585602\n",
      "Validation: Epoch [24], Batch [179/938], Loss: 0.32911187410354614\n",
      "Validation: Epoch [24], Batch [180/938], Loss: 0.4435488283634186\n",
      "Validation: Epoch [24], Batch [181/938], Loss: 0.4675660729408264\n",
      "Validation: Epoch [24], Batch [182/938], Loss: 0.3267597258090973\n",
      "Validation: Epoch [24], Batch [183/938], Loss: 0.5540303587913513\n",
      "Validation: Epoch [24], Batch [184/938], Loss: 0.2863169014453888\n",
      "Validation: Epoch [24], Batch [185/938], Loss: 0.5228492617607117\n",
      "Validation: Epoch [24], Batch [186/938], Loss: 0.28975093364715576\n",
      "Validation: Epoch [24], Batch [187/938], Loss: 0.3017422556877136\n",
      "Validation: Epoch [24], Batch [188/938], Loss: 0.3035324811935425\n",
      "Validation: Epoch [24], Batch [189/938], Loss: 0.3409042954444885\n",
      "Validation: Epoch [24], Batch [190/938], Loss: 0.3715895116329193\n",
      "Validation: Epoch [24], Batch [191/938], Loss: 0.5610074996948242\n",
      "Validation: Epoch [24], Batch [192/938], Loss: 0.1959722936153412\n",
      "Validation: Epoch [24], Batch [193/938], Loss: 0.33695733547210693\n",
      "Validation: Epoch [24], Batch [194/938], Loss: 0.2810167372226715\n",
      "Validation: Epoch [24], Batch [195/938], Loss: 0.5370669364929199\n",
      "Validation: Epoch [24], Batch [196/938], Loss: 0.2773846387863159\n",
      "Validation: Epoch [24], Batch [197/938], Loss: 0.3453207314014435\n",
      "Validation: Epoch [24], Batch [198/938], Loss: 0.5596880912780762\n",
      "Validation: Epoch [24], Batch [199/938], Loss: 0.6151423454284668\n",
      "Validation: Epoch [24], Batch [200/938], Loss: 0.3556278944015503\n",
      "Validation: Epoch [24], Batch [201/938], Loss: 0.34085899591445923\n",
      "Validation: Epoch [24], Batch [202/938], Loss: 0.31008216738700867\n",
      "Validation: Epoch [24], Batch [203/938], Loss: 0.33779722452163696\n",
      "Validation: Epoch [24], Batch [204/938], Loss: 0.5321945548057556\n",
      "Validation: Epoch [24], Batch [205/938], Loss: 0.32170194387435913\n",
      "Validation: Epoch [24], Batch [206/938], Loss: 0.3903609812259674\n",
      "Validation: Epoch [24], Batch [207/938], Loss: 0.31915080547332764\n",
      "Validation: Epoch [24], Batch [208/938], Loss: 0.22412414848804474\n",
      "Validation: Epoch [24], Batch [209/938], Loss: 0.30474501848220825\n",
      "Validation: Epoch [24], Batch [210/938], Loss: 0.27976733446121216\n",
      "Validation: Epoch [24], Batch [211/938], Loss: 0.44984492659568787\n",
      "Validation: Epoch [24], Batch [212/938], Loss: 0.3716658651828766\n",
      "Validation: Epoch [24], Batch [213/938], Loss: 0.44165804982185364\n",
      "Validation: Epoch [24], Batch [214/938], Loss: 0.4741884469985962\n",
      "Validation: Epoch [24], Batch [215/938], Loss: 0.3300047516822815\n",
      "Validation: Epoch [24], Batch [216/938], Loss: 0.30144554376602173\n",
      "Validation: Epoch [24], Batch [217/938], Loss: 0.5659059882164001\n",
      "Validation: Epoch [24], Batch [218/938], Loss: 0.37823212146759033\n",
      "Validation: Epoch [24], Batch [219/938], Loss: 0.33302026987075806\n",
      "Validation: Epoch [24], Batch [220/938], Loss: 0.28203028440475464\n",
      "Validation: Epoch [24], Batch [221/938], Loss: 0.27870914340019226\n",
      "Validation: Epoch [24], Batch [222/938], Loss: 0.39937132596969604\n",
      "Validation: Epoch [24], Batch [223/938], Loss: 0.4417933225631714\n",
      "Validation: Epoch [24], Batch [224/938], Loss: 0.34544551372528076\n",
      "Validation: Epoch [24], Batch [225/938], Loss: 0.34083986282348633\n",
      "Validation: Epoch [24], Batch [226/938], Loss: 0.34607169032096863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [227/938], Loss: 0.38881585001945496\n",
      "Validation: Epoch [24], Batch [228/938], Loss: 0.29584556818008423\n",
      "Validation: Epoch [24], Batch [229/938], Loss: 0.3822852373123169\n",
      "Validation: Epoch [24], Batch [230/938], Loss: 0.36022716760635376\n",
      "Validation: Epoch [24], Batch [231/938], Loss: 0.24087965488433838\n",
      "Validation: Epoch [24], Batch [232/938], Loss: 0.5233091115951538\n",
      "Validation: Epoch [24], Batch [233/938], Loss: 0.2579587697982788\n",
      "Validation: Epoch [24], Batch [234/938], Loss: 0.548497200012207\n",
      "Validation: Epoch [24], Batch [235/938], Loss: 0.4055319130420685\n",
      "Validation: Epoch [24], Batch [236/938], Loss: 0.635955274105072\n",
      "Validation: Epoch [24], Batch [237/938], Loss: 0.4552651047706604\n",
      "Validation: Epoch [24], Batch [238/938], Loss: 0.4259219765663147\n",
      "Validation: Epoch [24], Batch [239/938], Loss: 0.47219622135162354\n",
      "Validation: Epoch [24], Batch [240/938], Loss: 0.24156071245670319\n",
      "Validation: Epoch [24], Batch [241/938], Loss: 0.3702045679092407\n",
      "Validation: Epoch [24], Batch [242/938], Loss: 0.3358723521232605\n",
      "Validation: Epoch [24], Batch [243/938], Loss: 0.36908113956451416\n",
      "Validation: Epoch [24], Batch [244/938], Loss: 0.24744290113449097\n",
      "Validation: Epoch [24], Batch [245/938], Loss: 0.23957833647727966\n",
      "Validation: Epoch [24], Batch [246/938], Loss: 0.41749027371406555\n",
      "Validation: Epoch [24], Batch [247/938], Loss: 0.48386940360069275\n",
      "Validation: Epoch [24], Batch [248/938], Loss: 0.4993327856063843\n",
      "Validation: Epoch [24], Batch [249/938], Loss: 0.3878457546234131\n",
      "Validation: Epoch [24], Batch [250/938], Loss: 0.27386537194252014\n",
      "Validation: Epoch [24], Batch [251/938], Loss: 0.39359527826309204\n",
      "Validation: Epoch [24], Batch [252/938], Loss: 0.5147895216941833\n",
      "Validation: Epoch [24], Batch [253/938], Loss: 0.4262096881866455\n",
      "Validation: Epoch [24], Batch [254/938], Loss: 0.7429852485656738\n",
      "Validation: Epoch [24], Batch [255/938], Loss: 0.4359632432460785\n",
      "Validation: Epoch [24], Batch [256/938], Loss: 0.3239343762397766\n",
      "Validation: Epoch [24], Batch [257/938], Loss: 0.39975637197494507\n",
      "Validation: Epoch [24], Batch [258/938], Loss: 0.3274606466293335\n",
      "Validation: Epoch [24], Batch [259/938], Loss: 0.5318981409072876\n",
      "Validation: Epoch [24], Batch [260/938], Loss: 0.5228861570358276\n",
      "Validation: Epoch [24], Batch [261/938], Loss: 0.4072403311729431\n",
      "Validation: Epoch [24], Batch [262/938], Loss: 0.3258521556854248\n",
      "Validation: Epoch [24], Batch [263/938], Loss: 0.3873400092124939\n",
      "Validation: Epoch [24], Batch [264/938], Loss: 0.625114381313324\n",
      "Validation: Epoch [24], Batch [265/938], Loss: 0.2354462444782257\n",
      "Validation: Epoch [24], Batch [266/938], Loss: 0.24657586216926575\n",
      "Validation: Epoch [24], Batch [267/938], Loss: 0.2225230634212494\n",
      "Validation: Epoch [24], Batch [268/938], Loss: 0.2813032269477844\n",
      "Validation: Epoch [24], Batch [269/938], Loss: 0.39702433347702026\n",
      "Validation: Epoch [24], Batch [270/938], Loss: 0.38625985383987427\n",
      "Validation: Epoch [24], Batch [271/938], Loss: 0.4678233861923218\n",
      "Validation: Epoch [24], Batch [272/938], Loss: 0.37506532669067383\n",
      "Validation: Epoch [24], Batch [273/938], Loss: 0.3923989236354828\n",
      "Validation: Epoch [24], Batch [274/938], Loss: 0.3646555542945862\n",
      "Validation: Epoch [24], Batch [275/938], Loss: 0.3922741711139679\n",
      "Validation: Epoch [24], Batch [276/938], Loss: 0.3077755570411682\n",
      "Validation: Epoch [24], Batch [277/938], Loss: 0.3135766386985779\n",
      "Validation: Epoch [24], Batch [278/938], Loss: 0.44614243507385254\n",
      "Validation: Epoch [24], Batch [279/938], Loss: 0.23861363530158997\n",
      "Validation: Epoch [24], Batch [280/938], Loss: 0.6053191423416138\n",
      "Validation: Epoch [24], Batch [281/938], Loss: 0.4270915389060974\n",
      "Validation: Epoch [24], Batch [282/938], Loss: 0.3836916387081146\n",
      "Validation: Epoch [24], Batch [283/938], Loss: 0.42537304759025574\n",
      "Validation: Epoch [24], Batch [284/938], Loss: 0.28188133239746094\n",
      "Validation: Epoch [24], Batch [285/938], Loss: 0.4201502799987793\n",
      "Validation: Epoch [24], Batch [286/938], Loss: 0.4265112280845642\n",
      "Validation: Epoch [24], Batch [287/938], Loss: 0.34105563163757324\n",
      "Validation: Epoch [24], Batch [288/938], Loss: 0.3811068832874298\n",
      "Validation: Epoch [24], Batch [289/938], Loss: 0.37312912940979004\n",
      "Validation: Epoch [24], Batch [290/938], Loss: 0.5391640663146973\n",
      "Validation: Epoch [24], Batch [291/938], Loss: 0.5831714272499084\n",
      "Validation: Epoch [24], Batch [292/938], Loss: 0.5027774572372437\n",
      "Validation: Epoch [24], Batch [293/938], Loss: 0.34448349475860596\n",
      "Validation: Epoch [24], Batch [294/938], Loss: 0.3344556987285614\n",
      "Validation: Epoch [24], Batch [295/938], Loss: 0.33080315589904785\n",
      "Validation: Epoch [24], Batch [296/938], Loss: 0.540177583694458\n",
      "Validation: Epoch [24], Batch [297/938], Loss: 0.34298819303512573\n",
      "Validation: Epoch [24], Batch [298/938], Loss: 0.3685292601585388\n",
      "Validation: Epoch [24], Batch [299/938], Loss: 0.21417765319347382\n",
      "Validation: Epoch [24], Batch [300/938], Loss: 0.4073135554790497\n",
      "Validation: Epoch [24], Batch [301/938], Loss: 0.42875730991363525\n",
      "Validation: Epoch [24], Batch [302/938], Loss: 0.4260868430137634\n",
      "Validation: Epoch [24], Batch [303/938], Loss: 0.4689249098300934\n",
      "Validation: Epoch [24], Batch [304/938], Loss: 0.20248758792877197\n",
      "Validation: Epoch [24], Batch [305/938], Loss: 0.2610361576080322\n",
      "Validation: Epoch [24], Batch [306/938], Loss: 0.25006985664367676\n",
      "Validation: Epoch [24], Batch [307/938], Loss: 0.21860679984092712\n",
      "Validation: Epoch [24], Batch [308/938], Loss: 0.19640570878982544\n",
      "Validation: Epoch [24], Batch [309/938], Loss: 0.35482776165008545\n",
      "Validation: Epoch [24], Batch [310/938], Loss: 0.29141056537628174\n",
      "Validation: Epoch [24], Batch [311/938], Loss: 0.257427841424942\n",
      "Validation: Epoch [24], Batch [312/938], Loss: 0.30713772773742676\n",
      "Validation: Epoch [24], Batch [313/938], Loss: 0.4050673544406891\n",
      "Validation: Epoch [24], Batch [314/938], Loss: 0.3817020654678345\n",
      "Validation: Epoch [24], Batch [315/938], Loss: 0.4101106524467468\n",
      "Validation: Epoch [24], Batch [316/938], Loss: 0.3438724875450134\n",
      "Validation: Epoch [24], Batch [317/938], Loss: 0.34145909547805786\n",
      "Validation: Epoch [24], Batch [318/938], Loss: 0.28955134749412537\n",
      "Validation: Epoch [24], Batch [319/938], Loss: 0.3254016637802124\n",
      "Validation: Epoch [24], Batch [320/938], Loss: 0.41334986686706543\n",
      "Validation: Epoch [24], Batch [321/938], Loss: 0.38209593296051025\n",
      "Validation: Epoch [24], Batch [322/938], Loss: 0.4026432931423187\n",
      "Validation: Epoch [24], Batch [323/938], Loss: 0.2751743495464325\n",
      "Validation: Epoch [24], Batch [324/938], Loss: 0.45750898122787476\n",
      "Validation: Epoch [24], Batch [325/938], Loss: 0.26899150013923645\n",
      "Validation: Epoch [24], Batch [326/938], Loss: 0.3618466258049011\n",
      "Validation: Epoch [24], Batch [327/938], Loss: 0.20872549712657928\n",
      "Validation: Epoch [24], Batch [328/938], Loss: 0.5468318462371826\n",
      "Validation: Epoch [24], Batch [329/938], Loss: 0.24669328331947327\n",
      "Validation: Epoch [24], Batch [330/938], Loss: 0.2990496754646301\n",
      "Validation: Epoch [24], Batch [331/938], Loss: 0.34761011600494385\n",
      "Validation: Epoch [24], Batch [332/938], Loss: 0.2796085476875305\n",
      "Validation: Epoch [24], Batch [333/938], Loss: 0.2261231541633606\n",
      "Validation: Epoch [24], Batch [334/938], Loss: 0.34368327260017395\n",
      "Validation: Epoch [24], Batch [335/938], Loss: 0.3419567346572876\n",
      "Validation: Epoch [24], Batch [336/938], Loss: 0.3423476219177246\n",
      "Validation: Epoch [24], Batch [337/938], Loss: 0.3825775980949402\n",
      "Validation: Epoch [24], Batch [338/938], Loss: 0.4812021851539612\n",
      "Validation: Epoch [24], Batch [339/938], Loss: 0.3005576729774475\n",
      "Validation: Epoch [24], Batch [340/938], Loss: 0.40101704001426697\n",
      "Validation: Epoch [24], Batch [341/938], Loss: 0.2812609374523163\n",
      "Validation: Epoch [24], Batch [342/938], Loss: 0.40213724970817566\n",
      "Validation: Epoch [24], Batch [343/938], Loss: 0.2064029574394226\n",
      "Validation: Epoch [24], Batch [344/938], Loss: 0.29784995317459106\n",
      "Validation: Epoch [24], Batch [345/938], Loss: 0.2696804106235504\n",
      "Validation: Epoch [24], Batch [346/938], Loss: 0.409670889377594\n",
      "Validation: Epoch [24], Batch [347/938], Loss: 0.2605343163013458\n",
      "Validation: Epoch [24], Batch [348/938], Loss: 0.3841676115989685\n",
      "Validation: Epoch [24], Batch [349/938], Loss: 0.41047996282577515\n",
      "Validation: Epoch [24], Batch [350/938], Loss: 0.35999730229377747\n",
      "Validation: Epoch [24], Batch [351/938], Loss: 0.5702921152114868\n",
      "Validation: Epoch [24], Batch [352/938], Loss: 0.40630388259887695\n",
      "Validation: Epoch [24], Batch [353/938], Loss: 0.19418232142925262\n",
      "Validation: Epoch [24], Batch [354/938], Loss: 0.2788679003715515\n",
      "Validation: Epoch [24], Batch [355/938], Loss: 0.41988712549209595\n",
      "Validation: Epoch [24], Batch [356/938], Loss: 0.5263628363609314\n",
      "Validation: Epoch [24], Batch [357/938], Loss: 0.38437139987945557\n",
      "Validation: Epoch [24], Batch [358/938], Loss: 0.30779701471328735\n",
      "Validation: Epoch [24], Batch [359/938], Loss: 0.13275347650051117\n",
      "Validation: Epoch [24], Batch [360/938], Loss: 0.5565304756164551\n",
      "Validation: Epoch [24], Batch [361/938], Loss: 0.22935986518859863\n",
      "Validation: Epoch [24], Batch [362/938], Loss: 0.3133789896965027\n",
      "Validation: Epoch [24], Batch [363/938], Loss: 0.22993436455726624\n",
      "Validation: Epoch [24], Batch [364/938], Loss: 0.4429425001144409\n",
      "Validation: Epoch [24], Batch [365/938], Loss: 0.32791417837142944\n",
      "Validation: Epoch [24], Batch [366/938], Loss: 0.14973998069763184\n",
      "Validation: Epoch [24], Batch [367/938], Loss: 0.30336666107177734\n",
      "Validation: Epoch [24], Batch [368/938], Loss: 0.38462120294570923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [369/938], Loss: 0.26157671213150024\n",
      "Validation: Epoch [24], Batch [370/938], Loss: 0.27782589197158813\n",
      "Validation: Epoch [24], Batch [371/938], Loss: 0.39947396516799927\n",
      "Validation: Epoch [24], Batch [372/938], Loss: 0.32285675406455994\n",
      "Validation: Epoch [24], Batch [373/938], Loss: 0.5181463360786438\n",
      "Validation: Epoch [24], Batch [374/938], Loss: 0.3033722937107086\n",
      "Validation: Epoch [24], Batch [375/938], Loss: 0.5450171828269958\n",
      "Validation: Epoch [24], Batch [376/938], Loss: 0.4103347063064575\n",
      "Validation: Epoch [24], Batch [377/938], Loss: 0.43793267011642456\n",
      "Validation: Epoch [24], Batch [378/938], Loss: 0.3793827295303345\n",
      "Validation: Epoch [24], Batch [379/938], Loss: 0.4459165930747986\n",
      "Validation: Epoch [24], Batch [380/938], Loss: 0.4961026906967163\n",
      "Validation: Epoch [24], Batch [381/938], Loss: 0.4866948425769806\n",
      "Validation: Epoch [24], Batch [382/938], Loss: 0.39589032530784607\n",
      "Validation: Epoch [24], Batch [383/938], Loss: 0.4025689363479614\n",
      "Validation: Epoch [24], Batch [384/938], Loss: 0.4115491509437561\n",
      "Validation: Epoch [24], Batch [385/938], Loss: 0.4462960958480835\n",
      "Validation: Epoch [24], Batch [386/938], Loss: 0.34572917222976685\n",
      "Validation: Epoch [24], Batch [387/938], Loss: 0.24425336718559265\n",
      "Validation: Epoch [24], Batch [388/938], Loss: 0.5758352875709534\n",
      "Validation: Epoch [24], Batch [389/938], Loss: 0.2532406151294708\n",
      "Validation: Epoch [24], Batch [390/938], Loss: 0.23364537954330444\n",
      "Validation: Epoch [24], Batch [391/938], Loss: 0.35805460810661316\n",
      "Validation: Epoch [24], Batch [392/938], Loss: 0.3425695598125458\n",
      "Validation: Epoch [24], Batch [393/938], Loss: 0.29927217960357666\n",
      "Validation: Epoch [24], Batch [394/938], Loss: 0.47271835803985596\n",
      "Validation: Epoch [24], Batch [395/938], Loss: 0.32976987957954407\n",
      "Validation: Epoch [24], Batch [396/938], Loss: 0.37031930685043335\n",
      "Validation: Epoch [24], Batch [397/938], Loss: 0.20761224627494812\n",
      "Validation: Epoch [24], Batch [398/938], Loss: 0.37458986043930054\n",
      "Validation: Epoch [24], Batch [399/938], Loss: 0.4640078842639923\n",
      "Validation: Epoch [24], Batch [400/938], Loss: 0.28568530082702637\n",
      "Validation: Epoch [24], Batch [401/938], Loss: 0.39966511726379395\n",
      "Validation: Epoch [24], Batch [402/938], Loss: 0.3716907799243927\n",
      "Validation: Epoch [24], Batch [403/938], Loss: 0.2698117792606354\n",
      "Validation: Epoch [24], Batch [404/938], Loss: 0.47273293137550354\n",
      "Validation: Epoch [24], Batch [405/938], Loss: 0.35310783982276917\n",
      "Validation: Epoch [24], Batch [406/938], Loss: 0.3393253684043884\n",
      "Validation: Epoch [24], Batch [407/938], Loss: 0.32607412338256836\n",
      "Validation: Epoch [24], Batch [408/938], Loss: 0.6005721688270569\n",
      "Validation: Epoch [24], Batch [409/938], Loss: 0.43586236238479614\n",
      "Validation: Epoch [24], Batch [410/938], Loss: 0.3488219678401947\n",
      "Validation: Epoch [24], Batch [411/938], Loss: 0.35497885942459106\n",
      "Validation: Epoch [24], Batch [412/938], Loss: 0.6054559350013733\n",
      "Validation: Epoch [24], Batch [413/938], Loss: 0.3945409655570984\n",
      "Validation: Epoch [24], Batch [414/938], Loss: 0.3805898427963257\n",
      "Validation: Epoch [24], Batch [415/938], Loss: 0.4713653326034546\n",
      "Validation: Epoch [24], Batch [416/938], Loss: 0.3504236936569214\n",
      "Validation: Epoch [24], Batch [417/938], Loss: 0.3103870451450348\n",
      "Validation: Epoch [24], Batch [418/938], Loss: 0.4222700595855713\n",
      "Validation: Epoch [24], Batch [419/938], Loss: 0.48842522501945496\n",
      "Validation: Epoch [24], Batch [420/938], Loss: 0.32043904066085815\n",
      "Validation: Epoch [24], Batch [421/938], Loss: 0.3636402487754822\n",
      "Validation: Epoch [24], Batch [422/938], Loss: 0.3261239528656006\n",
      "Validation: Epoch [24], Batch [423/938], Loss: 0.3944902718067169\n",
      "Validation: Epoch [24], Batch [424/938], Loss: 0.33115342259407043\n",
      "Validation: Epoch [24], Batch [425/938], Loss: 0.19433441758155823\n",
      "Validation: Epoch [24], Batch [426/938], Loss: 0.4180174171924591\n",
      "Validation: Epoch [24], Batch [427/938], Loss: 0.14842984080314636\n",
      "Validation: Epoch [24], Batch [428/938], Loss: 0.5418703556060791\n",
      "Validation: Epoch [24], Batch [429/938], Loss: 0.4547940790653229\n",
      "Validation: Epoch [24], Batch [430/938], Loss: 0.28987520933151245\n",
      "Validation: Epoch [24], Batch [431/938], Loss: 0.34012526273727417\n",
      "Validation: Epoch [24], Batch [432/938], Loss: 0.22503162920475006\n",
      "Validation: Epoch [24], Batch [433/938], Loss: 0.24080124497413635\n",
      "Validation: Epoch [24], Batch [434/938], Loss: 0.35287904739379883\n",
      "Validation: Epoch [24], Batch [435/938], Loss: 0.6242110729217529\n",
      "Validation: Epoch [24], Batch [436/938], Loss: 0.46788281202316284\n",
      "Validation: Epoch [24], Batch [437/938], Loss: 0.27412891387939453\n",
      "Validation: Epoch [24], Batch [438/938], Loss: 0.444561630487442\n",
      "Validation: Epoch [24], Batch [439/938], Loss: 0.2739466726779938\n",
      "Validation: Epoch [24], Batch [440/938], Loss: 0.3611830472946167\n",
      "Validation: Epoch [24], Batch [441/938], Loss: 0.4369993209838867\n",
      "Validation: Epoch [24], Batch [442/938], Loss: 0.37100163102149963\n",
      "Validation: Epoch [24], Batch [443/938], Loss: 0.4125020503997803\n",
      "Validation: Epoch [24], Batch [444/938], Loss: 0.32565218210220337\n",
      "Validation: Epoch [24], Batch [445/938], Loss: 0.4223584532737732\n",
      "Validation: Epoch [24], Batch [446/938], Loss: 0.33800971508026123\n",
      "Validation: Epoch [24], Batch [447/938], Loss: 0.3204573690891266\n",
      "Validation: Epoch [24], Batch [448/938], Loss: 0.38500505685806274\n",
      "Validation: Epoch [24], Batch [449/938], Loss: 0.3066166043281555\n",
      "Validation: Epoch [24], Batch [450/938], Loss: 0.459068238735199\n",
      "Validation: Epoch [24], Batch [451/938], Loss: 0.44711440801620483\n",
      "Validation: Epoch [24], Batch [452/938], Loss: 0.21873372793197632\n",
      "Validation: Epoch [24], Batch [453/938], Loss: 0.2839900553226471\n",
      "Validation: Epoch [24], Batch [454/938], Loss: 0.3680638074874878\n",
      "Validation: Epoch [24], Batch [455/938], Loss: 0.4424159526824951\n",
      "Validation: Epoch [24], Batch [456/938], Loss: 0.2681945264339447\n",
      "Validation: Epoch [24], Batch [457/938], Loss: 0.2428656667470932\n",
      "Validation: Epoch [24], Batch [458/938], Loss: 0.5549378395080566\n",
      "Validation: Epoch [24], Batch [459/938], Loss: 0.42439717054367065\n",
      "Validation: Epoch [24], Batch [460/938], Loss: 0.3925970196723938\n",
      "Validation: Epoch [24], Batch [461/938], Loss: 0.3436669111251831\n",
      "Validation: Epoch [24], Batch [462/938], Loss: 0.5248655080795288\n",
      "Validation: Epoch [24], Batch [463/938], Loss: 0.4202064573764801\n",
      "Validation: Epoch [24], Batch [464/938], Loss: 0.2838938236236572\n",
      "Validation: Epoch [24], Batch [465/938], Loss: 0.27490511536598206\n",
      "Validation: Epoch [24], Batch [466/938], Loss: 0.3102707862854004\n",
      "Validation: Epoch [24], Batch [467/938], Loss: 0.45228227972984314\n",
      "Validation: Epoch [24], Batch [468/938], Loss: 0.24780024588108063\n",
      "Validation: Epoch [24], Batch [469/938], Loss: 0.5779529809951782\n",
      "Validation: Epoch [24], Batch [470/938], Loss: 0.3057560920715332\n",
      "Validation: Epoch [24], Batch [471/938], Loss: 0.40832769870758057\n",
      "Validation: Epoch [24], Batch [472/938], Loss: 0.3132855296134949\n",
      "Validation: Epoch [24], Batch [473/938], Loss: 0.6427827477455139\n",
      "Validation: Epoch [24], Batch [474/938], Loss: 0.31848007440567017\n",
      "Validation: Epoch [24], Batch [475/938], Loss: 0.3758072257041931\n",
      "Validation: Epoch [24], Batch [476/938], Loss: 0.43376946449279785\n",
      "Validation: Epoch [24], Batch [477/938], Loss: 0.27506256103515625\n",
      "Validation: Epoch [24], Batch [478/938], Loss: 0.38743138313293457\n",
      "Validation: Epoch [24], Batch [479/938], Loss: 0.21200299263000488\n",
      "Validation: Epoch [24], Batch [480/938], Loss: 0.4121500849723816\n",
      "Validation: Epoch [24], Batch [481/938], Loss: 0.29380592703819275\n",
      "Validation: Epoch [24], Batch [482/938], Loss: 0.4196840524673462\n",
      "Validation: Epoch [24], Batch [483/938], Loss: 0.4019511342048645\n",
      "Validation: Epoch [24], Batch [484/938], Loss: 0.2385140061378479\n",
      "Validation: Epoch [24], Batch [485/938], Loss: 0.2839648425579071\n",
      "Validation: Epoch [24], Batch [486/938], Loss: 0.45626968145370483\n",
      "Validation: Epoch [24], Batch [487/938], Loss: 0.287352979183197\n",
      "Validation: Epoch [24], Batch [488/938], Loss: 0.35320836305618286\n",
      "Validation: Epoch [24], Batch [489/938], Loss: 0.3848143517971039\n",
      "Validation: Epoch [24], Batch [490/938], Loss: 0.33186841011047363\n",
      "Validation: Epoch [24], Batch [491/938], Loss: 0.2964620888233185\n",
      "Validation: Epoch [24], Batch [492/938], Loss: 0.21984535455703735\n",
      "Validation: Epoch [24], Batch [493/938], Loss: 0.27777227759361267\n",
      "Validation: Epoch [24], Batch [494/938], Loss: 0.38976389169692993\n",
      "Validation: Epoch [24], Batch [495/938], Loss: 0.2562386393547058\n",
      "Validation: Epoch [24], Batch [496/938], Loss: 0.26412397623062134\n",
      "Validation: Epoch [24], Batch [497/938], Loss: 0.39137938618659973\n",
      "Validation: Epoch [24], Batch [498/938], Loss: 0.41163426637649536\n",
      "Validation: Epoch [24], Batch [499/938], Loss: 0.6678462028503418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [500/938], Loss: 0.4872264862060547\n",
      "Validation: Epoch [24], Batch [501/938], Loss: 0.310945063829422\n",
      "Validation: Epoch [24], Batch [502/938], Loss: 0.3202512860298157\n",
      "Validation: Epoch [24], Batch [503/938], Loss: 0.35364294052124023\n",
      "Validation: Epoch [24], Batch [504/938], Loss: 0.299714595079422\n",
      "Validation: Epoch [24], Batch [505/938], Loss: 0.46364954113960266\n",
      "Validation: Epoch [24], Batch [506/938], Loss: 0.38216137886047363\n",
      "Validation: Epoch [24], Batch [507/938], Loss: 0.400923490524292\n",
      "Validation: Epoch [24], Batch [508/938], Loss: 0.29796433448791504\n",
      "Validation: Epoch [24], Batch [509/938], Loss: 0.5833168029785156\n",
      "Validation: Epoch [24], Batch [510/938], Loss: 0.6195743680000305\n",
      "Validation: Epoch [24], Batch [511/938], Loss: 0.5343124866485596\n",
      "Validation: Epoch [24], Batch [512/938], Loss: 0.3640746772289276\n",
      "Validation: Epoch [24], Batch [513/938], Loss: 0.5189875364303589\n",
      "Validation: Epoch [24], Batch [514/938], Loss: 0.27177757024765015\n",
      "Validation: Epoch [24], Batch [515/938], Loss: 0.4965149462223053\n",
      "Validation: Epoch [24], Batch [516/938], Loss: 0.5168787837028503\n",
      "Validation: Epoch [24], Batch [517/938], Loss: 0.3763093948364258\n",
      "Validation: Epoch [24], Batch [518/938], Loss: 0.4780161678791046\n",
      "Validation: Epoch [24], Batch [519/938], Loss: 0.3630743622779846\n",
      "Validation: Epoch [24], Batch [520/938], Loss: 0.42691367864608765\n",
      "Validation: Epoch [24], Batch [521/938], Loss: 0.37250950932502747\n",
      "Validation: Epoch [24], Batch [522/938], Loss: 0.399788498878479\n",
      "Validation: Epoch [24], Batch [523/938], Loss: 0.2147354632616043\n",
      "Validation: Epoch [24], Batch [524/938], Loss: 0.3023068308830261\n",
      "Validation: Epoch [24], Batch [525/938], Loss: 0.5016261339187622\n",
      "Validation: Epoch [24], Batch [526/938], Loss: 0.41814449429512024\n",
      "Validation: Epoch [24], Batch [527/938], Loss: 0.23694118857383728\n",
      "Validation: Epoch [24], Batch [528/938], Loss: 0.35359159111976624\n",
      "Validation: Epoch [24], Batch [529/938], Loss: 0.465120792388916\n",
      "Validation: Epoch [24], Batch [530/938], Loss: 0.2122502475976944\n",
      "Validation: Epoch [24], Batch [531/938], Loss: 0.4863300621509552\n",
      "Validation: Epoch [24], Batch [532/938], Loss: 0.33042699098587036\n",
      "Validation: Epoch [24], Batch [533/938], Loss: 0.36108192801475525\n",
      "Validation: Epoch [24], Batch [534/938], Loss: 0.17599323391914368\n",
      "Validation: Epoch [24], Batch [535/938], Loss: 0.3618372082710266\n",
      "Validation: Epoch [24], Batch [536/938], Loss: 0.1659131646156311\n",
      "Validation: Epoch [24], Batch [537/938], Loss: 0.5246375203132629\n",
      "Validation: Epoch [24], Batch [538/938], Loss: 0.5414392948150635\n",
      "Validation: Epoch [24], Batch [539/938], Loss: 0.3608492612838745\n",
      "Validation: Epoch [24], Batch [540/938], Loss: 0.3726109266281128\n",
      "Validation: Epoch [24], Batch [541/938], Loss: 0.26888757944107056\n",
      "Validation: Epoch [24], Batch [542/938], Loss: 0.25818586349487305\n",
      "Validation: Epoch [24], Batch [543/938], Loss: 0.23066295683383942\n",
      "Validation: Epoch [24], Batch [544/938], Loss: 0.4753517508506775\n",
      "Validation: Epoch [24], Batch [545/938], Loss: 0.4555838704109192\n",
      "Validation: Epoch [24], Batch [546/938], Loss: 0.3353823125362396\n",
      "Validation: Epoch [24], Batch [547/938], Loss: 0.41493427753448486\n",
      "Validation: Epoch [24], Batch [548/938], Loss: 0.22134731709957123\n",
      "Validation: Epoch [24], Batch [549/938], Loss: 0.4228821098804474\n",
      "Validation: Epoch [24], Batch [550/938], Loss: 0.3381970524787903\n",
      "Validation: Epoch [24], Batch [551/938], Loss: 0.26257771253585815\n",
      "Validation: Epoch [24], Batch [552/938], Loss: 0.3595869541168213\n",
      "Validation: Epoch [24], Batch [553/938], Loss: 0.3959747850894928\n",
      "Validation: Epoch [24], Batch [554/938], Loss: 0.32517537474632263\n",
      "Validation: Epoch [24], Batch [555/938], Loss: 0.36458057165145874\n",
      "Validation: Epoch [24], Batch [556/938], Loss: 0.4013591706752777\n",
      "Validation: Epoch [24], Batch [557/938], Loss: 0.32242804765701294\n",
      "Validation: Epoch [24], Batch [558/938], Loss: 0.40446457266807556\n",
      "Validation: Epoch [24], Batch [559/938], Loss: 0.30932241678237915\n",
      "Validation: Epoch [24], Batch [560/938], Loss: 0.33907264471054077\n",
      "Validation: Epoch [24], Batch [561/938], Loss: 0.35479915142059326\n",
      "Validation: Epoch [24], Batch [562/938], Loss: 0.3350880444049835\n",
      "Validation: Epoch [24], Batch [563/938], Loss: 0.4750348627567291\n",
      "Validation: Epoch [24], Batch [564/938], Loss: 0.41780561208724976\n",
      "Validation: Epoch [24], Batch [565/938], Loss: 0.4320315420627594\n",
      "Validation: Epoch [24], Batch [566/938], Loss: 0.3543057441711426\n",
      "Validation: Epoch [24], Batch [567/938], Loss: 0.3362334966659546\n",
      "Validation: Epoch [24], Batch [568/938], Loss: 0.27034687995910645\n",
      "Validation: Epoch [24], Batch [569/938], Loss: 0.3026729226112366\n",
      "Validation: Epoch [24], Batch [570/938], Loss: 0.47805315256118774\n",
      "Validation: Epoch [24], Batch [571/938], Loss: 0.2719876170158386\n",
      "Validation: Epoch [24], Batch [572/938], Loss: 0.3228139281272888\n",
      "Validation: Epoch [24], Batch [573/938], Loss: 0.43449750542640686\n",
      "Validation: Epoch [24], Batch [574/938], Loss: 0.4841770827770233\n",
      "Validation: Epoch [24], Batch [575/938], Loss: 0.2987735867500305\n",
      "Validation: Epoch [24], Batch [576/938], Loss: 0.3864569067955017\n",
      "Validation: Epoch [24], Batch [577/938], Loss: 0.17977897822856903\n",
      "Validation: Epoch [24], Batch [578/938], Loss: 0.4629562497138977\n",
      "Validation: Epoch [24], Batch [579/938], Loss: 0.3782958984375\n",
      "Validation: Epoch [24], Batch [580/938], Loss: 0.36344462633132935\n",
      "Validation: Epoch [24], Batch [581/938], Loss: 0.32889336347579956\n",
      "Validation: Epoch [24], Batch [582/938], Loss: 0.3403315544128418\n",
      "Validation: Epoch [24], Batch [583/938], Loss: 0.3116419017314911\n",
      "Validation: Epoch [24], Batch [584/938], Loss: 0.3897266983985901\n",
      "Validation: Epoch [24], Batch [585/938], Loss: 0.49346718192100525\n",
      "Validation: Epoch [24], Batch [586/938], Loss: 0.36870884895324707\n",
      "Validation: Epoch [24], Batch [587/938], Loss: 0.40338334441185\n",
      "Validation: Epoch [24], Batch [588/938], Loss: 0.3750717341899872\n",
      "Validation: Epoch [24], Batch [589/938], Loss: 0.40894415974617004\n",
      "Validation: Epoch [24], Batch [590/938], Loss: 0.25464093685150146\n",
      "Validation: Epoch [24], Batch [591/938], Loss: 0.483866423368454\n",
      "Validation: Epoch [24], Batch [592/938], Loss: 0.2517988681793213\n",
      "Validation: Epoch [24], Batch [593/938], Loss: 0.616462230682373\n",
      "Validation: Epoch [24], Batch [594/938], Loss: 0.3149494528770447\n",
      "Validation: Epoch [24], Batch [595/938], Loss: 0.3729363679885864\n",
      "Validation: Epoch [24], Batch [596/938], Loss: 0.48647964000701904\n",
      "Validation: Epoch [24], Batch [597/938], Loss: 0.43618083000183105\n",
      "Validation: Epoch [24], Batch [598/938], Loss: 0.3720517158508301\n",
      "Validation: Epoch [24], Batch [599/938], Loss: 0.5091478824615479\n",
      "Validation: Epoch [24], Batch [600/938], Loss: 0.39656272530555725\n",
      "Validation: Epoch [24], Batch [601/938], Loss: 0.30390533804893494\n",
      "Validation: Epoch [24], Batch [602/938], Loss: 0.2967824637889862\n",
      "Validation: Epoch [24], Batch [603/938], Loss: 0.4654046893119812\n",
      "Validation: Epoch [24], Batch [604/938], Loss: 0.33653688430786133\n",
      "Validation: Epoch [24], Batch [605/938], Loss: 0.3632407784461975\n",
      "Validation: Epoch [24], Batch [606/938], Loss: 0.3830282390117645\n",
      "Validation: Epoch [24], Batch [607/938], Loss: 0.4289960563182831\n",
      "Validation: Epoch [24], Batch [608/938], Loss: 0.5064989328384399\n",
      "Validation: Epoch [24], Batch [609/938], Loss: 0.4639696180820465\n",
      "Validation: Epoch [24], Batch [610/938], Loss: 0.5383075475692749\n",
      "Validation: Epoch [24], Batch [611/938], Loss: 0.381618857383728\n",
      "Validation: Epoch [24], Batch [612/938], Loss: 0.28440025448799133\n",
      "Validation: Epoch [24], Batch [613/938], Loss: 0.2378312647342682\n",
      "Validation: Epoch [24], Batch [614/938], Loss: 0.3894418179988861\n",
      "Validation: Epoch [24], Batch [615/938], Loss: 0.31014442443847656\n",
      "Validation: Epoch [24], Batch [616/938], Loss: 0.2692987024784088\n",
      "Validation: Epoch [24], Batch [617/938], Loss: 0.2775440216064453\n",
      "Validation: Epoch [24], Batch [618/938], Loss: 0.4966626763343811\n",
      "Validation: Epoch [24], Batch [619/938], Loss: 0.4061073660850525\n",
      "Validation: Epoch [24], Batch [620/938], Loss: 0.35273975133895874\n",
      "Validation: Epoch [24], Batch [621/938], Loss: 0.3731805980205536\n",
      "Validation: Epoch [24], Batch [622/938], Loss: 0.4069734215736389\n",
      "Validation: Epoch [24], Batch [623/938], Loss: 0.406686395406723\n",
      "Validation: Epoch [24], Batch [624/938], Loss: 0.4202878475189209\n",
      "Validation: Epoch [24], Batch [625/938], Loss: 0.31616246700286865\n",
      "Validation: Epoch [24], Batch [626/938], Loss: 0.21151143312454224\n",
      "Validation: Epoch [24], Batch [627/938], Loss: 0.41224154829978943\n",
      "Validation: Epoch [24], Batch [628/938], Loss: 0.3414977490901947\n",
      "Validation: Epoch [24], Batch [629/938], Loss: 0.3450932502746582\n",
      "Validation: Epoch [24], Batch [630/938], Loss: 0.4019072949886322\n",
      "Validation: Epoch [24], Batch [631/938], Loss: 0.4135172963142395\n",
      "Validation: Epoch [24], Batch [632/938], Loss: 0.36989882588386536\n",
      "Validation: Epoch [24], Batch [633/938], Loss: 0.3834328055381775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [634/938], Loss: 0.34243127703666687\n",
      "Validation: Epoch [24], Batch [635/938], Loss: 0.19553929567337036\n",
      "Validation: Epoch [24], Batch [636/938], Loss: 0.42745572328567505\n",
      "Validation: Epoch [24], Batch [637/938], Loss: 0.45595335960388184\n",
      "Validation: Epoch [24], Batch [638/938], Loss: 0.4135105311870575\n",
      "Validation: Epoch [24], Batch [639/938], Loss: 0.31608858704566956\n",
      "Validation: Epoch [24], Batch [640/938], Loss: 0.3152346909046173\n",
      "Validation: Epoch [24], Batch [641/938], Loss: 0.4233495593070984\n",
      "Validation: Epoch [24], Batch [642/938], Loss: 0.2530232071876526\n",
      "Validation: Epoch [24], Batch [643/938], Loss: 0.3769080638885498\n",
      "Validation: Epoch [24], Batch [644/938], Loss: 0.402574360370636\n",
      "Validation: Epoch [24], Batch [645/938], Loss: 0.43433070182800293\n",
      "Validation: Epoch [24], Batch [646/938], Loss: 0.31589221954345703\n",
      "Validation: Epoch [24], Batch [647/938], Loss: 0.1489604413509369\n",
      "Validation: Epoch [24], Batch [648/938], Loss: 0.3236836791038513\n",
      "Validation: Epoch [24], Batch [649/938], Loss: 0.2286856770515442\n",
      "Validation: Epoch [24], Batch [650/938], Loss: 0.27723926305770874\n",
      "Validation: Epoch [24], Batch [651/938], Loss: 0.27516454458236694\n",
      "Validation: Epoch [24], Batch [652/938], Loss: 0.4562870264053345\n",
      "Validation: Epoch [24], Batch [653/938], Loss: 0.30677706003189087\n",
      "Validation: Epoch [24], Batch [654/938], Loss: 0.3166119456291199\n",
      "Validation: Epoch [24], Batch [655/938], Loss: 0.4172670543193817\n",
      "Validation: Epoch [24], Batch [656/938], Loss: 0.42314666509628296\n",
      "Validation: Epoch [24], Batch [657/938], Loss: 0.2590807378292084\n",
      "Validation: Epoch [24], Batch [658/938], Loss: 0.24061375856399536\n",
      "Validation: Epoch [24], Batch [659/938], Loss: 0.3479957580566406\n",
      "Validation: Epoch [24], Batch [660/938], Loss: 0.4968334436416626\n",
      "Validation: Epoch [24], Batch [661/938], Loss: 0.3497871160507202\n",
      "Validation: Epoch [24], Batch [662/938], Loss: 0.40622177720069885\n",
      "Validation: Epoch [24], Batch [663/938], Loss: 0.3892812132835388\n",
      "Validation: Epoch [24], Batch [664/938], Loss: 0.40660858154296875\n",
      "Validation: Epoch [24], Batch [665/938], Loss: 0.4797312319278717\n",
      "Validation: Epoch [24], Batch [666/938], Loss: 0.33638983964920044\n",
      "Validation: Epoch [24], Batch [667/938], Loss: 0.2705339789390564\n",
      "Validation: Epoch [24], Batch [668/938], Loss: 0.24330508708953857\n",
      "Validation: Epoch [24], Batch [669/938], Loss: 0.41140419244766235\n",
      "Validation: Epoch [24], Batch [670/938], Loss: 0.3020814061164856\n",
      "Validation: Epoch [24], Batch [671/938], Loss: 0.46140748262405396\n",
      "Validation: Epoch [24], Batch [672/938], Loss: 0.40031906962394714\n",
      "Validation: Epoch [24], Batch [673/938], Loss: 0.3097259998321533\n",
      "Validation: Epoch [24], Batch [674/938], Loss: 0.2588636577129364\n",
      "Validation: Epoch [24], Batch [675/938], Loss: 0.32033616304397583\n",
      "Validation: Epoch [24], Batch [676/938], Loss: 0.42400842905044556\n",
      "Validation: Epoch [24], Batch [677/938], Loss: 0.449287086725235\n",
      "Validation: Epoch [24], Batch [678/938], Loss: 0.2960776090621948\n",
      "Validation: Epoch [24], Batch [679/938], Loss: 0.24597446620464325\n",
      "Validation: Epoch [24], Batch [680/938], Loss: 0.3710878789424896\n",
      "Validation: Epoch [24], Batch [681/938], Loss: 0.32862186431884766\n",
      "Validation: Epoch [24], Batch [682/938], Loss: 0.3378799259662628\n",
      "Validation: Epoch [24], Batch [683/938], Loss: 0.1661883145570755\n",
      "Validation: Epoch [24], Batch [684/938], Loss: 0.3570539355278015\n",
      "Validation: Epoch [24], Batch [685/938], Loss: 0.4292079508304596\n",
      "Validation: Epoch [24], Batch [686/938], Loss: 0.2796659767627716\n",
      "Validation: Epoch [24], Batch [687/938], Loss: 0.3305961489677429\n",
      "Validation: Epoch [24], Batch [688/938], Loss: 0.21820423007011414\n",
      "Validation: Epoch [24], Batch [689/938], Loss: 0.5649585127830505\n",
      "Validation: Epoch [24], Batch [690/938], Loss: 0.3174084722995758\n",
      "Validation: Epoch [24], Batch [691/938], Loss: 0.389060914516449\n",
      "Validation: Epoch [24], Batch [692/938], Loss: 0.2917438745498657\n",
      "Validation: Epoch [24], Batch [693/938], Loss: 0.26263970136642456\n",
      "Validation: Epoch [24], Batch [694/938], Loss: 0.3270277976989746\n",
      "Validation: Epoch [24], Batch [695/938], Loss: 0.3961127996444702\n",
      "Validation: Epoch [24], Batch [696/938], Loss: 0.49784573912620544\n",
      "Validation: Epoch [24], Batch [697/938], Loss: 0.5247103571891785\n",
      "Validation: Epoch [24], Batch [698/938], Loss: 0.2947450280189514\n",
      "Validation: Epoch [24], Batch [699/938], Loss: 0.49613386392593384\n",
      "Validation: Epoch [24], Batch [700/938], Loss: 0.373282253742218\n",
      "Validation: Epoch [24], Batch [701/938], Loss: 0.4441680610179901\n",
      "Validation: Epoch [24], Batch [702/938], Loss: 0.29490751028060913\n",
      "Validation: Epoch [24], Batch [703/938], Loss: 0.3467620611190796\n",
      "Validation: Epoch [24], Batch [704/938], Loss: 0.44250524044036865\n",
      "Validation: Epoch [24], Batch [705/938], Loss: 0.2894180417060852\n",
      "Validation: Epoch [24], Batch [706/938], Loss: 0.30941182374954224\n",
      "Validation: Epoch [24], Batch [707/938], Loss: 0.2747838795185089\n",
      "Validation: Epoch [24], Batch [708/938], Loss: 0.5135868191719055\n",
      "Validation: Epoch [24], Batch [709/938], Loss: 0.4759184420108795\n",
      "Validation: Epoch [24], Batch [710/938], Loss: 0.3932136595249176\n",
      "Validation: Epoch [24], Batch [711/938], Loss: 0.30659008026123047\n",
      "Validation: Epoch [24], Batch [712/938], Loss: 0.39499738812446594\n",
      "Validation: Epoch [24], Batch [713/938], Loss: 0.45983773469924927\n",
      "Validation: Epoch [24], Batch [714/938], Loss: 0.41167616844177246\n",
      "Validation: Epoch [24], Batch [715/938], Loss: 0.3971143066883087\n",
      "Validation: Epoch [24], Batch [716/938], Loss: 0.31845757365226746\n",
      "Validation: Epoch [24], Batch [717/938], Loss: 0.3867311477661133\n",
      "Validation: Epoch [24], Batch [718/938], Loss: 0.30218514800071716\n",
      "Validation: Epoch [24], Batch [719/938], Loss: 0.3510556221008301\n",
      "Validation: Epoch [24], Batch [720/938], Loss: 0.4639730453491211\n",
      "Validation: Epoch [24], Batch [721/938], Loss: 0.3337029218673706\n",
      "Validation: Epoch [24], Batch [722/938], Loss: 0.3760221302509308\n",
      "Validation: Epoch [24], Batch [723/938], Loss: 0.3563295006752014\n",
      "Validation: Epoch [24], Batch [724/938], Loss: 0.4360610842704773\n",
      "Validation: Epoch [24], Batch [725/938], Loss: 0.2591874897480011\n",
      "Validation: Epoch [24], Batch [726/938], Loss: 0.29245322942733765\n",
      "Validation: Epoch [24], Batch [727/938], Loss: 0.2991616129875183\n",
      "Validation: Epoch [24], Batch [728/938], Loss: 0.3610408306121826\n",
      "Validation: Epoch [24], Batch [729/938], Loss: 0.4329392910003662\n",
      "Validation: Epoch [24], Batch [730/938], Loss: 0.2400946319103241\n",
      "Validation: Epoch [24], Batch [731/938], Loss: 0.3403085768222809\n",
      "Validation: Epoch [24], Batch [732/938], Loss: 0.38076159358024597\n",
      "Validation: Epoch [24], Batch [733/938], Loss: 0.5839219689369202\n",
      "Validation: Epoch [24], Batch [734/938], Loss: 0.5008939504623413\n",
      "Validation: Epoch [24], Batch [735/938], Loss: 0.4419691264629364\n",
      "Validation: Epoch [24], Batch [736/938], Loss: 0.42242923378944397\n",
      "Validation: Epoch [24], Batch [737/938], Loss: 0.22720643877983093\n",
      "Validation: Epoch [24], Batch [738/938], Loss: 0.43358975648880005\n",
      "Validation: Epoch [24], Batch [739/938], Loss: 0.43159574270248413\n",
      "Validation: Epoch [24], Batch [740/938], Loss: 0.4175105094909668\n",
      "Validation: Epoch [24], Batch [741/938], Loss: 0.35252347588539124\n",
      "Validation: Epoch [24], Batch [742/938], Loss: 0.3637760579586029\n",
      "Validation: Epoch [24], Batch [743/938], Loss: 0.4441339671611786\n",
      "Validation: Epoch [24], Batch [744/938], Loss: 0.29824936389923096\n",
      "Validation: Epoch [24], Batch [745/938], Loss: 0.3999834656715393\n",
      "Validation: Epoch [24], Batch [746/938], Loss: 0.32408490777015686\n",
      "Validation: Epoch [24], Batch [747/938], Loss: 0.3400960862636566\n",
      "Validation: Epoch [24], Batch [748/938], Loss: 0.5278502702713013\n",
      "Validation: Epoch [24], Batch [749/938], Loss: 0.42028817534446716\n",
      "Validation: Epoch [24], Batch [750/938], Loss: 0.34949982166290283\n",
      "Validation: Epoch [24], Batch [751/938], Loss: 0.4050523340702057\n",
      "Validation: Epoch [24], Batch [752/938], Loss: 0.2916106581687927\n",
      "Validation: Epoch [24], Batch [753/938], Loss: 0.362292617559433\n",
      "Validation: Epoch [24], Batch [754/938], Loss: 0.2642607092857361\n",
      "Validation: Epoch [24], Batch [755/938], Loss: 0.3005809187889099\n",
      "Validation: Epoch [24], Batch [756/938], Loss: 0.2392035871744156\n",
      "Validation: Epoch [24], Batch [757/938], Loss: 0.5711084008216858\n",
      "Validation: Epoch [24], Batch [758/938], Loss: 0.5227382183074951\n",
      "Validation: Epoch [24], Batch [759/938], Loss: 0.18358729779720306\n",
      "Validation: Epoch [24], Batch [760/938], Loss: 0.3356642723083496\n",
      "Validation: Epoch [24], Batch [761/938], Loss: 0.4242265224456787\n",
      "Validation: Epoch [24], Batch [762/938], Loss: 0.34478434920310974\n",
      "Validation: Epoch [24], Batch [763/938], Loss: 0.35913535952568054\n",
      "Validation: Epoch [24], Batch [764/938], Loss: 0.23505966365337372\n",
      "Validation: Epoch [24], Batch [765/938], Loss: 0.447640061378479\n",
      "Validation: Epoch [24], Batch [766/938], Loss: 0.3934759497642517\n",
      "Validation: Epoch [24], Batch [767/938], Loss: 0.34468919038772583\n",
      "Validation: Epoch [24], Batch [768/938], Loss: 0.5258958339691162\n",
      "Validation: Epoch [24], Batch [769/938], Loss: 0.27388325333595276\n",
      "Validation: Epoch [24], Batch [770/938], Loss: 0.28292474150657654\n",
      "Validation: Epoch [24], Batch [771/938], Loss: 0.4256807565689087\n",
      "Validation: Epoch [24], Batch [772/938], Loss: 0.21776381134986877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [773/938], Loss: 0.2815276086330414\n",
      "Validation: Epoch [24], Batch [774/938], Loss: 0.2502294182777405\n",
      "Validation: Epoch [24], Batch [775/938], Loss: 0.4761385917663574\n",
      "Validation: Epoch [24], Batch [776/938], Loss: 0.4179295301437378\n",
      "Validation: Epoch [24], Batch [777/938], Loss: 0.3938119113445282\n",
      "Validation: Epoch [24], Batch [778/938], Loss: 0.3697640597820282\n",
      "Validation: Epoch [24], Batch [779/938], Loss: 0.32679808139801025\n",
      "Validation: Epoch [24], Batch [780/938], Loss: 0.35486114025115967\n",
      "Validation: Epoch [24], Batch [781/938], Loss: 0.46850547194480896\n",
      "Validation: Epoch [24], Batch [782/938], Loss: 0.32805192470550537\n",
      "Validation: Epoch [24], Batch [783/938], Loss: 0.2782195806503296\n",
      "Validation: Epoch [24], Batch [784/938], Loss: 0.4111863374710083\n",
      "Validation: Epoch [24], Batch [785/938], Loss: 0.5249059200286865\n",
      "Validation: Epoch [24], Batch [786/938], Loss: 0.46103745698928833\n",
      "Validation: Epoch [24], Batch [787/938], Loss: 0.3004263937473297\n",
      "Validation: Epoch [24], Batch [788/938], Loss: 0.25489258766174316\n",
      "Validation: Epoch [24], Batch [789/938], Loss: 0.40719902515411377\n",
      "Validation: Epoch [24], Batch [790/938], Loss: 0.38968420028686523\n",
      "Validation: Epoch [24], Batch [791/938], Loss: 0.328389048576355\n",
      "Validation: Epoch [24], Batch [792/938], Loss: 0.28831547498703003\n",
      "Validation: Epoch [24], Batch [793/938], Loss: 0.3606741428375244\n",
      "Validation: Epoch [24], Batch [794/938], Loss: 0.40874719619750977\n",
      "Validation: Epoch [24], Batch [795/938], Loss: 0.2755841016769409\n",
      "Validation: Epoch [24], Batch [796/938], Loss: 0.43565961718559265\n",
      "Validation: Epoch [24], Batch [797/938], Loss: 0.4826139211654663\n",
      "Validation: Epoch [24], Batch [798/938], Loss: 0.46933382749557495\n",
      "Validation: Epoch [24], Batch [799/938], Loss: 0.370860755443573\n",
      "Validation: Epoch [24], Batch [800/938], Loss: 0.33945125341415405\n",
      "Validation: Epoch [24], Batch [801/938], Loss: 0.2665644884109497\n",
      "Validation: Epoch [24], Batch [802/938], Loss: 0.3611740171909332\n",
      "Validation: Epoch [24], Batch [803/938], Loss: 0.5152535438537598\n",
      "Validation: Epoch [24], Batch [804/938], Loss: 0.45308107137680054\n",
      "Validation: Epoch [24], Batch [805/938], Loss: 0.346084862947464\n",
      "Validation: Epoch [24], Batch [806/938], Loss: 0.3911076486110687\n",
      "Validation: Epoch [24], Batch [807/938], Loss: 0.32610273361206055\n",
      "Validation: Epoch [24], Batch [808/938], Loss: 0.43139776587486267\n",
      "Validation: Epoch [24], Batch [809/938], Loss: 0.4426315724849701\n",
      "Validation: Epoch [24], Batch [810/938], Loss: 0.3466532230377197\n",
      "Validation: Epoch [24], Batch [811/938], Loss: 0.39821842312812805\n",
      "Validation: Epoch [24], Batch [812/938], Loss: 0.4865986108779907\n",
      "Validation: Epoch [24], Batch [813/938], Loss: 0.6486303806304932\n",
      "Validation: Epoch [24], Batch [814/938], Loss: 0.4162891209125519\n",
      "Validation: Epoch [24], Batch [815/938], Loss: 0.3749234974384308\n",
      "Validation: Epoch [24], Batch [816/938], Loss: 0.24517296254634857\n",
      "Validation: Epoch [24], Batch [817/938], Loss: 0.503119945526123\n",
      "Validation: Epoch [24], Batch [818/938], Loss: 0.463263601064682\n",
      "Validation: Epoch [24], Batch [819/938], Loss: 0.27467215061187744\n",
      "Validation: Epoch [24], Batch [820/938], Loss: 0.2807610332965851\n",
      "Validation: Epoch [24], Batch [821/938], Loss: 0.3091422915458679\n",
      "Validation: Epoch [24], Batch [822/938], Loss: 0.3179382085800171\n",
      "Validation: Epoch [24], Batch [823/938], Loss: 0.3302333652973175\n",
      "Validation: Epoch [24], Batch [824/938], Loss: 0.28690242767333984\n",
      "Validation: Epoch [24], Batch [825/938], Loss: 0.4935569167137146\n",
      "Validation: Epoch [24], Batch [826/938], Loss: 0.2867177128791809\n",
      "Validation: Epoch [24], Batch [827/938], Loss: 0.37344688177108765\n",
      "Validation: Epoch [24], Batch [828/938], Loss: 0.3718941807746887\n",
      "Validation: Epoch [24], Batch [829/938], Loss: 0.26692649722099304\n",
      "Validation: Epoch [24], Batch [830/938], Loss: 0.34630292654037476\n",
      "Validation: Epoch [24], Batch [831/938], Loss: 0.3755788803100586\n",
      "Validation: Epoch [24], Batch [832/938], Loss: 0.5900169014930725\n",
      "Validation: Epoch [24], Batch [833/938], Loss: 0.4544627070426941\n",
      "Validation: Epoch [24], Batch [834/938], Loss: 0.3304590582847595\n",
      "Validation: Epoch [24], Batch [835/938], Loss: 0.4153361916542053\n",
      "Validation: Epoch [24], Batch [836/938], Loss: 0.22282551229000092\n",
      "Validation: Epoch [24], Batch [837/938], Loss: 0.41027432680130005\n",
      "Validation: Epoch [24], Batch [838/938], Loss: 0.2921196222305298\n",
      "Validation: Epoch [24], Batch [839/938], Loss: 0.36290252208709717\n",
      "Validation: Epoch [24], Batch [840/938], Loss: 0.4431740939617157\n",
      "Validation: Epoch [24], Batch [841/938], Loss: 0.3324565589427948\n",
      "Validation: Epoch [24], Batch [842/938], Loss: 0.272373765707016\n",
      "Validation: Epoch [24], Batch [843/938], Loss: 0.24198535084724426\n",
      "Validation: Epoch [24], Batch [844/938], Loss: 0.4893777370452881\n",
      "Validation: Epoch [24], Batch [845/938], Loss: 0.2987768054008484\n",
      "Validation: Epoch [24], Batch [846/938], Loss: 0.3596583306789398\n",
      "Validation: Epoch [24], Batch [847/938], Loss: 0.27106374502182007\n",
      "Validation: Epoch [24], Batch [848/938], Loss: 0.4933030903339386\n",
      "Validation: Epoch [24], Batch [849/938], Loss: 0.40015214681625366\n",
      "Validation: Epoch [24], Batch [850/938], Loss: 0.3427498936653137\n",
      "Validation: Epoch [24], Batch [851/938], Loss: 0.4256315529346466\n",
      "Validation: Epoch [24], Batch [852/938], Loss: 0.37137770652770996\n",
      "Validation: Epoch [24], Batch [853/938], Loss: 0.3694426119327545\n",
      "Validation: Epoch [24], Batch [854/938], Loss: 0.37922513484954834\n",
      "Validation: Epoch [24], Batch [855/938], Loss: 0.20744705200195312\n",
      "Validation: Epoch [24], Batch [856/938], Loss: 0.44686102867126465\n",
      "Validation: Epoch [24], Batch [857/938], Loss: 0.40397709608078003\n",
      "Validation: Epoch [24], Batch [858/938], Loss: 0.228211909532547\n",
      "Validation: Epoch [24], Batch [859/938], Loss: 0.38203030824661255\n",
      "Validation: Epoch [24], Batch [860/938], Loss: 0.4651252031326294\n",
      "Validation: Epoch [24], Batch [861/938], Loss: 0.6191740036010742\n",
      "Validation: Epoch [24], Batch [862/938], Loss: 0.38712212443351746\n",
      "Validation: Epoch [24], Batch [863/938], Loss: 0.5377559661865234\n",
      "Validation: Epoch [24], Batch [864/938], Loss: 0.6165909767150879\n",
      "Validation: Epoch [24], Batch [865/938], Loss: 0.42842361330986023\n",
      "Validation: Epoch [24], Batch [866/938], Loss: 0.25290587544441223\n",
      "Validation: Epoch [24], Batch [867/938], Loss: 0.3426235020160675\n",
      "Validation: Epoch [24], Batch [868/938], Loss: 0.2832331359386444\n",
      "Validation: Epoch [24], Batch [869/938], Loss: 0.35556527972221375\n",
      "Validation: Epoch [24], Batch [870/938], Loss: 0.5666905045509338\n",
      "Validation: Epoch [24], Batch [871/938], Loss: 0.33529144525527954\n",
      "Validation: Epoch [24], Batch [872/938], Loss: 0.42042675614356995\n",
      "Validation: Epoch [24], Batch [873/938], Loss: 0.41575056314468384\n",
      "Validation: Epoch [24], Batch [874/938], Loss: 0.38421404361724854\n",
      "Validation: Epoch [24], Batch [875/938], Loss: 0.44014570116996765\n",
      "Validation: Epoch [24], Batch [876/938], Loss: 0.36255180835723877\n",
      "Validation: Epoch [24], Batch [877/938], Loss: 0.3740786910057068\n",
      "Validation: Epoch [24], Batch [878/938], Loss: 0.3711870312690735\n",
      "Validation: Epoch [24], Batch [879/938], Loss: 0.3456050455570221\n",
      "Validation: Epoch [24], Batch [880/938], Loss: 0.4077860414981842\n",
      "Validation: Epoch [24], Batch [881/938], Loss: 0.40069225430488586\n",
      "Validation: Epoch [24], Batch [882/938], Loss: 0.2936578094959259\n",
      "Validation: Epoch [24], Batch [883/938], Loss: 0.39732587337493896\n",
      "Validation: Epoch [24], Batch [884/938], Loss: 0.5421820878982544\n",
      "Validation: Epoch [24], Batch [885/938], Loss: 0.3087703585624695\n",
      "Validation: Epoch [24], Batch [886/938], Loss: 0.46326225996017456\n",
      "Validation: Epoch [24], Batch [887/938], Loss: 0.3220517635345459\n",
      "Validation: Epoch [24], Batch [888/938], Loss: 0.4022118151187897\n",
      "Validation: Epoch [24], Batch [889/938], Loss: 0.3782076835632324\n",
      "Validation: Epoch [24], Batch [890/938], Loss: 0.2663462460041046\n",
      "Validation: Epoch [24], Batch [891/938], Loss: 0.41358673572540283\n",
      "Validation: Epoch [24], Batch [892/938], Loss: 0.3785111606121063\n",
      "Validation: Epoch [24], Batch [893/938], Loss: 0.3064429759979248\n",
      "Validation: Epoch [24], Batch [894/938], Loss: 0.30499380826950073\n",
      "Validation: Epoch [24], Batch [895/938], Loss: 0.24313925206661224\n",
      "Validation: Epoch [24], Batch [896/938], Loss: 0.33505573868751526\n",
      "Validation: Epoch [24], Batch [897/938], Loss: 0.3236708641052246\n",
      "Validation: Epoch [24], Batch [898/938], Loss: 0.4199402630329132\n",
      "Validation: Epoch [24], Batch [899/938], Loss: 0.336714506149292\n",
      "Validation: Epoch [24], Batch [900/938], Loss: 0.3136330246925354\n",
      "Validation: Epoch [24], Batch [901/938], Loss: 0.41471242904663086\n",
      "Validation: Epoch [24], Batch [902/938], Loss: 0.2671973705291748\n",
      "Validation: Epoch [24], Batch [903/938], Loss: 0.4677687883377075\n",
      "Validation: Epoch [24], Batch [904/938], Loss: 0.38825029134750366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch [24], Batch [905/938], Loss: 0.29582294821739197\n",
      "Validation: Epoch [24], Batch [906/938], Loss: 0.4074578285217285\n",
      "Validation: Epoch [24], Batch [907/938], Loss: 0.30837282538414\n",
      "Validation: Epoch [24], Batch [908/938], Loss: 0.6400419473648071\n",
      "Validation: Epoch [24], Batch [909/938], Loss: 0.4540669023990631\n",
      "Validation: Epoch [24], Batch [910/938], Loss: 0.35834068059921265\n",
      "Validation: Epoch [24], Batch [911/938], Loss: 0.3255329132080078\n",
      "Validation: Epoch [24], Batch [912/938], Loss: 0.39888402819633484\n",
      "Validation: Epoch [24], Batch [913/938], Loss: 0.37264716625213623\n",
      "Validation: Epoch [24], Batch [914/938], Loss: 0.3288416266441345\n",
      "Validation: Epoch [24], Batch [915/938], Loss: 0.14738726615905762\n",
      "Validation: Epoch [24], Batch [916/938], Loss: 0.2650589346885681\n",
      "Validation: Epoch [24], Batch [917/938], Loss: 0.29640042781829834\n",
      "Validation: Epoch [24], Batch [918/938], Loss: 0.2867565155029297\n",
      "Validation: Epoch [24], Batch [919/938], Loss: 0.4794453978538513\n",
      "Validation: Epoch [24], Batch [920/938], Loss: 0.4803403317928314\n",
      "Validation: Epoch [24], Batch [921/938], Loss: 0.2580786347389221\n",
      "Validation: Epoch [24], Batch [922/938], Loss: 0.27275970578193665\n",
      "Validation: Epoch [24], Batch [923/938], Loss: 0.16131891310214996\n",
      "Validation: Epoch [24], Batch [924/938], Loss: 0.37942129373550415\n",
      "Validation: Epoch [24], Batch [925/938], Loss: 0.520362377166748\n",
      "Validation: Epoch [24], Batch [926/938], Loss: 0.49504587054252625\n",
      "Validation: Epoch [24], Batch [927/938], Loss: 0.5264730453491211\n",
      "Validation: Epoch [24], Batch [928/938], Loss: 0.3835333585739136\n",
      "Validation: Epoch [24], Batch [929/938], Loss: 0.32163992524147034\n",
      "Validation: Epoch [24], Batch [930/938], Loss: 0.3764268159866333\n",
      "Validation: Epoch [24], Batch [931/938], Loss: 0.27362576127052307\n",
      "Validation: Epoch [24], Batch [932/938], Loss: 0.2756752371788025\n",
      "Validation: Epoch [24], Batch [933/938], Loss: 0.459453821182251\n",
      "Validation: Epoch [24], Batch [934/938], Loss: 0.38558250665664673\n",
      "Validation: Epoch [24], Batch [935/938], Loss: 0.38841378688812256\n",
      "Validation: Epoch [24], Batch [936/938], Loss: 0.3258863091468811\n",
      "Validation: Epoch [24], Batch [937/938], Loss: 0.4296157956123352\n",
      "Validation: Epoch [24], Batch [938/938], Loss: 0.4753383994102478\n",
      "Accuracy of test set: 0.8685166666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([2088.0768060684204,\n",
       "  1423.2744981050491,\n",
       "  912.7695209383965,\n",
       "  784.9550172686577,\n",
       "  716.431152433157,\n",
       "  650.8587863147259,\n",
       "  600.4662269353867,\n",
       "  566.3247152268887,\n",
       "  541.7453320026398,\n",
       "  521.706277102232,\n",
       "  503.0299856364727,\n",
       "  487.2078967690468,\n",
       "  472.9157083183527,\n",
       "  457.00474809110165,\n",
       "  445.26452837884426,\n",
       "  435.20742106437683,\n",
       "  424.61880838871,\n",
       "  416.50756745040417,\n",
       "  406.04087883234024,\n",
       "  396.82211987674236,\n",
       "  388.4086793512106,\n",
       "  380.4257443547249,\n",
       "  374.6783468276262,\n",
       "  365.81681057810783,\n",
       "  359.48318752646446],\n",
       " [0.19343769068717956,\n",
       "  0.10468817863464355,\n",
       "  0.08218478825092315,\n",
       "  0.07438255194425583,\n",
       "  0.0716054204761982,\n",
       "  0.06313921822607517,\n",
       "  0.05847307220101357,\n",
       "  0.055183471268415454,\n",
       "  0.053933488097786905,\n",
       "  0.051347768035531045,\n",
       "  0.048994957891106604,\n",
       "  0.047237525904178616,\n",
       "  0.04527337358444929,\n",
       "  0.04507615320086479,\n",
       "  0.04377811040580273,\n",
       "  0.04215586481839419,\n",
       "  0.04156230172514915,\n",
       "  0.04086666735112667,\n",
       "  0.0398466001406312,\n",
       "  0.038447994612157345,\n",
       "  0.03887525069415569,\n",
       "  0.040117026124894616,\n",
       "  0.03620498508512974,\n",
       "  0.03526042954176664,\n",
       "  0.034901096758246423],\n",
       " [0.4068833333333333,\n",
       "  0.44521666666666665,\n",
       "  0.618,\n",
       "  0.6785333333333333,\n",
       "  0.7196,\n",
       "  0.7465166666666667,\n",
       "  0.7689,\n",
       "  0.7852333333333333,\n",
       "  0.7964333333333333,\n",
       "  0.8047833333333333,\n",
       "  0.81385,\n",
       "  0.8205666666666667,\n",
       "  0.82555,\n",
       "  0.8306833333333333,\n",
       "  0.8344666666666667,\n",
       "  0.83695,\n",
       "  0.8428166666666667,\n",
       "  0.8458833333333333,\n",
       "  0.8494,\n",
       "  0.8519,\n",
       "  0.8553,\n",
       "  0.8574333333333334,\n",
       "  0.8591333333333333,\n",
       "  0.8623666666666666,\n",
       "  0.8649666666666667],\n",
       " [0.2838833333333333,\n",
       "  0.58655,\n",
       "  0.6543333333333333,\n",
       "  0.7058166666666666,\n",
       "  0.7158666666666667,\n",
       "  0.7415833333333334,\n",
       "  0.7792833333333333,\n",
       "  0.7937,\n",
       "  0.7932,\n",
       "  0.80925,\n",
       "  0.8218166666666666,\n",
       "  0.8266333333333333,\n",
       "  0.8339666666666666,\n",
       "  0.8311,\n",
       "  0.8370333333333333,\n",
       "  0.84365,\n",
       "  0.84595,\n",
       "  0.8464833333333334,\n",
       "  0.8516,\n",
       "  0.8577333333333333,\n",
       "  0.8543333333333333,\n",
       "  0.8489,\n",
       "  0.8649333333333333,\n",
       "  0.8662833333333333,\n",
       "  0.8685166666666667])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceb5783f",
   "metadata": {
    "id": "ceb5783f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8685166666666667\n"
     ]
    }
   ],
   "source": [
    "print(f'Final test accuracy: {test_accuracies[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e128ed",
   "metadata": {
    "id": "a5e128ed"
   },
   "source": [
    "## Visualization of the labels and predictions\n",
    "\n",
    "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d88cdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x700 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_labels_predictions(data_loader, model, parameters):\n",
    "    classes = train_set.classes  \n",
    "    \n",
    "    class_images = {class_name: None for class_name in classes}\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for images, labels in data_loader:\n",
    "        # Move images to the device of parameters\n",
    "        images = images.to(next(iter(parameters.values())).device)  \n",
    "        # Predicting the labels using the trained model\n",
    "        outputs = model(images, parameters)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Iterating over the batch\n",
    "        for image, label, prediction in zip(images, labels, predicted):\n",
    "            class_name = classes[label]\n",
    "            # If there is no image for this class, we store the current image\n",
    "            if class_images[class_name] is None:\n",
    "                class_images[class_name] = (image, label.item(), prediction.item())\n",
    "                break\n",
    "    \n",
    "    # Visualizing one image from each class along with actual and predicted labels\n",
    "    fig, axs = plt.subplots(2, len(classes) // 2, figsize=(15, 7))\n",
    "    for i, (class_name, (image, label, prediction)) in enumerate(class_images.items()):\n",
    "        ax = axs[i // (len(classes) // 2), i % (len(classes) // 2)]\n",
    "        # Move the image to CPU for plotting\n",
    "        ax.imshow(image.squeeze().cpu(), cmap='gray')  \n",
    "        ax.set_title(f'Actual: {classes[label]}\\nPredicted: {classes[prediction]}')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_labels_predictions(test_loader, model, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd4097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
